<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data](https://arxiv.org/abs/2512.19864)
*Shashi Kant Gupta,Arijeet Pramanik,Jerrin John Thomas,Regina Schwind,Lauren Wiener,Avi Raju,Jeremy Kornbluth,Yanshan Wang,Zhaohui Su,Hrituraj Singh*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大语言模型的智能代理框架，实现对电子健康记录中非结构化肿瘤学临床笔记的结构化数据提取，性能优异且大幅降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的非结构化肿瘤学笔记包含丰富临床信息，但由于术语复杂、格式不一以及信息矛盾，结构化提取极具挑战，且手工标注费用高昂不可扩展。

Method: 通过构建一个智能代理框架，将复杂的数据提取任务模块化，使用具备上下文检索和迭代综合能力的大语言模型作为推理代理，进行详尽的临床变量提取。

Result: 在涵盖2,250名癌症患者40多万条临床笔记及扫描PDF的真实大规模数据集上，整体F1达到0.93，103个肿瘤学变量中100个超过0.85，关键变量（如生物标志物与药物）超过0.95，手工审核直接通过率达0.94。

Conclusion: 本研究首次实现了基于大语言模型代理的多场景端到端肿瘤学临床结构化数据提取，效果卓越且显著降低人工标注成本，具有广泛应用前景。

Abstract: Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale

</details>


### [2] [How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse](https://arxiv.org/abs/2512.19903)
*Kirk Vanacore,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 本研究比较了六种大型语言模型在课堂教学对话中的教学行为分类任务上的基线表现，发现少样本提示显著提升了模型性能，但效果有限且表现不均衡。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型广泛应用于教育领域，了解它们在未经定制的真实教育场景中的表现对于设定预期和评估标准至关重要。

Method: 选取六个大型语言模型，采用零样本、一样本和少样本提示方法，在真实课堂对话文本中分类教学举措，评估其性能。

Result: 少样本提示明显提升了模型性能，最优模型实现了Cohen's Kappa=0.58，但不同教学举措的表现差异显著，且召回率提升通常伴随更高的误报率。

Conclusion: 基础模型在教学话语理解上具备一定能力，提示设计能提升表现，但仍存在可靠性限制，尚需进一步优化。

Abstract: Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.

</details>


### [3] [Counterfactual LLM-based Framework for Measuring Rhetorical Style](https://arxiv.org/abs/2512.19908)
*Jingyi Qiu,Hong Chen,Zongyi Li*

Main category: cs.CL

TL;DR: 本文引入基于大语言模型(LLM)的反事实框架，量化机器学习论文中的修辞风格，发现具有远见的修辞表达显著影响论文引用和媒体关注度，并指出2023年后修辞强度急剧上升与LLM写作辅助的采用有关。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，机器学习论文中的"炒作"问题日益引发关注，但难以独立量化修辞风格与实质内容，区分两者成为挑战。

Method: 设计一个基于LLM的反事实框架，利用多个LLM修辞角色生成基于相同实质内容的反事实文本，随后LLM评审通过配对比较评估修辞风格，结果用Bradley--Terry模型聚合。

Result: 基于ICLR 2017-2025年8485篇论文的250000多份反事实文本分析，发现远见性修辞对引用和媒体关注有显著预测效果，2023年后修辞强度显著上升，且与LLM写作辅助使用相关。

Conclusion: 该框架用LLM作为工具，有效分离并量化科学论文的修辞风格与实质内容，验证了LLM评判与人工注释高度相关，为科学评价提供了新方法。

Abstract: The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.

</details>


### [4] [PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation](https://arxiv.org/abs/2512.19933)
*Zhixiang Lu,Xueyuan Deng,Yiran Liu,Yulong Li,Qiang Yan,Imran Razzak,Jionglong Su*

Main category: cs.CL

TL;DR: 该论文提出了PRISM模型，通过结合情绪演化的随机微分方程与基于MBTI的个性化马尔可夫决策过程，从而更真实地模拟在线极化中的心理异质性。


<details>
  <summary>Details</summary>
Motivation: 传统基于个体的意见动态模型过于简单，忽略了个体认知偏差和信息传播间的复杂作用，导致难以机制性理解意识形态分歧的加剧。

Method: 引入PRISM模型，结合情绪的连续演化（随机微分方程）与基于MBTI的个性条件部分可观测马尔可夫决策过程，利用大规模社交媒体数据初始化多模式大语言模型代理的认知策略。

Result: PRISM表现出优越的人格一致性，且显著优于传统同质性模型和“大五人格”基准，能够有效复制理性抑制和情绪共鸣等社会现象。

Conclusion: PRISM为深入分析复杂社交媒体生态系统中的心理异质性和极化提供了强有力的工具。

Abstract: Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.

</details>


### [5] [Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems](https://arxiv.org/abs/2512.19950)
*Heet Bodara,Md Masum Mushfiq,Isma Farah Siddiqui*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在对话系统中表现出的语调偏见，发现即使在中性提示下也存在一致的语调倾向。通过合成对话数据集和基于DistilBERT的弱监督标注，训练了高效的语调分类器，证明了语调偏见具有系统性和可测量性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话系统中的回应往往带有细微的语调偏见，这影响用户对系统的信任、同理心和公平感，因此需要识别和理解这种隐含的行为特征。

Method: 利用可控的大型语言模型生成中性和带有正负语调的合成对话数据集，结合预训练的DistilBERT进行弱监督语调标签，然后训练多种分类器进行检测，最终通过集成模型提升分类性能。

Result: 发现即使中性提示下对话数据也呈现一致的语调偏向，分类器的宏F1分数最高达到0.92，表明语调偏见是系统性的且可被有效检测。

Conclusion: 语调偏见作为大型语言模型中的隐形行为特征，是可量化且显著的，理解和控制这种偏见对设计公平、可信的对话AI至关重要。

Abstract: Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.

</details>


### [6] [Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/abs/2512.19995)
*Ming Li,Chenrui Fan,Yize Cheng,Soheil Feizi,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出了ThinkARM框架，通过中间层次的推理步骤抽象，揭示了大语言模型在数学问题解决中的认知结构和动态差异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽能暴露推理轨迹，但难以深入识别和分析其认知结构及推理步骤，仅停留在表面统计层面。

Method: 本文采用Schoenfeld的Episode Theory作为理论基础，设计ThinkARM框架，将推理轨迹抽象为分析、探索、实现、验证等功能性步骤，并应用于多模型的数学推理。

Result: 抽象推理步骤揭示出推理模型与非推理模型之间的结构性差异和可复现的思维动态，探索步骤被发现是与正确性密切相关的关键分支步骤，且效率导向方法主要抑制评估反馈步骤。

Conclusion: 基于情节级别的表示使推理步骤显式化，有助于系统分析现代语言模型中推理的结构、稳定性及其变化。

Abstract: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.

</details>


### [7] [Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents](https://arxiv.org/abs/2512.20092)
*Yiming Du,Baojun Wang,Yifan Xiang,Zhaowei Wang,Wenyu Huang,Boyang Xue,Bin Liang,Xingshan Zeng,Fei Mi,Haoli Bai,Lifeng Shang,Jeff Z. Pan,Yuxin Jiang,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 这篇论文提出了Memory-T1框架，通过强化学习选择时间感知的记忆片段来改善长对话中的时间推理能力，显著提升了模型在Time-Dialog基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型在长对话历史中难以准确识别与时间相关的信息，导致推理性能下降。

Method: 采用从粗到细的策略，先用时间和相关性过滤对话历史，再用强化学习代理选择证据片段，训练时结合答案准确率、证据基础和时间一致性奖励。

Result: Memory-T1使7B模型达到67.0%的整体分数，超越14B基线10.2%，在噪声多的长对话中表现稳健，消融实验表明时间一致性与证据基础奖励带来15.0%的性能提升。

Conclusion: 通过时间感知的记忆选择和多层次奖励，Memory-T1有效提升了多轮长对话的时间推理能力，具备较强的鲁棒性和实用价值。

Abstract: Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/

</details>


### [8] [A Novel Graph-Sequence Learning Model for Inductive Text Classification](https://arxiv.org/abs/2512.20097)
*Zuo Wang,Ye Yuan*

Main category: cs.CL

TL;DR: 本文提出了一个新颖的图序列学习模型TextGSL，用于归纳文本分类，通过多类型边的自适应消息传递和Transformer捕获序列信息，显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的文本分类方法未充分利用词对间多样的结构信息且忽视序列信息，难以处理含新词和新关系的文本。

Method: 构建文本级别图，定义多种边类型，设计自适应多边消息传递机制聚合结构信息，并通过Transformer层融合序列信息，提升文本表征能力。

Result: TextGSL在多个基准数据集上与强 baselines比较，表现出更高的分类准确率。

Conclusion: 提出的TextGSL有效整合多种结构与序列信息，提升了文本分类性能，具有较强的推广应用价值。

Abstract: Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.

</details>


### [9] [ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language](https://arxiv.org/abs/2512.20111)
*Aly Lidayan,Jakob Bjorner,Satvik Golechha,Kartik Goyal,Alane Suhr*

Main category: cs.CL

TL;DR: 本文提出ABBEL框架，通过自然语言表达的信念状态替代长序列历史，用于长序列决策任务中的上下文压缩，并结合强化学习优化性能。


<details>
  <summary>Details</summary>
Motivation: 长序列决策任务中，全历史上下文存储计算代价高，且长历史易导致信息处理困难。需要方法在有限内存下有效表示关键信息。

Method: 提出ABBEL框架，用自然语言信念状态代替多步交互历史。每步先根据环境观测更新信念，再基于信念选择行为。结合强化学习，加入信念质量奖励和信念长度惩罚，提升信念生成及行动性能。

Result: ABBEL在多环境下实现可解释信念生成及内存使用稳定，但信念更新错误影响性能。强化学习优化后，ABBEL性能超越全上下文方法且占用更少内存。

Conclusion: ABBEL有效减少长序列任务中的内存需求，结合强化学习能缓解误差传播问题，提升多步决策性能，具有广泛应用潜力。

Abstract: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.

</details>


### [10] [M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2512.20136)
*Hyeongcheol Park,Jiyoung Seo,Jaewon Mun,Hogun Park,Wonmin Byeon,Sung June Kim,Hyeonsoo Im,JeungSub Lee,Sangpil Kim*

Main category: cs.CL

TL;DR: 本文提出了M$^3$KG-RAG方法，通过构建多跳多模态知识图谱并结合GRASP机制，提升了多模态语言模型在音视频领域的检索和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有音视频多模态知识图谱覆盖有限且多跳连接不足，检索仅基于共享嵌入空间相似度，无法有效过滤无关或冗余知识，导致多模态推理效果受限。

Method: 设计轻量级多代理管道构建多跳多模态知识图谱（M$^3$KG），包含丰富上下文的多模态实体三元组，支持基于查询的模态特定检索；引入GRASP机制，实现精确实体定位、相关性评估与冗余知识剪枝。

Result: 在多个多模态基准测试中，M$^3$KG-RAG显著提升了多模态语言模型的推理深度和答案可信度，优于现有方法。

Conclusion: 通过融合多跳多模态知识图谱和精细检索剪枝机制，M$^3$KG-RAG有效增强了音视频多模态语言模型的知识推理和定位能力，推动了该领域发展。

Abstract: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.

</details>


### [11] [Multi-hop Reasoning via Early Knowledge Alignment](https://arxiv.org/abs/2512.20144)
*Yuxin Wang,Shicheng Fang,Bo Wang,Qi Luo,Xuanjing Huang,Yining Zheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出Early Knowledge Alignment (EKA)模块，通过在迭代检索增强生成系统中于规划前与检索集对齐，提升多跳复杂问题的检索和推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有迭代RAG系统未充分利用检索语料信息，导致检索和推理链效率低下及性能受限。

Method: 设计EKA模块，使大语言模型在规划前结合上下文相关的检索知识，实现知识对齐，提高推理基础。

Result: EKA在六个标准RAG数据集上的实验显示显著提升检索精准度，减少错误传播，增强性能和效率，并具备无训练、易扩展的优点。

Conclusion: EKA有效改善迭代RAG系统的推理与探索过程，推动该领域技术进步，同时验证了结构化推理与高效探索的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.

</details>


### [12] [Retrieval-augmented Prompt Learning for Pre-trained Foundation Models](https://arxiv.org/abs/2512.20145)
*Xiang Chen,Yixin Ou,Quan Feng,Lei Li,Piji Li,Haibo Ye,Sheng-Jun Huang,Shuofei Qiao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为RetroPrompt的新方法，通过结合检索机制与知识库，提升预训练基础模型在零样本和少样本任务中的泛化能力，减少了对死记硬背的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练基础模型的提示学习方法仍依赖参数化学习，容易在有限数据上过拟合，难以充分利用非典型样本，影响泛化能力。

Method: 提出RetroPrompt方法，利用从训练数据生成的公开知识库和检索机制，在输入、训练及推理阶段动态检索相关上下文信息，平衡记忆与泛化。

Result: 在自然语言处理和计算机视觉多个数据集上的零样本和少样本任务中，RetroPrompt表现优越，有效减少死记硬背依赖，提高了模型泛化性。

Conclusion: 通过引入知识检索机制，RetroPrompt成功缓解了传统提示学习的稳定性问题，提升了模型在少数据情况下的泛化能力。

Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.

</details>


### [13] [Fun-Audio-Chat Technical Report](https://arxiv.org/abs/2512.20156)
*Qian Chen,Luyao Cheng,Chong Deng,Xiangang Li,Jiaqing Liu,Chao-Hong Tan,Wen Wang,Junhao Xu,Jieping Ye,Qinglin Zhang,Qiquan Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了Fun-Audio-Chat，一种大规模音频语言模型，通过双分辨率语音表示和核心训练方法解决了语音与文本令牌时序匹配问题，显著提升模型效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有联合语音-文本模型存在语音令牌与文本令牌时序分辨率不匹配，导致语义丢失、高昂计算代价及文本知识遗忘等问题。

Method: 引入双分辨率语音表示（DRSR）和核心鸡尾酒训练方法（Core-Cocktail Training），结合多任务DPO训练，提升模型效率和保持文本大模型知识。

Result: Fun-Audio-Chat在语音转文本和语音转语音任务上表现出竞争力，并在口语问答和语音功能调用等多项任务上取得优异成绩。全双工版本Fun-Audio-Chat-Duplex支持更流畅的语音交互。

Conclusion: Fun-Audio-Chat有效解决了时序分辨率不匹配及知识遗忘问题，通过多阶段训练实现了高效且强大的音频语言理解与生成能力，并已开源供社区使用。

Abstract: Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.

</details>


### [14] [AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications](https://arxiv.org/abs/2512.20164)
*Honglin Mu,Jinghao Liu,Kaiyang Wan,Rui Xing,Xiuying Chen,Timothy Baldwin,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在简历筛选中的脆弱性，发现对抗性指令可以导致模型偏离任务，并提出了相应防御方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码审查等领域已有成熟防御措施，但在简历筛选等领域缺乏有效防护，存在较高的安全风险。

Method: 本文设计了针对简历筛选的漏洞基准，评估了两种防御机制：基于提示的防御和通过LoRA适配的FIDS（外部指令分离检测），并探讨了两者结合的效果。

Result: 实验证明，针对简历筛选的对抗攻击成功率超过80%；基于提示的防御减少攻击10.1%但假拒率增加12.5%；FIDS减少攻击15.4%，假拒率增加10.4%；两者结合后攻击减少26.3%。

Conclusion: 训练时防御机制优于推理时缓解方法，能够更有效地降低攻击风险并保持模型性能。

Abstract: Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by "adversarial instructions" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.

</details>


### [15] [FaithLens: Detecting and Explaining Faithfulness Hallucination](https://arxiv.org/abs/2512.20182)
*Shuzheng Si,Qingyi Wang,Haozhe Zhao,Yuzhuo Bai,Guanqiao Chen,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: FaithLens是一个高效且有效的忠实度虚假信息检测模型，能同时提供二元预测和解释，显著提升了大语言模型输出的可信度。


<details>
  <summary>Details</summary>
Motivation: 识别大型语言模型输出中的忠实度虚假信息对于实际应用至关重要，以提升生成内容的可靠性。

Method: 通过先进的大型语言模型合成带有解释的训练数据，并采用数据过滤策略保证标签和解释质量；基于这些数据进行微调，并通过基于规则的强化学习进一步优化预测和解释质量。

Result: FaithLens在12个不同任务中表现优于GPT-4.1等先进模型，且能够生成高质量解释，在可信度、效率和效果之间达到良好平衡。

Conclusion: FaithLens通过联合预测和解释，为大语言模型输出的忠实度检测提供了一种兼具成本效益和高性能的解决方案，提升了实际应用中的可靠性。

Abstract: Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.

</details>


### [16] [Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings](https://arxiv.org/abs/2512.20204)
*Marko Čechovič,Natália Komorníková,Dominik Macháček,Ondřej Bojar*

Main category: cs.CL

TL;DR: 该论文介绍了一个用于无共同语言交流者之间会议的跨语言对话语料库，并提出了误解自动检测方法。


<details>
  <summary>Details</summary>
Motivation: 为了评估自动跨语言语音处理和翻译系统，需要一个多样且真实的评估语料库，同时关注跨语言误解的自动检测问题。

Method: 创建包含12种语言的5小时录音及相应转录和翻译语料，提供会议书面总结，并通过人工注释和大语言模型（如Gemini）检测误解。

Result: Gemini模型能以77%的召回率和47%的精确率自动识别包含误解的文本片段。

Conclusion: 本研究构建了实际应用背景下的跨语种会议语料库，验证了大语言模型在误解检测中的潜力，为多语言会议理解和总结提供了基础。

Abstract: Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.
  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.

</details>


### [17] [SlideTailor: Personalized Presentation Slide Generation for Scientific Papers](https://arxiv.org/abs/2512.20292)
*Wenzheng Zeng,Mingyu Ouyang,Langyuan Cui,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文提出了一种基于用户偏好的自动论文到幻灯片生成框架SlideTailor，通过一个低需求的示例对和视觉模板输入，实现个性化、可编辑的幻灯片生成，并引入链式语音机制以增强内容与口述的匹配。


<details>
  <summary>Details</summary>
Motivation: 现有自动幻灯片生成技术未充分考虑不同用户的偏好，导致生成结果往往不能满足个别用户需求。为了更好地定制化幻灯片内容与风格，亟需一种能够结合用户偏好进行生成的新方法。

Method: 提出了基于人类行为启发的代理式框架SlideTailor，用户提供一个论文-幻灯片示例对和视视觉模板，该系统从中隐式提取丰富的用户偏好，用于指导个性化幻灯片逐步生成。同时引入链式语音机制，确保幻灯片内容与计划的口述内容紧密对齐。

Result: 通过构建包含多样化用户偏好的基准数据集和设计可解释评价指标，系统性地验证了SlideTailor的有效性，生成的幻灯片质量显著提升，支持后续视频演示等应用。

Conclusion: SlideTailor成功实现了基于用户偏好的个性化幻灯片生成，显著提升内容和视觉风格的匹配度及表达质量，拓展了自动幻灯片生成的适用性和用户体验。

Abstract: Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.

</details>


### [18] [AprielGuard](https://arxiv.org/abs/2512.20293)
*Jaykumar Kasundra,Anjaneya Praharaj,Sourabh Surana,Lakshmi Sirisha Chodisetty,Sourav Sharma,Abhigya Verma,Abhishek Bhardwaj,Debasish Kanhar,Aakash Bhagat,Khalil Slimi,Seganrasan Subramanian,Sathwik Tejaswi Madhusudhan,Ranga Prasad Chenna,Srinivas Sunkara*

Main category: cs.CL

TL;DR: 本文提出了AprielGuard，一种统一处理大型语言模型安全性和对抗性威胁的保护模型，能够有效检测有害内容和对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的安全检测工具将安全风险与对抗威胁视为独立问题，限制了其鲁棒性和泛化能力。为了解决这一问题，作者提出了一个统一的解决方案。

Method: AprielGuard基于8B参数规模，采用统一的分类体系和学习框架，结合多样的开源和合成数据进行训练，涵盖单次提示、多轮对话及代理工作流，并引入结构化推理轨迹以提升模型解释性。

Result: 在多个公开和专有的基准测试中，AprielGuard表现优异，准确检测有害内容和对抗操纵，超过现有开源保护工具如Llama-Guard和Granite Guardian，特别是在多步推理和复杂场景中表现突出。

Conclusion: 通过发布AprielGuard模型，促进了大型语言模型安全保护领域的透明和可复现研究，为提升模型安全性提供了有效工具。

Abstract: Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.

</details>


### [19] [Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives](https://arxiv.org/abs/2512.20298)
*Karolina Drożdż,Kacper Dudzic,Anna Sterna,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: 本文比较了先进的大型语言模型与心理健康专家在诊断边缘型和自恋型人格障碍方面的表现，发现模型在整体准确率上优于专家，但模型严重低估了自恋型人格障碍的诊断。


<details>
  <summary>Details</summary>
Motivation: 随着对大型语言模型在精神健康自评中的依赖增加，研究其能否准确解读患者的定性叙述成为必要。

Method: 使用波兰语第一人称自传体文本，直接比较顶尖的Gemini Pro模型与心理健康专业人士对边缘型人格障碍（BPD）和自恋型人格障碍（NPD）的诊断准确率和表现。

Result: Gemini Pro模型的诊断准确率比人类专家高出21.91个百分点；两者均擅长诊断BPD，但模型严重低估NPD，表现出对“自恋”这一价值判断词汇的回避。

Conclusion: 大型语言模型在解读复杂的第一人称临床数据方面表现出高度能力，但仍存在可靠性和偏见等关键问题需解决。

Abstract: Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term "narcissism." Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.

</details>


### [20] [SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision](https://arxiv.org/abs/2512.20308)
*Maxime Poli,Mahi Luthra,Youssef Benchekroun,Yosuke Higuchi,Martin Gleize,Jiayi Shen,Robin Algayres,Yu-An Chung,Mido Assran,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 该论文提出了SpidR，一种自监督语音表示模型，能够高效学习包含音素信息的表示，适用于无文本的语音语言建模。


<details>
  <summary>Details</summary>
Motivation: 当前语言建模和语音表示学习的进步使得直接从语音中学习语言成为可能，这需要提取直接反映语义的语音表示。

Method: SpidR基于原始波形，采用蒙版预测目标结合自蒸馏和在线聚类进行训练。学生模型的中间层预测教师模型中间层派生的聚类分配，通过此方法稳定在线聚类流程，提高代码本质量。

Result: SpidR在多个下游语言建模基准上优于wav2vec 2.0、HuBERT、WavLM和DinoSR，且显著缩短预训练时间（从一周减少到一天）。同时验证了语音单元质量指标与语言建模性能的相关性。

Conclusion: SpidR不仅提升了语音表示的质量和语言建模性能，还提高了预训练效率，促进了无文本语音语言模型的研究和应用。代码和模型已开源。

Abstract: The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.

</details>


### [21] [Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://arxiv.org/abs/2512.20324)
*Nurul Labib Sayeedi,Md. Faiyaz Abdullah Sayeedi,Khushnur Binte Jahangir,Swakkhar Shatabda,Sarah Masud Preum*

Main category: cs.CL

TL;DR: 本文介绍了针对孟加拉语传统谜语推理能力的新基准BanglaRiddleEval，包含1244个谜语，评测多个模型在生成问答、多项选择和歧义解析任务上的表现，发现当前大语言模型性能虽有提示，但远不及人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在比喻性、文化背景和低资源语言环境中的推理能力尚未充分研究，特别是孟加拉语谜语推理领域存在空白。

Method: 建立包含1244个传统孟加拉谜语的数据集，设计四个相关任务，并通过链式思维解释、语义干扰选项和细粒度歧义注释增强数据；测试多种开源和闭源模型在不同提示策略下的表现。

Result: 生成问答存在适度语义重叠但正确率低，多项选择题准确率最高约56%，低于83%的人类基线，歧义解析准确率在26%至68%间波动，高质量解释仅见于最强模型。

Conclusion: 现有大语言模型能够捕捉部分孟加拉谜语推理线索，但整体表现落后于人类，BanglaRiddleEval为低资源语言的比喻推理提供了富有挑战性的基准。

Abstract: Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.

</details>


### [22] [Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation](https://arxiv.org/abs/2512.20352)
*Nilesh Jain,Seyi Adeyinka,Leor Roseman,Aza Allsop*

Main category: cs.CL

TL;DR: 本文提出了一种结合多模型验证与双重可靠性指标的LLM主题分析框架，显著提升定性研究的一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统定性研究依赖多人工编码，耗时且一致性一般，需提升可靠性和效率。

Method: 引入结合Cohen's Kappa和余弦相似度的多模型多轮次验证框架，支持参数调节与灵活提示设计，实现JSON格式主题共识提取。

Result: 在迷幻艺术疗法访谈数据上，Gemini模型表现最佳（κ=0.907，余弦95.3%），GPT-4o和Claude紧随其后，均显示高一致性（κ>0.80），多轮次集成验证有效。

Conclusion: 该框架为AI辅助定性研究提供了透明、灵活且结构中立的可靠性保障，具备广泛应用潜力。

Abstract: Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.

</details>


### [23] [Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining](https://arxiv.org/abs/2512.20404)
*Junyi Liu,Stanley Kok*

Main category: cs.CL

TL;DR: 本文提出了一种融合情感信号的文本摘要框架，改进了抽取式与生成式摘要在处理非结构化、情感丰富的社交媒体短文本时的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的文本摘要方法主要针对结构化新闻数据，难以有效处理包含噪声和情感信息的用户生成短文本，而情感信息对品牌监控和市场分析等信息系统任务至关重要。

Method: 提出了一个情感感知的框架，将情感信号嵌入到抽取式(TextRank)和生成式(UniLM)摘要方法的排序和生成过程中，以提升对情感细节和主题相关性的捕捉。

Result: 该方法能够生成简洁且富含情感的摘要，有助于及时干预和战略决策，改善了动态在线环境中的舆情监测效果。

Conclusion: 融合情感信息的摘要框架有效提升了非结构化、短文本的摘要质量，为信息系统中的情感分析和决策支持提供了新思路。

Abstract: With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.

</details>


### [24] [Step-DeepResearch Technical Report](https://arxiv.org/abs/2512.20491)
*Chen Hu,Haikuo Du,Heng Wang,Lin Lin,Mingrui Chen,Peng Liu,Ruihang Miao,Tianchi Yue,Wang You,Wei Ji,Wei Yuan,Wenjin Deng,Xiaojian Yuan,Xiaoyun Zhang,Xiangyu Liu,Xikai Liu,Yanming Xu,Yicheng Cao,Yifei Zhang,Yongyao Wang,Yubo Shu,Yurong Zhang,Yuxiang Zhang,Zheng Gong,Zhichao Chang,Binyan Li,Dan Ma,Furong Jia,Hongyuan Wang,Jiayu Liu,Jing Bai,Junlan Liu,Manjiao Liu,Na Wang,Qiuping Wu,Qinxin Du,Shiwei Li,Wen Sun,Yifeng Gong,Yonglin Chen,Yuling Zhao,Yuxuan Lin,Ziqi Ren,Zixuan Wang,Aihu Zhang,Brian Li,Buyun Ma,Kang An,Li Xie,Mingliang Li,Pan Li,Shidong Yang,Xi Chen,Xiaojia Liu,Yuchu Luo,Yuan Song,YuanHao Ding,Yuanwei Liang,Zexi Li,Zhaoning Zhang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Step-DeepResearch的端到端自主智能体，利用原子能力的数据合成策略和分阶段训练方法提升深度研究能力，在中文领域建立了ADR-Bench基准，实验表明该模型在多项指标上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有学术基准无法满足开放式深度研究对意图识别、长远决策和跨源验证的高要求，且缺乏适用于中文领域的评估体系。

Method: 提出基于原子能力的数据合成策略加强规划与报告写作，采用从agentic中期训练到SFT和强化学习的渐进式训练路径，并结合清单式评判器提升模型鲁棒性，构建适用于中文深度研究的ADR-Bench评测基准。

Result: Step-DeepResearch(32B)在Scale AI Research Rubrics上得分61.4%，在ADR-Bench上显著优于同类模型并可媲美OpenAI及Gemini 等闭源顶尖模型。

Conclusion: 通过精细训练，中等规模模型能够以行业领先的成本效率，实现专家级深度研究能力，推动LLM自主代理的发展。

Abstract: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.

</details>


### [25] [Distilling to Hybrid Attention Models via KL-Guided Layer Selection](https://arxiv.org/abs/2512.20569)
*Yanhong Li,Songlin Yang,Shawn Tan,Mayank Mishra,Rameswar Panda,Jiawei Zhou,Yoon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于层重要性得分的简单高效的层选择方法，用于将预训练的softmax Transformer蒸馏成高效的软max和线性注意力混合架构，从而提高大型语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型推理效率，避免从零开始昂贵的预训练，通过蒸馏将softmax注意力层转换为线性注意力层，并合理选择转换层是关键。

Method: 利用少量通用文本训练计算层重要性得分，选择重要层进行转换；采用RADLADS蒸馏流程，包括注意力权重传递、隐藏状态对齐、KL散度分布匹配和微调。

Result: 所提方法在层选择效果上优于均匀插入线性注意力层的启发式方法和依赖专用诊断数据集的复杂方法。

Conclusion: 基于层重要性得分的选择方法简单有效，结合RADLADS蒸馏流程能够更好地提升模型推理效率，为混合注意力架构蒸馏提供实用方案。

Abstract: Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.

</details>


### [26] [Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits](https://arxiv.org/abs/2512.20578)
*Amirhosein Ghasemabadi,Di Niu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Gnosis的轻量级自我认知机制，使冻结的大型语言模型能够通过解码隐藏状态和注意力模式中的信号，实现内在的自我验证，从而高效预测自身生成内容的正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽能生成流畅复杂的文本，但常常无法识别自己的错误和幻觉。现有方法依赖外部评判器、多样本一致性或基于文本的自我批判，计算成本高且与真实正确性相关性弱。作者希望模型能通过推理过程中的内部状态预测自身失败。

Method: 设计Gnosis机制，通过被动观察模型内部的隐藏状态和注意力模式轨迹，将其压缩成固定预算的描述符，进而预测输出的正确性。该机制仅增加约500万个参数，推理成本极低，且与序列长度无关。

Result: 在数学推理、开放领域问答和学术知识基准测试中，使用从1.7B到20B参数规模的冻结模型骨干，Gnosis均优于强大的内部基线和大型外部评判器，在准确率和校准度上表现更佳。同时能零样本泛化至部分生成，实现对失败轨迹的早期检测和计算资源感知控制。

Conclusion: 生成过程本身包含可靠的正确性线索，可通过高效的方法从内部状态中提取，无需外部监督。Gnosis证明了这种内在信号的可利用性，为大型语言模型的自我纠错开辟了新路径。

Abstract: Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.

</details>


### [27] [Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs](https://arxiv.org/abs/2512.20595)
*Dhruv Anand,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 本文提出了Cube Bench，一个用于评估多模态大语言模型空间和序列推理能力的魔方基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在解决复杂空间和序列推理任务时表现有限，亟需一个统一且可重复的基准进行评估。

Method: Cube Bench分解为五项技能测试，包括图文魔方面重建、最佳下一步选择、候选动作结果预测、多步计划执行及错误检测与修改，通过统一的魔方状态、提示和解析器评估七种模型。

Result: 研究发现随着问题复杂度增加，模型准确率显著下降，闭源模型整体优于开源模型，高面部重建准确率并不能保证动作选择和多步执行能力，有自我反思机制的模型表现略有提升但存在过度思考问题。

Conclusion: Cube Bench作为一个紧凑且可复现的基准，为多模态大语言模型的序列空间推理能力提供了有效的测试工具，揭示了模型在复杂任务中的局限和改进方向。

Abstract: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.

</details>


### [28] [MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts](https://arxiv.org/abs/2512.20604)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: MoE-DiffuSeq是一种基于专家混合的扩散模型框架，专注于提升长文档生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散文本生成模型在长序列应用时计算成本高且内存需求大，影响实用性。

Method: 融合稀疏注意力机制和专家混合架构，设计定制的稀疏注意力以降低复杂度，并在扩散过程中引入软吸收状态以加速序列重构。

Result: 实验显示，MoE-DiffuSeq在训练效率和采样速度上显著优于现有模型，特别适用于长文档生成场景。

Conclusion: MoE-DiffuSeq在效率、速度、准确性及表达力上均有提升，推动扩散模型在高质量长文本生成中的实用化进程。

Abstract: We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [29] [Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://arxiv.org/abs/2512.19758)
*Wang Bin,Ao Yang,Kedan Li,Aofan Liu,Hui Li,Guibo Luo,Weixiang Huang,Yan Zhuang*

Main category: cs.SE

TL;DR: 本文提出了利用大语言模型的注意力距离度量，提升定向灰盒模糊测试的效率，在多项漏洞复现实验中显著优于传统物理距离方法和现有先进工具。


<details>
  <summary>Details</summary>
Motivation: 现有定向灰盒模糊测试方法仅依赖物理距离，忽视了代码段之间的逻辑关系，导致指导方向冗余或误导，降低实用效果。

Method: 引入基于大语言模型上下文分析的注意力距离度量，利用注意力分数揭示代码元素间的内在联系，并替换传统物理距离用于种子执行路径和目标位置的距离计算。

Result: 在38个真实漏洞复现实验中，使用注意力距离的测试效率较传统方法提升3.43倍，相较DAFL和WindRanger分别提升2.89倍和7.13倍。将注意力距离集成到DAFL和WindRanger中也获得了性能提升。

Conclusion: 注意力距离有效提升了定向灰盒模糊测试的效率和泛化能力，证明了逻辑关系在引导测试中的重要性，为软件安全测试方法提供了有力改进。

Abstract: In the domain of software security testing, Directed Grey-Box Fuzzing (DGF) has garnered widespread attention for its efficient target localization and excellent detection performance. However, existing approaches measure only the physical distance between seed execution paths and target locations, overlooking logical relationships among code segments. This omission can yield redundant or misleading guidance in complex binaries, weakening DGF's real-world effectiveness. To address this, we introduce \textbf{attention distance}, a novel metric that leverages a large language model's contextual analysis to compute attention scores between code elements and reveal their intrinsic connections. Under the same AFLGo configuration -- without altering any fuzzing components other than the distance metric -- replacing physical distances with attention distances across 38 real vulnerability reproduction experiments delivers a \textbf{3.43$\times$} average increase in testing efficiency over the traditional method. Compared to state-of-the-art directed fuzzers DAFL and WindRanger, our approach achieves \textbf{2.89$\times$} and \textbf{7.13$\times$} improvements, respectively. To further validate the generalizability of attention distance, we integrate it into DAFL and WindRanger, where it also consistently enhances their original performance. All related code and datasets are publicly available at https://github.com/TheBinKing/Attention\_Distance.git.

</details>


### [30] [A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows](https://arxiv.org/abs/2512.19769)
*Ivan Daunis*

Main category: cs.SE

TL;DR: 本文提出了一种声明式系统，通过统一DSL分离Agent工作流规范与实现，实现多语言多环境部署，提升开发效率和部署速度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM Agent系统将Agent逻辑与特定编程语言及部署模式紧耦合，导致开发和部署复杂且效率低下。

Method: 设计一个统一的声明式DSL，抽象常见工作流模式，将Agent开发从编写代码转变为配置流水线，并支持多语言后端及部署环境，及A/B测试功能。

Result: 在PayPal电商工作流中验证，开发时间减少60%，部署速度提升3倍，复杂工作流仅需50行DSL代码，相比代码实现大幅简化。

Conclusion: 声明式DSL降低了Agent开发门槛和维护成本，使非工程师也能安全修改行为，且保持低延迟，适合现实复杂场景应用。

Abstract: Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises).
  Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison.
  We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.

</details>


### [31] [Larger Is Not Always Better: Leveraging Structured Code Diffs for Comment Inconsistency Detection](https://arxiv.org/abs/2512.19883)
*Phong Nguyen,Anh M. T. Bui,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种基于CodeT5+的新方法，用于即时检测代码与注释的不一致性，显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 代码修改后注释未更新导致语义不一致，影响程序理解和维护，现有方法忽视代码演变的结构复杂性且存在隐私及资源问题。

Method: 将代码变更分解为有序的替换、删除、添加等修改活动序列，利用CodeT5+捕捉修改与过时注释之间语义关联。

Result: 在JITDATA和CCIBENCH两个公开数据集上，所提方法F1分数最高提升13.54%，优于多种调优后大型语言模型。

Conclusion: 引入修改活动序列分解策略的即时检测方法有效提升了代码注释不一致性检测性能，具备较强的实用价值。

Abstract: Ensuring semantic consistency between source code and its accompanying comments is crucial for program comprehension, effective debugging, and long-term maintainability. Comment inconsistency arises when developers modify code but neglect to update the corresponding comments, potentially misleading future maintainers and introducing errors. Recent approaches to code-comment inconsistency (CCI) detection leverage Large Language Models (LLMs) and rely on capturing the semantic relationship between code changes and outdated comments. However, they often ignore the structural complexity of code evolution, including historical change activities, and introduce privacy and resource challenges. In this paper, we propose a Just-In-Time CCI detection approach built upon the CodeT5+ backbone. Our method decomposes code changes into ordered sequences of modification activities such as replacing, deleting, and adding to more effectively capture the correlation between these changes and the corresponding outdated comments. Extensive experiments conducted on publicly available benchmark datasets-JITDATA and CCIBENCH--demonstrate that our proposed approach outperforms recent state-of-the-art models by up to 13.54% in F1-Score and achieves an improvement ranging from 4.18% to 10.94% over fine-tuned LLMs including DeepSeek-Coder, CodeLlama and Qwen2.5-Coder.

</details>


### [32] [Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?](https://arxiv.org/abs/2512.19980)
*Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 本文研究了代码语言模型中神经元的内部可解释性，发现存在语言专用神经元和跨语言概念层，并基于此提升了代码生成、克隆检测和代码总结任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言处理神经元可解释方法不适用于程序语言代码，因其形式化、层次化和可执行特性，需针对代码特性进行专门研究。

Method: 通过分析Llama-3.1-8B和Qwen2.5-Coder-32B模型在多语言代码输入上的神经元选择性和层次贡献，定位语言专用神经元和编码语言无关语义的概念层，并基于此设计神经元引导的微调、克隆检测和迁移学习方案。

Result: 发现了个别语言专用神经元和支持通用生成的神经元子集；低层编码语言特定语法，中层形成共享的语义概念层；基于这些发现的方法在多语言代码生成、克隆检测和代码总结任务上均取得一致性能提升。

Conclusion: 本研究揭示了代码语言模型内部的语言和概念层次结构，可指导针对多语言代码任务的优化方法，促进代码模型的可解释性和性能提升。

Abstract: Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.

</details>


### [33] [Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing](https://arxiv.org/abs/2512.20083)
*Wenzhao Wu,Yahui Tang,Mingfei Cheng,Wenbing Tang,Yuan Zhou,Yang Liu*

Main category: cs.SE

TL;DR: 本文针对实体代理任务规划中完成任务但效率低下的问题，提出了一种基于多样性指导变形测试的非最优决策检测框架NoD-DGMT，显著提升了非最优决策的检测率。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法只关注功能正确性，忽视了生成计划的非功能性最优性，导致性能下降和资源浪费。

Method: 提出NoD-DGMT框架，通过设计四个变形关系（位置绕行子最优性、动作最优性完备性、条件细化单调性、场景扰动不变性）检测非最优决策，并引入多样性指导的测试用例选择策略提高检测效率。

Result: 在AI2-THOR模拟平台上对四个先进规划模型进行测试，NoD-DGMT平均检测违规率31.9%，比六个基线方法最高提升16.8%，多样性筛选策略带来检测率和多样性评分显著提升。

Conclusion: NoD-DGMT有效检测实体代理任务规划中的非最优决策，表现优于现有方法，适用于不同模型和任务复杂度，推动资源受限应用中代理决策的优化。

Abstract: As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities.

</details>


### [34] [AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration](https://arxiv.org/abs/2512.20159)
*Ruiqi Wang,Xinchen Wang,Cuiyun Gao,Chun Yong Chong,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 本文提出了AXIOM，一种基于扰动的代码评估基准构建框架，通过规则引导的扰动与多源质量校准，实现了代码评分的精细控制与分布平衡，提升了代码质量评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的代码评估基准存在标签过于粗糙或主观定义模糊的问题，且数据合成方法不受控，导致评估结果不可靠，无法准确反映代码生成的真实质量。

Method: 提出AXIOM框架，使用规则引导的扰动对高质量代码进行功能及质量修改以精确控制目标分数，实现评分分布平衡；随后采用多源质量校准来优化评分的准确性和一致性。

Result: 构建的基准数据集呈现多样且均衡的质量分布，显著提高了手动注释流程的效率和评分的可靠性，增强了对代码生成质量的评估能力。

Conclusion: AXIOM有效解决了现有基准的局限性，为代码评估任务提供了更精确、客观和多样化的评价依据，有助于推动LLM生成代码质量的深入研究。

Abstract: Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.
  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...

</details>


### [35] [Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair](https://arxiv.org/abs/2512.20203)
*Zhenlei Ye,Xiaobing Sun,Sicong Cao,Lili Bo,Bin Li*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型的漏洞修复方法\sysname，通过定位需修复位置和质量评估机制提升修复效果，在C/C++漏洞数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的漏洞修复方法忽视补丁位置定位及对生成补丁质量的评价，导致修复效果有限。

Method: \sysname结合漏洞定位信息指导修复位置，并引入漏洞再引入与污染语句覆盖率两个维度对候选补丁质量进行评估，改进迭代修复策略。

Result: 在包含40个漏洞的真实C/C++漏洞修复数据集VulnLoc+上，\sysname生成了27个合理补丁，超越现有多种先进方法8至22个，且修复了8至13个额外漏洞。

Conclusion: 定位修复位置和质量评估机制有效提升了基于大语言模型的自动漏洞修复性能，\sysname为实用漏洞修复提供了有力支持。

Abstract: The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.
  To tackle the two limitations, we propose \sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.

</details>


### [36] [Toward Explaining Large Language Models in Software Engineering Tasks](https://arxiv.org/abs/2512.20328)
*Antonio Vitale,Khai-Nguyen Nguyen,Denys Poshyvanyk,Rocco Oliveto,Simone Scalabrino,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 本文提出了一种名为FeatureSHAP的自动化、模型无关的软件工程任务可解释性框架，基于Shapley值，通过系统输入扰动与任务特定的相似性比较，揭示大型语言模型输出背后的高层输入特征。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型虽然推动了软件工程任务自动化，但其黑盒特性限制了在安全关键领域的应用，缺乏与软件工程实践者推理方式相符的领域专用解释。

Method: FeatureSHAP使用Shapley值，通过系统性输入扰动和任务特定相似性比较，将模型输出归因于高层输入特征，兼容开源及专有LLM。

Result: 在代码生成与代码总结两个双模态软件工程任务中的评估显示，FeatureSHAP对无关特征赋予更低重要性，解释具有更高的真实性；37名从业者调查也表明其有助于更好理解模型输出并做出明智决策。

Conclusion: FeatureSHAP是实现软件工程中实用可解释人工智能的重要一步，有助于增强模型透明度与信任度。

Abstract: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.

</details>


### [37] [Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation](https://arxiv.org/abs/2512.20334)
*Yuan Huang,Yukang Zhou,Xiangping Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: 本研究评估了含缺陷的注释代码（CO代码）如何影响AI编程助手GitHub Copilot和Cursor生成代码的缺陷率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注代码上下文对缺陷代码生成的影响，忽视了注释代码中的缺陷对AI生成代码的影响。

Method: 通过实验模拟含缺陷的注释代码作为上下文，观察GitHub Copilot和Cursor生成代码的缺陷情况，并测试工具对忽视注释代码的响应。

Result: 含缺陷的注释代码使生成的缺陷代码比例最高达到58.17%，工具通过推理补全缺陷模式，且即使明确指示忽略注释代码，缺陷减少也不超过21.84%。

Conclusion: AI编程助手对含缺陷的注释代码敏感，表现出推理生成缺陷代码的行为，强调了提升AI编码工具鲁棒性和安全性的必要。

Abstract: With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.
  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.

</details>


### [38] [A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems](https://arxiv.org/abs/2512.20345)
*Xiaoxue Ma,Wanwei Zhan,Jiale Chen,Yishu Li,Jacky Keung,Federica Sarro*

Main category: cs.SE

TL;DR: 本文对分布式深度学习框架中的实际问题进行了大规模实证分析，构建了问题症状、根因及修复模式的分类体系，揭示了分布式训练中特有的主要问题并提出解决建议。


<details>
  <summary>Details</summary>
Motivation: 传统通用深度学习框架的分布式功能较为附加，使用复杂且易出问题，缺乏对分布式深度学习专用框架中实际挑战的系统性理解。

Method: 分析了DeepSpeed、Megatron-LM和Colossal-AI三个主流分布式框架中849个真实问题，归纳出34类症状、28类根因和6类修复模式，并建立症状、根因与修复之间的映射关系。

Result: 发现45.1%的问题症状是分布式框架特有，常见包括配置失败、内存问题和性能异常；通信配置阶段95%的问题仅在分布式环境出现；超过60%的问题可通过版本依赖管理和通信调优解决。

Conclusion: 通过系统化的问题分类和分析，本文为分布式深度学习框架的开发和维护提供了有针对性的实践指导和改进建议。

Abstract: In today's data-driven era, deep learning is vital for processing massive datasets, yet single-device training is constrained by computational and memory limits. Distributed deep learning overcomes these challenges by leveraging multiple GPUs or machines in parallel. While general-purpose frameworks (e.g., TensorFlow and PyTorch) provide distributed capabilities, these are often add-on features that demand significant manual effort for advanced parallelism, underscoring the need for specialized frameworks. This study conducts the first large-scale empirical analysis of practitioner challenges in dedicated distributed frameworks. We examine 849 real-world issues from DeepSpeed, Megatron-LM, and Colossal-AI and construct a taxonomy of 34 bug symptoms, 28 root causes, and 6 fix patterns. Crucially, we establish explicit mappings between symptoms, causes, and fixes across distributed training stages, enabling a systematic understanding of how issues emerge and are resolved. Our results show that 45.1\% of bug symptoms are unique to distributed frameworks, with setup failures, memory issues, and performance anomalies being the most prevalent. Moreover, 95\% of issues in the communication setup stage occur exclusively in distributed contexts. We also find over 60\% of cases can be resolved through version and dependency management, and distributed feature, API, and communication tuning. Based on these findings, we provide actionable implications.

</details>


### [39] [Identifying Appropriately-Sized Services with Deep Reinforcement Learning](https://arxiv.org/abs/2512.20381)
*Syeda Tasnim Fabiha,Saad Shafiq,Wesley Klewerton Guez Assunção,Nenad Medvidović*

Main category: cs.SE

TL;DR: 本文提出了一种基于深度强化学习的服务分解方法Rake，用于从遗留系统源码和文档中自动识别合理大小且模块化的服务，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统服务分解方法依赖文档完备性、项目人员访问或事先知道服务数量，现实中难以满足；因此需要一种无此限制的自动化、语言无关的服务分解技术。

Method: 提出Rake方法，利用深度强化学习结合系统文档和源码，从实现方法级别指导服务划分，支持定制化目标函数以权衡模块化质量与业务能力匹配。

Result: 在四个开源遗留项目上，Rake相比两种技术提升7-14%模块化质量，18-22%业务能力匹配度。此外，单一优化业务匹配度会降低紧耦合系统的分解质量。

Conclusion: Rake能够在无需特殊文档或人员介入情况下，有效且灵活地实现高质量服务分解，强调了优化指标平衡的重要性，为遗留系统现代化提供新的技术路径。

Abstract: Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.

</details>


### [40] [SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization](https://arxiv.org/abs/2512.20482)
*Revanth Gangi Reddy,Ye Liu,Wenting Zhao,JaeHyeok Doo,Tarun Suresh,Daniel Lee,Caiming Xiong,Yingbo Zhou,Semih Yavuz,Shafiq Joty*

Main category: cs.SE

TL;DR: 该论文提出SweRank+框架，通过多语言代码排名工具SweRankMulti和多轮推理搜索机制SweRankAgent，实现更精准的多语言代码问题定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法多针对Python且只进行单次搜索，无法充分利用多轮推理提升问题定位精度。

Method: 设计了SweRankMulti用于多语言代码嵌入检索和列表式大语言模型重排序，并通过多轮记忆搜索的SweRankAgent实现迭代式定位。

Result: 在多语言问题定位基准测试中，SweRankMulti达到最新最优性能，SweRankAgent进一步提升了定位准确性。

Conclusion: 结合多语言大规模数据训练的代码排名和多轮推理搜索机制，能够显著提升跨语言代码库中的问题定位效果。

Abstract: Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [41] [A Multi-Agent Retrieval-Augmented Framework for Work-in-Progress Predictio](https://arxiv.org/abs/2512.19841)
*Yousef Mehrdad Bibalan,Behrouz Far,Mohammad Moshirpour,Bahareh Ghiyasian*

Main category: cs.MA

TL;DR: 本文提出了一种结合检索增强生成和多智能体推理的工作进行中(WiP)预测框架，通过将结构化事件日志转换为语义丰富的自然语言故事，利用语义向量存储历史上下文，实现了动态检索和多智能体协作推理，提升了预测准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: WiP预测对于工作负载波动的准确预判和优化运营计划至关重要，现有方法在引入历史上下文和多智能体协同推理方面存在不足。

Method: 构建了一个结合检索增强生成(RAG)和多智能体协作推理的框架，包括将事件日志生成自然语言叙述、语义向量存储历史过程记忆、多个预测智能体利用检索历史上下文、决策助理智能体提取高层次信息，并通过融合智能体综合ReAct风格推理生成预测结果。

Result: 在两个真实世界基准数据集上的实验表明，所提框架在预测准确率方面表现优异，单个数据集上的平均绝对百分比误差(MAPE)达到1.50%，优于时间卷积网络(TCN)、长短期记忆(LSTM)及持久性基线方法。

Conclusion: 结合检索机制和多智能体推理的WiP预测框架显著提升了预测的准确性和鲁棒性，验证了该方法在实际应用中的有效性。

Abstract: Work-in-Progress (WiP) prediction is critical for predictive process monitoring, enabling accurate anticipation of workload fluctuations and optimized operational planning. This paper proposes a retrieval-augmented, multi-agent framework that combines retrieval-augmented generation (RAG) and collaborative multi-agent reasoning for WiP prediction. The narrative generation component transforms structured event logs into semantically rich natural language stories, which are embedded into a semantic vector-based process memory to facilitate dynamic retrieval of historical context during inference. The framework includes predictor agents that independently leverage retrieved historical contexts and a decision-making assistant agent that extracts high-level descriptive signals from recent events. A fusion agent then synthesizes predictions using ReAct-style reasoning over agent outputs and retrieved narratives. We evaluate our framework on two real-world benchmark datasets. Results show that the proposed retrieval-augmented multi-agent approach achieves competitive prediction accuracy, obtaining a Mean Absolute Percentage Error (MAPE) of 1.50\% on one dataset, and surpassing Temporal Convolutional Networks (TCN), Long Short-Term Memory (LSTM), and persistence baselines. The results highlight improved robustness, demonstrating the effectiveness of integrating retrieval mechanisms and multi-agent reasoning in WiP prediction.

</details>


### [42] [When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)](https://arxiv.org/abs/2512.20457)
*Marco Aruta,Francesco Improta,Vadim Malvone,Aniello Murano*

Main category: cs.MA

TL;DR: 本文提出了HumanATLF逻辑，结合模糊语义和资源限制的动作，改进了多智能体系统中策略推理的现实性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统策略假设过于理想化，忽视动作成本和事实感知不确定性，与现实人类决策不符。

Method: 引入HumanATLF逻辑，定义具有模糊语义和动作成本的自然策略，提供形式化语法语义，分析模型检测复杂度，并实现开源工具VITAMIN。

Result: 证明了在固定策略复杂度和资源预算时模型检测为P复杂度，允许布尔目标算子时为NP完全，策略复杂度和预算变化时为Delta^P_2完全，回忆策略可在PSPACE内判定。并在资源约束无人机救援场景中验证了算法。

Conclusion: HumanATLF有效弥补了多智能体系统策略推理与现实决策之间的差距，提供了兼具模糊性和资源感知的新模型，并将其成功应用于实际验证。

Abstract: In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.

</details>
