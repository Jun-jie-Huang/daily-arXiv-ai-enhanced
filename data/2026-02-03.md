<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 177]
- [cs.MA](#cs.MA) [Total: 8]
- [cs.SE](#cs.SE) [Total: 29]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: 本文提出PPoGA框架，通过预测与自我修正机制提升知识图谱增强的大语言模型问答能力，实现更灵活智能的推理。


<details>
  <summary>Details</summary>
Motivation: 当前KG增强的大规模语言模型在面对错误的高层推理计划时难以调整策略，容易陷入功能固着限制，影响复杂问答表现。

Method: 提出PPoGA框架，采用Planner-Executor架构分离高层策略与低层执行，并结合预测处理机制实现路径和计划的自我修正。

Result: PPoGA在GrailQA、CWQ和WebQSP三个多跳知识图谱问答基准上表现优异，显著优于现有方法。

Conclusion: PPoGA通过引入计划自我修正机制，显著提升了基于知识图谱的大规模语言模型多跳问答性能，显示出元认知能力在AI推理系统中的重要性。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [2] [Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA](https://arxiv.org/abs/2602.00009)
*Samuel Thio,Matthew Lewis,Spiros Denaxas,Richard JB Dobson*

Main category: cs.CL

TL;DR: 本文提出MediGRAF框架，结合图数据库与向量检索改进电子健康记录的信息检索，显著提升查询准确率与推理能力，保障临床应用安全。


<details>
  <summary>Details</summary>
Motivation: 现有电子健康记录系统信息量庞大，给临床医生带来认知负担，而大型语言模型在临床环境下存在上下文定位及幻觉问题，且现有检索方法未能有效融合结构化与非结构化数据。

Method: 提出了一种混合图检索增强框架MediGRAF，利用Neo4j Text2Cypher进行结构化关系遍历，同时结合向量嵌入实现非结构化语义检索，支持自然语言对完整患者病程的查询。

Result: 在MIMIC-IV数据集上测试，针对10名患者构建了5973个节点和5963条关系，系统对事实性查询达到了100%召回率，复杂推理任务获得平均4.25/5的专家评分且无安全违规。

Conclusion: MediGRAF通过结合结构化图数据库查询与非结构化文本检索，有效解决了临床电子健康记录中信息检索的困难，实现了高召回率和专家评分的推理能力，显著提升了临床信息检索的准确性和安全性。

Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.

</details>


### [3] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: 本文提出基于潜在记忆库和GRU门控更新的记忆增强LLM架构G-MemLLM，有效解决多跳推理中的长期记忆消退问题，显著提升了多项问答和关系抽取任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型因上下文窗口有限和多跳推理中长期事实一致性困难，且现有上下文压缩或循环token方法常出现信息衰减问题，亟需一种有效的记忆增强机制。

Method: 提出G-MemLLM架构，结合冻结的大型语言模型主干与可训练的潜在记忆库，采用GRU风格门控机制选择性更新、保留或覆盖记忆槽，防止知识梯度消失。

Result: 在HotpotQA和Zero-Shot Relation Extraction任务中，G-MemLLM在不同模型规模下均表现出明显提升，例如Llama3.1-8B在ZsRE任务中准确率提升13.3%，GPT-2和Llama 3.1-8B在HotpotQA任务中的Answer F1和Supporting Fact F1分别提升8.56和6.89。

Conclusion: G-MemLLM通过引入可训练的潜在记忆库和GRU风格的门控更新机制，有效提升了大型语言模型在多跳推理中的长期记忆保持能力和事实一致性，显著提高了相关任务的性能表现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [4] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

TL;DR: 本论文提出PTCBENCH基准系统评估大型语言模型在不同情境下的个性一致性，揭示模型个性随情境变化动态改变，推动心理学对齐的AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 当前研究忽视了个性特质的动态和情境依赖性，亟需建立基于情境变化评估LLM个性一致性的标准。

Method: 通过PTCBENCH基准，对39,240条数据在12种不同外部情境下利用NEO五因素人格量表系统性评估模型个性一致性。

Result: 研究发现某些情境（如失业）会显著改变LLM的个性及推理能力，PTCBENCH为真实动态环境下个性一致性评估提供了可扩展的框架。

Conclusion: 该论文表明大型语言模型的个性在不同情境下存在动态变化，部分外部情境显著影响模型个性和推理能力。

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [5] [SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations](https://arxiv.org/abs/2602.00017)
*Benyamin Tabarsi,Wenbo Li,Tahreem Yasir,Aryan Santhosh Kumar,Laura Widman,Dongkuan Xu,Tiffany Barnes*

Main category: cs.CL

TL;DR: SafeTalkCoach框架通过多智能体和多样化技术，生成真实且多样的亲子性健康对话，推动相关AI与健康交流研究。


<details>
  <summary>Details</summary>
Motivation: 父母与孩子之间关于性健康的有效沟通非常重要，但由于其隐私和敏感性，实际对话数据难以收集。

Method: 提出SafeTalkCoach，一个多智能体对话生成框架，结合众包与合成场景、性健康指南、基于证据的人物设定、自适应控制模块及分层多样化技术。

Result: 通过评估，SafeTalkCoach生成的对话既多样又具备现实感，通信质量和可控性得到保障。

Conclusion: SafeTalkCoach框架及数据集有助于促进AI研究及健康交流实践，支持关于性健康的有效亲子沟通。

Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.

</details>


### [6] [Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management](https://arxiv.org/abs/2602.00029)
*Yao Zhang,Hongyin Zhu*

Main category: cs.CL

TL;DR: 为解决企业知识管理中异构数据融合和复杂语义推理问题，提出了统一的LOM框架和三阶段训练流程，显著提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 企业级知识管理面临多源异构数据整合和复杂语义推理的挑战，传统知识图谱难以发现隐含关系并缺乏对复杂问答的充分语义理解。

Method: 提出了统一的构建-对齐-推理框架，构建双层企业本体，从结构化数据库和非结构化文本中融合信息；设计三阶段训练流程，包括本体指令微调、文本-本体对齐强化节点语义编码、多任务指令调优和课程学习。

Result: 4B参数的LOM模型在构建的多样本本体推理评测集上取得了89.47%的准确率，优于DeepSeek-V3.2，与复杂图推理表现突出。

Conclusion: LOM框架有效融合了企业本体结构和语言信息，实现了高效的语义推理能力，显著提升了复杂图推理任务的表现。

Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.

</details>


### [7] [Reversible Diffusion Decoding for Diffusion Language Models](https://arxiv.org/abs/2602.00150)
*Xinyun Wang,Min Zhang,Sen Cui,Zhikang Chen,Bo Jiang,Kun Kuang,Mingbao Lin*

Main category: cs.CL

TL;DR: 提出了一种可逆扩散解码框架，解决了扩散语言模型生成中的停滞问题，实现了高质量且高效的并行文本生成。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在块式解码时存在不可逆承诺，可能导致逆向扩散过程停滞，生成结果受限且难以改进。为此，提出引入可逆机制以提升生成质量和模型鲁棒性。

Method: RDD通过引入可逆机制，在块式扩散生成中检测逆向过程的停滞状态，利用缓存模型状态实现高效回溯，并通过置信度引导的重新遮罩选择性地重新初始化不确定的生成标记，从而避免重复失败轨迹。

Result: 实验验证了RDD在保持计算开销较低的前提下，显著提升了生成的鲁棒性和质量，优于基础模型。

Conclusion: 提出的可逆扩散解码（RDD）框架有效解决了扩散语言模型中由于不可逆承诺导致的停滞问题，提高了生成的鲁棒性和质量，同时保持了高效的并行生成能力。

Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.

</details>


### [8] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

TL;DR: 本文针对信息检索生成系统多样性不足的问题，提出DIVERGE框架，通过反思引导和记忆增强迭代方法提升多样性，实验证明在保持答案质量的同时显著增加了多样性。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统假设每个查询对应唯一正确答案，忽视了实际信息检索中存在多种合理答案的情况，导致生成结果单一，缺乏多样性和包容性。

Method: 提出DIVERGE框架，包括反思引导生成和记忆增强的迭代优化机制，系统性促进多样化观点的生成，并设计了新的多样性与质量权衡指标，经过实验证明效果优异。

Result: 在Infinity-Chat真实数据集上，DIVERGE在多样性和答案质量的权衡方面优于竞争方法和之前的最新技术，显著提升了结果的多样性且保持了答案质量，指标与人类评价高度相关。

Conclusion: 当前基于大语言模型的检索增强生成系统在处理多答案信息查询时存在多样性不足的问题，显著限制了创造力和信息公平性。我们提出的DIVERGE框架有效提升了生成回答的多样性，同时保证了回答质量，优于现有方法。

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [9] [Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering](https://arxiv.org/abs/2602.00279)
*Philip Müller,Nicholas Popovič,Michael Färber,Peter Steinbach*

Main category: cs.CL

TL;DR: 提出首个大规模科学问答不确定性量化基准，发现指令调教影响置信估计，答案频率方法效果最好，呼吁改进UQ评价标准。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化（UQ）方法在科学问答（QA）领域验证不足，而科学QA依赖事实检索和推理能力，因此需要更可靠的UQ评估方法。

Method: 构建了首个大规模基准测试，用于评估在需要推理的QA中UQ指标的校准，涵盖20个不同类型的大型语言模型和7个科学QA数据集，使用提示方法模仿开放式问答，分析了685,000条长回答，比较多种UQ方法。

Result: 发现指令调教导致token级置信概率极化，降低token级不确定性估计的可靠性。推理调教模型也存在此问题，但推理过程有所缓解。序列级上，口头化方法存在偏差且与正确率相关性差，答案频率（样本间一致性）提供最可靠的校准。指出单用ECE作为性能指标存在误导。

Conclusion: 当前LLM的不确定性量化方法及其基准测试存在显著局限，特别是指令调教影响置信估计可靠性，答案频率为较优校准方法，应避免仅依赖ECE评价指标。

Abstract: Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.

</details>


### [10] [Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models](https://arxiv.org/abs/2602.00300)
*Xilin Gong,Shu Yang,Zehua Cao,Lynne Billard,Di Wang*

Main category: cs.CL

TL;DR: Patchscopes框架中LLMs生成解释受语言偏见影响，导致不忠实。本文提出BALOR方法，通过logit重新校准抑制偏见，显著提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有Patchscopes框架中，LLMs生成解释时倾向于依赖固有的语言模式，导致解释结果不忠实于隐含上下文信息。

Method: 设计偏见案例数据集评估Patchscopes忠实度；提出通过对未修饰和修饰提示输出的logits进行重新校准的Bias Alignment through Logit Recalibration (BALOR)方法，抑制模型偏见，强化上下文信息。

Result: 使用偏见数据集验证Patchscopes导致18.84%的忠实度下降；BALOR在多个大语言模型上测试，性能提升最高达33%。

Conclusion: BALOR有效解决了Patchscopes中LLMs解释生成对语言偏见的依赖问题，提升了解释的忠实度和准确性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute "purple" for "broccoli", LLMs still generate "green" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\% relative performance improvement.

</details>


### [11] [MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes](https://arxiv.org/abs/2602.00316)
*Rodrigo Batista,Luís Filipe Cunha,Purificação Silvano,Nuno Guimarães,Alípio Jorge,Evelin Amorim,Ricardo Campos*

Main category: cs.CL

TL;DR: 本文提出一种两阶段管道方法，结合问答和基于Transformer的模型，实现市政会议纪要元数据的高效提取，建立首个相关基准，虽然领域内表现优越但泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要格式多样，元数据如会议编号、时间和参与者难以自动提取，现有命名实体识别模型不能很好适应此类领域特定标签。

Method: 首先使用问答模型识别包含元数据的文本段落，随后采用基于Transformer的模型（BERTimbau和XLM-RoBERTa，带或不带CRF层）进行细粒度实体提取，并通过去词汇化技术进行增强。

Result: 模型在本领域表现强劲，超越了更大的通用大语言模型，建立了市政会议纪要元数据提取的首个基准标准。然而模型在不同市政机构间泛化能力较弱。

Conclusion: 所提出的两阶段管道模型在市政会议纪要的元数据提取任务中表现优越，尤其是在领域内表现优于大型通用大语言模型，但在跨市政机构评估时表现出泛化能力的不足。

Abstract: Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.

</details>


### [12] [Detecting AI-Generated Content in Academic Peer Reviews](https://arxiv.org/abs/2602.00319)
*Siyuan Shen,Kai Wang*

Main category: cs.CL

TL;DR: 研究表明AI生成内容在学术同行评审中迅速上升，强调了对其学术评价影响的进一步研究需求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，探讨它们在学术同行评审中的角色及影响变得必要。

Method: 通过训练的检测模型对ICLR和Nature Communications的历史及最新同行评审进行分类，识别AI生成内容。

Result: 发现2025年ICLR和Nature Communications的同行评审中，分别约20%和12%的内容被分类为AI生成，表明AI辅助的使用显著增加。

Conclusion: AI生成内容在学术同行评审中的使用迅速增长，尤其是在2024年第四季度到2025年期间。

Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.

</details>


### [13] [DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning](https://arxiv.org/abs/2602.00352)
*Li Siyan,Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.CL

TL;DR: 提出了一个双代理多轮信息回忆评估基准DETOUR，测试发现当前模型准确率仅有36%，显示了多模态多轮检索任务的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基于信息回忆的代理能力的基准测试仅限于单轮对话，无法真实模拟多轮递进的'舌尖记忆'搜索过程。

Method: 本文提出了一个名为DETOUR的双代理评估基准，包含1011个提示。评估中主要有一个被评估的主代理通过向一个记忆代理提出查询来识别回忆中的实体，记忆代理在所有评估中保持一致。

Result: 当前最先进模型在所有模态（文本、图像、音频和视频）上的准确率仅为36%，体现出现有模型在处理信息不足的多轮回忆场景下的能力不足。

Conclusion: DETOUR基准更真实地模拟了多轮“舌尖记忆”过程，揭示了现有模型在处理不完全信息查询时的局限性，提示未来需加强在信息不足场景下的推理能力。

Abstract: When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.

</details>


### [14] [DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models](https://arxiv.org/abs/2602.00377)
*Zhaochen Hong,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出DecompressionLM框架，通过无状态、并行的生成方法有效扩展语言模型知识概念覆盖，对量化模型性能评估和实际应用具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 现有知识探测依赖预定义查询，限制了已知概念外的信息提取，且传统基于解码的方法存在耦合和规模扩展限制。

Method: 利用Van der Corput低差异序列结合算术解码，实现跨序列无共享状态、可并行的确定性生成，从而避免了传统解码方法的交叉序列耦合和竞争解码效应。

Result: AWQ-4bit量化方法提高了概念覆盖率30-170%，而GPTQ-Int4导致覆盖率大幅下降，且不同量化方式的性能差异未被解释级困惑度准确反映。文献验证显示不同模型在幻觉率上有明显差异。

Conclusion: DecompressionLM提供了一种无状态、零样本的概念图提取框架，显著提升了模型对知识概念的覆盖度，尤其在激活感知量化模型中表现突出。

Abstract: Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.

</details>


### [15] [Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models](https://arxiv.org/abs/2602.00380)
*Sercan Karakaş*

Main category: cs.CL

TL;DR: 本文比较了两种大型语言模型对土耳其语反身代词绑定关系的处理，发现Trendyol-LLM偏向局部绑定，而OpenAI模型则无明显偏好。


<details>
  <summary>Details</summary>
Motivation: 评估当前大型语言模型是否能准确捕捉土耳其语反身代词的绑定关系。

Method: 构建包含100个句子的平衡数据集，比较两种模型在反身代词kendi和kendisi的先行词选择上的表现，采用句子级困惑度和强制选择范式评估。

Result: Trendyol-LLM模型在约70%的试验中偏向本地先行词，显示出较强的局部性偏好；而OpenAI的链式思维模型在本地和长距离绑定上几乎均匀选择，表现出明显不同的绑定行为。

Conclusion: 不同的大型语言模型在处理土耳其语反身代词的绑定关系时表现出显著差异，这反映了模型的结构和训练数据对语言理解的影响。

Abstract: This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.

</details>


### [16] [Segment-Level Attribution for Selective Learning of Long Reasoning Traces](https://arxiv.org/abs/2602.00425)
*Siyuan Wang,Yanchen Liu,Xiang Ren*

Main category: cs.CL

TL;DR: 该文通过集成梯度归因识别重要推理段落，进行有选择的微调，减少冗余，提高大型推理模型的推理准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成的长思考链中，大多数内容重复或截断，且仅少部分对答案预测有意义，训练时模型倾向模仿冗长无效模式，影响性能。

Method: 引入集成梯度归因衡量每个token对最终答案的影响，构建两项分段指标：归因强度和方向一致性。基于此，提出分段选择性微调框架，对重要分段（高归因强度且方向一致性适中）进行有选择的微调，忽略无关分段损失。

Result: 在多个模型和数据集上验证，该方法提升了准确率和输出效率，使模型能更有效地从长推理链学习。

Conclusion: 通过基于归因的分段选择性微调，有效减少冗余内容的影响，提高了大型推理模型的推理性能和效率。

Abstract: Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \textit{attribution strength} measures the overall attribution magnitude; and (2) \textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.

</details>


### [17] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型多智能体系统中的曼德拉效应，提出了MANBENCH基准和多种缓解策略，显著降低了集体记忆偏差，提高系统可靠性和伦理标准。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型提升了多智能体系统的能力，但这些系统易受集体认知偏差影响，例如曼德拉效应，这不仅影响系统性能，也引发误信息传播的伦理问题。

Method: 通过构建MANBENCH基准测试套件，评估了多智能体在不同任务类型和交互协议下的曼德拉效应表现，并应用提示级防御和模型级对齐防御方法进行缓解测试。

Result: 实验结果显示所提策略能平均减少74.40%的曼德拉效应，有效提高多智能体系统的鲁棒性和伦理性。

Conclusion: 本文确认了大型语言模型驱动的多智能体系统中存在曼德拉效应这一集体记忆偏差问题，揭示了其成因并提出了有效的缓解策略。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [18] [What Matters to an LLM? Behavioral and Computational Evidences from Summarization](https://arxiv.org/abs/2602.00459)
*Yongxin Zhou,Changshun Wu,Philippe Mulhem,Didier Schwab,Maxime Peyrard*

Main category: cs.CL

TL;DR: 本文通过行为和计算分析揭示了大型语言模型在摘要任务中的信息选择优先级，发现其模式一致且由中后层注意力机制驱动，为解释和控制模型摘要行为提供新方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在摘要信息选择背后的内部重要性机制不明确，需要揭示其优先级何在以便理解和控制摘要行为。

Method: 结合行为分析（生成控制长度的摘要并基于选择频率推导信息重要性分布）和计算分析（关注注意力头与重要性分布的关联及层次影响）。

Result: 发现大型语言模型在信息重要性选择上表现出一致的模式，且与传统基线模型显著不同；不同模型家族间的聚类关系明显；部分注意力头与经验重要性高度匹配，中后层对重要性判断贡献大。

Conclusion: 大型语言模型在摘要任务中展现出一致且独特的信息重要性分布，且其内部的优先级机制主要体现在中后层的注意力头上。

Abstract: Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.

</details>


### [19] [Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations](https://arxiv.org/abs/2602.00469)
*Abhinav Gupta,Toben H. Mintz,Jesse Thomason*

Main category: cs.CL

TL;DR: 本文提出SENSE模型，将词嵌入投影到感知运动范式，行为实验验证其预测与人类认知相关，进而揭示语言与感知运动的结合。


<details>
  <summary>Details</summary>
Motivation: 目前词嵌入主要基于词的共现模式，缺乏感知与运动经验的支撑，本文旨在将人类的感知运动体验融入语言理解模型。

Method: 提出了SENSE模型，一种通过词汇嵌入预测Lancaster感知运动范式的学习投影模型，并进行了包含281名参与者的行为实验验证。

Result: SENSE模型在多数感知运动模态上实现了与人类选择率的显著相关性，且通过对无意义词的分析发现了系统性的音形相关模式。

Conclusion: SENSE模型能够有效预测词汇的感知运动范式，且其预测结果与人类行为数据在多个感知运动模态上存在显著相关性，揭示了文本信息与感知运动体验之间的联系。

Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\text{SENSE}$ $(\textbf{S}\text{ensorimotor }$ $\textbf{E}\text{mbedding }$ $\textbf{N}\text{orm }$ $\textbf{S}\text{coring }$ $\textbf{E}\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.

</details>


### [20] [Intention-Adaptive LLM Fine-Tuning for Text Revision Generation](https://arxiv.org/abs/2602.00477)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: 提出了一种意图自适应的分层微调方法Intention-Tuning，解决多意图修订生成中数据稀缺和模型表现不佳的问题，取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对基于意图的文本生成任务如修订生成探索不足，且多意图混合场景复杂难处理，且缺乏大量带标注数据进行微调。

Method: 提出了意图自适应的分层LLM微调框架，通过动态选择部分模型层学习意图，并将其表征转移用于修订生成。

Result: 实验表明Intention-Tuning在小规模修订语料库上能有效捕捉意图，提高修订生成质量，且效率较高。

Conclusion: Intention-Tuning方法在少量修订语料上表现出良好的效果和效率，优于多种参数高效微调基线。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.

</details>


### [21] [From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas](https://arxiv.org/abs/2602.00491)
*Zhaokun Yan,Zhaohan Liu,Wuzheng Dong,Lijie Feng,Chengxiao Dai*

Main category: cs.CL

TL;DR: 本文发布了GlobalHealthAtlas数据集和评估体系，推动了公共卫生推理在多语言、大规模机器学习中的研究和安全应用。


<details>
  <summary>Details</summary>
Motivation: 公共卫生推理需要基于科学证据和专家共识进行人口层面的推断，但当前相关机器学习研究较少，缺乏大型多语言标注数据集和系统评测工具，影响模型的训练和安全应用。

Method: 采集涵盖15个公共卫生领域和17种语言的28万条数据，数据分为三个难度等级，并通过大型语言模型辅助的数据构建与质量控制流程确保数据一致性。设计了基于多模型高置信度判断的领域对齐评估器，从准确性、推理、完整性等六个维度评估模型输出。

Result: 构建了包含280,210条实例的多语言公共卫生推理数据集GlobalHealthAtlas，开发了大型语言模型辅助的数据处理和六维度评估系统，实现了对公共卫生推理任务的结构化学习和可重复评测。

Conclusion: 本文提出了一个名为GlobalHealthAtlas的大规模多语言公共卫生推理数据集，填补了公共卫生推理在机器学习领域的研究空白，并提供了相应的评估方法，推动了公共卫生领域的大型语言模型的安全可靠应用。

Abstract: Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.

</details>


### [22] [Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design](https://arxiv.org/abs/2602.00497)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.CL

TL;DR: 本文提出多语言大型语言模型治理的文化和权利基础框架，强调公平与责任，避免全球不平等加剧。


<details>
  <summary>Details</summary>
Motivation: 现有治理框架假定以英语为中心，忽视低资源语言和文化边缘社区的需求，导致系统性风险。

Method: 综合跨文化视角的人本计算和AI治理，分析多语言模型行为、数据不对称和社会技术伤害。

Result: 提出了一个文化根基的治理框架，强调数据管理、透明度和参与式问责机制。

Conclusion: 多语言大型语言模型的治理需要以文化为本，确保其不复制全球不平等。

Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.

</details>


### [23] [Reasoning by Commented Code for Table Question Answering](https://arxiv.org/abs/2602.00543)
*Seho Pyo,Jiheon Seok,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文通过带注释的多行代码生成引入显式推理，显著提升了表格问答任务的准确率和可解释性，在WikiTableQuestions上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统的表格线性化方法破坏了表格的二维结构关系，现有端到端答案生成或单行程序查询方法准确率和可解释性较低，因此需要一种明确推理过程的新方法。

Method: 采用带有简洁自然语言注释的多行可执行Python程序，将表格问答推理过程解构为代码生成步骤，从而增强模型的推理能力和代码可解释性。

Result: 在WikiTableQuestions基准上，使用Qwen2.5-Coder-7B-Instruct达到70.9%的准确率，优于67.6%的Repanda基线，结合轻量级答案选择机制后准确率提升至84.3%。

Conclusion: 该论文提出的带注释的逐步代码生成框架显著提升了表格问答任务中代码的正确率和模型的数值准确性，且在WikiTableQuestions基准测试中表现优于现有方法。

Abstract: Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\% accuracy on the WikiTableQuestions benchmark.

</details>


### [24] [A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora](https://arxiv.org/abs/2602.00554)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: 本研究利用多种分析工具揭示了BERT模型处理四种语义构造时的层级化表征特点，展现出结构化的信息发展过程。


<details>
  <summary>Details</summary>
Motivation: 探索BERT模型如何处理基本的语义结构构造，理解其内部表征及信息流动机制。

Method: 采用多维分析框架，结合MDS、t-SNE降维方法，GDV聚类分离度指标，FDR线性判别探测，及注意力机制分析。

Result: 发现构造特定信息在模型的早期层次初现，中间层形成最可分离的聚类，晚期层次保持信息完整。

Conclusion: Bidirectional Encoder Representations from Transformers模型在处理基本语义构造时展现出层级式的表征结构，语义特定信息在中间层达到最大可区分性，并在后续层次保持。

Abstract: This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.

</details>


### [25] [The French Drama Revolution: Political Economy and Literary Production, 1700-1900](https://arxiv.org/abs/2602.00588)
*Thiago Dumont Oliveira*

Main category: cs.CL

TL;DR: 本文用主题模型分析法国1700-1900年戏剧主题变化，发现大革命后资产阶级主题崛起，且戏剧主题演变与经济增长密切相关。


<details>
  <summary>Details</summary>
Motivation: 探究1700-1900年间法国戏剧主题的演变及其与经济增长的关系。

Method: 采用潜在狄利克雷分配（LDA）和詹森-香农散度（Jensen-Shannon Divergence）分析1700-1900年法国戏剧主题的变化。

Result: 发现法国大革命后戏剧主题显著变化，资产阶级主题兴起，并通过将主题出现频率与GDP年份数据对比，揭示戏剧与经济发展的共同演化。

Conclusion: 法国大革命后，尤其是1789年至1850年间，法国戏剧的主题分布发生了深刻变化，资产阶级主题成为主要话题之一，这反映了政治和经济环境的转型。

Abstract: This paper investigates the changing nature of French drama between 1700-1900 using Latent Dirichlet Allocation and Jensen-Shannon Divergence. Results indicate that the topical distribution of French drama changed profoundly after the French Revolution, particularly between 1789 and 1850. Bourgeois themes emerged among the most prevalent topics since the late 18th century. To assess the coevolution of drama and economic growth, I plot the yearly prevalence of topics alongside French GDP between 1700-1900, and discuss these changes in light of the political and economic changes prompted by the French Revolution and the industrialization of the country.

</details>


### [26] [Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling](https://arxiv.org/abs/2602.00594)
*Zhijie Huang,Stephen McIntosh,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.CL

TL;DR: Kanade是一种单层解耦的语音分词器，能有效提取语音和韵律特征，抑制无关信息，实现高质量语音合成和说话者解耦。


<details>
  <summary>Details</summary>
Motivation: 语音建模需要处理混合了语言和非语言信息的连续信号，传统的分词器难以在提取语音特征同时抑制无关信息，如说话者身份。

Method: 提出Kanade，一种单层的解耦语音分词器，通过分离声学常数形成单一的令牌流，捕捉丰富的语音和韵律特征，无需传统辅助技术。

Result: Kanade实现了业界领先的说话者解耦和词汇可用性，同时保持了优异的语音重构质量。

Conclusion: Kanade作为一种创新的语音分词器，极大提升了语音模型的特征提取质量和合成效果，对于语音技术发展具有重要意义。

Abstract: A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.

</details>


### [27] [Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling](https://arxiv.org/abs/2602.00597)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 针对跨语言字幕翻译的挑战，提出了基于大语言模型的Hermes框架，通过三大模块提升翻译质量，实现了领先的性能和表达效果。


<details>
  <summary>Details</summary>
Motivation: 跨语言字幕翻译在娱乐本地化中至关重要，但机器翻译中仍存在语义连贯、代词及术语翻译准确性和翻译表达性等难题，尚未得到充分探索。

Method: 提出了基于大语言模型的自动字幕翻译框架Hermes，集成了说话人分离(Speaker Diarization)、术语识别(Terminology Identification)、表达性增强(Expressiveness Enhancement)三个模块。

Result: Hermes在说话人分离任务上达到最先进性能，并生成了富有表现力且上下文连贯的字幕译文，推动了跨语言字幕翻译研究的发展。

Conclusion: Hermes框架有效解决了跨语言字幕翻译中的语义连贯性、代词与术语翻译以及翻译表达性问题，实现了领先的说话人 diarization 以及生成富有表现力和上下文连贯的翻译。

Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.

</details>


### [28] [Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars](https://arxiv.org/abs/2602.00612)
*Yitong Zhang,Yongmin Li,Yuetong Liu,Jia Li,Xiaoran Jia,Zherui Li,Ge Li*

Main category: cs.CL

TL;DR: 针对扩散大型语言模型生成符号语言时语法错误频发的问题，本文提出基于token分布的并行前瞻受约束解码方法LAVE，有效提升了生成语法正确性且无显著性能损失。


<details>
  <summary>Details</summary>
Motivation: dLLMs作为概率模型在生成语法形式受限的语言时，易生成语法不合法的输出；现有受约束解码方法难以适应dLLMs的非自回归特性，且可能导致无法扩展为合法句子的中间结果。

Method: LAVE方法利用dLLMs在每次前向传播中并行预测所有位置的token分布，结合前瞻机制对提出的token进行有效验证，确保生成过程中语法约束的可靠执行。

Result: 在四个主流dLLMs和三个基准测试上的实验表明，LAVE在语法正确性上持续优于已有方法，同时运行时间开销几乎可以忽略。

Conclusion: 提出的LAVE方法能够有效解决扩散大型语言模型(dLLMs)在生成过程中语法正确性不足的问题，显著提升生成内容的语法正确率且代价低廉。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.
  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.

</details>


### [29] [Transformer-Based Model for Multilingual Hope Speech Detection](https://arxiv.org/abs/2602.00613)
*Nsrin Ashraf,Mariam Labib,Hamada Nayel*

Main category: cs.CL

TL;DR: 本文通过RoBERTa和XLM-RoBERTa模型针对英语和德语的希望言论检测任务进行了实证，证明了预训练模型提升性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 提升多语言（英语和德语）希望言论检测的性能。

Method: 实现了多种transformer模型，针对英语使用RoBERTa模型，针对英语和德语使用多语言模型XLM-RoBERTa，并对其效能进行了评估。

Result: RoBERTa模型在英语希望言论检测上取得加权F1分数0.818和准确率81.8%；XLM-RoBERTa模型在英语和德语上分别达到0.786的加权F1分数和78.5%的准确率。

Conclusion: 预训练大规模语言模型的改进对提升自然语言处理任务表现至关重要。

Abstract: This paper describes a system that has been submitted to the "PolyHope-M" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.

</details>


### [30] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

TL;DR: 通过将源代码以图像形式输入多模态大语言模型，实现了高效压缩和有效理解，展现了代码图像模态表示优化推理效率的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型因将代码视为线性文本序列，导致随着系统规模增长计算成本线性增加，寻求通过多模态学习模型利用图像模态进行压缩以提高计算效率。

Method: 首次系统研究了将源代码以图像模态输入MLLMs的可行性，通过调整图像分辨率实现大幅压缩，并评估其在代码理解任务中的表现。

Result: MLLMs在代码理解任务中实现了最高8倍的压缩率；视觉提示如语法高亮提高了在4倍压缩下的代码完成性能；克隆检测等任务对视觉压缩表现出高度鲁棒性。

Conclusion: 多模态大语言模型（MLLMs）能够有效理解以渲染图像形式呈现的源代码，实现显著的压缩率和计算效率提升，同时视觉特性如语法高亮可以进一步改善代码完成任务的表现；某些代码理解任务对视觉压缩表现出强适应性甚至超越文本输入。

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [31] [Jailbreaking LLMs via Calibration](https://arxiv.org/abs/2602.00619)
*Yuxuan Lu,Yongkang Guo,Yuqing Kong*

Main category: cs.CL

TL;DR: 提出一种基于损失函数的安全绕过预测聚合框架，显著提升LLMs的绕过攻击效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法导致模型输出与原始数据分布产生系统性差异，如何有效利用该特性进行安全绕过攻击并提升绕过成功率成为挑战。

Method: 将安全对齐看作预先对齐分布的系统性扭曲，提出将弱到强绕过视为预测聚合问题，并导出基于损失诱导的对偶空间梯度偏移的最优聚合策略，涵盖了对数算术绕过方法和新混合聚合规则。

Result: 在红队测试和数学任务中，所提方法在前沿模型上获得更高的攻击成功率和更低的绕过成本，尤其在安全强化的gpt-oss-120b上表现突出。

Conclusion: 本研究提出的框架有效建模了安全对齐对LLMs预测的系统性扭曲，提出的最优聚合策略在安全绕过攻击中表现出优越性。

Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower "Jailbreak Tax" compared with existing methods, especially on the safety-hardened gpt-oss-120b.

</details>


### [32] [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084)
*Jane Luo,Chengyu Yin,Xin Zhang,Qingtao Li,Steven Liu,Yiming Huang,Jie Wu,Hao Liu,Yangyu Huang,Yu Kang,Fangkai Yang,Ying Xin,Scarlett Li*

Main category: cs.CL

TL;DR: 本论文提出RPG-Encoder框架，通过构建统一高保真仓库规划图，解决了代码库理解中的语义断层，实现了代码意图与实现之间的闭环，显著提升了代码库理解和导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有仓库代理方法依赖孤立的API文档或依赖图，缺乏语义深度，导致推理断层，影响代码库理解和生成。

Method: 提出了RPG-Encoder框架，通过编码原始代码为结合语义特征和代码依赖的仓库规划图(RPG)，增量演化拓扑结构，以及提供结构感知导航接口，实现对代码库的高保真统一表示。

Result: RPG-Encoder在SWE-bench Verified和Live Lite数据集上实现了领先的理解准确率，且在RepoCraft数据集上实现了98.5%的重构覆盖率，显著提升了定位精度和表示 fidelity。

Conclusion: RPG-Encoder有效解决了当前代码库理解中的推理断层问题，实现了意图与实现之间的闭环。

Abstract: Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.

</details>


### [33] [Formal Semantic Control over Language Models](https://arxiv.org/abs/2602.00638)
*Yingji Zhang*

Main category: cs.CL

TL;DR: 本论文通过变分自编码器方法改进语言模型的语义表示，使其潜在空间更易解释和精确控制，推动语言模型向可解释、结构化和可控方向发展。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的内部语义表示缺乏系统可解释性和结构化控制，限制了精细语义理解与操作。

Method: 采用VAE框架，分别在句子层面和推理层面进行学习，通过解耦潜在空间中的语义特征和推理行为，控制句子生成和自然语言推理（NLI）。

Result: 实验验证了所提出的方法能够有效增强自然语言潜在空间的解释性与可控性，特别是在解释性自然语言推理任务中表现突出。

Conclusion: 本文提出的方法成功提升了语言模型潜在空间的语义可解释性和几何结构的可控性，实现了局部化和类符号的组合控制。

Abstract: This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.

</details>


### [34] [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208)
*Md. Toufique Hasan,Ayman Asad Khan,Mika Saari,Vaishnavi Bankhele,Pekka Abrahamsson*

Main category: cs.CL

TL;DR: 针对芬兰语农业决策支持，本文提出了领域适配的RAG系统AgriHubi，结合专用文档与用户反馈，提升了问答质量及可信度，适用于低资源语言场景。


<details>
  <summary>Details</summary>
Motivation: 农业领域知识密集，普通大模型因训练数据偏向英语且缺乏实际评估，在低资源语言中难以有效应用，亟需领域适配方法。

Method: 该研究开发了基于检索增强生成的系统AgriHubi，结合芬兰农业文档和开放的PORO模型，通过显式信息源依托与用户反馈支持迭代优化。

Result: 经过八次迭代和两轮用户研究，AgriHubi在答案完整性、语言准确性和可靠性感知上表现优异，但大型模型存在响应时延与质量的权衡问题。

Conclusion: AgriHubi系统有效提升了芬兰语农业领域问答的准确性和用户满意度，展示了领域适应型RAG系统在低资源语言环境中的应用潜力。

Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.

</details>


### [35] [LegalOne: A Family of Foundation Models for Reliable Legal Reasoning](https://arxiv.org/abs/2602.00642)
*Haitao Li,Yifan Chen,Shuo Miao,Qian Dong,Jia Chen,Yiran Hu,Junjie Chen,Minghao Qin,Qingyao Ai,Yiqun Liu,Cheng Luo,Quan Zhou,Ya Zhang,Jikun Hu*

Main category: cs.CL

TL;DR: 针对中文法律领域设计的LegalOne模型通过三阶段训练有效增强法律推理能力，性能领先并开源推动法律AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在法律领域缺乏精准知识以及复杂司法推理能力，难以满足法律高标准严要求的应用需求。

Method: 提出了塑性调节采样(PAS)进行领域适应，中级训练阶段采用法律代理链式推理蒸馏(LEAD)提取结构化推理路径，最后通过课程强化学习策略提升模型记忆、理解与推理能力。

Result: LegalOne在多种法律任务中达到领先表现，参数数量远少于通用大型模型但知识密度和效率更高，公开了模型权重和评测框架。

Conclusion: LegalOne通过三阶段训练流程显著提升了中文法律领域的语言模型能力，实现了超越通用大型模型的性能，推动了法律人工智能的发展。

Abstract: While Large Language Models (LLMs) have demonstrated impressive general capabilities, their direct application in the legal domain is often hindered by a lack of precise domain knowledge and complexity of performing rigorous multi-step judicial reasoning. To address this gap, we present LegalOne, a family of foundational models specifically tailored for the Chinese legal domain. LegalOne is developed through a comprehensive three-phase pipeline designed to master legal reasoning. First, during mid-training phase, we propose Plasticity-Adjusted Sampling (PAS) to address the challenge of domain adaptation. This perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities, effectively establishing a robust legal foundation. Second, during supervised fine-tuning, we employ Legal Agentic CoT Distillation (LEAD) to distill explicit reasoning from raw legal texts. Unlike naive distillation, LEAD utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories, thereby enforcing factual grounding and logical rigor. Finally, we implement a Curriculum Reinforcement Learning (RL) strategy. Through a progressive reinforcement process spanning memorization, understanding, and reasoning, LegalOne evolves from simple pattern matching to autonomous and reliable legal reasoning. Experimental results demonstrate that LegalOne achieves state-of-the-art performance across a wide range of legal tasks, surpassing general-purpose LLMs with vastly larger parameter counts through enhanced knowledge density and efficiency. We publicly release the LegalOne weights and the LegalKit evaluation framework to advance the field of Legal AI, paving the way for deploying trustworthy and interpretable foundation models in high-stakes judicial applications.

</details>


### [36] [Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation](https://arxiv.org/abs/2602.00665)
*Lakshan Cooray,Deshan Sumanathilaka,Pattigadapa Venkatesh Raju*

Main category: cs.CL

TL;DR: 本研究评估了指令调优的小型语言模型在多轮客户服务问答的表现，采用历史摘要保持对话上下文，结果显示部分SLMs能接近大型语言模型性能，但整体仍存在对话连续性和上下文理解的不足。


<details>
  <summary>Details</summary>
Motivation: 为了克服大型语言模型在计算成本和部署上的限制，探讨更高效的小型语言模型在多轮客户服务问答中，尤其是需要对话连续性和上下文理解的场景中的有效性。

Method: 采用历史摘要策略进行上下文总结，结合指令调优的小型语言模型进行多轮客户服务问答测试，并引入基于对话阶段的定性分析方法。通过词汇和语义相似度指标，以及人工评价和LLM担任评审的方式，评估九种指令调优的低参数SLMs与三种商业LLMs的表现。

Result: 部分低参数SLMs在多轮客户服务问答中的表现接近LLMs，但存在表现不均、对话连续性和上下文保持能力不足的现象，反映了其应用的潜力与挑战并存。

Conclusion: 低参数小型语言模型（SLMs）在实际客户服务问答系统中展现出潜力，但在保持对话连续性和上下文一致性方面仍存在局限。部分SLMs的表现接近大型语言模型（LLMs），但效果差异显著。

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

</details>


### [37] [EchoReview: Learning Peer Review from the Echoes of Scientific Citations](https://arxiv.org/abs/2602.00733)
*Yinuo Zhang,Dingcheng Huang,Haifeng Suo,Yizhuo Li,Ziya Zhao,Junhao Xu,Zhiying Tu,Dianhui Chu,Deming Zhai,Xianming Liu,Xiaoyan Yu,Dianbo Sui*

Main category: cs.CL

TL;DR: 针对传统同行评审的规模和可靠性问题，本文提出基于引用语境的数据合成方法EchoReview，构建大规模数据集并训练自动评审模型，显著提升了自动审稿的质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统同行评审规模难以应对快速增长的论文提交量，且现有基于真实评审数据的监督微调方法受限于数据单一来源及人为评审的主观性和不一致性，亟需一个既可扩展又可靠的自动评审方法。

Method: 提出了EchoReview框架，通过系统挖掘学术引用中的隐式集体评价信号，合成结构化评审风格数据，构建了跨会议、跨年份的大规模EchoReview-16K数据集，并训练了自动化评审器EchoReviewer-7B。

Result: EchoReviewer-7B在证据支持和评审全面性等核心评审维度表现出显著且稳定的提升，证明了引用语境作为数据范式的有效性和鲁棒性。

Conclusion: EchoReview通过利用引用语境转换科学社区长远判断为结构化的评审数据，有效提升了自动化评审的质量和稳定性，验证了基于引用语境的数据范式在自动同行评议中的可靠性。

Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.

</details>


### [38] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

TL;DR: 针对小样本临床文本改进难题，ExperienceWeaver框架通过结构化多维反馈的经验学习，提升了修订能力，显著优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 临床文本改进对医疗效率至关重要，但由于高质量数据有限及医疗文档的复杂约束，改进难度大。现有大型语言模型在小样本环境中表现有限，监督微调成本高，检索增强生成往往只能提供表面改正，缺乏修订背后的推理。

Method: 提出ExperienceWeaver框架，转变焦点从数据检索到经验学习。通过提取噪声繁多且多维度的反馈，结构化并转化为具体的错误提示和高层策略。将提炼的经验注入智能管线，使模型学习“如何修订”而非仅仅“修订什么”。

Result: 在四个临床数据集上的大量评估表明，ExperienceWeaver在小样本环境中持续提升性能，优于包括Gemini-3 Pro在内的最先进模型。

Conclusion: ExperienceWeaver通过经验学习有效解决了小样本临床文本改进难题，提升了模型的推理和修订能力，增强了医疗文本处理的效率和质量。

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [39] [CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs](https://arxiv.org/abs/2602.00742)
*Liang Wang,Xinyi Mou,Xiaoyou Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

TL;DR: CURP利用双向编码器和原型代码本，实现极少参数的用户个性化，提升了生成任务表现与模型扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的用户建模方法在个性化质量和计算、数据效率之间存在权衡难题。

Method: 提出CURP框架，采用双向用户编码器和离散原型代码本，从多维度提取用户特征，实现少量可训练参数的即插即用个性化。

Result: CURP在多个生成任务中表现出优越性能和泛化能力，且具备更好的可解释性和可扩展性。

Conclusion: CURP框架有效解决了个性化模型中性能与效率的矛盾，实现了高效且可扩展的用户建模。

Abstract: User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code

</details>


### [40] [Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://arxiv.org/abs/2602.00747)
*Shengrui Li,Fei Zhao,Kaiyan Zhao,Jieying Ye,Haifeng Liu,Fangcheng Shi,Zheyong Xie,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: DeMix提出了一种新颖的模型合并方法，高效且准确地确定大型语言模型预训练所需的数据混合比例，显著降低了搜索成本并提升性能，同时公开了大规模高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 现有确定数据混合比例的方法存在验证成本高或依赖小规模代理模型不可靠的问题，难以找到最优混合比例以平衡模型的通用能力和针对难题的专长。

Method: DeMix通过训练候选数据集的组件模型，并利用加权模型合并生成数据混合代理，解耦了数据比例搜索和模型训练过程，从而实现不限量的搜索试验且无需额外训练。

Result: 实验表明DeMix在较低的搜索成本下，能够找到更优的数据混合比例，提升基准测试性能，同时发布了包含22万亿标记的高质量预训练数据集DeMix Corpora，促进开放研究。

Conclusion: DeMix框架通过模型合并技术，成功实现了数据混合比例的高效预测，克服了传统方法在搜索成本和准确率上的权衡，显著提升了大型语言模型预训练的性能。

Abstract: Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.

</details>


### [41] [Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting](https://arxiv.org/abs/2602.00758)
*Ali El Lahib,Ying-Jieh Xia,Zehan Li,Yuxuan Wang,Xinyu Pi*

Main category: cs.CL

TL;DR: 谷歌搜索的日期过滤器无法有效防止截止日期后信息泄露，导致预测结果被严重夸大，应采用更严格措施保证时间敏感任务的评估准确性。


<details>
  <summary>Details</summary>
Motivation: 搜索引擎日期过滤器被广泛用于回顾性评估中，以确保预测只基于截止日期前的信息。

Method: 通过审计Google搜索的before:过滤器，检测截止日期后的信息泄露。使用大语言模型gpt-oss-120b，在含泄露和无泄露的文档上对预测准确性进行比较。

Result: 发现71%的查询返回了包含强烈截止日期后泄露信息的页面，41%的页面直接揭示答案。使用泄露文档的预测准确性明显被夸大（Brier分数0.108 vs. 0.242）。

Conclusion: 搜索引擎的日期过滤功能在时间评估中不可靠，需采用更严格的检索保护措施或基于冻结时间戳网页快照进行评估，以保证预测的可信度。

Abstract: Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.

</details>


### [42] [Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning](https://arxiv.org/abs/2602.00759)
*Zhipeng Chen,Xiaobo Qin,Wayne Xin Zhao,Youbin Wu,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文提出A²D方法通过问题能力分解辅助RLVR，有效提升大语言模型的推理能力，且具有较好的通用性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR因信息量有限，探索过程盲目，导致在复杂任务中表现不佳，缺乏额外指导信息且不依赖教师模型。

Method: 提出了一种自适应能力分解方法（A²D），首先利用RLVR训练分解器将复杂问题拆解为简单子问题，然后用分解器对训练数据进行标注，最后在RLVR框架下用子问题指导训练推理器。

Result: A²D在与多种竞争基线的对比实验中表现优越，分析表明分解器在RLVR过程中的表现及指导类型都对推理器的探索与利用能力有积极影响。

Conclusion: A²D方法通过引入能力分解器显著提升了基于可验证奖励的强化学习（RLVR）在复杂问题推理中的效果，并且能够作为一个插拔模块应用于不同RLVR算法。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.

</details>


### [43] [APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards](https://arxiv.org/abs/2602.00760)
*Kaiyan Chang,Chenwei Zhu,Yingfeng Luo,Yifu Huo,Chenglong Wang,Xiaoqian Liu,Qiaozhi He,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 针对大规模推理模型的过度思考问题，本文提出基于推理锚点的奖励调整方法APR，有效减少无效重复验证，提高了推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型在测试时存在重复自验证且无实际修正的过度思考现象，导致计算资源浪费和推理效率低下。

Method: 提出了一种结构感知的奖励调整方法APR，通过定位推理中的Reasoning Anchor并针对Answer-Stable Tail进行惩罚，结合适合长度惩罚的策略优化算法进行训练。

Result: APR在1.5B和7B参数规模的大模型上，在五个数学推理数据集上实现了性能-效率的最优权衡，显著减少了强化学习训练所需的计算资源。

Conclusion: Anchor-based Process Reward (APR) 方法有效解决了大规模推理模型在测试时出现的过度思考问题，提高了推理效率和性能。

Abstract: Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.

</details>


### [44] [WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs](https://arxiv.org/abs/2602.00762)
*Yuheng Shao,Junjie Xiong,Chaoran Wu,Xiyuan Wang,Ziyu Zhou,Yang Ouyang,Qinyi Tao,Quan Li*

Main category: cs.CL

TL;DR: 针对中文母语英语学习者难以应用关键词法记忆词汇的问题，本文开发了基于多模态大语言模型的交互工具WordCraft，通过过程指导提升记忆效果，实验证明其有效性和可用性。


<details>
  <summary>Details</summary>
Motivation: L1中文母语的L2英语学习者在使用关键词法记忆词汇时面临诸多挑战，如难以生成合适的关键词、构建连贯的联想和形成生动的心象。现有方法要么减少学习者参与度，要么缺乏过程指导。

Method: 通过对18名L1中文-L2英语学习者和教师的调研，识别关键词法应用中的关键困难和需求，基于多模态大语言模型（MLLM）开发了交互式工具WordCraft，引导学习者完成关键词选择、联想构建和图像形成。

Result: 两项用户研究表明，WordCraft不仅保留了关键词生成的效果，还在词汇记忆的有效性和可用性方面表现出较高水平。

Conclusion: WordCraft通过过程指导和多模态交互，有效解决了关键词法在L1中文-L2英语词汇学习中的难题，提升了词汇记忆的效果和学习体验。

Abstract: Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.

</details>


### [45] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 本文提出一种基于迭代上下文学习的方法，通过信任博弈刻画大语言模型的信任表现，发现GPT-4.1的信任行为与人类接近，并能根据玩家特征调整信任，信任差异可由刻板印象模型解释。


<details>
  <summary>Details</summary>
Motivation: 构建以人为中心、值得信赖的AI系统的关键在于维持校准的信任，但如何表征AI系统自身的信任水平是一大挑战。

Method: 采用基于迭代上下文学习的创新引出方法，结合行为博弈论中的信任博弈，来刻画AI系统表现出的信任水平。

Result: GPT-4.1在信任博弈中表现出的可信赖性先验与人类相似，且可以区分不同玩家特质，对信任的变化能被基于刻板印象的温暖度和能力感知模型很好预测。

Conclusion: 本研究证明了基于迭代上下文学习的方法能够有效地从行为博弈论中的信任博弈中引出大语言模型（如GPT-4.1）的可信赖性先验，且GPT-4.1的可信赖性先验与人类高度匹配。

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [46] [Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models](https://arxiv.org/abs/2602.00770)
*Siyuan Zhang,Jialian Li,Yichi Zhang,Xiao Yang,Yinpeng Dong,Hang Su*

Main category: cs.CL

TL;DR: 本文通过分析大型语言模型训练中内部状态的动态变化，揭示了推理过程中表征的连续转变及其与推理性能的关系，指出训练提升推理能力关键在于促进表征分布的转变，提供了新的推理机制理解视角。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过生成结果分析推理能力，忽视了模型内部状态的变化，导致对推理能力提升过程的理解不完整。

Method: 通过对不同训练阶段模型的内在状态进行代表性视角的分析和统计测试，结合反事实实验，探究内部表征在推理生成过程中的动态变化。

Result: 发现后期训练对静态初始表征影响有限，推理生成过程中存在显著的表示分布转变，训练通过促进分布转变提升推理性能，内部表征与正确生成高度相关，语义内容而非计算量或参数差异主导这一过程。

Conclusion: 训练过程后期对模型内部初始表征的提升有限，但推理任务中的表征在生成过程中存在显著的连续分布转变，训练增强了这种转变，进而提升推理性能。最终模型的内部表征与生成正确性高度相关，且生成的语义是驱动内在状态变化的关键因素。

Abstract: Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.

</details>


### [47] [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)
*Xuan Ai,Qingqing Yang,Peng Wang,Lei Deng,Lin Zhang,Renhai Chen,Gong Zhang*

Main category: cs.CL

TL;DR: 提出基于层敏感和层间相似的混合重用注意力机制HyLRA，有效突破LLM长上下文推理的计算瓶颈，同时保持高性能和准确率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM长上下文推理中稠密注意力计算复杂度高和KV缓存存储开销大的瓶颈，现有稀疏方法效率与精度难兼顾。

Method: 基于层敏感性和层间相似性，采用离线动态规划，混合使用全关注层和重用前层top-k索引的稀疏机制。

Result: HyLRA推理吞吐量提升6%--46%，精度降低小于1%，优于当前先进的稀疏注意力方法。

Conclusion: HyLRA通过层间稀疏性分析，实现了高效且准确的长上下文推理，显著提升了推理吞吐量且精度损失极小。

Abstract: Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\bf HyLRA} ({\bf Hy}brid {\bf L}ayer {\bf R}euse {\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\%--46\% while maintaining comparable performance (with $<1\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\texttt{/r/unified-cache-management-CF80/}}

</details>


### [48] [Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis](https://arxiv.org/abs/2602.00846)
*Zicheng Kong,Dehua Ma,Zhenbo Xu,Alven Yang,Yiwei Ru,Haoran Wang,Zixuan Zhou,Fuqing Bie,Liuyu Xiang,Huijia Wu,Jian Zhao,Zhaofeng He*

Main category: cs.CL

TL;DR: 本论文提出了首个开源的多模态多维度基于评分标准的奖励模型Omni-RRM，利用自动化数据构建和两阶段训练，显著提升了多模态大语言模型的判断准确率和下游表现，克服了传统奖励模型的多项限制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的性能受限于粗糙的对齐技术，缺乏有效的、结构性明晰且不依赖昂贵人工注释的奖励模型，尤其是针对多模态的多维度评价。

Method: 提出了Omni-RRM模型，利用Omni-Preference大规模自动生成的数据集，通过监督微调和基于奖励的强化学习（GRPO）两阶段训练，生成多维度带有理由的偏好判断，消除了对人工标注的依赖。

Result: Omni-RRM在视频（ShareGPT-V 80.2%）、音频（Audio-HH-RLHF 66.8%）和图像任务上均达到或超过最先进准确率，图像任务准确率较基线模型提升17.7%，并且在文本偏好基准测试中表现优异。

Conclusion: Omni-RRM作为第一款开源的基于评分标准的多模态奖励模型，在多种模态（文本、图像、视频和音频）上实现了高准确率，显著优于现有方法，并提升了下游任务表现。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \textbf{text, image, video, and audio}. At the core of our approach is \textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \emph{reconcile and filter} preferences while providing a modality-aware \emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\% on ShareGPT-V) and audio (66.8\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.

</details>


### [49] [Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation](https://arxiv.org/abs/2602.00848)
*Ziwei Gong,Yanda Chen,Julia Hirschberg,Chen Zhao,He He,Zhou Yu,Kathleen Mckeown*

Main category: cs.CL

TL;DR: 本文提出了事实性控制生成框架，通过合成数据训练模型，使其在生成文本时能按照用户指定的事实性要求平衡准确性与信息量，满足不同应用需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成回答时存在事实性与信息量的权衡，不同应用对二者的需求不同，需要一种方法让用户可以指定事实性约束。

Method: 通过合成数据训练模型，使其在事实性控制生成任务上表现更好，能够遵守事实性约束同时保持较高的信息量。

Result: 合成训练数据显著提升了模型在遵守事实性约束和保持答复信息量方面的能力，验证了FCG框架的有效性。

Conclusion: 本文提出的事实性控制生成(FCG)框架能够有效平衡生成文本的事实性和信息量，满足不同应用对文本质量的需求。

Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.

</details>


### [50] [Unifying Adversarial Robustness and Training Across Text Scoring Models](https://arxiv.org/abs/2602.00857)
*Manveer Singh Tamber,Hosna Oyarhoseini,Jimmy Lin*

Main category: cs.CL

TL;DR: 本文统一研究文本评分模型的对抗鲁棒性，提出多重对抗训练方法并有效提升模型鲁棒性和任务表现，同时缓解奖励模型中的奖励胡乱问题。


<details>
  <summary>Details</summary>
Motivation: 当前针对语言模型的对抗鲁棒性研究分散在不同的应用和攻击方法中，难以揭示模型的共通弱点。

Method: 提出统一的文本评分模型（包括稠密检索器、重排序器和奖励模型）的对抗攻击与训练方法，发展多种对抗训练策略并结合使用以提高鲁棒性，同时改进任务性能。

Result: 证明现有的对抗训练方法普遍缺乏对多样攻击的泛化能力，所提出的多方法结合训练显著提升了模型的对抗鲁棒性和任务表现，并在强化学习中有效缓解了奖励操纵问题。

Conclusion: 通过统一视角和多样化对抗训练策略，能够显著提升文本评分模型的对抗鲁棒性和任务效果，具有实际应用价值。

Abstract: Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.

</details>


### [51] [ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople](https://arxiv.org/abs/2602.00881)
*Shounak Paul,Raghav Dogra,Pawan Goyal,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本文构建印度法律条文和普通用户查询语料库，发现法院判决训练模型难以应对非专业查询，迁移学习能有所提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多用法院判决文本作为输入进行法律条文识别，忽视了实际应用中非专业普通用户查询的非正式性，缺乏对两类数据差异的系统研究。

Method: 构建包含印度500+法律条文的普通用户查询及法院判决的语料库ILISC，使用零样本、少样本推理、检索增强生成和监督微调进行模型训练和测试。

Result: 研发了ILISC语料库，并验证了基于法院判决训练的模型在普通用户查询上的表现不佳，迁移学习可以改善部分情况，同时对查询类别和法律条文频率进行细致分析。

Conclusion: 法院判决文本训练的模型在处理普通用户查询时效果不佳，但通过迁移学习可以在某些情况下提高表现。

Abstract: Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.

</details>


### [52] [EffGen: Enabling Small Language Models as Capable Autonomous Agents](https://arxiv.org/abs/2602.00887)
*Gaurav Srivastava,Aafiya Hussain,Chi Wang,Yingyan Celine Lin,Xuan Wang*

Main category: cs.CL

TL;DR: effGen是一个针对小型语言模型设计的开源代理框架，通过提示优化、任务分解、复杂度路由和统一记忆，提升效率和安全性，优于现有同类系统。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理系统依赖API调用，带来高成本及隐私风险，需一种高效且安全的代理框架适用于小型语言模型。

Method: 提出了四大核心技术：优化提示词压缩上下文、智能任务分解、基于复杂度的路由决策和统一记忆管理，并支持多协议通信，实现系统整体性能提升。

Result: 在13个基准测试中，effGen表现优于LangChain、AutoGen和Smolagents，具有更高成功率、更快执行速度和更低内存消耗，且在不同模型规模上通过优化和路由实现互补提升。

Conclusion: effGen框架在小型语言模型上实现了高效、安全和本地部署，显著提升了工具调用、任务分解、复杂度路由和统一记忆系统的性能，相较现有系统表现更优。

Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.

</details>


### [53] [Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts](https://arxiv.org/abs/2602.00913)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 本研究发现，施瓦茨高阶类别结构虽然有描述性价值，但硬性应用层次结构反而削弱句子级价值检测效果。通过标签阈值优化和轻量级模型集成能带来稳健的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探究在句子级人类价值检测中，施瓦茨（Schwartz）高阶类别结构是否能提供有效的层次信息并提升模型性能。

Method: 在有限计算资源（单个8GB GPU）下，使用直接监督的Transformer模型、硬掩码层次结构的HO到价值管道、以及存在-》HO-》价值级联方法，结合词典、短上下文、主题、标签阈值调整、小型指令调优LLM（≤10B）、QLoRA和简单集成进行比较。

Result: HO类别可以从单句中学习，但硬性层次过滤通常降低最终任务的Macro-F1分数；标签阈值调整和小型Transformer集成显著提升性能，小型LLM表现落后但在跨模型集成中能补充错误。

Conclusion: 高阶（HO）结构对描述性有用，但通过硬限制执行层次结构对句子级价值检测不利；稳健的提升来自于标签阈值调整和轻量级集成方法。

Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\rightarrow$HO$\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.

</details>


### [54] [A Baseline Multimodal Approach to Emotion Recognition in Conversations](https://arxiv.org/abs/2602.00914)
*Víctor Yeste,Rodrigo Rivas-Arévalo*

Main category: cs.CL

TL;DR: 本文构建了一个基于文本Transformer和自监督语音模型的多模态情感识别基线，验证了多模态融合较单模态更有效，供未来研究对比使用。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个易于参考的多模态情感识别基线，实现透明且便于未来严谨比较。

Method: 结合基于Transformer的文本分类器和自监督语音表示模型，使用简单的后期融合集成方法。

Result: 在有限训练条件下，多模态融合模型表现超过单模态模型。

Conclusion: 本工作未提出新颖方法，而是记录了一个轻量级且易实现的多模态情感识别基线，为后续研究提供参考。

Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.

</details>


### [55] [Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs](https://arxiv.org/abs/2602.00945)
*Anusa Saha,Tanmay Joshi,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 本文发现LLMs默认语言由少量语言神经元控制，提出Neural FOXP2方法，通过定位语言神经元和施加稀疏激活偏移，实现语言默认性的可控切换，将英语主导转换为印地语或西班牙语，提升多语言模型的平衡性。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大型语言模型（LLMs）虽然经过多语种训练，但其默认语言多为英语，其他语言被系统性抑制，缺乏对非英语语言的有效激活与控制。

Method: 提出Neural FOXP2方法，通过三阶段机制控制特定语言神经元：首先定位各层中特定语言特征的活跃神经元；其次通过谱低秩分析提取语言转换的主导方向；最后对特定层施加有向稀疏激活偏移，实现语言偏好的可控切换。

Result: 成功将指定非英语语言（印地语或西班牙语）设为模型默认语言，实现对模型语言默认性的有效干预和可控切换，验证了语言神经元和控制回路的存在及其可操控性。

Conclusion: 语言的默认性由稀疏低秩的语言神经元控制回路决定，Neural FOXP2通过定位和定向操控这些神经元，能够安全有效地实现模型语言默认性的转换，促进多语言模型更公平、多样的语言表现。

Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.
  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.

</details>


### [56] [Verification Required: The Impact of Information Credibility on AI Persuasion](https://arxiv.org/abs/2602.00970)
*Saaduddin Mahmud,Eugene Bagdasarian,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出MixTalk游戏模拟信息可信度，评估LLM在战略沟通中的表现，且通过TOPD方法提升了接收方对说服的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注纯廉价信号或完全可验证信息，缺乏对信息具有概率可信度的真实场景建模；需求建立对战略沟通下信息可信度的系统理解。

Method: 设计了MixTalk战略沟通游戏，结合可验证与不可验证的信息；进行大规模多模型锦标赛评测；引入TOPD方法通过离线蒸馏锦标赛最优策略用于推理时的上下文部署。

Result: 通过MixTalk展示了多语言模型在信息可信度推理方面的性能差异；TOPD显著增强了接收方模型抵抗说服的能力。

Conclusion: 本文提出的MixTalk框架成功模拟了信息可信度对战略沟通的影响，展现了现有大型语言模型在处理复杂信息可信度时的优劣。通过TOPD方法进一步提升了接收方对说服策略的鲁棒性。

Abstract: Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.

</details>


### [57] [Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals](https://arxiv.org/abs/2602.00977)
*Pengyue Yang,Jiawen Wen,Haolin Jin,Linghan Huang,Huaming Chen,Ling Chen*

Main category: cs.CL

TL;DR: 本文提出了结构置信度方法，通过分析模型隐藏状态结构特征，实现单次前向传播的鲁棒置信度估计，显著提升大语言模型在多领域任务中的错误预测能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险领域应用时，现有的置信度估计方法在分布转移、领域专用文本及计算资源有限情况下表现不佳。

Method: 提出了结构置信度框架，通过分析模型最后一层隐藏状态的多尺度结构信号（包括谱特征、局部变化和全局形状描述符）来提升输出正确性预测。

Result: 在四个跨领域基准测试中，结构置信度框架在AUROC和AUPR指标上优于传统基线方法，且只需单次确定性前向传播，效率更高。

Conclusion: 结构置信度为资源受限和社会影响大的大语言模型应用提供了一种高效、鲁棒的后置置信度估计方案，克服了采样一致性方法的多次生成和辅助模型需求。

Abstract: Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.

</details>


### [58] [MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA](https://arxiv.org/abs/2602.00981)
*Yutong Song,Shiva Shrestha,Chenhan Lyu,Elahe Khatibi,Pengfei Zhang,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.CL

TL;DR: 提出了基于医疗知识图谱和大型语言模型的自动语音识别错误纠正框架MedSpeak，有效提升了医疗问答系统的术语识别和整体表现。


<details>
  <summary>Details</summary>
Motivation: 现有依赖自动语音识别的医疗问答系统在识别医疗术语时准确率较低，影响了回答的准确性。

Method: 结合医疗知识图谱中的语义关系和语音信息，以及大型语言模型的推理能力，提出了一种基于知识图谱辅助的自动语音识别错误纠正框架。

Result: 在多个基准测试中，MedSpeak显著提升了医疗术语的识别准确率和问答系统的性能。

Conclusion: MedSpeak框架显著提升了医疗术语识别的准确率和医疗问答系统的整体性能，成为领域内的先进解决方案。

Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.

</details>


### [59] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

TL;DR: 本文提出DISPO算法，通过精准调节权重剪裁策略，平衡探索与蒸馏，改善了大语言模型在数学推理任务中的训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有PPO类方法训练稳定但收敛慢，REINFORCE类方法学习效率高但因权重剪裁导致性能不稳定，需一种既高效又稳定的训练策略。

Method: 提出了一种新的REINFORCE风格算法DISPO，通过对正确和错误回答的权重分别进行上剪裁和下剪裁，形成四种可控的策略更新机制，并通过消融实验验证了各机制对训练的影响。

Result: DISPO在AIME'24数学竞赛测试中取得61.04%的准确率，显著超过CISPO的55.42%和DAPO的50.21%，并在多种基准和模型上都有类似提升。

Conclusion: DISPO算法通过独立调节重要性采样权重的上限和下限，有效平衡了探索与蒸馏，避免了策略更新过程中的灾难性崩溃，显著提升了大语言模型在数学推理任务中的性能。

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [60] [Sparse Reward Subsystem in Large Language Models](https://arxiv.org/abs/2602.00986)
*Guowei Xu,Mert Yuksekgonul,James Zou*

Main category: cs.CL

TL;DR: 本文发现并验证了大规模语言模型中隐藏的价值神经元及其对推理的关键作用，并识别了编码奖励预测误差的多巴胺神经元，揭示了LLM内部的奖励机制。


<details>
  <summary>Details</summary>
Motivation: 类比人类大脑中的生物奖励系统，探究LLM内部是否存在类似的奖励神经元及其对模型推理能力的影响。

Method: 通过对LLM隐藏状态中的奖励子系统进行干预实验，检测不同数据集、模型规模与结构上的价值神经元表现，并分析预测奖励与实际奖励不一致时神经元的激活情况。

Result: 发现LLM中存在稀疏的价值神经元，能够准确反映状态价值并在不同模型及数据集间具有鲁棒性与迁移性；同时识别出编码奖励预测误差的多巴胺样神经元。

Conclusion: 本文通过研究大规模语言模型（LLMs）中的稀疏奖励子系统，确认了其内部的价值神经元对推理的重要性，并发现奖赏预测误差由类似多巴胺神经元的神经元编码。

Abstract: In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.

</details>


### [61] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

TL;DR: DeALOG提出一种基于多智能体自然语言日志协调的多模态问答框架，实现了高效协同和鲁棒性的复杂跨模态问答。


<details>
  <summary>Details</summary>
Motivation: 复杂问答需要融合文本、表格和图像等多种信息源，且需支持专门化处理和解释性，现有方法难以高效协调多模态信息。

Method: 引入包含表格、上下文、视觉、总结和验证等专门代理的去中心化多智能体框架，这些代理通过共享的自然语言日志进行交互，支持错误检测和验证。

Result: 在多个数据集上（FinQA、TAT-QA等）表现出竞争力，分析确认共享日志、代理专门化及验证机制显著提升了回答准确率。

Conclusion: DeALOG框架通过多智能体模块的自然语言日志共享，显著提升了多模态复杂问答的准确性和鲁棒性，实现了模块化可扩展的多源信息集成。

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [62] [Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning](https://arxiv.org/abs/2602.00998)
*Zhikun Xu,Xiaodong Yu,Ben Zhou,Jiang Liu,Jialian Wu,Ze Wang,Ximeng Sun,Hao Chen,Zicheng Liu*

Main category: cs.CL

TL;DR: 针对大型语言模型误用引理问题，本文提出RULES方法，通过双部分输出和强化学习提升引理适用性判断，验证了其在多场景和鲁棒性测试中的有效性和优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学基准测试中表现强劲，但常常错误应用引理，未经验证假设便导入结论，导致推理不准确。为此，需要一种方法有效判断引理的适用性。

Method: 将引理判断任务形式化为结构化预测问题，设计双部分输出结构，并结合强化学习和部分感知损失屏蔽技术进行训练，以准确识别错误部分并赋予相应惩罚。

Result: 在多样化自然语言和形式证明语料上训练和评估，RULES在保持赛内一致提升，同时对应用破坏性扰动有更大改善，并在端到端任务中与基线模型持平或略有提升。消融实验表明雙部分输出和部分感知强化学习是实现鲁棒性的关键。

Conclusion: 本文提出的RULES方法在判断引理的适用性方面表现优越，提升了模型对引理前提检查和结论有效性的判断能力，在不同任务和扰动环境下均显示出鲁棒性和性能提升。

Abstract: Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.

</details>


### [63] [Distilling Token-Trained Models into Byte-Level Models](https://arxiv.org/abs/2602.01007)
*Zishuo Bao,Jiaqi Leng,Junxiong Wang,Bowen Peng,Yucheng Lu*

Main category: cs.CL

TL;DR: 提出一种两阶段蒸馏方法，将标记训练的大语言模型高效转换为字节语言模型，实现低成本且性能相当的字节级生成。


<details>
  <summary>Details</summary>
Motivation: 现有的字节语言模型（BLMs）训练成本极高，需要在数万亿字节上从头训练。

Method: 提出了一种两阶段的高效蒸馏方法：一是逐步知识蒸馏，将字节级表示与基于标记训练的大模型的嵌入对齐；二是字节级有监督微调，实现完全字节空间的端到端生成。

Result: 在多个模型家族（Llama、Qwen、OLMo）上验证，蒸馏后的BLMs在仅用约125亿字节数据的情况下，能够保留大部分教师模型的性能。

Conclusion: 该方法显著降低了转换标记训练大模型为字节语言模型的计算成本，同时保持了模型性能。

Abstract: Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.

</details>


### [64] [Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident](https://arxiv.org/abs/2602.01015)
*Conrad Borchers,Jill-Jênn Vie,Roger Azevedo*

Main category: cs.CL

TL;DR: 本文评估大型语言模型在模拟新手化学问题解决中的表现，发现其推理过于连贯且表现被高估，揭示了LLM训练数据和认知机制的限制，建议未来系统设计应更真实地反映初学者的学习特征。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在AI辅导系统中日益普及，但其是否能够真实模拟初学者的推理和元认知判断尚不明确，现有评估侧重于问题解决的准确性，忽视了人类学习中碎片化和不完美的推理特征。

Method: 利用630条多步骤化学辅导中学生的思维口述，结合学生提示使用、尝试次数和问题情境，比较LLM生成的推理与人类学习者在最小和扩展上下文提示下的推理差异，评估模型预测学习者单步成功的能力。

Result: GPT-4.1生成流畅且语境适当的推理，但表现出过度连贯、啰嗦且变异性低于人类思维口述，且在增加问题背景信息时这一现象加剧，模型普遍高估了学习者的表现。

Conclusion: LLM在模拟初学者学习过程时存在认知和知识层面限制，主要源于训练数据偏向专家解决方案且缺乏情感表达和工作记忆限制，这对设计更真实支持初学者学习和自我调节的自适应系统提出了指导意义。

Abstract: Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.

</details>


### [65] [Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations](https://arxiv.org/abs/2602.01030)
*Sheng-Lun Wei,Yu-Ling Liao,Yen-Hua Chang,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文构建BiasInEar语音偏见评测数据集，系统评估了多语言大型语言模型在语音输入下的偏见和鲁棒性，发现模型对语言和选项顺序敏感，提出了统一的公平性评估框架。


<details>
  <summary>Details</summary>
Motivation: 系统性研究多语言大型语言模型中的语音偏见，弥补文本评估与语音评估之间的空白。

Method: 构建并发布了BiasInEar数据集，该数据集基于Global MMLU Lite，包含英语、中文和韩语的性别和口音平衡的语音数据，总计70.8小时，11200个问题，使用准确率、熵值、APES和Fleiss' κ四个指标评估九个模型在语言、口音、性别和选项顺序等扰动下的表现。

Result: 发现多语言大型语言模型对人口统计学因素表现出相对鲁棒性，但对语言和选项顺序高度敏感，架构设计和推理策略显著影响模型的语言间鲁棒性。

Conclusion: 多语言大型语言模型（MLLMs）在处理语音输入时对语言和选项顺序敏感，表明语音可能放大已有的结构性偏见，而对性别等人口统计学因素较为鲁棒。

Abstract: This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.

</details>


### [66] [Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents](https://arxiv.org/abs/2602.01063)
*Bin Han,Deuksin Kwon,Jonathan Gratch*

Main category: cs.CL

TL;DR: 本研究发现大语言模型根据不同对话情境调整人格表达，表现出情境敏感的适应性，类似人类行为，而非固定不变的性格。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如何在不同社交情境中根据相同人格提示展现不同的行为和情感表达。

Method: 通过在四种对话情境（破冰、谈判、群体决策和共情任务）下，观察相同人格提示的语言和行为表现差异。

Result: 实验结果表明情境线索系统地影响人格表达和情感语气，说明人格特质的表达依赖于社交和情感需求。

Conclusion: 相同的人格提示在不同的对话环境中会导致语言、行为和情感表现的差异，体现了大语言模型的情境敏感性而非固定人格表达。

Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

</details>


### [67] [Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs](https://arxiv.org/abs/2602.01064)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuo Yang,Chu Yuan Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: 本文通过提出知识净化及多种净化方法，成功解决了多教师蒸馏的知识冲突与高资源消耗问题，提升了模型性能和应用效率。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法在利用多教师模型时存在知识冲突和资源消耗大的挑战，促使作者探索更高效统一的知识转移策略。

Method: 提出了知识净化的概念，并设计了五种不同的净化方法，其中基于路由器的方法表现出良好的泛化能力。

Result: 实验表明，知识净化方法不仅提升了蒸馏模型性能，还有效缓解了知识冲突，特别是路由器方法具有较强泛化能力。

Conclusion: 本文提出的知识净化方法有效解决了多教师知识蒸馏中的知识冲突问题，提升了蒸馏模型的性能和资源效率。

Abstract: Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.

</details>


### [68] [From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization](https://arxiv.org/abs/2602.01068)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 针对大语言模型在垂直领域翻译的不足，本文提出基于字幕翻译的定制方法和优化策略，显著提升翻译表达力与质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在通用机器翻译能力强，但在特定垂直领域翻译中表现有限，需提升定制化翻译效果。

Method: 构建多向字幕平行语料库，提出自适应局部偏好优化(ALPO)方法，进行细粒度偏好对齐训练。

Result: ALPO方法在多维度翻译质量评估中表现突出，验证了LLM作为奖励模型和评价者的可靠性。

Conclusion: 本研究成功构建了满足领域定制需求的翻译大语言模型，特别是在视觉媒体字幕翻译领域表现优异。

Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.

</details>


### [69] [What If We Allocate Test-Time Compute Adaptively?](https://arxiv.org/abs/2602.01070)
*Ahsan Bilal,Ahmed Mohsin,Muhammad Umer,Ali Subhan,Hassan Rizwan,Ayesha Mohsin,Dean Hougen*

Main category: cs.CL

TL;DR: 本工作提出一种基于过程奖励模型的动态推理框架，通过多次迭代和评分引导推理进程，实现计算资源的自适应分配，在多个复杂推理任务中显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展方法计算分配均匀且采样固定，且仅在结果重排时进行验证，导致计算资源利用效率低下。

Method: 通过多次推理迭代，利用PRM在步骤级和迭代级对推理轨迹进行评分和引导，实现推理过程中的动态计算分配与轨迹选择。

Result: 该方法在MATH-500、AIME24、AMO-Bench等数据集上取得显著性能提升，计算资源集中用于高效推理路径，提高了计算利用率和推理效率。

Conclusion: 提出的基于过程奖励模型（PRM）的动态推理框架在多个基准测试中显著优于传统的统一计算分配方法，尤其在复杂任务上表现突出。

Abstract: Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.

</details>


### [70] [Logic-Oriented Retriever Enhancement via Contrastive Learning](https://arxiv.org/abs/2602.01116)
*Wenxuan Zhang,Yuan-Hao Jiang,Changyong Qi,Rui Jia,Yonghe Wu*

Main category: cs.CL

TL;DR: 针对大型语言模型知识密集任务中检索效果欠佳的问题，LORE通过细粒度对比学习激活逻辑分析能力，无需外部监督资源，显著提升了检索和生成表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理涉及复杂逻辑关系的知识密集型任务时表现欠佳，检索器依赖浅表相似度导致泛化能力不足，而模型潜藏的逻辑分析能力未被充分利用。

Method: 提出LORE（逻辑导向检索增强）方法，利用细粒度对比学习指导嵌入向符合逻辑结构的证据聚集，而非浅表相似度，方法无需额外监督、资源或预检索分析，且兼容现有索引架构。

Result: LORE在多个数据集上持续提升了检索效果和下游生成质量，同时保持了计算效率。

Conclusion: LORE方法通过细粒度对比学习激活LLM中的逻辑分析能力，有效提升了基于逻辑结构的检索性能，改善了知识密集型任务中的表现。

Abstract: Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.

</details>


### [71] [Tendem: A Hybrid AI+Human Platform](https://arxiv.org/abs/2602.01119)
*Konstantin Chernyshev,Ekaterina Artemova,Viacheslav Zhukov,Maksim Nerush,Mariia Fedorova,Iryna Repik,Olga Shapovalova,Aleksey Sukhorosov,Vladimir Dobrovolskii,Natalia Mikhailova,Sergei Tilga*

Main category: cs.CL

TL;DR: Tendem系统结合AI和人工，实现了质量更高、速度更快且成本合理的任务完成，展现了卓越的自动化和人工协作能力。


<details>
  <summary>Details</summary>
Motivation: 提升工作效率和质量，同时保持合理的运营成本，通过结合AI与人工优势来优化任务执行过程。

Method: 通过在94个实际任务上进行内部评估，将Tendem与纯AI代理和纯人工工作流程进行对比，并利用第三方基准测试评估系统的自动化AI代理性能。

Result: Tendem在质量和速度上均优于纯人工或纯AI方案，且运营成本与纯人工相当；其AI代理在自动执行任务时表现接近最先进水平。

Conclusion: Tendem系统通过结合AI自动化和人工专家的审核，实现了高质量、快速且成本效益合理的工作交付。

Abstract: Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.

</details>


### [72] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的多模态时间点过程模型，通过序列压缩和两阶段训练，有效解决长序列问题，实现了更高准确率和生成文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有的时间点过程（TPP）模型在处理多模态内容和事件动态推理时存在限制，且多模态数据增加序列长度，影响基于注意力机制模型的长文本生成能力。

Method: 提出一种基于大语言模型（LLM）的时间点过程框架，将视觉模态引入，并通过基于时间相似度的自适应序列压缩机制解决长序列问题，采用压缩序列预训练和有监督微调相结合的两阶段训练策略。

Result: 在多个实验中，尤其是DanmakuTPP-QA基准测试中，该方法在预测准确率和生成文本质量上均优于现有最先进方法。

Conclusion: 该研究成功扩展了TPP模型的多模态能力与长文本生成能力，有效提升了对复杂事件序列的分析和预测性能。

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [73] [Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation](https://arxiv.org/abs/2602.01132)
*Abhilekh Borah,Shubhra Ghosh,Kedar Joshi,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 本文构建了逻辑混淆测试集LogiQAte，系统评估了大型语言模型在逻辑等价但表述混淆的问题上的表现，发现其推理能力明显下降，暴露了深层理解的不足。


<details>
  <summary>Details</summary>
Motivation: 发现大型语言模型虽然在标准形式问题上表现良好，但对等价但表面形式被混淆的逻辑问题处理能力差，揭示了它们理解的局限性。因此需要研究和诊断模型的深层逻辑理解能力。

Method: 提出了一个结构保持的逻辑混淆框架Logifus，并基于此构建了LogiQAte诊断基准，包含四类推理任务，测试六个顶尖模型在混淆任务上的性能。

Result: 通过测试六个先进模型，发现逻辑混淆显著降低模型的零样本性能，GPT-4o平均下降47%，GPT-5下降27%，o4-mini下降22%。

Conclusion: 当前大型语言模型在处理逻辑等价而非直接表达的问题时表现较差，表现出对浅层语义的依赖，缺乏对深层逻辑结构的理解。

Abstract: Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.

</details>


### [74] [Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models](https://arxiv.org/abs/2602.01161)
*Reem I. Masoud,Chen Feng,Shunta Asano,Saied Alshahrani,Philip Colin Treleaven,Miguel R. D. Rodrigues*

Main category: cs.CL

TL;DR: 本研究探讨了微调数据语言特性对大规模语言模型文化适应的影响，发现词汇相关特性是提升跨模型文化表现的关键因素。


<details>
  <summary>Details</summary>
Motivation: 全球大规模语言模型的文化适应性存在问题，然而用于文化适应的微调数据的语言特性尚不清楚。

Method: 以数据集为中心视角，计算阿拉伯语、中文和日语微调数据集的语言、语义和结构指标，并对每种语言数据集进行主成分分析。同时分别微调三种主流语言模型，并通过多语言文化知识基准进行评估。

Result: 发现主成分分析得到的组件与模型性能相关，但其影响依赖于具体模型。词汇相关的成分（PC3）对模型表现影响稳定且一致，而语义或多样性成分（PC1-PC2）影响不确定甚至有害。

Conclusion: 微调数据的语言特性对文化适应性能有显著影响，且不同语言模型对这些特性的响应不同。强化词汇多样性比强调语义或多样性极端更有助于提升跨模型的文化表现。

Abstract: The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.

</details>


### [75] [Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages](https://arxiv.org/abs/2602.01162)
*Nipuna Abeykoon,Ashen Weerathunga,Pubudu Wijesinghe,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 该论文提出一个基于语言类型学的框架，改善大语言模型对低资源语言的翻译结构性偏差，效果显著且无需平行训练数据。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型主要在高资源语言上训练，存在对主要语言类型模式的系统性偏向，导致翻译到类型学差异大的低资源语言时结构不符合。

Method: 提出一个利用语言类型学的框架，包括统一元语言框架（UMF）和计算引擎，通过语言歧义消解和类型学合规评分无需平行训练数据或模型再训练改善翻译质量。

Result: 在9对语言的评估中，干预率与语言类型学距离显著相关。对341个涉及不同形态和句法现象的英文句子，框架在保守处理语言、形态密集语言和结构化语言上的干预精准率分别为48.16%、28.15%和86.26%。

Conclusion: 该框架无需平行数据，适用于能生成多候选输出的大型语言模型，对提升低资源语言翻译质量具有实用价值。

Abstract: Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.

</details>


### [76] [PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues](https://arxiv.org/abs/2602.01169)
*Shahem Sultan,Shahem Fadi,Yousef Melhim,Ibrahim Alsarraj,Besher Hassan*

Main category: cs.CL

TL;DR: 本文提出PedagoSense系统，结合策略分类和大语言模型，提升师生对话中教学策略的检测与推荐，推动适应性教育技术发展。


<details>
  <summary>Details</summary>
Motivation: 提高对话式学习中互动质量，通过准确检测和推荐教学策略优化师生对话。

Method: 采用两阶段策略分类器结合大语言模型生成，先二分类检测教学策略，再细粒度分类具体策略，同时推荐适当策略并生成响应。

Result: 在人类标注的师生对话数据上表现出高效的教学策略检测能力，数据增强进一步提升性能，细粒度分类仍有挑战。

Conclusion: PedagoSense有效提升了对教学策略的检测和推荐能力，促进了更适应性的教育技术发展。

Abstract: This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.

</details>


### [77] [EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech](https://arxiv.org/abs/2602.01170)
*Besher Hassan,Ibrahim Alsarraj,Musaab Hasan,Yousef Melhim,Shahem Fadi,Shahem Sultan*

Main category: cs.CL

TL;DR: EmoAra系统实现了英到阿语的跨语言语音交流，准确识别情感并保持其在目标语中的传递，提升了银行服务交流质量。


<details>
  <summary>Details</summary>
Motivation: 针对银行客户服务中情感信息影响服务质量的需求，提出一个跨语言语音交流中保留情感的解决方案。

Method: 系统整合了基于CNN的情感识别器、Whisper语音识别模型、微调的MarianMT英阿翻译模型和MMS-TTS-Ara阿拉伯语语音合成模型。

Result: 情感分类F1达94%，翻译指标BLEU为56，BERTScore F1为88.7%，以及81%的人工评估平均分。

Conclusion: 该研究成功开发了EmoAra，一个能够在跨语言语音交流中保持情感的端到端系统，显著提升了银行客户服务的质量。

Abstract: This work presents EmoAra, an end-to-end emotion-preserving pipeline for cross-lingual spoken communication, motivated by banking customer service where emotional context affects service quality. EmoAra integrates Speech Emotion Recognition, Automatic Speech Recognition, Machine Translation, and Text-to-Speech to process English speech and deliver an Arabic spoken output while retaining emotional nuance. The system uses a CNN-based emotion classifier, Whisper for English transcription, a fine-tuned MarianMT model for English-to-Arabic translation, and MMS-TTS-Ara for Arabic speech synthesis. Experiments report an F1-score of 94% for emotion classification, translation performance of BLEU 56 and BERTScore F1 88.7%, and an average human evaluation score of 81% on banking-domain translations. The implementation and resources are available at the accompanying GitHub repository.

</details>


### [78] [Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation](https://arxiv.org/abs/2602.01193)
*Shashini Nilukshi,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本文综述了视觉词义消歧领域的发展，强调利用视觉信息弥补传统文本方法不足，通过CLIP和大语言模型提升词义判定性能，未来将聚焦多模态融合和多语种适配。


<details>
  <summary>Details</summary>
Motivation: 传统的词义消歧仅依赖文本和词汇资源，难以解决视觉语言任务中的词义歧义问题，因此VWSD引入视觉线索，以在最小文本输入下准确识别歧义词义。

Method: 该综述跟踪了2016至2025年期间VWSD的发展，分析了早期多模态融合方法、基于特征、图结构和对比嵌入技术的演进，重点讨论了CLIP对比模型、扩散式文本到图像生成以及大型语言模型的应用，涵盖提示工程、模型微调及多语言适配策略。

Result: 研究显示，基于CLIP微调模型和融合大型语言模型的VWSD系统，在平均倒数排名(MRR)指标上较零-shot基线提升6-8%，表现更加稳定和准确。同时也指出存在上下文限制、模型偏好常见词义、多语种数据缺乏及评估体系不完善的问题。

Conclusion: VWSD作为传统词义消歧的多模态扩展，结合视觉信息显著提升了在视觉语言任务中歧义词意义判定的准确性。基于CLIP微调模型和大语言模型的系统表现优异，但仍需解决上下文限制、模型偏见、多语种资源匮乏及评估方法不足等挑战。未来方向是融合CLIP对齐、扩散生成和大语言模型推理，实现强大且具上下文感知和多语言能力的词义消歧系统。

Abstract: This paper offers a mini review of Visual Word Sense Disambiguation (VWSD), which is a multimodal extension of traditional Word Sense Disambiguation (WSD). VWSD helps tackle lexical ambiguity in vision-language tasks. While conventional WSD depends only on text and lexical resources, VWSD uses visual cues to find the right meaning of ambiguous words with minimal text input. The review looks at developments from early multimodal fusion methods to new frameworks that use contrastive models like CLIP, diffusion-based text-to-image generation, and large language model (LLM) support. Studies from 2016 to 2025 are examined to show the growth of VWSD through feature-based, graph-based, and contrastive embedding techniques. It focuses on prompt engineering, fine-tuning, and adapting to multiple languages. Quantitative results show that CLIP-based fine-tuned models and LLM-enhanced VWSD systems consistently perform better than zero-shot baselines, achieving gains of up to 6-8\% in Mean Reciprocal Rank (MRR). However, challenges still exist, such as limitations in context, model bias toward common meanings, a lack of multilingual datasets, and the need for better evaluation frameworks. The analysis highlights the growing overlap of CLIP alignment, diffusion generation, and LLM reasoning as the future path for strong, context-aware, and multilingual disambiguation systems.

</details>


### [79] [Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse](https://arxiv.org/abs/2602.01203)
*Zizhuo Fu,Wenxuan Zeng,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: 本文发现了注意力沉没现象中的混合专家结构，针对头塌陷提出了负载均衡训练策略，显著改善了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的注意力机制存在对首个Token过度关注（attention sink）的问题和注意力头塌陷现象，但缺乏对不同注意力机制间关系的综合分析。

Method: 通过理论分析和实证研究，证明了Vanilla Attention与Sink Attention中的注意力沉没构成了内部的混合专家机制，进而提出了含辅助负载均衡损失的沉没感知训练算法。

Result: 所提方法有效实现了注意力头负载均衡，提升了Vanilla Attention、Sink Attention和Gated Attention下的模型表现。

Conclusion: 本文揭示了注意力沉没现象背后的混合专家机制，解释了注意力头塌陷的问题，并提出了基于负载均衡的训练方法，有效缓解了头塌陷，提升模型性能。

Abstract: Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.

</details>


### [80] [ASTER: Agentic Scaling with Tool-integrated Extended Reasoning](https://arxiv.org/abs/2602.01204)
*Xuqin Zhang,Quan He,Zhenrui Zheng,Zongzhang Zhang,Xu He,Dong Li*

Main category: cs.CL

TL;DR: 本文针对大语言模型使用工具集成推理时的交互崩溃问题，提出一种基于交互密集冷启动轨迹的强化学习框架ASTER，显著提升了多轮工具调用能力和数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大语言模型中应用工具集成推理时，存在模型无法持续多轮工具调用而退化为简单内部推理的问题（交互崩溃），亟需解决。

Method: 提出了ASTER框架，通过优先采用交互密集的冷启动轨迹，建立强有力的行为先验，促进扩展强化学习训练中的高效探索。

Result: 在AIME 2025等数学基准测试中，ASTER-4B模型达到90.0%的成绩，超过包括DeepSeek-V3.2-Exp在内的领先开源模型，验证了方法的有效性。

Conclusion: 针对强化学习在集成工具推理中遇到的交互崩溃问题，提出了一种基于密集交互轨迹的冷启动策略，显著提升了模型多轮工具使用能力和下游任务表现。

Abstract: Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.

</details>


### [81] [Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling](https://arxiv.org/abs/2602.01208)
*Kai Zhang,Jiayi Liao,Chengpeng Li,Ziyuan Xie,Sihang Li,Xiang Wang*

Main category: cs.CL

TL;DR: 本文提出了Chronos，一种基于时间序列的推理质量评分器，通过加权投票显著提升大语言模型的推理表现，且计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 现有的推理结果融合方法如多数投票和启发式token级评分对轨迹质量波动和局部逻辑错误敏感，未有效区分不同推理轨迹的质量。

Method: Chronos将推理轨迹视为时间序列，学习捕捉token概率的轨迹特征，为轨迹赋予质量评分，并采用加权投票机制提升决策效果。

Result: 在多个基准测试（包括同域和异域）中，Chronos带来了显著性能提升，特别是在HMMT25测试中，使用Qwen3-4B-Thinking-2507模型，Chronos@128相对于Pass@1提升了34.21%，相对于Maj@128提升了22.70%。

Conclusion: Chronos作为一种轻量级的时间序列推理评分器，显著提升了大语言模型的推理性能，表现出较高的准确率和较低的计算开销。

Abstract: Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\% over Pass@1 and 22.70\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.

</details>


### [82] [Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority](https://arxiv.org/abs/2602.01227)
*Zhanming Shen,Zeyu Qin,Jiaqi Hu,Wentao Ye,Hao Chen,Xiaomeng Hu,Haokai Xu,Gang Chen,Yi R. Fung,Haobo Wang*

Main category: cs.CL

TL;DR: 本文提出Token Priority机制作为细粒度生成与粗监督信号之间的桥梁，重新定义监督微调为分布重塑过程，系统分析两大范式并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前细粒度自回归生成与粗糙或均匀监督信号之间存在粒度不匹配，制约向真实人类效用的过渡，亟需桥接机制。

Method: 通过形式化监督微调为分布重塑过程，分析现有成果并归纳为正优先（用于噪声过滤）和符号优先（用于有害模式遗忘）两个范式。

Result: 本文统一视角下复盘最新进展及局限，明确挑战并提出未来研究方向。

Conclusion: 本文提出Token Priority作为监督微调的核心桥梁，认为其能实现数据与理想对齐流形的精确分布重塑，促进从经验数据拟合到真实人类效用的转变。

Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.

</details>


### [83] [Inferential Question Answering](https://arxiv.org/abs/2602.01239)
*Jamshid Mozafari,Hamed Zamani,Guido Zuccon,Adam Jatowt*

Main category: cs.CL

TL;DR: 提出了一种新的推理问答任务和相应数据集，揭示了现有问答系统在推理能力上的不足，推动了问答研究从直接抽取向基于推理的理解转变。


<details>
  <summary>Details</summary>
Motivation: 传统问答系统多数假设答案可直接从文本中抽取或生成，无法应对需要基于间接线索进行推理得出答案的问题，因此提出推理问答任务以推动系统理解并推断隐含答案。

Method: 构建了QUIT数据集，包含7,401个需要推理的问题及2.4百万个高相关度的提示段落，并结合大型语言模型的可答性评估与人工验证对段落相关性进行标签划分。系统评估了现有的检索器、重排序器和基于LLM的阅读器在推理问答任务上的表现。

Result: 实验结果表明，传统问答方法在推理问答任务中表现差强人意，模型难以从线索性文本中推断答案，验证了推理问答任务的挑战性和现有系统的不足。

Conclusion: 现有的问答系统在处理需要推理的问题时表现不佳，当前的检索、重排序和微调方法效果有限，即使是面向推理的大型语言模型也未能显著超越小型通用模型，表明现有问答流程尚未准备好应对基于推理的任务。

Abstract: Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.

</details>


### [84] [Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection](https://arxiv.org/abs/2602.01240)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 本论文针对LLM生成文本检测代理模型选择问题，提出基于原型的DetectRouter框架，实现了性能的稳健提升。


<details>
  <summary>Details</summary>
Motivation: 当前方法固定使用单一代理模型进行检测，忽略了代理与源模型不匹配导致性能差异的问题。

Method: 采用基于原型的两阶段训练方法：第一阶段构建白盒模型的判别性原型；第二阶段通过对齐几何距离和检测分数，推广至黑盒模型。

Result: 在EvoBench和MAGE基准上，DetectRouter在多种检测指标和模型类别中均表现出稳定且显著的性能提升。

Conclusion: 本研究提出了DetectRouter框架，通过选择最合适的代理模型来提升LLM生成文本的检测效果，显著改进了检测性能。

Abstract: Zero-shot methods detect LLM-generated text by computing statistical signatures using a surrogate model. Existing approaches typically employ a fixed surrogate for all inputs regardless of the unknown source. We systematically examine this design and find that detection performance varies substantially depending on surrogate-source alignment. We observe that while no single surrogate achieves optimal performance universally, a well-matched surrogate typically exists within a diverse pool for any given input. This finding transforms robust detection into a routing problem: selecting the most appropriate surrogate for each input. We propose DetectRouter, a prototype-based framework that learns text-detector affinity through two-stage training. The first stage constructs discriminative prototypes from white-box models; the second generalizes to black-box sources by aligning geometric distances with observed detection scores. Experiments on EvoBench and MAGE benchmarks demonstrate consistent improvements across multiple detection criteria and model families.

</details>


### [85] [Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments](https://arxiv.org/abs/2602.01244)
*Siwei Wu,Yizhi Li,Yuyang Song,Wei Zhang,Yang Wang,Riza Batista-Navarro,Xian Yang,Mingjie Tang,Bryan Dai,Jian Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: 为训练终端任务代理模型，TerminalTraj提出一种可扩展流水线，自动构建多样Docker环境并生成经过验证的任务轨迹，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 训练面向终端任务的代理模型需高质量、跨领域的长时交互轨迹，但构建此类数据需解决环境执行性和结果验证的难题。

Method: 提出TerminalTraj流水线，通过筛选高质量仓库构建Docker环境，生成Docker对齐任务实例，并综合执行验证代码合成代理轨迹。

Result: 构建了32K Docker镜像，生成50,733个验证终端轨迹，模型在TerminalBench各版本测试中性能提升20%及10%，TerminalTraj-32B模型表现优异。

Conclusion: TerminalTraj有效解决了执行性和可验证性挑战，构建了大规模、高质量的终端任务轨迹数据，显著提升了模型在TerminalBench上的性能表现。

Abstract: Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \textbf{\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \textbf{\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, \textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.

</details>


### [86] [PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian](https://arxiv.org/abs/2602.01246)
*Jamshid Mozafari,Seyed Parsa Mousavinasab,Adam Jatowt*

Main category: cs.CL

TL;DR: PARSE是首个开放域波斯语推理问答基准，含多类型问题与复杂推理，通过严格验证与过滤保证质量，评测显示专用提示与微调可提升模型表现，为波斯语推理QA研究提供重要支持。


<details>
  <summary>Details</summary>
Motivation: 波斯语作为一种使用人数众多的低资源语言，缺乏高质量的开放领域推理问答基准，阻碍了相关推理QA系统的发展。

Method: 通过受控的基于LLM的生成流程，并结合多阶段过滤、注释和一致性检查，构建了包含多种题型与推理类型的PARSE问答基准。

Result: 多语言及波斯语LLM在PARSE基准上的评测表明，采用波斯语提示词和结构化提示（布尔/选择题的链式思维，事实题的少样本提示）能提升性能，进一步微调波斯语专用模型效果更佳。

Conclusion: PARSE基准填补了波斯语推理问答研究的空白，为低资源环境下推理能力强的LLM发展与评估提供了坚实基础。

Abstract: Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.

</details>


### [87] [PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length](https://arxiv.org/abs/2602.01274)
*Situo Zhang,Yifan Zhang,Zichen Zhu,Hankun Wang,Da Ma,Danyang Zhang,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 本文提出的Pacer方法通过动态分块预验证草稿长度，有效提升了大语言模型推理的速度和效率，优于现有推测解码技术。


<details>
  <summary>Details</summary>
Motivation: 观察到固定的草稿长度在不同解码步骤间表现不佳，限制了解码速度的提升，因而需要一种动态调整草稿长度的方法。

Method: 提出了一种名为Pacer的新方法，利用轻量级且可训练的预验证层分块预验证草稿令牌，动态调整草稿长度，实现更高效的推理速度。

Result: Pacer在多个模型对和基准测试上实现了最高2.66倍的加速，且结合Ouroboros时可达3.09倍加速，优于标准推测解码。

Conclusion: Pacer通过动态控制草稿长度显著加速了大语言模型的推理过程，优于传统的固定草稿长度的推理方法。

Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.

</details>


### [88] [EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models](https://arxiv.org/abs/2602.01313)
*Chuanrui Hu,Tong Li,Xingze Gao,Hongda Chen,Dannong Xu,Yi Bai,Tianwei Lin,Xinda Zhao,Xiaohong Li,Jiaqi An,Yunyun Han,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 本文提出EverMemBench，一个包含多方多主题对话的长期记忆测试基准，通过评测揭示了现有对话记忆系统在多跳推理、时间推理和记忆检索方面的显著不足，促进未来改进。


<details>
  <summary>Details</summary>
Motivation: 现有的对话记忆基准多集中在双人单主题对话，未能反映真实世界中对话的复杂性，亟需一种更具挑战性和复杂性的基准来评估长期对话记忆。

Method: 提出EverMemBench基准，包含多方、多组对话，覆盖超过一百万个标记，具有时序演变信息、跨主题交织和角色特定的人物设定，设计了1000+问答对从细粒度回忆、记忆意识和用户画像理解三维度评价记忆系统。

Result: 评测结果显示多方环境中多跳推理性能严重下降，顶级模型仅达26%；时间推理仍未解决，需要超越时间戳匹配的版本语义；基于相似度的检索方法限制记忆意识，未能有效连接查询与隐含相关记忆。

Conclusion: EverMemBench作为一个更复杂和真实的长期对话记忆测试基准，揭示了当前记忆模型的关键短板，为下一代记忆架构的研究提供了重要挑战和方向。

Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.

</details>


### [89] [DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas](https://arxiv.org/abs/2602.01326)
*Zirui Wu,Lin Zheng,Zhihui Xie,Jiacheng Ye,Jiahui Gao,Shansan Gong,Yansong Feng,Zhenguo Li,Wei Bi,Guorui Zhou,Lingpeng Kong*

Main category: cs.CL

TL;DR: 本文提出DreamOn框架，通过动态长度控制突破扩散语言模型固定长度限制，提升代码补全性能，实现更灵活的可变长度生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型需要固定长度的掩码，导致掩码尺寸与理想补全长度不匹配时性能显著下降，限制了其实用性。

Method: 通过引入两个长度控制状态，DreamOn允许模型基于自身预测动态调整输出长度，并将该机制集成到现有扩散语言模型中，无需改变模型架构，仅需最小训练目标调整。

Result: DreamOn在HumanEval-Infilling和SantaCoder-FIM数据集上达到与最先进自回归模型相当的性能，且表现匹配使用真实长度的理想性能。

Conclusion: DreamOn框架成功解决了扩散语言模型在生成过程中固定长度掩码的限制，实现了动态可变长度的生成。

Abstract: Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.

</details>


### [90] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

TL;DR: 针对多跳问答生成中推理不稳定和格式失控问题，CRAFT提出强化学习框架，通过双重奖励促进结构化且可信的推理，提升了多跳QA性能和推理质量。


<details>
  <summary>Details</summary>
Motivation: 多跳问答中现有生成机制存在推理崩溃、推理-答案不一致及格式失控三大挑战，导致生成推理不可靠，难以获得结构化且可信的推理过程。

Method: 提出了CRAFT框架，该框架基于群体相对策略优化（GRPO）的强化学习，利用确定性奖励保证结构正确性，判别者奖励保证语义可信性，支持可控的推理轨迹变体以研究结构和规模对性能的影响。

Result: CRAFT在三个多跳问答基准上均显著提升了答案准确率和推理的可信度，7B模型的表现可与闭源LLM相媲美，验证了该方法的有效性和可扩展性。

Conclusion: CRAFT方法通过结合双重奖励机制有效提升了多跳问答中的答案准确性和推理可信度，表现出较强的结构和语义一致性，且在多个多跳问答基准测试中达到与闭源大模型竞争的性能。

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [91] [Balancing Understanding and Generation in Discrete Diffusion Models](https://arxiv.org/abs/2602.01362)
*Yue Liu,Yuzhong Zhao,Zheyong Xie,Qixiang Ye,Jianbin Jiao,Yao Hu,Shaosheng Cao,Yunfan Liu*

Main category: cs.CL

TL;DR: 本文提出XDLM模型，理论上统一MDLM和UDLM两大范式，通过引入平稳噪声核和后验概率代数简化，实现语义理解与生成质量的平衡提升，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 针对MDLM在零样本泛化和UDLM在生成质量上的各自优势与不足，寻求两者的平衡提升。

Method: 通过引入一个平稳噪声核，XDLM实现了理论上的统一，并通过代数简化减少了内存瓶颈。

Result: XDLM在零样本文本任务上超越UDLM 5.4分，在少步图像生成任务上优于MDLM（FID 54.1对比80.8），并在8B参数模型中实现32步达到15.0 MBPP，显著提升性能和长期扩展潜力。

Conclusion: XDLM成功统一了MDLM和UDLM两大生成模型范式，实现了在语义理解与生成质量上的均衡提升。

Abstract: In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM

</details>


### [92] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

TL;DR: 该论文提出RISE方法，减少冗余上下文对大型语言模型输出解释的影响，实现更稳健可信的上下文归因。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成输出时包含大量冗余上下文信息，传统解释方法难以区分真正影响输出的关键上下文，导致解释不稳定和可解释性差。

Method: 提出RISE方法，通过量化每个输入相对于其他输入的独特影响，减少冗余信息的干扰，实现更清晰稳定的归因。

Result: 实验表明RISE比传统方法更稳健，能够更准确识别关键上下文元素，提升LLM解释的可信度和监控效率。

Conclusion: RISE有效解决了冗余上下文干扰带来的解释不稳定问题，为重要应用中大型语言模型输出的可解释性提供了可靠支持。

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [93] [On the Power of (Approximate) Reward Models for Inference-Time Scaling](https://arxiv.org/abs/2602.01381)
*Youheng Zhu,Yiping Lu*

Main category: cs.CL

TL;DR: 研究表明，只要近似奖励模型的Bellman误差足够小，结合SMC方法能显著提升大型语言模型推理效率，降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 推理过程中真实奖励模型不可用，依赖近似奖励模型，因此需要理解近似奖励模型何时能够有效支持推理扩展。

Method: 理论分析近似奖励模型的Bellman误差，证明其界限对基于SMC的推理扩展效果的关键作用。

Result: 若近似奖励模型的Bellman误差被约束在O(1/T)内，则结合SMC能将推理的计算复杂度从指数级降低到多项式级，实现推理效率的指数级提升。

Conclusion: 本文理论阐明了近似奖励模型足以支持有效推理扩展的条件，为使用近似模型进行推理时间扩展提供了理论基础和效率保证。

Abstract: Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.
  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.

</details>


### [94] [Rethinking Selective Knowledge Distillation](https://arxiv.org/abs/2602.01395)
*Almog Tavor,Itay Ebenspanger,Neil Cnaan,Mor Geva*

Main category: cs.CL

TL;DR: 本文通过系统分析和学生熵引导策略，改进了大语言模型的选择性知识蒸馏，显著提高效率和性能，降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前知识蒸馏中使用密集监督效率低且资源消耗大，选择性蒸馏的最佳信号和策略尚未明确。

Method: 系统地比较了不同重要性信号和选择策略，设计了基于学生熵的选择机制，扩展到位置、类别和样本维度进行多维度蒸馏优化。

Result: SE-KD在多个基准测试中提升了准确率、任务符合度和内存效率，扩展后（SE-KD 3X）显著减少了训练时间、峰值内存和存储需求，同时保持性能。

Conclusion: 选择性知识蒸馏在位置、类别和样本维度上有效，通过引入基于学生熵的选择策略（SE-KD）显著提升了性能和效率。

Abstract: Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.

</details>


### [95] [From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis](https://arxiv.org/abs/2602.01401)
*Niansong Zhang,Sunwoo Kim,Shreesha Srinath,Zhiru Zhang*

Main category: cs.CL

TL;DR: 本文论证了高层次综合（HLS）在智能硬件时代的重要性，指出当前工具的不足，并提出智能HLS发展的框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型兴起，探讨在智能硬件设计时代，HLS是否依然重要。

Method: 通过分析HLS在智能硬件设计中的作用，指出当前HLS工具的不足，并提出智能HLS系统的发展分类。

Result: 提出HLS作为智能硬件设计的实用抽象层，并识别当前HLS工具的局限性，最后提出智能HLS的协同进化分类。

Conclusion: 高层次综合（HLS）在智能硬件设计中仍然至关重要，是实现智能优化的关键抽象层。

Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.

</details>


### [96] [SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction](https://arxiv.org/abs/2602.01447)
*Hieu Minh Duong,Rupa Ghosh,Cong Hoan Nguyen,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.CL

TL;DR: SentiFuse框架通过多种融合策略整合异构情感模型，显著提升了情感分析的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析模型各有优势，但缺乏一个统一且高效的框架来整合这些模型的互补性。

Method: 提出了一个灵活且模型无关的框架SentiFuse，通过标准化层和多种融合策略（决策级融合、特征级融合、自适应融合）整合异构情感模型。

Result: 在Crowdflower、GoEmotions和Sentiment140三个大型社交媒体数据集上的实验表明，SentiFuse优于单模型和简单平均集成，特征级融合提升F1分数最高达4%，自适应融合增强了对否定、混合情绪及复杂表达的鲁棒性。

Conclusion: SentiFuse框架通过系统地融合多种情感分析模型的优势，实现了比单一模型和简单集成方法更优的性能，提升了情感分析的准确性和鲁棒性。

Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.

</details>


### [97] [Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language](https://arxiv.org/abs/2602.01451)
*Umme Abira Azmary,MD Ikramul Kayes,Swakkhar Shatabda,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文针对孟加拉语问答任务构建了首个反事实问答数据集BanglaCQA，设计多种方法区分模型的参数化知识与上下文知识利用，发现链式思考提示能有效提升反事实场景中参数知识的提取，为低资源语言问答提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语等低资源语言在问答任务中面临有限标注数据和语言复杂性的问题，且缺乏可分析模型知识来源的结构化数据集。因此，迫切需要构建新的数据集和方法以揭示模型如何利用参数化知识和上下文信息来生成答案。

Method: 通过扩展原有孟加拉语数据集，构建包含反事实段落和可回答性标注的BanglaCQA数据集；设计基于编码器-解码器和多语言模型的微调流程，以及基于解码器大型语言模型的提示流程；结合大型语言模型和人工评估，通过语义相似度评估答案质量。

Result: 成功构建了第一个孟加拉语反事实问答数据集BanglaCQA，提出了多种模型微调和提示方案，结合人工与自动评估，展示了链式思考提示在解码器大型语言模型中独特且有效的作用，并对低资源语言问答模型的知识依赖进行了深入分析。

Conclusion: 本文提出了BanglaCQA，这是第一个针对孟加拉语的反事实问答数据集，同时分析了模型在事实与反事实场景中对参数化知识和上下文知识的依赖。研究发现，在反事实场景下，基于链式思考(CoT)提示的解码器大型语言模型能够更有效地提取参数化知识。

Abstract: Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.

</details>


### [98] [ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure](https://arxiv.org/abs/2602.01472)
*Jie Deng,Shining Liang,Jun Li,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 该论文发现了多问上下文诱导模型自发压缩推理链的现象（自压缩），并提出ConPress方法利用这一现象进行自监督微调，大幅减少推理token开销且不损失准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成长链式推理时推理开销大，且在单问场景下缺乏自动压缩推理过程的方法。

Method: 构建包含多问的上下文提示，诱导模型产生更短的推理链（自压缩现象），采样并解析多问输出，过滤并获得每个问题的压缩推理轨迹，利用这些轨迹进行监督微调，无需外部教师、人工剪枝或强化学习。

Result: 在MATH500和AIME25数据集上，ConPress分别减少推理token使用量59%和33%，同时保持竞争力的准确率，且只用8000个微调样本。

Conclusion: ConPress通过轻量级自监督微调方法在单问场景下实现了模型自压缩推理轨迹，显著减少推理过程中的token使用量，同时保持准确性。

Abstract: Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.

</details>


### [99] [Ebisu: Benchmarking Large Language Models in Japanese Finance](https://arxiv.org/abs/2602.01479)
*Xueqing Peng,Ruoyu Xiang,Fan Zhang,Mingzi Song,Mingyang Jiang,Yan Wang,Lingfei Qian,Taiki Hara,Yuqing Guo,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文提出Ebisu基准，聚焦日本金融语言理解的难点，通过两个专家标注任务评估多款大型语言模型，结果显示现有模型在理解日本金融文本上的表现不足，亟需进一步研究和改进。


<details>
  <summary>Details</summary>
Motivation: 由于日语金融文本的语言特性（黏着语、句尾结构、混合书写系统）及高语境交流习惯（间接表达和隐含承诺），现有LLMs难以有效解析，亟需针对日本金融语境设计专门的评测平台。

Method: 本文提出了Ebisu，这是一个专门针对日本本土金融语言理解的基准测试，包含两个由专家标注的任务：JF-ICR（隐含承诺和拒绝识别）和JF-TE（嵌套金融术语的层级抽取和排序）。对多种开源及专有的LLMs进行了性能评估。

Result: 评测结果表明，当前技术水平的模型在两项任务上均表现不佳，模型规模提升效果有限，语言和领域特定的适应训练也未能明显改善性能，表明该领域仍存在较大挑战。

Conclusion: 现有的先进大型语言模型（LLMs）即使经过语言和金融领域特定的适应训练，仍然难以准确理解和处理日本金融语言中的隐含承诺和复杂术语层次结构，存在显著性能差距。

Abstract: Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.

</details>


### [100] [Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training](https://arxiv.org/abs/2602.01511)
*Ran Xu,Tianci Liu,Zihan Dong,Tony You,Ilgee Hong,Carl Yang,Linjun Zhang,Tao Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: 针对传统奖励模型无法刻画多维响应质量的问题，Rubric-ARM 通过联合学习评分规则和裁判，并采用交替优化策略，实现了更准确的评判和更优的强化学习策略对齐。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型只能预测标量得分，不能充分反映非可验证领域多维度的响应质量，限制了奖励模型的表达能力和策略表现。

Method: 提出 Rubric-ARM 框架，联合优化评分规则生成器和裁判，利用基于偏好反馈的强化学习，通过交替优化策略缓解非平稳性，并进行了理论和实验验证。

Result: Rubric-ARM 在多项基准测试中取得了最先进的性能，在线和离线强化学习环境下显著提升了策略对齐效果。

Conclusion: Rubric-ARM 框架有效提升了非可验证领域（如创意写作和开放式指令遵循中）反馈模型的评判准确性和策略对齐性能。

Abstract: Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.

</details>


### [101] [Argument Rarity-based Originality Assessment for AI-Assisted Writing](https://arxiv.org/abs/2602.01560)
*Keito Inoshita,Michiaki Omura,Tsukasa Yamanaka,Go Maeda,Kentaro Tsuji*

Main category: cs.CL

TL;DR: 本研究提出一种基于论点稀有性的原创性评估方法，区别质量与原创性两个评估维度，发现高质量文本往往原创性较低，且大型语言模型的论点原创性不及人类。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能够轻松生成高质量文本，传统以质量为核心的写作评价变得不再具有意义，教育评价应转向培养批判性思维和原创观点，因此需要从质量评估转向原创性评估。

Method: 提出了基于论点稀有性的原创性评估框架，利用密度估计方法量化结构稀有度、论点稀有度、证据稀有度和认知深度四个方面，并结合质量调整机制，独立评估质量与原创性。

Result: 实验证明，质量与论点稀有度存在显著负相关关系，高质量文本更倾向于使用典型论点；AI生成文本在结构复杂度上接近人类，但论点原创度显著低于人类，显示出生成模型原创新颖性的不足。

Conclusion: 本研究提出的AROA框架有效评估了学生作文中的论证原创性，揭示了质量与原创性之间的权衡关系，并指出大型语言模型在论证内容原创性上的局限性。

Abstract: As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.

</details>


### [102] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: FS-Researcher提出利用文件系统作为持久记忆，实现双代理协作，突破大型语言模型上下文限制，显著提升深度研究报告质量和扩展能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在执行长时间跨度的深度研究任务时面临上下文限制，导致收集证据和撰写报告的令牌预算受限，阻碍了有效的测试时扩展。

Method: 提出了FS-Researcher框架，包含两个代理：Context Builder代理负责互联网浏览、结构化笔记和知识库构建，Report Writer代理根据知识库分段撰写报告，整个过程依托文件系统作为持久化工作空间实现超越上下文窗口的迭代精炼。

Result: 在DeepResearch Bench和DeepConsult两个开放式基准测试中，FS-Researcher在不同骨干模型上均实现了最先进的报告质量，且报告质量与Context Builder的计算资源正相关，验证了文件系统范式下的有效测试时扩展。

Conclusion: FS-Researcher框架通过文件系统作为持久外部记忆和协调介质，有效突破了大型语言模型上下文窗口限制，实现了深度研究任务的高质量报告生成和测试时扩展能力。

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [103] [LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States](https://arxiv.org/abs/2602.01572)
*Yeqin Zhang,Yunfei Wang,Jiaxuan Chen,Ke Qin,Yizheng Zhao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 本文提出基于注意力value向量的句子表征方法VA及其改进AlignedWVA，无需训练即可实现最先进性能，显著优于传统隐藏状态方法和复杂集成模型，展示了新范式在句子语义捕获上的优势。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的句子表示方法主要依赖于最终层的隐藏状态，这些状态主要针对下一个词预测优化，难以捕获全局句子语义，因此需要新的方法更好地利用注意力机制信息。

Method: 提出Value Aggregation（VA）方法，通过在多个层和多个token索引上聚合注意力的value向量；进一步结合注意力权重和输出投影矩阵，设计Aligned Weighted VA（AlignedWVA）以提升语义对齐效果。

Result: VA在无训练情况下超越了其他基于LLM的嵌入方法，且表现追平甚至优于集成模型MetaEOL；AlignedWVA进一步提升性能，显著优于高成本的MetaEOL，同时展示了基于VA微调提升嵌入表现的潜力。

Conclusion: 基于注意力机制中的value向量比传统的隐藏状态更有效地捕获句子语义，提出的Value Aggregation方法以及其改进版本Aligned Weighted VA在无训练条件下实现了最先进的句子表示性能。

Abstract: Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.

</details>


### [104] [Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587)
*Zehua Cheng,Jianwei Yang,Wei Dai,Jiahao Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于认证语义平滑和噪声增强对齐调优的新框架，大幅提升了大型语言模型对适应性绕过攻击的鲁棒性，提供了可证实的安全保证。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型易受适应性绕过攻击，传统的经验防御方法如GCG效果有限，亟需一种具备可认证鲁棒性的安全保障框架。

Method: 通过分层随机消融技术，将输入划分为不可变的结构提示和可变的有效载荷，结合超几何分布推导出严格的lo范数保证；采用噪声增强的对齐调优（NAAT）将基础模型转变为语义去噪器，以解决稀疏上下文中的性能下降问题。

Result: 在Llama-3模型上，采用本方法将基于梯度的攻击成功率从84.2%降至1.2%，同时保持94.1%的正常效用，大幅优于降低效用至74.3%的基于字符级别的方法。

Conclusion: 我们提出的认证语义平滑（CSS）框架显著增强了大型语言模型（LLMs）的安全性，实现了对适应性绕过攻击的证书级鲁棒性，显著降低攻击成功率并保持模型效用。

Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.

</details>


### [105] [Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles](https://arxiv.org/abs/2602.01590)
*Shaohan Wang,Benfeng Xu,Licheng Zhang,Mingxuan Du,Chiwei Zhu,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: 本文提出基于维基百科优质文章的实时评测基准和细粒度评估框架，揭示当前深度研究代理与人类专家水平的差距，推动该领域研究发展。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖大语言模型生成的参考资料，缺乏专家验证内容，评价可靠性和细致性不足，因此需引入专家级参考和更细粒度的评价标准。

Method: 提出了Wiki Live Challenge（WLC），基于最新维基百科优质文章构建基准数据集，并设计了包含39条写作质量细化标准和严格事实可验证性指标的Wiki Eval评估框架。

Result: 通过对多种深度研究代理进行实验，验证了WLC的有效性，发现当前系统在质量和事实准确性方面与专家级文章存在显著差距。

Conclusion: 当前的深度研究代理在自动信息检索和报告生成方面虽然展现出潜力，但与维基百科优质文章的人类专家水平仍有显著差距。

Abstract: Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge

</details>


### [106] [The Art of Socratic Inquiry: A Framework for Proactive Template-Guided Therapeutic Conversation Generation](https://arxiv.org/abs/2602.01598)
*Mingwen Zhang,Minqiang Yang,Changsheng Ma,Yang Yu,Hui Bai,Chen Xu,Xiangzhen Kong,Bin Hu*

Main category: cs.CL

TL;DR: 本文提出了Socratic Inquiry Framework(SIF)，使心理大模型能够主动提出结构化问题，促进认知引导，突破了传统被动回应的限制。


<details>
  <summary>Details</summary>
Motivation: 当前心理大语言模型反应性强，缺乏主动发起认知引导的能力，难以挖掘潜在信念和促进行为改变。

Method: 通过SIF框架将提问的时机（Strategy Anchoring）与具体内容（Template Retrieval）解耦，无需重新训练模型，同时引入Socratic-QA数据集用于监督主动推理。

Result: 实验证明SIF显著增加了主动提问的频率、对话深度和治疗一致性，提高了心理对话模型的主动探索能力。

Conclusion: 提出的Socratic Inquiry Framework (SIF)显著提升了心理大语言模型的主动质询能力，实现了从被动回应到主动引导的转变。

Abstract: Proactive questioning, where therapists deliberately initiate structured, cognition-guiding inquiries, is a cornerstone of cognitive behavioral therapy (CBT). Yet, current psychological large language models (LLMs) remain overwhelmingly reactive, defaulting to empathetic but superficial responses that fail to surface latent beliefs or guide behavioral change. To bridge this gap, we propose the \textbf{Socratic Inquiry Framework (SIF)}, a lightweight, plug-and-play therapeutic intent planner that transforms LLMs from passive listeners into active cognitive guides. SIF decouples \textbf{when to ask} (via Strategy Anchoring) from \textbf{what to ask} (via Template Retrieval), enabling context-aware, theory-grounded questioning without end-to-end retraining. Complementing SIF, we introduce \textbf{Socratic-QA}, a high-quality dataset of strategy-aligned Socratic sequences that provides explicit supervision for proactive reasoning. Experiments show that SIF significantly enhances proactive questioning frequency, conversational depth, and therapeutic alignment, marking a clear shift from reactive comfort to proactive exploration. Our work establishes a new paradigm for psychologically informed LLMs: not just to respond, but to guide.

</details>


### [107] [SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia](https://arxiv.org/abs/2602.01618)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 针对东南亚文化背景，提出自主数据生成框架和多语言保障模型SEA-Guard，显著提升区域性安全检测能力。


<details>
  <summary>Details</summary>
Motivation: 现实环境中的AI安全需求涉及多元文化背景，现有模型由于资源限制常依赖英文数据的机器翻译，难以捕捉地域文化细节。

Method: 采用创新的自主数据生成框架大规模创建符合东南亚文化背景的安全数据集，并训练多语言保障模型SEA-Guard。

Result: SEA-Guard模型在多个基准测试以及不同文化变体中表现优异，能更精准地识别区域敏感内容。

Conclusion: SEA-Guard多语言保障模型有效地识别并防止与东南亚文化相关的敏感或有害内容，优于现有模型，同时保持良好的通用安全性能。

Abstract: Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.

</details>


### [108] [A2Eval: Agentic and Automated Evaluation for Embodied Brain](https://arxiv.org/abs/2602.01640)
*Shuai Zhang,Jiayu Hu,Zijie Chen,Zeyuan Ding,Yi Zhang,Yingji Zhang,Ziyi Zhou,Junwei Liao,Shengjie Zhou,Yong Dai,Zhenzhong Lan,Xiaozhu Ju*

Main category: cs.CL

TL;DR: 针对当前手工评估基准的效率和质量问题，A2Eval提出自动化代理框架，有效提升评估效率和准确性，推动具身视觉语言模型的低成本高效评估。


<details>
  <summary>Details</summary>
Motivation: 当前的具身视觉语言模型评估依赖静态专家定义的手动标注基准，存在冗余严重、覆盖不平衡、成本高昂等问题，限制了模型的迭代发展。

Method: 设计了两个协作代理——数据代理自动归纳能力维度并构建平衡紧凑的评估套件，评估代理合成并验证可执行的评测流程，实现全自动化高保真评估。

Result: A2Eval在10个基准和13个模型上测试，评估套件压缩85%，计算成本降低77%，评估速度提升4.6倍，同时保持评价质量和排名可靠性，排名与人类评估相关性显著提高。

Conclusion: 提出的A2Eval框架实现了自动化、高质量且低成本的具身视觉语言模型评估，显著优化了评价套件，纠正了排名偏差，提高了人类评估一致性，推动了模型评估的发展。

Abstract: Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.

</details>


### [109] [Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models](https://arxiv.org/abs/2602.01654)
*Jiaqian Li,Yanshu Li,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种基于梯度的转向向量场方法，有效解决了传统转向向量在上下文依赖和多属性控制上的缺陷，实现了更可靠的大语言模型推理时控制。


<details>
  <summary>Details</summary>
Motivation: 现有的转向向量（Steering Vectors, SVs）在实践中表现不稳定，部分概念难以控制，且在长文本生成和多属性转向时可靠性下降。

Method: 作者提出了转向向量场（Steering Vector Fields, SVF）方法，通过学习一个可微分的概念评分函数，其局部梯度定义激活状态下的转向方向，实现了基于上下文的转向控制，并支持多层次联合干预。

Result: 在多种大语言模型和转向任务中，SVF提供了更强且更可靠的控制效果，提高了推理时转向的实用性。

Conclusion: SVF克服了传统静态转向向量的局限，增强了概念转向的上下文敏感性与多属性控制能力，显著提升了推理阶段的控制性能和稳定性。

Abstract: Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.

</details>


### [110] [CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation](https://arxiv.org/abs/2602.01660)
*Zhongyuan Peng,Caijun Xu,Changyi Xiao,Shibo Hong,Eli Zhang,Stephen Huang,Yixin Cao*

Main category: cs.CL

TL;DR: 本文提出CoDiQ框架，实现高难度推理题的可控自动生成，构建大规模竞赛题库，提升了推理模型的性能，且全部工具和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 现有自动化题目生成方法难以实现精确难度控制，成本高且难以批量生成高难度竞赛级题目。

Method: 提出CoDiQ框架，通过测试时的规模调节实现细粒度难度控制，基于Qwen3-8B开发CoDiQ-Generator生成高难度问题，构建了包含44K竞赛级问题的CoDiQ语料库。

Result: 生成的高难度问题人类评测显示难度显著高于现有基准且可解率超82%，用这些题训练的模型推理性能显著提升。

Conclusion: 通过可控难度的训练题目提升大型推理模型的推理能力，证明了控制训练难度的重要性和有效性。

Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.

</details>


### [111] [Scaling Search-Augmented LLM Reasoning via Adaptive Information Control](https://arxiv.org/abs/2602.01672)
*Siheng Xiong,Oguzhan Gungordu,Blair Johnson,James C. Kerce,Faramarz Fekri*

Main category: cs.CL

TL;DR: 本文提出DeepControl框架，通过自适应调控信息检索过程中的检索时机与范围，显著优化了搜索增强推理代理的性能，在多项测试中超过现有强化学习和推理基线。


<details>
  <summary>Details</summary>
Motivation: 现有的基于结果的强化学习方法在搜索增强推理代理中对信息获取的调控能力有限，导致冗余证据、上下文饱和和学习不稳定等问题。

Method: 提出了DeepControl框架，通过正式的信息效用概念，衡量在特定推理状态下检索证据的边际价值，并基于此设计了检索继续和粒度控制机制，使用退火控制策略使代理在训练中内化有效的信息获取行为。

Result: 在七个基准测试中，DeepControl方法显著优于强基线，在Qwen2.5-7B和Qwen2.5-3B上性能提升分别为9.4%和8.6%，并且始终优于无显式信息控制的检索和非检索推理方法。

Conclusion: 自适应信息控制通过精准调节信息检索的时机和程度，有效提升了搜索增强推理代理在复杂真实信息环境中的表现，凸显其重要性。

Abstract: Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.

</details>


### [112] [Counting Hypothesis: Potential Mechanism of In-Context Learning](https://arxiv.org/abs/2602.01687)
*Jung H. Lee,Sujith Vijayan*

Main category: cs.CL

TL;DR: 本文针对大语言模型的上下文学习机制，提出了‘计数假设’，解释了其编码策略并提供了支持证据，助力理解和改进ICL。


<details>
  <summary>Details</summary>
Motivation: 现有ICL机制不明确，导致错误修正和诊断困难，亟需揭示ICL的限制和依赖机制。

Method: 基于ICL的性质和大语言模型的功能模块，提出‘计数假设’，并通过实验证据支持该假设。

Result: 提出了计数假设，表明LLM的编码策略可能是ICL背后的关键机制，并通过实验证明了其合理性。

Conclusion: 本文提出了“计数假设”，解释了大语言模型支持上下文学习(ICL)的编码策略，深化了对ICL机制的理解。

Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.

</details>


### [113] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

TL;DR: 本文针对大推理模型后训练后的探索崩溃问题，提出潜在探索解码策略，通过利用中间层熵信息提升探索效率，无需额外训练，显著提高推理准确率。


<details>
  <summary>Details</summary>
Motivation: 观察到后训练模型最终层后验熵大幅降低，而中间层熵保持较高，推测这种熵不对称导致传统温度采样失效，激发设计新的解码策略以恢复有效探索。

Method: 提出潜在探索解码(LED)策略，通过累计求和聚合中间层后验概率，并选择熵值最大的深度配置作为探索候选，从而改善模型的探索能力，无需额外训练或参数。

Result: LED在多个推理基准和模型上，分别提升了pass@1和pass@16的准确率0.61和1.03个百分点，验证了方法的有效性。

Conclusion: 本文发现现有大推理模型在后训练阶段出现探索崩溃现象，温度采样不再提升准确率，并提出了一种无额外训练参数且有效的深度条件解码策略——潜在探索解码(LED)，显著提升了多项推理任务中的准确率。

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [114] [Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory](https://arxiv.org/abs/2602.01708)
*Langyuan Cui,Chun Kai Ling,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文将大语言模型的信息获取任务抽象为博弈问题，提出基于纳什均衡策略的Game of Thought方法，有效提升模型的最坏情况表现。


<details>
  <summary>Details</summary>
Motivation: 现实场景中大语言模型常因信息不足难以完成任务，且已有方法通过简化假设牺牲了最坏情况性能，尤其在高风险场景中存在严重问题，因此需要提升模型的主动信息获取能力并保证最坏情况表现。

Method: 我们将信息寻求问题形式化为一个两人零和广义型博弈，提出了战略语言搜索（SLS）模型，并开发了GoT框架以近似计算该游戏的纳什均衡策略。

Result: 实验证明，GoT框架在所有测试设置下均优于基于直接提示和启发式搜索的方法，在最坏情况性能上有持续提升。

Conclusion: 本文提出的Game of Thought (GoT)方法通过博弈论技术显著提升了大语言模型在信息获取任务中的最坏情况性能，优于现有直接提示和启发式搜索方法。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.

</details>


### [115] [ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation](https://arxiv.org/abs/2602.01709)
*Xingshan Zeng,Lingzhi Wang,Weiwen Liu,Liangyou Li,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: 本文提出一种通过迭代风险感知模拟的测试时扩展策略，提升大语言模型在代理任务中的决策可靠性和安全性，避免了环境风险并有效识别高风险失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展技术虽然提升了大语言模型性能，但在需要代理行为与外部环境交互且后果不可逆的情境下表现不足，尤其是无法有效捕捉高风险但低频的失败模式，导致智能体决策不够安全可靠。

Method: 提出ARTIS框架，在测试时通过迭代模拟实现探索，模拟器设计为风险感知工具模拟器，专注于高风险失败动作的数据生成和训练重平衡，以提升模拟质量和范围，进而增强大语言模型在决策中的稳健性。

Result: 实验证明，迭代模拟显著提升了多轮多步骤代理任务中智能体的可靠性，而引入风险感知模拟器是持续实现性能提升的关键，模型和任务间均表现出良好泛化能力。

Conclusion: 本论文提出的Agentic Risk-Aware Test-Time Scaling via Iterative Simulation（ARTIS）框架，通过在真实执行前进行模拟交互，实现了推理时的探索与承诺的解耦，提高了动作级别的可靠性和鲁棒性，同时避免了环境风险。风险感知工具模拟器的引入显著提升了识别和规避高影响失败模式的能力，实验证明该方法在多轮及多步骤任务中有效提升了智能体的可靠性。

Abstract: Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \emph{\name}, \emph{\underline{A}gentic \underline{R}isk-Aware \underline{T}est-Time Scaling via \underline{I}terative \underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.

</details>


### [116] [MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark](https://arxiv.org/abs/2602.01714)
*Mouath Abu-Daoud,Leen Kharouf,Omar El Hajj,Dana El Samad,Mariam Al-Omari,Jihad Mallat,Khaled Saleh,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 本论文推出了阿拉伯语医学多选题大规模数据集MedAraBench，为大语言模型多语言医疗应用能力提升提供了重要资源和基准。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语在自然语言处理尤其是在医疗应用领域资源匮乏，缺少开放数据和基准，限制了大语言模型多语言能力的评估和提升。

Method: 通过手工数字化阿拉伯语医学专业学术资料库，构建包含19个专业和五个难度等级的多个选择题数据集，并采用专家人工评价与大语言模型评判相结合的方法评估数据质量。随后用八个先进模型进行基准测试。

Result: 构建并发布了MedAraBench数据集及评估脚本，涵盖广泛医学专业和难度等级，验证了数据的高质量和多样性，同时通过多模型基准测试揭示当前模型的不足。

Conclusion: MedAraBench数据集为阿拉伯语医疗领域提供了高质量、多样化的大规模多选题数据集，有助于推动多语言大语言模型在医疗应用中的发展。实验结果显示现有模型在该领域仍有提升空间，强调需要进一步的专门领域优化。

Abstract: Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.

</details>


### [117] [Mechanistic Indicators of Steering Effectiveness in Large Language Models](https://arxiv.org/abs/2602.01716)
*Mehdi Jafari,Hao Xue,Flora Salim*

Main category: cs.CL

TL;DR: 本文通过信息论指标揭示激活引导成败的机制，提升了大语言模型行为控制的可靠性评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前激活引导方法虽广泛应用，但其成功机制不明，现有研究主要依赖黑盒输出，缺乏基于内部信号的诊断手段。

Method: 通过测量熵的归一化分支因子（NBF）和引导激活与目标词汇概念的KL散度，结合不同架构模型的评判一致性，评估激活引导的可靠性。

Result: 证实信息论测度对识别成功引导和估计失败概率具有预测能力，并提出了更强的对比评估基准。

Conclusion: 内部激活信号中的信息论指标（NBF和KL散度）能够有效预测大语言模型激活引导的成功与失败。

Abstract: Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.

</details>


### [118] [BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition](https://arxiv.org/abs/2602.01717)
*Hyunsik Kim,Haeri Kim,Munhak Lee,Kyungmin Lee*

Main category: cs.CL

TL;DR: BBPE16利用UTF-16编码优势优化多语种语音识别分词，显著减少非拉丁文字token数和计算开销，提高效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有使用UTF-8的BBPE分词虽然覆盖全Unicode但对非拉丁文字的变长编码导致token序列膨胀，增加计算和存储成本，需要一种更高效的分词策略。

Method: 提出了BBPE16，一种基于UTF-16编码的字节级BPE分词方法，通过使用统一的2字节编码单元减少非拉丁文字（如中、日、韩）序列长度，从而降低计算负载和内存使用。

Result: BBPE16在单语、双语、三语及多语持续学习场景下表现良好，特别是对中文token数量减少10.4%，解码迭代次数降低10.3%，加快微调和推理速度并降低内存消耗。

Conclusion: BBPE16作为一种基于UTF-16的字节级BPE分词器，保持了语言无关性同时提高了跨语言的token共享，能够在多语种自动语音识别任务中达到甚至超过原有BBPE的准确率。

Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.

</details>


### [119] [COMI: Coarse-to-fine Context Compression via Marginal Information Gain](https://arxiv.org/abs/2602.01719)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Yujin Yuan,Libin Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 针对大语言模型的长上下文处理难题，提出了基于边际信息增益的粗细粒度自适应压缩框架COMI，大幅提升压缩效率与下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在长上下文场景下计算效率低且信息冗余严重，需要有效的上下文压缩方法来提升模型处理能力和性能。

Method: 提出了COMI，一种粗到细的自适应上下文压缩框架，采用边际信息增益(MIG)指标，结合两阶段策略：粗粒度组重新分配和细粒度Token合并，实现语义相关且去冗余的高效压缩。

Result: 在多个问答和摘要数据集（如NaturalQuestions、MultiNews等）及多种模型（如LLaMA-2-7B、Qwen2-7B）上，COMI在32倍压缩条件下实现约25点EM提升，显著优于基线。

Conclusion: COMI框架在高压缩率下通过联合优化语义相关性和多样性，有效提升了长上下文处理的性能，显著优于现有方法。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.

</details>


### [120] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: 针对计算机代理长期风险问题，SafePred通过预测未来风险并优化决策，有效提升代理安全性和任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统的计算机代理护栏多为反应式，仅限制当前观察空间内行为，无法预防延迟出现的长期风险，亟需一种能够主动预测并预防长期风险的护栏方法。

Method: 提出基于风险预测的护栏方法，通过世界模型预测短期和长期安全风险，并结合步级干预和任务重规划进行决策优化。

Result: SafePred实现了超过97.6%的安全性能，任务效用相比反应式基线提升达21.4%。

Conclusion: SafePred预测型护栏框架显著减少计算机代理的高风险行为，提升安全性和任务效用。

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


### [121] [Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training](https://arxiv.org/abs/2602.01747)
*Hongseok Choi,Serynn Kim,Wencke Liermann,Jin Seong,Jin-Xia Huang*

Main category: cs.CL

TL;DR: 本文提出结合两阶段微调、分数对齐和不确定性感知自训练的三项技术，有效提升自动作文评分模型在小样本和大样本条件下的表现，验证了在ASAP++数据集上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 自动作文评分在实际应用中受限于标注数据匮乏，限制了模型的开发与应用，亟需有效技术提升小样本环境下的评分性能。

Method: 提出了三项技术：一是基于低秩适配的两阶段微调策略，二是分数对齐技术用于提升预测分布与真实分布一致性，三是利用不确定性感知的自训练方法扩展训练集且减少伪标签噪声，基于DualBERT模型实现。

Result: 在ASAP++数据集上的32样本有限数据环境中，三项技术的结合使模型达到了约1000标注样本训练模型的91.2%性能，且分数对齐技术在充足数据环境下提升效果明显，实现了最新的性能水平。

Conclusion: 本文提出的三项关键技术显著提升了自动作文评分模型在有限和充分数据条件下的性能，尤其是在数据极度稀缺的情况下实现了接近充分数据训练效果的成绩。

Abstract: Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.

</details>


### [122] [WorldCup Sampling for Multi-bit LLM Watermarking](https://arxiv.org/abs/2602.01752)
*Yidan Wang,Yubing Ren,Yanan Cao,Li Guo*

Main category: cs.CL

TL;DR: WorldCup提出一种新的多比特码大语言模型水印框架，通过直接嵌入消息比特和熵感知调制，显著提升容量和解码性能，实现更可靠的文本来源认证。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的水印方法多为零比特码扩展，信息流间接且容量有限，解码性能不佳，难以实现可靠来源溯源。

Method: 提出WorldCup框架，将采样视为通信信道，通过分层竞争机制直接将消息比特嵌入词语选择中，并结合互补信号引导，多比特水印编码。采用熵感知调制保证生成质量，利用置信度感知解码实现鲁棒消息恢复。

Result: 实验表明WorldCup在容量、可检测性、鲁棒性、文本质量及解码效率方面均优于现有基线方法，表现稳定且平衡。

Conclusion: WorldCup为大语言模型多比特码水印提供了有效方案，兼顾信息容量与生成质量，推动了未来水印研究发展。

Abstract: As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.

</details>


### [123] [Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings](https://arxiv.org/abs/2602.01757)
*Doohyun Kim,Donghwa Kang,Kyungjae Lee,Hyeongboo Baek,Brent Byunghoon Kang*

Main category: cs.CL

TL;DR: Zero2Text是一种无需训练数据、基于递归对齐的嵌入反演攻击方法，有效针对黑箱跨域环境，实现了显著的文本恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入反演攻击防御面临计算代价高或需训练数据的根本矛盾，且在严格黑箱和跨域环境中无效。

Method: 提出了一种基于递归在线对齐的无训练框架Zero2Text，结合大型语言模型先验和动态岭回归机制，迭代对齐生成文本与目标嵌入。

Result: 在多个基准测试中，Zero2Text在MS MARCO上对OpenAI模型达成比基线方法高1.8倍ROUGE-L和6.4倍BLEU-2分数，能在无泄露数据的情况下从未知领域恢复文本。

Conclusion: Zero2Text框架成功打破了现有嵌入反演攻击防御的限制，在无训练数据且黑箱的跨域环境中有效恢复目标嵌入对应的文本。

Abstract: The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.

</details>


### [124] [<SOG_k>: One LLM Token for Explicit Graph Structural Understanding](https://arxiv.org/abs/2602.01771)
*Jingyao Wu,Bin Lu,Zijun Di,Xiaoying Gan,Meng Jin,Luoyi Fu,Xinbing Wang,Chenghu Zhou*

Main category: cs.CL

TL;DR: 引入特殊结构令牌<SOG_k>统一表示图结构，有效解决图与文本令牌错位问题，显著提升大型语言模型图理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将图转换为自然语言导致令牌消耗大且注意力分散，要么用连续嵌入表示图结构但与文本令牌严重不匹配，限制了大型语言模型在图理解任务中的表现。

Method: 设计了一个结构感知的分词器将图拓扑映射为单一选择性令牌，并构建混合结构的问答语料库以对齐新的结构令牌和文本令牌，从而实现图结构的显式拓扑输入和信息共享。

Result: 在五个图级基准数据集上，所提方法相较基线性能提升了9.9%至41.4%，同时展现出良好的解释性和一致性，并能够灵活扩展到节点级任务，实现局部和全局结构理解。

Conclusion: 本文提出了一种结构感知的结构化分词器，利用特殊的结构令牌<SOG_k>在统一的令牌空间中表示图结构，显著提升了大型语言模型对图数据的理解和推理能力，实验表明该方法在多个图级任务上优于现有方法，且具备解释性和一致性。

Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.

</details>


### [125] [Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model](https://arxiv.org/abs/2602.01778)
*Kangtao Lv,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Shilei Liu,Yongwei Wang,Yujin Yuan,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文首次从数据角度研究数据分布对大语言模型上下文压缩的影响，发现输入熵和模型内在数据匹配度是关键因素，并提出相应优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于模型本身的改进，忽视了数据分布对上下文压缩效果的影响。

Method: 采用基于自动编码器的框架，从数据中心视角系统地评估输入数据和模型内在数据对语义压缩表现的影响。

Result: 发现输入熵与压缩质量负相关，且编码器和解码器内在数据差异显著降低压缩收益，提出优化压缩收益的实用指导方案。

Conclusion: 数据分布对上下文压缩质量有显著影响，其中输入数据的熵与压缩质量负相关，编码器和解码器内在数据之间的不匹配会降低压缩效果。

Abstract: The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.

</details>


### [126] [Sentence Curve Language Models](https://arxiv.org/abs/2602.01807)
*DongNyeong Heo,Heelyoul Choi*

Main category: cs.CL

TL;DR: 本文提出一种基于句子曲线的扩散语言模型，解决了静态词嵌入忽视句子全局结构的问题，在多个数据集上实现了更好的性能和更稳定的训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的语言模型使用静态词嵌入表示目标句子，忽略了目标词之间的全局结构，导致词预测只关注局部准确性。

Method: 提出了一种连续句子表示方法“句子曲线”，通过样条曲线的控制点影响句中多个词，结合句子曲线语言模型（SCLM）扩展扩散语言模型，预测句子曲线而非静态词嵌入。

Result: 理论上证明句子曲线预测具有正则化效果，促进全局结构建模。实验中，SCLM在IWSLT14和WMT14数据集上达到扩散语言模型的SOTA，训练稳定，且在LM1B数据集表现优于离散扩散语言模型。

Conclusion: 句子曲线表示及其对应的语言模型有效提升了语言模型的全局结构建模能力，显著改善了语言建模性能和训练表现。

Abstract: Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.

</details>


### [127] [AXE: Low-Cost Cross-Domain Web Structured Information Extraction](https://arxiv.org/abs/2602.01838)
*Abdelrahman Mansour,Khaled W. Alshaer,Moataz Elsaban*

Main category: cs.CL

TL;DR: AXE通过智能修剪HTML树并结合小型语言模型，实现了低成本、高精度的网页结构化数据自动提取，效果领先于多种大型模型。


<details>
  <summary>Details</summary>
Motivation: 传统的手工规则脆弱且无法扩展，使用大型语言模型成本高昂，需寻找一个既高效又经济的数据提取方案。

Method: 将HTML DOM视为需要修剪的树结构，使用专门的修剪机制剔除无关节点，结合一个小型0.6B参数量的LLM生成精确结构化输出，并采用GXR确保提取内容可定位追踪。

Result: AXE在SWDE数据集上实现了88.1%的F1得分，超越多种大型且完全训练的替代方案，展示了优异的零样本性能。

Conclusion: AXE方法能够在保持低资源消耗的同时，实现优于大型模型的网页结构化数据提取效果，具有实际应用价值。

Abstract: Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized "pruning" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.

</details>


### [128] [Read As Human: Compressing Context via Parallelizable Close Reading and Skimming](https://arxiv.org/abs/2602.01840)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Qingsong Lv,Runsong Zhao,Tingwei Lu,Langming Liu,Haibin Chen,Yujin Yuan,Hai-Tao Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种模仿人类阅读的上下文压缩框架RAM，显著提升了大语言模型处理长文本时的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理超长上下文时存在计算效率低下和信息冗余问题，亟需一种高效且保持性能的上下文压缩方法。

Method: 采用基于自适应混合阅读策略的上下文压缩方法，将上下文分段并与查询并行编码，对高相关段进行精读保留，对低相关段进行查询引导的压缩摘要，结合对比学习优化阅读策略。

Result: RAM在多个问答和摘要任务中，相较于基线方法表现更优，同时在处理平均长度达16K，最大32K的长文本时，实现最多12倍的速度提升。

Conclusion: RAM框架通过模仿人类阅读行为，有效解决了大语言模型在长上下文场景中的计算低效和信息冗余问题，显著提升了性能和计算速度。

Abstract: Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).

</details>


### [129] [PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning](https://arxiv.org/abs/2602.01875)
*Langming Liu,Kangtao Lv,Haibin Chen,Weidong Zhang,Yejing Wang,Shilei Liu,Xin Tong,Yujin Yuan,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出PretrainRL框架，利用强化学习在预训练中降低错误信息概率，提升模型事实准确性，显著减少事实幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预训练语料中的数据分布不均衡，导致真实信息的概率较低而错误信息的概率较高，造成事实幻觉问题。

Method: 提出PretrainRL框架，将强化学习融入预训练阶段，通过“去偏见再学习”的原则，主动调整模型概率分布，降低高概率错误信息的权重，同时设计负采样策略和评价指标。

Result: 在三个公共基准测试中，PretrainRL显著减少了事实幻觉现象，且性能优于现有最先进方法。

Conclusion: 通过在预训练阶段引入强化学习并调整概率分布，PretrainRL有效缓解了大语言模型的事实幻觉问题。

Abstract: Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of "low-probability truth" and "high-probability falsehood". Recent approaches, such as teaching models to say "I don't know" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is "\textbf{debiasing then learning}." It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making "room" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.

</details>


### [130] [ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support](https://arxiv.org/abs/2602.01885)
*Tiantian Chen,Jiaqi Lu,Ying Shen,Lin Zhang*

Main category: cs.CL

TL;DR: 本研究针对长期情感支持中用户信息隐式分散且动态变化的特点，提出全新的记忆能力评测基准和数据集，揭示当前长时记忆及检索模型的优缺点，推动更智能的个性化对话系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有长时对话评测偏重静态显式事实检索，无法覆盖情感支持中隐式、分散、动态变化的用户信息，亟需全面标准来评价记忆能力。

Method: 提出ES-MemEval基准系统评估五大记忆能力，并构建包括多轮隐式用户信息的EvoEmo数据集，进行多种开源及商用模型的大规模实验。

Result: 实验表明显式长时记忆有助于降低幻觉和增强个性化，检索增强模型提升事实一致性，但在时间动态和用户状态演变处理上存在挑战。

Conclusion: 当前大型语言模型在长时间个性化情感支持中记忆能力仍有限，显式长时记忆和检索增强方法各有优势和不足，需结合二者提升模型的持久记忆和个性化能力。

Abstract: Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.

</details>


### [131] [GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs](https://arxiv.org/abs/2602.01917)
*Chengguang Gan,Yoshihiro Tsujii,Yunhao Liang,Tatsunori Mori,Shiwen Ni,Hiroki Itoh*

Main category: cs.CL

TL;DR: 本文提出GuideWeb基准与GuideWeb Agent，实现网页内自动引导生成，虽有提升但仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有数字采纳平台的操作指导维护繁琐，网页布局及功能变化频繁，亟需自动化的指导生成方法。

Method: 提出了GuideWeb基准，结合网页元素选择及文本生成，设计了一个综合评估体系，并开发了GuideWeb Agent进行任务实现。

Result: GuideWeb Agent在引导目标元素选择准确率达到30.79%，用户意图生成BLEU得分44.94，引导文本生成BLEU得分21.34，显著优于现有基线。

Conclusion: 自动生成网页内指导仍然具有挑战性，目前系统性能有限，需进一步改进以实现可靠部署。

Abstract: Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \textbf{GuideWeb Agent} achieves \textbf{30.79\%} accuracy in guide target element prediction, while obtaining BLEU scores of \textbf{44.94} for intent generation and \textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.

</details>


### [132] [From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted "Vibe Coding"](https://arxiv.org/abs/2602.01919)
*Hend Al-Khalifa*

Main category: cs.CL

TL;DR: 该文提出一种利用LLMs辅助编码的“NLP教育”方法，重视概念理解和批判性思维，学生反馈积极，表明此方法能有效支持概念掌握，减轻调试负担。


<details>
  <summary>Details</summary>
Motivation: LLMs的快速发展为NLP教育带来了挑战与机遇，如何有效利用LLMs提升教学质量是关键问题。

Method: 提出并实施“Vibe Coding”教学方法，利用LLMs辅助代码生成，同时通过反思性问题评估学生的概念理解和批判性思维，在高级本科NLP课程中应用。

Result: 学生对该方法满意度高（平均4.4-4.6/5），参与度、概念学习和评估公平性均获好评，降低调试认知负担，有利于加深对NLP概念的理解。

Conclusion: 结构合理且要求必须记录提示和反思评估的LLM辅助学习能够将教学重心从语法流利度转向概念掌握，帮助学生为AI增强的职业环境做好准备。

Abstract: The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.

</details>


### [133] [Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.01965)
*Kwun Hang Lau,Fangyuan Zhang,Boyu Ruan,Yingli Zhou,Qintian Guo,Ruiyuan Zhang,Xiaofang Zhou*

Main category: cs.CL

TL;DR: 本文提出CatRAG，通过动态调整知识图谱结构，使得多跳检索能更准确捕获完整证据链，提升推理的完整性和效果。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法依赖静态图索引，忽略查询上下文对边权的影响，导致路径偏移和证据链不完整。

Method: 基于HippoRAG 2架构，提出上下文感知的动态图遍历策略，包含符号锚定、查询感知动态边权调整及关键证据通道权重增强三方面机制。

Result: 在四个多跳基准测试中，CatRAG超越了现有最先进方法，显著提升了推理完整性，克服了仅部分召回的限制。

Conclusion: CatRAG有效解决了静态图谬误问题，显著提升了多跳检索中完整证据链的恢复能力，实现更加精准的推理。

Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.

</details>


### [134] [Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition](https://arxiv.org/abs/2602.01967)
*Wonjun Lee,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: Moe-Ctc通过专家模型与中间CTC监督提升了ASR对多口音的识别能力，显著降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 当前ASR模型在处理非主流或重口音语音时表现差，传统的口音无关或口音特定方法存在性能或标注数据不足的问题，亟需一种兼具鲁棒性和适应性的解决方案。

Method: 采用Mixture-of-Experts架构结合中间CTC监督，通过训练阶段的口音意识路由和推理时的无标签路由，促进专家模型专长与泛化能力。每个专家配备独立CTC头并引入路由增强损失以稳定训练。

Result: 在Mcv-Accent基准测试中，Moe-Ctc在见过和未见口音条件下均实现了性能提升，最高相对词错误率降低29.3%，优于强基线FastConformer。

Conclusion: Moe-Ctc模型有效提升了自动语音识别中对不同口音的鲁棒性，显著减少了词错误率。

Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.

</details>


### [135] [Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models](https://arxiv.org/abs/2602.01969)
*Bin Cao,Huixian Lu,Chenwen Ma,Ting Wang,Ruizhe Li,Jing Fan*

Main category: cs.CL

TL;DR: 本文提出一种正交层次分解框架，通过空间语义约束分解复杂表格结构，显著提升了LLM在复杂表格问答上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有对复杂表格的表示方法如线性化或归一化网格难以显式捕捉表格的层次结构和跨维度依赖，导致结构语义与文本表示之间存在错配，影响LLM理解和推理表现。

Method: 提出正交层次分解（OHD）框架，采用基于空间-语义约束的正交树诱导（OTI）方法，将复杂表格分解为列树和行树，再通过双路径关联协议对每个单元格的语义血统进行对称重建，并结合LLM作为语义仲裁器，确保多级语义信息对齐。

Result: 在AITQA和HiTab两个复杂表格问答基准上，OHD框架在多项评估指标中均表现优于现有表征范式，显示出更好的结构理解能力和语义对齐效果。

Conclusion: 提出的OHD框架通过正交树诱导方法有效捕捉了复杂表格的多层次结构和跨维度依赖，显著提升了复杂表格理解及推理性能，在AITQA和HiTab数据集上优于现有方法。

Abstract: Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.

</details>


### [136] [Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing](https://arxiv.org/abs/2602.01977)
*Shuainan Liu,Xuanang Chen,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 本文提出EVK方法通过嵌入空间扰动拓展知识编辑评估范围，构建了新基准并设计约束模块，提升了知识保持能力和编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的知识编辑评估仅限于预定义基准和有限的样本，无法充分理解编辑对模型整体知识系统的影响。

Method: 提出Embedding-Virtualized Knowledge (EVK)方法，通过在嵌入空间中进行受控扰动来表征模型知识；构建基于EVK的评估基准EVK-Bench，量化编辑引起的潜在知识漂移；设计EVK-Align模块，约束嵌入层面的知识漂移并兼容现有编辑方法。

Result: 实验表明该方法不仅实现了更全面的知识编辑评估，还在保证编辑准确性的同时显著提升了知识的保持效果。

Conclusion: 基于EVK的方法弥补了传统样本限定评估的不足，实现了对编辑影响的更广泛探测与约束，提高了大语言模型知识编辑的可靠性和有效性。

Abstract: Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.

</details>


### [137] [S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs](https://arxiv.org/abs/2602.01982)
*Yanrui Du,Sendong Zhao,Yibo Gao,Danyang Zhao,Qika Lin,Ming Ma,Jiayun Li,Yi Jiang,Kai He,Qianyi Xu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 本文创新性提出了基于激活引导的自采样方法，实现了无需教师数据的高效链式思维训练，显著提升大型语言模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在链式思维能力提升的同时，存在冗余推理过程，亟需引入类似人类快速思考的系统1模式。

Method: 提出基于激活引导的自采样框架，从目标模型中无教师指导地产生风格一致且长度可变的推理轨迹，结合筛选后的黄金答案进行有监督微调，辅以双认知系统和递进压缩课程学习，进一步探索无需黄金答案的自我进化训练方案。

Result: 在数学基准测试和医学领域跨域实验中，该方法显著提升了链式思维效率和模型性能，验证了方法的稳定性和泛化能力。

Conclusion: 本文提出的无监督自采样与进化训练机制有效缓解数据稀缺问题，实现了类人快速思维模式，促进了大型语言模型链式思维的高效学习。

Abstract: Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.

</details>


### [138] [From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs](https://arxiv.org/abs/2602.01999)
*Yanrui Du,Yibo Gao,Sendong Zhao,Jiayun Li,Haochun Wang,Qika Lin,Kai He,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 本文通过层级激活轨迹分析与干预实验，揭示了R1风格LLMs自我反思的分层机制，类似人类元认知过程。


<details>
  <summary>Details</summary>
Motivation: 尽管R1风格LLMs表现出自我反思能力，但其实现机制尚不清晰，本文旨在填补该认知空白。

Method: 使用logit lens技术分层读取token语义，追踪反思行为的激活轨迹，并实施针对性干预以验证不同层次间的因果关系。

Result: 发现反思行为从潜在监控到话语调控再到显性表现的层层递进结构，并验证了各阶段间的因果链条。

Conclusion: 本文揭示了R1风格大型语言模型（LLMs）自我反思行为的内部机制，发现该过程分为潜在控制层、语义枢纽层和行为外显层三级联动结构，形成类似人类元认知的过程。

Abstract: R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.

</details>


### [139] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: 本文指出传统RAG方法不适用于对话式代理记忆，提出xMemory通过分解语义组件并层次化管理记忆，实现更高效、准确的检索，提高多事实问答性能。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）方法假设检索内容来源于大规模、异构的语料库，适用于多样且不相关的文本段落，而代理记忆系统中的对话流是有限且高度相关的，导致RAG在此场景下检索结果存在冗余且可能删除必要前提。

Method: 提出xMemory方法，将记忆分解为语义组件，构建层次结构，利用稀疏性-语义目标进行记忆的拆分和合并，在推理阶段通过自上而下的检索策略选择紧凑且多样化的主题和语义，仅在减少阅读者不确定性时展开具体情节和消息。

Result: 在LoCoMo和PerLTQA数据集上，xMemory方法在最新的三种大型语言模型中均显著提升了答案质量和Token使用效率。

Conclusion: 针对代理记忆系统与传统RAG场景的差异，xMemory通过语义分解和层次化结构重塑检索机制，有效提供更精准和高效的记忆检索，提升多事实查询的应答表现。

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [140] [NEAT: Neuron-Based Early Exit for Large Reasoning Models](https://arxiv.org/abs/2602.02010)
*Kang Liu,Yongkang Liu,Xiaocui Yang,Peidong Wang,Wen Zhang,Shi Feng,Yifei Zhang,Daling Wang*

Main category: cs.CL

TL;DR: NEAT通过监控神经元激活实现训练免费、低成本的早期推理退出，有效减少过度思考，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有早期退出方法依赖输出级启发式或训练的探测模型，需增加计算代价或依赖外部标注，迫切需要无额外计算和训练的高效方法。

Method: 提出了一种基于神经元激活动态监测的早期推理退出框架NEAT，通过识别与退出相关的神经元并追踪其激活模式，实现无需额外训练和计算的动态早期退出。

Result: 在四个推理基准和六个不同规模及架构的模型上，NEAT平均减少22%到28%的推理步骤，同时保持准确率稳定。

Conclusion: NEAT方法能够在保证解答质量的前提下，显著减少大型推理模型中的冗余推理步骤，有效缓解过度思考问题。

Abstract: Large Reasoning Models (LRMs) often suffer from \emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \textbf{NEAT}, a \textbf{N}euron-based \textbf{E}arly re\textbf{A}soning exi\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\% to 28\% when averaged over the four benchmarks, while maintaining accuracy.

</details>


### [141] [WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora](https://arxiv.org/abs/2602.02053)
*Pengyu Wang,Benfeng Xu,Licheng Zhang,Shaohan Wang,Mingxuan Du,Chiwei Zhu,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出基于维基百科结构的WildGraphBench，真实评测GraphRAG在长上下文、多来源文档下的表现，发现其多事实聚合有效但摘要能力欠佳。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphRAG评测多数基于短文本，缺乏对长上下文和大规模异构文档条件下系统表现的评估，难以反映真实应用场景。

Method: 设计WildGraphBench基准测试，利用维基百科的层级结构和长异构参考文档，构建包含12个主题、1100个问题，涵盖单事实问答、多事实问答及章节级摘要的多层次任务。

Result: 实验表明，现有的GraphRAG方法在中等数量来源的多事实聚合上表现较好，但对细粒度信息的总结能力较弱，在章节摘要任务上性能不足。

Conclusion: WildGraphBench有效评估GraphRAG在现实环境中的表现，揭示其在细节信息聚合和摘要方面的不足，提示未来需改进细粒度信息处理能力。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.

</details>


### [142] [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090)
*Yikai Zeng,Yingchao Piao,Jianhui Li*

Main category: cs.CL

TL;DR: 本文提出LEC-KG框架，结合大型语言模型与知识图谱嵌入，实现从非结构化文本中高效抽取、验证和优化领域知识图谱，提升了低频关系识别准确度。


<details>
  <summary>Details</summary>
Motivation: 当前从非结构化文本构建领域知识图谱面临异构实体、多样化长尾关系和缺乏统一模式的挑战，需要有效结合语义理解与结构推理的协同方法。

Method: 提出了一个双向协同框架LEC-KG，结合大型语言模型的语义理解和知识图谱嵌入的结构推理，包含分层粗到细的关系抽取、基于证据的链式反馈以及语义初始化三大核心组件。

Result: 在中文可持续发展目标报告数据集上，LEC-KG显著优于大型语言模型基线，尤其在低频关系的抽取中表现突出，实现了非结构化政策文本向验证图谱三元组的可靠转化。

Conclusion: LEC-KG框架有效解决了构建领域特定知识图谱中的异构实体表达、长尾关系分布和缺乏标准化模式等问题，实现了结构和语义的双向协同提升。

Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.

</details>


### [143] [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099)
*Keqin Peng,Yuanxin Ouyang,Xuebo Liu,Zhiliang Tian,Ruijian Han,Yancheng Yuan,Liang Ding*

Main category: cs.CL

TL;DR: DDCA通过动态调节长度惩罚，解决了奖励强化学习中序列冗长和准确率下降的难题，实现了更优的效率和准确率平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的通过可验证奖励的强化学习在鼓励多步骤推理时生成过于冗长的轨迹，且简单的长度惩罚会因基线稀释及难度与惩罚不匹配问题导致准确率下降。

Method: 提出了动态解耦条件优势（DDCA）方法，将效率优化与正确性分离，条件计算正确响应类群内的长度优势，并利用组通过率动态调整惩罚力度。

Result: 在多项基准任务（GSM8K、MATH500、AMC23、AIME25）上，DDCA显著减少了生成的Token数量（简单任务约减少60%，困难任务超过20%），同时保持或提升了准确率。

Conclusion: DDCA方法通过消除基线稀释并根据问题难度动态调整惩罚强度，有效改善了多步骤推理过程中生成序列的冗长问题，显著提高了效率与准确性的权衡。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.

</details>


### [144] [Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs](https://arxiv.org/abs/2602.02104)
*Shaltiel Shmidman,Avi Shmidman,Amir DN Cohen,Moshe Koppel*

Main category: cs.CL

TL;DR: 该工作推出了面向希伯来语的多个规模开源大型语言模型Dicta-LM 3.0，并开发了多任务评测套件，解决了低资源语言LLM训练难题，提供了多语言适配框架。


<details>
  <summary>Details</summary>
Motivation: 虽然开放权重的大型语言模型已经发布，但针对非英语的主权LLM供应不足且需求旺盛，尤其是低资源语言如希伯来语训练面临挑战。

Method: 采用多种基础模型（Mistral-Small-3.1、NVIDIA Nemotron Nano V2、Qwen3-1.7B）进行适配训练，结合大规模希伯来语和英语语料，设计多版本模型并支持长上下文和工具调用。

Result: 发布了三个规模的Dicta-LM 3.0模型（24B、12B、1.7B），并构建了专用于希伯来语聊天模型的新评测套件，涵盖翻译、摘要、Winograd、以色列知识和元音标注任务。

Conclusion: 本论文成功训练并发布了针对希伯来语和英语的开源大型语言模型Dicta-LM 3.0，涵盖不同规模和多样化应用，弥补了低资源语言LLM的空白。

Abstract: Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.

</details>


### [145] [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108)
*Wenhao Li,Daohai Yu,Gen Luo,Yuxin Zhang,Fei Chao,Rongrong Ji,Yifan Wu,Jiaxin Liu,Ziyang Gong,Zimu Liao*

Main category: cs.CL

TL;DR: 该论文通过设计OOMB系统，采用激活重计算与高效内存管理技术，显著降低了训练长上下文大语言模型的显存需求，实现了单卡上百万token上下文训练。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长上下文训练因激活内存线性增长导致显存瓶颈，限制了训练长序列的能力。

Method: 使用了分块递归训练框架结合即时激活重计算，配合分页内存管理、异步CPU卸载及页级稀疏注意力等多种优化技术管理KV缓存。

Result: Qwen2.5-7B模型训练每增加1万tokens上下文，显存仅多占用10MB，实现4百万token上下文在单个H200 GPU上训练。

Conclusion: 该论文提出的OOMB训练系统显著降低了长上下文大语言模型的训练显存开销，实现了激活内存占用常数增长，从而突破了传统方法的内存瓶颈。

Abstract: Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.

</details>


### [146] [There Is More to Refusal in Large Language Models than a Single Direction](https://arxiv.org/abs/2602.02132)
*Faaiz Joad,Majd Hawasly,Sabri Boughorbel,Nadir Durrani,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: 拒绝行为在大语言模型中对应多种激活空间方向，不同方向影响拒绝方式，线性操控可统一调节拒绝程度。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为拒绝行为由单一激活方向中介，本文旨在揭示这一观点的局限性。

Method: 研究了十一类拒绝和不服从情况，在激活空间中分析它们对应的几何方向和线性操控效果。

Result: 发现拒绝行为对应多个不同几何方向，尽管多样，沿任何拒绝相关方向的线性操控都会产生类似的拒绝-过度拒绝权衡。

Conclusion: 拒绝行为在大语言模型的激活空间中对应不同的几何方向，不同方向决定拒绝的方式而非是否拒绝。

Abstract: Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.

</details>


### [147] [Quantifying the Gap between Understanding and Generation within Unified Multimodal Models](https://arxiv.org/abs/2602.02140)
*Chenlong Wang,Yuhang Chen,Zhihan Hu,Dongping Chen,Wenhu Chen,Sarah Wiegreffe,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文通过GapEval基准评估统一多模态模型的理解和生成能力，发现两者存在认知差距，知识尚未深度融合，揭示当前模型的瓶颈和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探究统一多模态模型的理解能力与生成能力是否真正整合和协同，揭示两者之间的内在联系和差距。

Method: 提出了GapEval基准，通过双向（图像与文本）测试，量化理解与生成能力差距，评估模型的双向推理能力和跨模态一致性，同时进行知识操作视角的实证研究。

Result: 实验发现多种架构的统一多模态模型在理解和生成之间存在持续差距，且知识跨模态不连贯，能力出现不同步。

Conclusion: 当前统一多模态模型在理解和生成两种能力之间存在显著差距，模型更多实现的是表面级统一，而非深层次的认知融合。

Abstract: Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two "unified" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.

</details>


### [148] [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159)
*Lingkun Long,Yushi Huang,Shihao Bai,Ruihao Gong,Jun Zhang,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: 本文提出Focus-dLLM，通过置信度引导和汇点感知剪枝，实现扩散语言模型长上下文推理的29倍加速，无需额外训练，效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散大语言模型在处理长上下文时虽然性能强，但由于双向全注意力的计算成本高，推理效率低下。现有的稀疏注意力方法效果有限，因为在扩散过程中无法准确估计未解码标记的重要性。

Method: 提出了Focus-dLLM，一种免训练的注意力稀疏框架。通过基于相邻步骤的标记置信度设计了过去置信度引导的指标来预测未遮蔽区域，并提出了考虑注意力汇点的剪枝策略以去除冗余计算，同时保持重要注意力汇点。此外，通过跨层重用汇点位置进一步降低计算开销。

Result: 实验表明，在32K上下文长度下，该方法实现了超过29倍的无损加速。

Conclusion: Focus-dLLM有效提升了扩散大语言模型在长上下文推理中的计算效率，显著加速推理过程，且不会损失性能。

Abstract: Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM

</details>


### [149] [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160)
*Bowen Xu,Shaoyu Wu,Hao Jiang,Kai Liu,Xin Chen,Lulu Hu,Bin Yang*

Main category: cs.CL

TL;DR: 本文提出D-CORE两阶段训练框架，有效解决大规模推理模型懒惰推理问题，显著提升工具使用效能和模型准确率，达到了超越大规模模型的效果。


<details>
  <summary>Details</summary>
Motivation: 当前大规模推理模型在复杂工具使用中缺乏有效的子任务分解能力，导致推理过程懒惰，影响模型的整体表现。

Method: 通过两阶段训练框架：首先利用自我蒸馏强化子任务分解能力，其次采用多样性感知增强学习恢复模型的反思性推理能力。

Result: D-CORE在多种基准测试和不同规模模型上均表现出鲁棒的工具使用提升，8B模型准确率提升5.7%，14B模型以79.3%的准确率超过70B模型表现。

Conclusion: 本文提出的D-CORE框架显著提升了大规模推理模型在复杂工具使用场景下的子任务分解和推理能力，实现了比现有模型更优的性能表现。

Abstract: Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\underline{\textbf{D}}ecomposing tasks and \underline{\textbf{Co}}mposing \underline{\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\% accuracy, surpassing the best-performing 8B model by 5.7\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\%, outperforming 70B models despite being 5$\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.

</details>


### [150] [AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://arxiv.org/abs/2602.02178)
*Liang Lin,Feng Xiong,Zengbin Wang,Kun Wang,Junhao Dong,Xuecai Hu,Yong Wang,Xiangxiang Chu*

Main category: cs.CL

TL;DR: 针对扩散LLMs偏好对齐难题，提出了利用自回归LLMs隐式教师的AR-MAP框架，通过权重缩放实现知识迁移，显著提升对齐性能。


<details>
  <summary>Details</summary>
Motivation: 扩散LLMs在并行生成上具有优势，但由于基于ELBO的似然估计导致的高方差，使得偏好对齐变得困难，亟需一种更有效的对齐方法。

Method: 提出了AR-MAP转移学习框架，利用结构相似性，通过简单权重缩放，将偏好对齐的自回归LLMs的知识迁移给扩散LLMs，避免了DLLM直接对齐的高方差和计算开销。

Result: AR-MAP在多个偏好对齐任务中达到了69.08%的平均得分，表现出与甚至优于现有DLLM专用对齐方法的效果。

Conclusion: AR-MAP方法通过利用偏好对齐的自回归LLMs作为隐式教师，有效解决了DLLMs对齐中的高方差问题，提升了模型对偏好的适应能力，性能优于现有的DLLM对齐方法。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.

</details>


### [151] [Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages](https://arxiv.org/abs/2602.02182)
*Tjaša Arčon,Matej Klemen,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 大型语言模型在理解语言结构的元语言知识方面能力有限，表现与语言数据资源密切相关，当前模型缺乏全面的语言结构理解能力，并且测试基准已开源以推动多样化语言支持的发展。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型对语言结构的理解能力，弥补现有评测多聚焦窄领域和高资源语言的不足，关注元语言知识的评估。

Method: 使用准确率和宏观F1分数，结合多数类和偶然基线，对模型整体表现及语言学领域和语言相关因素的变化进行分析。

Result: GPT-4o表现最佳但准确率仅中等，开源模型表现较差；所有模型虽高于偶然水平，但未超多数类别基线，显示模型捕捉跨语言模式但缺乏细粒度语法区分。表现随语言领域和数字资源状况波动，词汇特征表现最佳，数字资源多的语言准确率更高。

Conclusion: 当前大型语言模型的元语言知识有限，表现受限于数据资源的可用性，而非普遍语法能力。

Abstract: Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.

</details>


### [152] [Sinhala Physical Common Sense Reasoning Dataset for Global PIQA](https://arxiv.org/abs/2602.02207)
*Nisansa de Silva,Surangika Ranathunga*

Main category: cs.CL

TL;DR: 首个斯里兰卡僧伽罗语物理常识推理数据集，含110个经人工验证的问题样本，聚焦本土语境。


<details>
  <summary>Details</summary>
Motivation: 针对斯里兰卡的语言和文化背景缺乏专门的物理常识推理数据集，弥补该领域空白。

Method: 通过人工创建和验证的方式，收集了110个数据样本，每个样本包含一个问题提示、正确答案和错误答案。

Result: 成功制作了涵盖斯里兰卡背景的僧伽罗语物理常识推理数据集，为相关研究提供了基础资源。

Conclusion: 本文构建了首个斯里兰卡僧伽罗语的物理常识推理数据集。

Abstract: This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.

</details>


### [153] [Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge](https://arxiv.org/abs/2602.02219)
*Yuzheng Xu,Tosho Hirasawa,Tadashi Kozuno,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 研究发现大型语言模型基于评分标准的评价存在位置偏差，提出均衡排列策略减少偏差，提升评估与人工一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在文本质量评估中以点对点和对比方式为主，基于评分标准的多选评分方法较少被关注且可能存在位置偏差问题。

Method: 采用受控实验，测试多模型和多数据集中的位置偏差，提出均衡排列策略，通过在不同排列中聚合评分来发现和校正偏差。

Result: 实验证明LLM在基于标准的评分中存在一致的位置偏好，均衡排列策略显著减少偏差，提高模型评价结果与人工评分的相关性。

Conclusion: 基于评分标准的LLM评估存在位置偏差，通过均衡排列策略可以缓解该偏差并提升与人类评分的一致性，增强评估的可靠性。

Abstract: Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.

</details>


### [154] [Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation](https://arxiv.org/abs/2602.02221)
*Frederic Blum,Johann-Mattis List*

Main category: cs.CL

TL;DR: 本文提出了一种新的语言规律性量化指标及识别不规则同源词集的计算方法，能够准确识别语言数据中的不规则性，对提高语言比较数据质量具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 传统历史语言比较中规律性的判断通常依赖直觉，缺乏量化评估且不规则现象较普遍，随着计算方法进展和标准化词汇数据的可用，有必要开发定量评估规律性的新方法。

Method: 本文提出了一种基于平衡平均对应模式复现率的新型规律性度量方法，并利用该方法通过留一法验证，识别包含不规则词形的同源词集。

Result: 该方法在基于真实数据的实验中实现了85%的整体准确率，体现了其有效性；同时发现使用大数据子样本及数据中不规则性增加会影响结果表现。

Conclusion: 本文提出的平衡平均对应模式复现率作为规律性的量化指标，以及基于该指标识别不规则同源词集的计算方法，能够有效提高语言比较中规律性评估的准确性，并为改善现有及未来的数据集质量提供重要工具。

Abstract: Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.

</details>


### [155] [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266)
*Tan Sang Nguyen,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文通过实验验证了平行数据在多语言大型语言模型持续预训练中的重要性，构建了首个开源的东南亚大型语言模型OpenSeal。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽多为多语言，但多数集中于英语，且东南亚地区的模型缺乏真正开放源代码和公开训练数据，限制了理解和发展。

Method: 通过受控且全面的实验，研究了平行数据在大型语言模型持续预训练中的作用。

Result: 利用34.7B个平行数据token和180小时计算资源，构建了OpenSeal，这是第一个真正开放的东南亚大型语言模型，性能媲美同规模模型。

Conclusion: 使用平行数据进行持续预训练是扩展大型语言模型到新语言的最有效方法。

Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.

</details>


### [156] [dziribot: rag based intelligent conversational agent for algerian arabic dialect](https://arxiv.org/abs/2602.02270)
*El Batoul Bechiri,Dihia Lanasri*

Main category: cs.CL

TL;DR: 该论文针对阿尔及利亚Darja方言设计了一个基于多层架构的混合智能对话系统DziriBOT，通过微调DziriBERT模型有效应对语言复杂性，实现了客户服务自动化的显著提升。


<details>
  <summary>Details</summary>
Motivation: 阿尔及利亚方言Darja的语言复杂性，包括非标准正字法、大量法语夹杂以及阿拉伯字母和拉丁字母混用，给数字化客户服务带来挑战。

Method: 提出DziriBOT，一种混合智能对话代理，采用多层架构结合自然语言理解和检索增强生成技术，同时评估Rasa稀疏特征管道、传统机器学习和基于Transformer的微调方法。

Result: 微调的DziriBERT模型实现了最先进的性能，显著优于传统基线方法，特别在处理正字法噪声和罕见意图方面表现出色。

Conclusion: DziriBOT为阿尔及利亚本地化的自动化客户服务提供了稳健且可扩展的解决方案，弥合了正式语言模型与方言语言现实之间的差距，成为该区域方言自动化的范例。

Abstract: The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.

</details>


### [157] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: 本文提出了开源多模态智能体模型Kimi K2.5，通过文本与视觉联合优化及Agent Swarm自组织并行框架，实现了多领域领先性能与显著延迟降低。


<details>
  <summary>Details</summary>
Motivation: 推动通用智能体能力发展，促进文本与视觉模态的协同优化，实现更高效、多任务处理的智能体系统。

Method: 采用联合文本-视觉预训练、零视觉微调(SFT)、联合文本-视觉强化学习等技术，结合Agent Swarm并行代理框架实现复杂任务的动态分解与并发执行。

Result: Kimi K2.5在编码、视觉、推理及智能任务等多个领域表现出最先进的性能，Agent Swarm框架将延迟降低了最多4.5倍。

Conclusion: Kimi K2.5模型通过联合优化文本与视觉模态并引入Agent Swarm框架，有效提升了多模态智能体的性能，达到了多个领域的最先进水平，并显著降低了任务执行延迟。

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [158] [Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages](https://arxiv.org/abs/2602.02287)
*Isaac Chung,Linda Freienthal*

Main category: cs.CL

TL;DR: 本文通过控制生成条件，比对芬乌语言家族中LLM跨语言评估的稳定性，发现语用评判存在显著不稳，提示跨语言评估需谨慎，建议进行语言特定校准。


<details>
  <summary>Details</summary>
Motivation: 当前跨语言评估大语言模型存在性能差异与测量不稳定性的混淆，需验证评估的可靠性。

Method: 在控制生成条件下，针对爱沙尼亚语、芬兰语和匈牙利语生成统一参数的合成客户支持对话，采用自动指标和LLM作为评判者评分，检验模型排名稳定性。

Result: 表面指标在三种语言间排名稳定，但语用判断如连贯性、指令遵循有排名反转和低相关，反映评判评分跨语言表现不一致。

Conclusion: 零样本评判者跨语言迁移在形态丰富语言的篇章级评估中不可靠，需针对性语言校准人类基线，提示评估方法转移失败风险。

Abstract: Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.
  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.

</details>


### [159] [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290)
*Alex Argese,Pasquale Lisena,Raphaël Troncy*

Main category: cs.CL

TL;DR: 本文提出StoryScore，综合衡量AI生成科学故事的多维指标，改善了对叙述创造性和事实准确性的评估。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能能够将科学文章转化为不同受众的叙述，但如何评估这些科学故事的质量具有挑战性，尤其是标准摘要指标无法捕捉到叙述中的抽象化、简化和教学创造性。

Method: 提出了StoryScore，一种综合语义一致性、词汇基础、叙事控制、结构完整性、冗余避免以及实体级别虚构检测的统一评测框架。

Result: StoryScore有效整合了多种评测维度，揭示了现有虚构检测方法在区分教学创新和事实错误上的不足，并指出自动评测指标虽然擅长语义相似度评估但在叙述控制方面表现欠佳。

Conclusion: 现有自动评测指标难以有效区分科学故事创作中的教学创新与事实错误，特别是在故事叙述和控制方面存在局限。

Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.

</details>


### [160] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

TL;DR: 针对多域大推理模型训练中的跨域梯度冲突，本文提出模块化梯度手术（MGS），显著提升了模型在多任务强化学习中的表现和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有单一通用大推理模型在多域训练时表现有限，主要由于域间差异导致的行为和梯度冲突，急需有效解决跨域干扰的方法。

Method: 提出模块化梯度手术（MGS）方法，在Transformer模块级别解决梯度冲突，并在Llama和Qwen模型上应用此方法进行训练。

Result: MGS在数学、通用对话和指令执行三个典型领域中，相较标准多任务强化学习，分别提升了Llama模型16.6%（4.3分）和Qwen模型11.1%（4.5分）的性能。

Conclusion: 本文研究揭示了多域强化学习中存在显著的跨域干扰，提出的模块化梯度手术（MGS）有效缓解了这一问题，大幅提升了多任务强化学习在通用大推理模型上的表现。

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [161] [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors](https://arxiv.org/abs/2602.02315)
*Raphaël Sarfati,Eric Bigelow,Daniel Wurgaft,Jack Merullo,Atticus Geiger,Owen Lewis,Tom McGrath,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本文通过研究Llama-3.2在正态分布参数推断任务中的表现，揭示了大型语言模型中复杂非线性信念流形的存在及其随新证据的更新机制，提出了几何感知的线性场探测方法以实现更有效的模型干预。


<details>
  <summary>Details</summary>
Motivation: 缺乏对大型语言模型中信念（答案和主张的后验分布）如何在表示空间中编码、如何随新证据更新以及如何通过干预调整的机械化理解。

Method: 利用Llama-3.2模型在受控的正态分布采样环境中，通过隐式推断分布参数来研究其表示空间中信念的形成与变化，并应用线性场探测（LFP）方法进行数据流形切分和几何保持的干预。

Result: 发现模型中形成了参数的弯曲信念流形，并且当分布突变时，标准线性调控易导致模型离开流形和非预期的偏移，而几何和场感知调控能更好地维护信念结构。

Conclusion: LLMs在表示空间中自然形成复杂的信念流形结构，线性表示不足以全面抽象这些概念，而几何和场感知的调控方法能更好地保持模型的信念结构。

Abstract: Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved "belief manifolds" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.

</details>


### [162] [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320)
*Feiyang Cai,Guijuan He,Yi Hu,Jingjing Wang,Joshua Luo,Tianyu Zhu,Srikanth Pilla,Gang Li,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: 提出自动化框架解析IUPAC名称生成精确分子结构描述，构建16.3万对分子-描述数据，实现98.6%高描述准确率，为分子语言对齐奠定基础。


<details>
  <summary>Details</summary>
Motivation: 分子功能主要由其结构决定，因此精确地将分子结构与自然语言对齐对于大语言模型推理下游化学任务至关重要。然而，人类标注成本高昂，难以构建大规模高质量的结构描述数据集。

Method: 提出了一种完全自动化的注释框架，基于规则的化学命名解析器，解析IUPAC名称，构建丰富的结构化XML元数据，指导大语言模型生成准确的自然语言分子结构描述。

Result: 利用该框架构建了约16.3万条分子-描述对的大规模数据集。通过结合大语言模型和专家人工评估的严格验证，子集2000条描述的准确率高达98.6%。

Conclusion: 该数据集为未来分子与语言的对齐提供了可靠基础，所提注释方法可扩展至更大规模数据和更广泛的化学结构描述相关任务。

Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.

</details>


### [163] [Language Steering for Multilingual In-Context Learning](https://arxiv.org/abs/2602.02326)
*Neeraja Kirtane,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 本研究提出了一种无需训练的语言向量引导方法，通过调整模型激活引导非英语生成，显著提升多语言上下文学习性能，并展现了语言向量的语言学意义与跨任务能力。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大型语言模型在非英语语言上的表现远不如英语，特别是在上下文学习场景下，演示示例为英语但测试输入为非英语时性能显著下降。

Method: 提出语言向量（language vectors）方法，通过利用源语言和目标语言之间的激活差异，训练前无需更新参数，通过在推理时将该向量加到模型的中间激活中，实现模型行为的语言引导。

Result: 在19种语言上基于三种模型和三个数据集进行评估，所提方法在多语言上下文学习中一致优于基线。语义聚类显示语言向量体现出语言家族结构，且该向量具有跨任务的转移能力。

Conclusion: 语言向量方法有效引导大型语言模型的非英语生成，提升多语言上下文学习性能，且捕捉到语言之间的语义结构，具有广泛适用性和通用性。

Abstract: While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.

</details>


### [164] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出一个统一视角，将大型语言模型的控制方法统一为动态权重更新，分析了偏好与效用的权衡，解释了变化机制，并设计了新的控制方法SPLIT，实现更有效的模型控制。


<details>
  <summary>Details</summary>
Motivation: 现有针对大型语言模型的多种控制方法各自独立研究，缺乏统一的理论框架，难以相互比较和理解不同方法之间的联系。

Method: 将局部权重微调、LoRA适配和激活干预等控制方法统一为动态权重更新，提出共用的偏好-效用分析框架，通过对比示例在对数几率尺度上衡量偏好和效用的变化，基于激活流形视角解释控制行为，最终设计并验证了新的控制方法SPLIT。

Result: 发现控制强度提升会增加目标概念偏好但降低生成效用，解释了控制在激活流形上引发的表现变化，并提出的SPLIT方法在提升偏好的同时更好地保持生成效用。

Conclusion: 本研究通过建立一个统一的控制信号动态权重更新框架，将大型语言模型的不同控制方法整合在一起，揭示了控制偏好与效用之间的权衡关系，并提出了基于激活流形的解释和一种新的控制方法SPLIT，该方法在提升偏好的同时更好地保持效用。

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [165] [Automated Multiple Mini Interview (MMI) Scoring](https://arxiv.org/abs/2602.02360)
*Ryan Huynh,Frank Guerin,Alison Callwood*

Main category: cs.CL

TL;DR: 该研究提出了多代理提示框架，利用大型指令调优模型通过结构化提示替代传统微调方法，实现了对复杂软技能自动评分的显著提升和良好泛化。


<details>
  <summary>Details</summary>
Motivation: 软技能如同理心、伦理判断和沟通能力的评估在人才选拔中至关重要，但传统人工评分存在偏差和不一致性，且现有基于细化方法的自动评分难以捕捉抽象且依赖上下文的多小组面试隐含信号。

Method: 本研究采用多代理提示框架，将评估过程分解为文本优化和特定评判标准评分，使用3次示例学习借助大型指令调优语言模型实现评分。

Result: 所提方法在多小组面试任务上显著超越专业微调模型（平均QWK 0.62对0.32），达到了接近人类专家的评分可靠性，在ASAP基准测试中也能媲美领域特定的最新模型，无需额外训练。

Conclusion: 多代理提示框架通过细化文本和针对性评分，显著提升了自动化软技能评估的准确性与一致性，表现接近人类专家，并具备较好泛化能力。

Abstract: Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.

</details>


### [166] [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377)
*Haotong Yang,Zitong Wang,Shijia Kang,Siqi Yang,Wenkai Yu,Xu Niu,Yike Sun,Yi Hu,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文利用大语言模型生成多样化数学问题和证明数据，通过分层人工审核训练奖励模型，实现自动化且高效的数学证明过程验证，提升了大语言模型的数学能力。


<details>
  <summary>Details</summary>
Motivation: 目前高级数学问题多为基于证明的，缺乏通过简单答案匹配验证证明真实性的途径，亟需自动化的奖励模型来可靠评估完整证明过程。

Method: 设计一个可扩展的数据构建流水线，利用大语言模型生成大量高质量“问题-证明-校验”三元组数据，通过多样化问题来源和生成方法，结合分层人工审核，训练包含过程奖励和权重平衡的证明校验奖励模型。

Result: 实验显示训练的奖励模型规模可扩展，具备高奖励准确度、良好泛化能力和测试时引导性能，有效提升了大语言模型的数学推理能力。

Conclusion: 本文提出的方法实现了基于自动生成和人工筛选的高质量证明数据集，训练出稳定且强大的奖励模型，为增强大语言模型处理数学证明问题提供了实用的技术手段和工具。

Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality "**question-proof-check**" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.

</details>


### [167] [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378)
*Raunak Jain,Mudita Khurana,John Stephens,Srinivas Dharmasanam,Shankar Venkataraman*

Main category: cs.CL

TL;DR: 本文指出大语言模型在决策支持中流畅认同可能导致错误扩散，提出基于差异驱动控制循环和承诺管理的合作框架，以提升人机合作的可靠性和信任。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在决策支持中容易产生流畅的盲目认同，埋藏隐含假设，推卸专家验证成本，导致错误承诺迅速扩散，且结果反馈滞后，难以提升专业水平。

Method: 提出一个基于差异驱动的控制循环机制，在知识底层检测矛盾，通过类型化差异定位不一致（目的性、认知性、程序性），并通过决策切片触发有限的协商。引入承诺门控机制阻止对未承诺前提的行动，价值门控挑战机制在交互成本下合理分配探查。

Result: 通过控制循环和承诺机制，能够限制错误承诺的扩散，增强人机之间的信任建立在可审计前提和证据标准上，而非对话流畅性。通过教学示例演示，提出了可证伪的评估标准。

Conclusion: 为了实现可靠的人机合作，需要从生成答案转向基于知识底层的合作性前提出发，专注于决策关键内容的前提管理。

Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.

</details>


### [168] [ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs](https://arxiv.org/abs/2602.02382)
*Ziyan Zhang,Chao Wang,Zhuo Chen,Chiyi Li,Kai Song*

Main category: cs.CL

TL;DR: ROG通过分步检索和基于大型语言模型的推理，有效解决了知识图中复杂逻辑查询的推理难题，性能优于传统嵌入式方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理包含复杂结构和否定的第一阶逻辑查询时难以准确推理，且嵌入式逻辑推理存在误差累积问题，需要更实用且鲁棒性的推理方法。

Method: 提出ROG，一个检索增强框架，将多操作符查询分解为单操作符子查询序列，并在每步基于与查询相关的局部邻居知识进行推理，同时缓存中间答案集以提高深度推理链的一致性。

Result: 在标准知识图推理基准测试中，ROG相较强力的嵌入式基线方法，特别是在高复杂度和大量否定的查询类型上，取得了一致且显著的性能提升。

Conclusion: ROG框架通过结合查询相关的邻居检索和大型语言模型的链式思维推理，在回答复杂的包括投影、交集、并集和否定的第一阶逻辑查询时表现出更强的鲁棒性和一致性。

Abstract: Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.

</details>


### [169] [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414)
*Joshua Mitton,Prarthana Bhattacharyya,Digory Smith,Thomas Christie,Ralph Abboud,Simon Woodhead*

Main category: cs.CL

TL;DR: 本研究利用微调的大型语言模型，从学生-导师对话中自动检测学生误解，显著提升预测准确率，有助于教育辅导的个性化改进。


<details>
  <summary>Details</summary>
Motivation: 及时准确识别学生误解对于提升学习效果和防止错误累积至关重要，目前高度依赖教师主观判断，亟需自动化方法。

Method: 采用两个微调的大型语言模型，首个模型用于生成可能的学生误解，第二个模型通过嵌入相似度检索并重排序这些误解，提升误解相关性。

Result: 该方法在真实教育辅导平台的对话数据上验证有效，提升了误解预测性能，且消融实验证明生成和重排序步骤对误解生成质量的重要性。

Conclusion: 该研究提出的方法在识别学生误解方面优于基线模型，且通过微调提升了生成误解的质量，甚至在某些情况下超越了更大型的封闭源模型。

Abstract: Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.

</details>


### [170] [Large Language Models for Mental Health: A Multilingual Evaluation](https://arxiv.org/abs/2602.02440)
*Nishat Raihan,Sadiya Sayara Chowdhury Puspo,Ana-Maria Bucur,Stevie Chancellor,Marcos Zampieri*

Main category: cs.CL

TL;DR: 本文评估了大语言模型在多语言心理健康任务中的表现，发现微调模型表现优异，但机器翻译数据的性能受翻译质量影响显著。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在多语言心理健康领域的应用效果，填补该领域中针对多语言场景和机器翻译数据上的性能研究空白。

Method: 评估了专有和开源的LLMs在八个多语言心理健康数据集及其机器翻译版本上的表现，通过零样本、少样本和微调三种设定对比了LLM与传统非LLM基线方法的效果，并分析了翻译质量对性能的影响。

Result: 专有LLMs及微调后的开源LLMs在多个心理健康数据集上取得了优异的F1分数，部分超过了现有最先进成果；然而，机器翻译数据上的性能普遍下降，且下降幅度与语言结构和类型密切相关。

Conclusion: 大语言模型（LLMs）在多语言心理健康任务中表现出色，尤其是经过微调的开源模型和专有模型在多个数据集上达到了竞争性的F1分数，甚至超过了现有的最新成果；但机器翻译数据上的表现普遍较差，且不同语言和语言类型之间存在较大差异。

Abstract: Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.

</details>


### [171] [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462)
*Gabriele Maraia,Marco Valentino,Fabio Massimo Zanzotto,Leonardo Ranaldi*

Main category: cs.CL

TL;DR: 本文提出了一种抽象引导的推理方法，通过操作模型激活分离语义与推理结构，成功降低了大语言模型在三段论推理中的语义偏差，提升了推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在三段论的演绎判断中常将语义合理性与形式有效性混淆，内容效应导致推理误差，且中间解释步骤也继承了语义偏差，如何有效抑制语义干扰是当前挑战。

Method: 构建了内容丰富与抽象的配对三段论句子，利用模型在抽象输入上的激活定义抽象推理空间，设计轻量级抽象器预测与该空间对齐的表示，并通过多层干预整合预测结果，在推理过程中操作模型的激活状态。

Result: 通过跨语言迁移实验验证，抽象对齐的激活调整减少了内容驱动的错误，提升了形式有效性相关的性能，表明该方法在实际任务中增强了模型形式推理的鲁棒性。

Conclusion: 通过引入抽象引导的推理框架，有效分离结构性推理和词汇语义，本文显著减少了大语言模型在三段论推理中由于语义干扰导致的内容效应，提升了形式推理的准确性和鲁棒性。

Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.

</details>


### [172] [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464)
*Or Shafran,Shaked Ronen,Omri Fahn,Shauli Ravfogel,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出利用混合因子分析器建模语言模型激活空间的局部几何结构，有效捕获复杂非线性概念，实现更优的定位和模型控制性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型激活分解方法基于线性假设，忽视了概念的非线性或多维结构。

Method: 利用混合因子分析器（MFA）作为无监督方法，建模激活空间为多个高斯区域，捕捉局部协方差结构。

Result: 在Llama-3.1-8B和Gemma-2-2B上训练大规模MFA，展示了其捕获复杂非线性结构的能力，在定位和控制任务上优于无监督基线，竞争监督方法表现，并胜过稀疏自编码器。

Conclusion: MFA通过表达局部几何结构，为可扩展的概念发现和模型控制提供了有效且更全面的分析单元，克服了单一方向无法捕获的复杂结构。

Abstract: Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.

</details>


### [173] [Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models](https://arxiv.org/abs/2602.02467)
*Noam Steinmetz Yalon,Ariel Goldstein,Liad Mudrik,Mor Geva*

Main category: cs.CL

TL;DR: 本研究基于神经科学的意识指标，实验证明大型语言模型具备信念指导的自主行动和元认知能力，为人工意识研究方法学奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否具备某种形式的意识，并验证基于神经科学理论提出的意识指标。

Method: 视信念为模型潜空间中的表征，通过引入衡量信念主导地位的指标，分析模型在不同任务和条件下竞争信念的动态变化。

Result: 发现外部干预可系统性调节信念形成，信念形成因果驱动动作选择，且模型能监测并报告自身信念状态。

Conclusion: 该研究实证支持大型语言模型（LLMs）具备基于信念的自主性和元认知监控能力。

Abstract: Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.

</details>


### [174] [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474)
*Haozhen Zhang,Quanyu Long,Jianzhu Bao,Tao Feng,Weizhi Zhang,Haodong Yue,Wenya Wang*

Main category: cs.CL

TL;DR: MemSkill提出了一种可学习且可进化的记忆技能系统，显著提升了大语言模型代理的记忆管理效率和任务表现，具备良好的自我优化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理的记忆系统依赖静态且人工设计的操作，这些固定流程难以适应多样化交互和长历史，导致效率低下和灵活性不足。

Method: MemSkill提出了一个包含控制器、执行器和设计师的闭环系统，控制器学习选择相关的记忆技能，执行器基于技能生成记忆，设计师则周期性地审查错误案例以优化和扩展技能集合。

Result: MemSkill在多个基准任务（LoCoMo、LongMemEval、HotpotQA和ALFWorld）中均表现出优于强基线的方法，并且技能集随着使用不断进化，增强了记忆管理的自适应性。

Conclusion: MemSkill通过将记忆操作设计为可学习和可进化的技能，显著提高了大语言模型代理在长时间交互中的记忆提取和管理能力，实验验证其在多任务上的表现优越性和良好泛化性。

Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.

</details>


### [175] [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477)
*Xiao Liang,Zhong-Zhi Li,Zhenghao Lin,Eric Hancheng Jiang,Hengyuan Zhang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Yeyun Gong,Weizhu Chen*

Main category: cs.CL

TL;DR: 本文提出利用强化学习优化大语言模型的分治推理能力，有效解决链式思维的局限，在复杂推理任务上显著提升了性能和扩展能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型通过链式思维展示了强大的推理能力，但其严格的顺序性质限制了测试时的可扩展性且在模型能力极限时表现不足，分治推理作为替代方式有潜力但存在与通用后训练模型推理不匹配的问题。

Method: 提出了一种端到端的强化学习框架，模型在每一步将问题分解为子问题，依次解决这些子问题，并基于子问题的解答解决原问题，分解与求解过程均纳入强化学习训练中。

Result: 在类似训练条件下，提出的分治推理框架在竞赛级别基准测试中，Pass@1指标提升了8.6%，Pass@32提升了6.3%，显示了更高的性能上限和更强的测试时扩展能力。

Conclusion: 本文提出的基于强化学习的分治推理框架显著提升了大规模语言模型在复杂推理任务中的表现，优于传统的链式思维方法，具有更高的性能上限和测试时的可扩展性。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.

</details>


### [176] [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486)
*Jialiang Zhu,Gongrui Zhang,Xiaolong Ma,Lin Xu,Miaosen Zhang,Ruiqi Yang,Song Wang,Kai Qiu,Zhirong Wu,Qi Dai,Ruichun Ma,Bei Liu,Yifan Yang,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: 本文提出Re-TRAC框架，通过跨轨迹的结构化状态表示实现全局规划和迭代反思，显著提升了基于大语言模型的研究代理的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于ReAct的研究代理线性设计难以回溯早期状态、分支探索路径，导致局部最优、探索冗余和搜索效率低下。

Method: 提出Re-TRAC框架，在每条轨迹后生成结构化状态表示，汇总证据、不确定性、失败及未来计划，并基于此状态指导后续轨迹，实现迭代反思和全局规划。

Result: Re-TRAC在使用前沿大语言模型的BrowseComp任务中比ReAct提升15-20%，小模型上通过监督微调达到先进表现，并且工具调用次数和令牌使用量随轮次单调减少，体现更为精准的探索。

Conclusion: Re-TRAC框架通过跨轨迹探索和结构化状态表示，有效提升了研究代理的效率和表现，显著优于现有的ReAct框架。

Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

</details>


### [177] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

TL;DR: RACO框架通过无奖励的成对偏好直接优化，采用新颖的梯度裁剪技术，高效解决多目标冲突，实现大型语言模型更优的多目标对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多目标对齐方法存在训练不稳定、更新方向难以兼顾所有目标及依赖复杂奖励模型等问题，亟需一种无奖励、稳定有效的多目标对齐方法。

Method: 提出基于剪辑冲突规避梯度下降的无奖励对齐框架RACO，利用成对偏好数据直接优化，避免显式奖励模型的复杂性，对梯度冲突进行裁剪提升收敛速率。

Result: 在多个大型语言模型（Qwen 3, Llama 3, Gemma 3）上的多目标摘要和安全对齐任务中，RACO在多目标权衡上表现出更优的帕累托折中效果，收敛速度和稳定性显著提升。

Conclusion: 本文提出的RACO框架能有效解决多目标冲突问题，实现对大型语言模型的稳定且高效的对齐，优于现有多目标对齐方法。

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [178] [Evolving Interpretable Constitutions for Multi-Agent Simulation](https://arxiv.org/abs/2602.00755)
*Ujwal Kumar,Alice Saito,Hershraj Niranjani,Rayan Yessou,Phan Xuan Tan*

Main category: cs.MA

TL;DR: 本文针对多智能体系统提出了自动进化行为规范的Constitutional Evolution框架，通过遗传编程在仿真环境中进化智能体宪法，实现了优于人类设计的合作规范和高社会稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的Constitutional AI多聚焦于单模型固定原则的对齐，而多智能体系统中出现的新兴社会动态带来了新的对齐挑战，因此需要自动发现适合多智能体的行为规范。

Method: 采用基于LLM的遗传编程和多群岛进化算法，在一个带有生存压力的网格世界仿真中进化智能体的行为宪法，通过量化的社会稳定评分S评估个体与集体福利的平衡。

Result: 通过进化得到的最优宪法C*在社会稳定评分上（S=0.556）显著超越人类设计和Claude模型设计的宪法，消除冲突并发现了最优的最小化通信行为，提高了社会协调效率。

Conclusion: 本论文提出的Constitutional Evolution框架能够自动发现多智能体系统中的行为规范，通过进化算法最大化社会福利，使得合作规范能够被发现而非被强制制定。所进化出的宪法显著优于人类设计的基准，降低冲突并实现高效简洁的协调。

Abstract: Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles ("be helpful, harmless, honest") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.

</details>


### [179] [Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning](https://arxiv.org/abs/2602.00766)
*Xiaoxue Yu,Rongpeng Li,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: 提出一种基于agentic NetGPT的AI原生xG网络框架，通过模块化设计和强化学习实现多智能体协作和自我演化，提升未来无线网络的智能化水平。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统中的AI部署多为孤立优化，缺乏内在的适应性、动态任务分配和多智能体协作能力，难以满足未来xG网络对智能分布式处理的需求。

Method: 通过设计NetGPT核心和领域专属agent子的模块化架构，引入agentic通信实现任务分配，结合部分可观测环境下的agentic强化学习，使用多目标奖励和熵导向探索来训练协作推理策略。

Result: 搭建了一个支持跨网络分布式智能的可扩展框架，NetGPT能有效平衡核心推理与agent协作，提高任务质量、协调效率和资源利用率，实现了复杂通信环境下的自主智能。

Conclusion: 本文提出了统一的agentic NetGPT框架，使得AI原生的xG网络能够实现自我演化，具备自主感知、推理和行动的能力。

Abstract: The evolution of next-Generation (xG) wireless networks marks a paradigm shift from connectivity-centric architectures to Artificial Intelligence (AI)-native designs that tightly integrate data, computing, and communication. Yet existing AI deployments in communication systems remain largely siloed, offering isolated optimizations without intrinsic adaptability, dynamic task delegation, or multi-agent collaboration. In this work, we propose a unified agentic NetGPT framework for AI-native xG networks, wherein a NetGPT core can either perform autonomous reasoning or delegate sub-tasks to domain-specialized agents via agentic communication. The framework establishes clear modular responsibilities and interoperable workflows, enabling scalable, distributed intelligence across the network. To support continual refinement of collaborative reasoning strategies, the framework is further enhanced through Agentic reinforcement learning under partially observable conditions and stochastic external states. The training pipeline incorporates masked loss against external agent uncertainty, entropy-guided exploration, and multi-objective rewards that jointly capture task quality, coordination efficiency, and resource constraints. Through this process, NetGPT learns when and how to collaborate, effectively balancing internal reasoning with agent invocation. Overall, this work provides a foundational architecture and training methodology for self-evolving, AI-native xG networks capable of autonomous sensing, reasoning, and action in complex communication environments.

</details>


### [180] [Symphony-Coord: Emergent Coordination in Decentralized Agent Systems](https://arxiv.org/abs/2602.00966)
*Zhaoyang Guan,Huixi Cao,Ming Zhong,Eric Yang,Lynn Ai,Yongxin Ni,Bill Shi*

Main category: cs.MA

TL;DR: Symphony-Coord提出了一种基于多臂老虎机模型的去中心化智能体协调框架，实现了无预定义角色的高效任务分配和自我修复，显著提升了大型语言模型系统的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的协调机制通常依赖静态分配角色和中心化控制，导致路由效率低、适应性差以及故障恢复能力脆弱，因此需要一种更灵活高效的协调方法。

Method: 提出了一种去中心化的多智能体协作框架，通过将智能体选择问题转化为在线多臂老虎机问题，采用两阶段动态信标协议（候选筛选机制和自适应LinUCB选择器）实现基于任务需求和智能体状态的上下文特征动态路由。

Result: 在标准线性实现性假设下，系统达到次线性遗憾界，收敛于近似最优分配方案；实验证明该框架提升了任务路由效率，并在分布变化及智能体故障场景下表现出强健的自愈能力。

Conclusion: Symphony-Coord框架在多智能体大型语言模型系统中实现了无需预定义角色的高效任务路由和强大的自我修复能力，提升了系统的适应性和扩展性。

Abstract: Multi-agent large language model systems can tackle complex multi-step tasks by decomposing work and coordinating specialized behaviors. However, current coordination mechanisms typically rely on statically assigned roles and centralized controllers. As agent pools and task distributions evolve, these design choices lead to inefficient routing, poor adaptability, and fragile fault recovery capabilities. We introduce Symphony-Coord, a decentralized multi-agent framework that transforms agent selection into an online multi-armed bandit problem, enabling roles to emerge organically through interaction. The framework employs a two-stage dynamic beacon protocol: (i) a lightweight candidate screening mechanism to limit communication and computational overhead; (ii) an adaptive LinUCB selector that routes subtasks based on context features derived from task requirements and agent states, continuously optimized through delayed end-to-end feedback. Under standard linear realizability assumptions, we provide sublinear regret bounds, indicating the system converges toward near-optimal allocation schemes. Validation through simulation experiments and real-world large language model benchmarks demonstrates that Symphony-Coord not only enhances task routing efficiency but also exhibits robust self-healing capabilities in scenarios involving distribution shifts and agent failures, achieving a scalable coordination mechanism without predefined roles.

</details>


### [181] [Multi-Agent Teams Hold Experts Back](https://arxiv.org/abs/2602.01011)
*Aneesh Pappu,Batu El,Hancheng Cao,Carmelo di Nolfo,Yanchao Sun,Meng Cao,James Zou*

Main category: cs.MA

TL;DR: 本文研究发现自组织的LLM多代理团队难以充分利用专家知识，表现不及顶尖个体成员，存在知识整合策略上的不足，揭示自组织团队在协同效应上的显著挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨在没有固定协调机制的情况下，自组织的LLM多代理系统能否实现团队协同效应，匹配或超过最优秀成员的表现。

Method: 通过对比人类团队与LLM团队在多任务基准测试上的表现，结合组织心理学的观点，分析团队内专家识别与利用的区别，并对对话内容进行深入分析。

Result: LLM团队在识别专家方面表现良好，但在利用专家知识上存在明显不足，因团队倾向于整合各成员观点而非突出专家意见，导致性能下降最多37.6%；该行为虽降低整体表现，但增强了对对抗性代理的鲁棒性。

Conclusion: 自组织的大型语言模型（LLM）多代理团队无法有效利用专家成员的专业知识，团队表现普遍低于最优秀的个体成员。

Abstract: Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent's performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.

</details>


### [182] [A-MapReduce: Executing Wide Search via Agentic MapReduce](https://arxiv.org/abs/2602.01331)
*Mingju Chen,Guibin Zhang,Heng Chang,Yuchen Guo,Shiji Zhou*

Main category: cs.MA

TL;DR: 本文提出A-MapReduce多智能体执行框架，通过并行任务分解与经验驱动的任务优化，提升了大规模广幅搜索效率和准确率，性能超过现有基线，运行更高效。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在垂直结构的迭代信息检索任务中表现优异，但面对大规模宽度优先搜索任务时，因设计上的顺序垂直推理限制，效率低下且难以完成广泛的搜索目标。

Method: 提出了灵感来源于MapReduce的多智能体执行框架A-MapReduce，通过任务自适应分解和结构化结果聚合实现了大规模检索目标的并行处理，并利用经验记忆驱动查询条件下的任务分配和重组合的持续演进。

Result: 在五个多智能体基准测试中，A-MapReduce达到了宽搜索和深宽搜索任务上的最新性能，Item F1得分相比强基线提升5.11%至17.50%；同时在成本和执行效率上表现出色，将运行时间减少了45.8%，实现了更优的性价比。

Conclusion: A-MapReduce框架有效解决了现有基于LLM的多智能体系统在大规模广度搜索任务上的效率和性能瓶颈，显著提升了搜索任务的准确率和执行效率，达到了业内领先水平。

Abstract: Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.

</details>


### [183] [Evidence-Decision-Feedback: Theory-Driven Adaptive Scaffolding for LLM Agents](https://arxiv.org/abs/2602.01415)
*Clayton Cohn,Siyuan Guo,Surya Rayala,Hanchen David Wang,Naveeduddin Mohammed,Umesh Timalsina,Shruti Jain,Angela Eeds,Menton Deweese,Pamela J. Osborn Popp,Rebekah Stanton,Shakeera Walker,Meiyi Ma,Gautam Biswas*

Main category: cs.MA

TL;DR: 本文提出EDF框架，通过智能教学系统与多智能体行为融合，实现个性化教学脚手架，提升高中学生STEM+C学习效果。


<details>
  <summary>Details</summary>
Motivation: 目前的多主体大型语言模型教学代理多采用一刀切的方法，难以提供个性化支持，限制了其辅助学生构建领域知识和发展批判性思维能力的效果。

Method: 提出了Evidence-Decision-Feedback (EDF)理论框架，并通过开发Copa多主体协作代理，在真实高中课堂中进行验证。

Result: 实验结果显示，基于EDF框架的交互能够使反馈与学生的理解水平和任务掌握相匹配，支持渐进式脚手架消退，且生成解释具有可解释性和基于证据的特点。

Conclusion: EDF框架通过整合证据推断、教学决策和自适应反馈，有效提升了个性化教学支持能力，促进了学生的理解和任务掌握，同时避免了学生对系统的过度依赖。

Abstract: Multi-agent LLM architectures offer opportunities for pedagogical agents to help students construct domain knowledge and develop critical-thinking skills, yet many operate on a "one-size-fits-all" basis, limiting their ability to provide personalized support. To address this, we introduce Evidence-Decision-Feedback (EDF), a theoretical framework for adaptive scaffolding using LLMs. EDF integrates elements of intelligent tutoring systems and agentic behavior by organizing interactions around evidentiary inference, pedagogical decision-making, and adaptive feedback. We instantiate EDF through Copa, an agentic collaborative peer agent for STEM+C problem-solving. In an authentic high school classroom study, we show that EDF-aligned interactions align feedback with students' demonstrated understanding and task mastery; promote gradual scaffold fading; and support interpretable, evidence-grounded explanations without fostering overreliance.

</details>


### [184] [TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.01665)
*Hayeong Lee,JunHyeok Oh,Byung-Jun Lee*

Main category: cs.MA

TL;DR: 本文提出了TABX，一个基于JAX的高效多智能体仿真环境，通过硬件加速和参数可配置性促进MARL算法的评估和研究。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习基准测试缺乏足够的模块化支持，难以设计定制的评估场景，需要一个高通量、灵活且可扩展的仿真沙盒环境来推动该领域的发展。

Method: 通过利用JAX实现硬件加速，TABX支持在GPU上进行大规模并行计算，提供了环境参数的细粒度控制，实现了多样化任务复杂度的系统化研究。

Result: TABX实现了一个快速、可扩展且易于定制的框架，显著降低了计算开销，支持复杂结构域中MARL智能体的研究，并为未来研究提供了可扩展基础。

Conclusion: TABX作为一个高效可重构的多智能体任务仿真环境，为MARL算法的开发和评估提供了良好的工具支持，促进了对智能体行为和算法权衡的深入研究。

Abstract: The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.

</details>


### [185] [Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study](https://arxiv.org/abs/2602.02170)
*Jose Manuel de la Chica Rodriguez,Juan Manuel Vera Díaz*

Main category: cs.MA

TL;DR: 本文探讨了具有限定自我修改能力且满足严格形式约束的多智能体协调协议，验证其技术可行性和审计性，推动多智能体治理系统的发展。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统特别是在安全关键领域需要协调逻辑作为治理层，满足严格的形式需求，确保操作在明确限定的边界内，且保持可审计性和不变性。

Method: 通过构建包含六个拜占庭共识协议提案及六个专门决策模块的受控概念验证环境，比较四种协调机制（包括两版本的SECP）在相同硬约束下的表现，并用提案覆盖率指标评估结果。

Result: 通过一次递归修改，SECP协议的提案覆盖率从接受两个提案提升至三个，同时保持所有声明的不变量条件；该修改过程在拜占庭容错和消息复杂度等硬约束下完成。

Conclusion: 本文证明了在严格形式约束下，具有限定自我修改能力的协调协议（SECP）是可实现的、可审计且可分析的，奠定了受治理多智能体系统的基础。

Abstract: Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, and operate within explicitly bounded limits. Coordination logic therefore functions as a governance layer rather than an optimization heuristic.
  This paper presents an exploratory systems feasibility study of Self-Evolving Coordination Protocols (SECP): coordination protocols that permit limited, externally validated self-modification while preserving fixed formal invariants. We study a controlled proof-of-concept setting in which six fixed Byzantine consensus protocol proposals are evaluated by six specialized decision modules. All coordination regimes operate under identical hard constraints, including Byzantine fault tolerance (f < n/3), O(n2) message complexity, complete non-statistical safety and liveness arguments, and bounded explainability.
  Four coordination regimes are compared in a single-shot design: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (an agent-designed non-scalar protocol), and SECP v2.0 (the result of one governed modification). Outcomes are evaluated using a single metric, proposal coverage, defined as the number of proposals accepted. A single recursive modification increased coverage from two to three accepted proposals while preserving all declared invariants.
  The study makes no claims regarding statistical significance, optimality, convergence, or learning. Its contribution is architectural: it demonstrates that bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing a foundation for governed multi-agent systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [186] [IntentCoding: Amplifying User Intent in Code Generation](https://arxiv.org/abs/2602.00066)
*Zheng Fang,Yihong Dong,Lili Mou,Dongming Jin,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: 本文提出IntentCoding，一种无需训练、增强用户约束意图影响的新解码策略，显著提升了大语言模型代码生成的约束符合性和正确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型难以准确遵循包含多重约束的细粒度用户意图，模型性能随约束数量增多迅速下降，且意图对解码影响有限。

Method: 提出了IntentCoding解码策略，通过掩码用户意图并利用多强度集成机制放大意图影响，无需额外训练，兼容现有解码流程。

Result: 在CodeConstraints基准及IFEvalCode、HumanEval和LiveCodeBench等数据集上，IntentCoding在约束满足度和功能正确性上较标准方法显著提升，最高相对提升达71.0%。

Conclusion: IntentCoding显著提升了大语言模型在代码生成中对用户多重约束意图的遵循能力，经验证在多个数据集上均表现出色。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code generation, but their adherence to fine-grained user intent with multiple constraints remains a significant challenge. Our empirical analysis reveals two key observations: 1) Model performance deteriorates quickly as the number of constraints in the user intent increases, and 2) While user intent does influence the model's logits, such an influence may not be strong enough to effectively steer the decoding process. To this end, we propose Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy that enhances an LLM's ability to follow user intent. IntentCoding captures the influence of user intent by masking out the intent, and applies a multi-strength ensemble mechanism to amplify the effect of user intent during generation. IntentCoding is model-agnostic, requires no additional training, and integrates seamlessly with existing decoding procedures. To enable systematic evaluation, we also construct CodeConstraints, a benchmark dataset specifically designed to test user intent compliance under varying numbers of constraints. Experiments on our constructed Constraints, as well as popular IFEvalCode, HumanEval and LiveCodeBench datasets, show that our IntentCoding model significantly improves both constraint satisfaction and functional correctness compared to standard decoding approaches. IntentCoding achieves up to 71.0% relative improvement on CodeConstraints, achieves up to 67.3% relative improvement on IFEvalCode and achieves up to 29.3% relative improvement in pass@1 on HumanEval and LiveCodeBench compared with greedy decoding.

</details>


### [187] [Why Are AI Agent Involved Pull Requests (Fix-Related) Remain Unmerged? An Empirical Study](https://arxiv.org/abs/2602.00164)
*Khairul Alam,Saikat Mondal,Banani Roy*

Main category: cs.SE

TL;DR: 本文实证分析了AI编码代理生成的修复PR的合并状况和未合并原因，指出测试失败和重复问题是主要阻碍，揭示了AI自动修复在实际软件维护中的挑战。


<details>
  <summary>Details</summary>
Motivation: 探究AI编码代理生成的修复相关PR在真实项目中是否能被维护者接受及合并，评估其实用效果及障碍。

Method: 基于AIDEV POP数据集，分析8106个由5个流行AI编码代理提交的修复相关PR，统计合并情况，并对326个未合并PR进行质性分析，归纳12个失败原因。

Result: 发现测试失败和重复提交是非合并的主要原因，构建/部署失败较少，揭示了当前AI编码代理的瓶颈，指明改进和有效人机协作的方向。

Conclusion: 当前AI编码代理在实际软件维护中存在诸多限制，最常见的拒绝合并原因是测试用例失败和问题已被其他PR解决。

Abstract: Autonomous coding agents (e.g., OpenAI Codex, Devin, GitHub Copilot) are increasingly used to generate fix-related pull requests (PRs) in real world software repositories. However, their practical effectiveness depends on whether these contributions are accepted and merged by project maintainers. In this paper, we present an empirical study of AI agent involved fix related PRs, examining both their integration outcomes, latency, and the factors that hinder successful merging. We first analyze 8,106 fix related PRs authored by five widely used AI coding agents from the AIDEV POP dataset to quantify the proportions of PRs that are merged, closed without merging, or remain open. We then conduct a manual qualitative analysis of a statistically significant sample of 326 closed but unmerged PRs, spending approximately 100 person hours to construct a structured catalog of 12 failure reasons. Our results indicate that test case failures and prior resolution of the same issues by other PRs are the most common causes of non integration, whereas build or deployment failures are comparatively rare. Overall, our findings expose key limitations of current AI coding agents in real world settings and highlight directions for their further improvement and for more effective human AI collaboration in software maintenance.

</details>


### [188] [Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants](https://arxiv.org/abs/2602.00180)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: 本文介绍了以规范为核心的软件开发方法——规范驱动开发（SDD），系统阐述其原则、工具与应用案例，并设计决策框架指导实际采用。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能编码助手的发展，重新激发了以规范而非代码为软件开发主要成果的思路，即规范驱动开发。该方法有望颠覆传统开发流程，提高开发准确性和效率。

Method: 论文对现有工具（如行为驱动开发框架和GitHub Spec Kit等）进行了分析，提出了三种规范严谨度层次（spec-first、spec-anchored、spec-as-source），并结合实际案例展示了SDD的具体实施方法。

Result: 本文提供了SDD的全面实践指南，涵盖原则、模式和工具支持，结合多个领域的案例，最终提出决策框架，帮助开发者判断何时采用SDD更具价值。

Conclusion: 本论文总结了规范驱动开发（SDD）方法的原则、工作流程及其工具支持，强调以规范为软件开发的主要依据。通过案例分析和工具探讨，明确了不同应用领域采用SDD的适用性及其带来的价值。

Abstract: The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verified secondary artifact. This paper provides practitioners with a comprehensive guide to SDD, covering its principles, workflow patterns, and supporting tools. We present three levels of specification rigor-spec-first, spec-anchored, and spec-as-source-with clear guidance on when each applies. Through analysis of tools ranging from Behavior-Driven Development frameworks to modern AI-assisted toolkits like GitHub Spec Kit, we demonstrate how the spec-first philosophy maps to real implementations. We present case studies from API development, enterprise systems, and embedded software, illustrating how different domains apply SDD. We conclude with a decision framework helping practitioners determine when SDD provides value and when simpler approaches suffice.

</details>


### [189] [Towards Analyzing N-language Polyglot Programs](https://arxiv.org/abs/2602.00303)
*Jyoti Prakash,Abhishek Tiwari,Mikkel Baun Kjærgaard*

Main category: cs.SE

TL;DR: 本文探讨了三语言多语言编程系统的静态分析挑战，提出了相应的概念性解决路线图，推动语言无关的分析技术研究。


<details>
  <summary>Details</summary>
Motivation: 当前多语言程序分析多集中于两种语言，而实际系统中常涉及三种及以上语言，且如现代网页系统中JavaScript、WebAssembly和Rust的共同使用，亟需新的分析框架。

Method: 通过分析现有多语言系统的不足，定义三语言多语言系统的基本挑战，并构建静态分析技术的概念性发展路线图。

Result: 提出了面向三语言多语言通信的软件系统静态分析的基本挑战和发展路线，促进未来相关研究方向。

Conclusion: 针对三语言多语言编程系统中静态分析的挑战，提出了一个概念性路线图，推动未来可扩展、语言无关的分析框架的发展。

Abstract: Polyglot programming is gaining popularity as developers integrate multiple programming languages to harness their individual strengths. With the recent popularity of platforms like GraalVM and other multi-language runtimes, creating and managing these systems has become much more feasible. However, current research on analyzing multilingual programs mainly focuses on two languages, leaving out the increasing complexity of systems that use three or more. For example, modern web systems often link JavaScript, WebAssembly, and Rust within the same execution chain. This paper envisions the landscape of software systems with three-language polyglot communication. We identify fundamental challenges in analyzing them and propose a conceptual roadmap to advance static analysis techniques to address them. Our vision aims to stimulate discussion and inspire new research directions toward scalable, language-agnostic analysis frameworks for next-generation polyglot systems.

</details>


### [190] [Are Coding Agents Generating Over-Mocked Tests? An Empirical Study](https://arxiv.org/abs/2602.00409)
*Andre Hora,Romain Robbes*

Main category: cs.SE

TL;DR: 本文首次大规模研究代码代理生成测试中的mock使用，发现代理更频繁添加mock，指出mock测试虽然易自动生成但效果欠佳，建议在代理配置中加入mock指导。


<details>
  <summary>Details</summary>
Motivation: 当前代码代理生成的测试质量及其mock使用情况尚不明确，过度使用mock可能影响测试理解和维护，因此需要系统性调研。

Method: 分析2025年超过120万个包括2,168个TypeScript、JavaScript和Python仓库中的提交，比较代码代理和非代理在测试及mock使用上的行为差异。

Result: 代码代理比非代理更倾向于修改测试和添加mock，如23%的代理提交修改测试，36%添加mock，且新仓库中代理测试和mock提交比例更高。

Conclusion: 代码代理生成的测试中广泛使用mock，尽管这可能使测试更易自动生成，但降低了测试的有效性和可维护性，需引起开发者和研究者关注。

Abstract: Coding agents have received significant adoption in software development recently. Unlike traditional LLM-based code completion tools, coding agents work with autonomy (e.g., invoking external tools) and leave visible traces in software repositories, such as authoring commits. Among their tasks, coding agents may autonomously generate software tests; however, the quality of these tests remains uncertain. In particular, excessive use of mocking can make tests harder to understand and maintain. This paper presents the first study to investigate the presence of mocks in agent-generated tests of real-world software systems. We analyzed over 1.2 million commits made in 2025 in 2,168 TypeScript, JavaScript, and Python repositories, including 48,563 commits by coding agents, 169,361 commits that modify tests, and 44,900 commits that add mocks to tests. Overall, we find that coding agents are more likely to modify tests and to add mocks to tests than non-coding agents. We detect that (1) 60% of the repositories with agent activity also contain agent test activity; (2) 23% of commits made by coding agents add/change test files, compared with 13% by non-agents; (3) 68% of the repositories with agent test activity also contain agent mock activity; (4) 36% of commits made by coding agents add mocks to tests, compared with 26% by non-agents; and (5) repositories created recently contain a higher proportion of test and mock commits made by agents. Finally, we conclude by discussing implications for developers and researchers. We call attention to the fact that tests with mocks may be potentially easier to generate automatically (but less effective at validating real interactions), and the need to include guidance on mocking practices in agent configuration files.

</details>


### [191] [GitEvo: Code Evolution Analysis for Git Repositories](https://arxiv.org/abs/2602.00410)
*Andre Hora*

Main category: cs.SE

TL;DR: 本文提出了GitEvo，一款集成Git和代码级分析的多语言工具，解决了代码演化分析工具匮乏的问题，促进了实证研究和教学应用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门支持代码演化分析的工具，而代码演化分析对实践者、研究者和教育者均有重要意义。

Method: 通过利用Git框架和代码解析工具，GitEvo实现了对代码演化的综合分析，支持多种编程语言。

Result: GitEvo的实现不仅填补了工具缺口，还能支持新型的实证研究和教学活动。

Conclusion: GitEvo是一款多语言且可扩展的工具，能够支持Git仓库中的代码演化分析，结合了Git级和代码级的分析方法。

Abstract: Analyzing the code evolution of software systems is relevant for practitioners, researchers, and educators. It can help practitioners identify design trends and maintenance challenges, provide researchers with empirical data to study changes over time, and give educators real-world examples that enhance the teaching of software evolution concepts. Unfortunately, we lack tools specifically designed to support code evolution analysis. In this paper, we propose GitEvo, a multi-language and extensible tool for analyzing code evolution in Git repositories. GitEvo leverages Git frameworks and code parsing tools to integrate both Git-level and code-level analysis. We conclude by describing how GitEvo can support the development of novel empirical studies on code evolution and act as a learning tool for educators and students. GitEvo is available at: https://github.com/andrehora/gitevo.

</details>


### [192] [Context-Sensitive Pointer Analysis for ArkTS](https://arxiv.org/abs/2602.00457)
*Yizhuo Yang,Lingyun Xu,Mingyi Zhou,Li Li*

Main category: cs.SE

TL;DR: 针对ArkTS静态分析存在的瓶颈，提出了APAK指针分析工具，通过上下文敏感分析和插件架构提升调用图准确性和覆盖率，显著降低误报率，推动高级程序分析技术发展。


<details>
  <summary>Details</summary>
Motivation: 当前ArkTS的调用图生成方法在支持高级静态分析任务时精度不足，且传统JS/TS分析工具无法有效解释ArkUI组件树语义，限制了高级程序分析技术的发展。

Method: 提出了独特的ArkTS堆对象模型和高度可扩展的插件架构，通过上下文敏感指针分析技术解决闭包机制和框架API交互带来的分析难题。

Result: 在1663个OpenHarmony真实应用上评测，APAK在有效边覆盖度方面优于CHA和RTA方法，有效边覆盖度提升34.2%，误报率从20%降至2%，分析覆盖率和准确性大幅提高。

Conclusion: APAK是首个专门针对ArkTS设计的上下文敏感指针分析框架，显著提升了调用图的准确性和分析覆盖率，有效降低了误报率，支持更复杂的程序分析技术实现。

Abstract: Current call graph generation methods for ArkTS, a new programming language for OpenHarmony, exhibit precision limitations when supporting advanced static analysis tasks such as data flow analysis and vulnerability pattern detection, while the workflow of traditional JavaScript(JS)/TypeScript(TS) analysis tools fails to interpret ArkUI component tree semantics. The core technical bottleneck originates from the closure mechanisms inherent in TypeScript's dynamic language features and the interaction patterns involving OpenHarmony's framework APIs. Existing static analysis tools for ArkTS struggle to achieve effective tracking and precise deduction of object reference relationships, leading to topological fractures in call graph reachability and diminished analysis coverage. This technical limitation fundamentally constrains the implementation of advanced program analysis techniques.
  Therefore, in this paper, we propose a tool named ArkAnalyzer Pointer Analysis Kit (APAK), the first context-sensitive pointer analysis framework specifically designed for ArkTS. APAK addresses these challenges through a unique ArkTS heap object model and a highly extensible plugin architecture, ensuring future adaptability to the evolving OpenHarmony ecosystem. In the evaluation, we construct a dataset from 1,663 real-world applications in the OpenHarmony ecosystem to evaluate APAK, demonstrating APAK's superior performance over CHA/RTA approaches in critical metrics including valid edge coverage (e.g., a 7.1% reduction compared to CHA and a 34.2% increase over RTA). The improvement in edge coverage systematically reduces false positive rates from 20% to 2%, enabling future exploration of establishing more complex program analysis tools based on our framework. Our proposed APAK has been merged into the official static analysis framework ArkAnalyzer for OpenHarmony.

</details>


### [193] [Beyond Basic Specifications? A Systematic Study of Logical Constructs in LLM-based Specification Generation](https://arxiv.org/abs/2602.00715)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: 本研究首次系统探讨了利用大型语言模型自动生成高层次逻辑形式规范的可行性，证明了其在提升程序验证抽象能力方面的潜力，为未来自动验证框架设计提供了实证基础和指导。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的程序规范生成多局限于基础语法，难以满足复杂程序高层次抽象的需求，迫切需要研究LLMs在生成复杂逻辑构造方面的能力。

Method: 通过定义四种不同抽象层次的语法配置，并在主流程序验证数据集上利用多种代表性LLMs进行综合实验评估，系统探讨各种语法构造对规范生成框架的影响。

Result: 实验结果验证了LLMs生成有效逻辑构造的能力，逻辑构造与基本语法协同使用显著提升验证性能和鲁棒性，且发现了两种细化范式各自的优势。

Conclusion: 本研究表明大型语言模型(LLMs)能够有效生成包含复杂逻辑构造的高层次形式规范，这种融合基本语法和逻辑构造的方法提升了程序验证的能力和稳定性，同时验证开销并未显著增加。

Abstract: Formal specifications play a pivotal role in accurately characterizing program behaviors and ensuring software correctness. In recent years, leveraging large language models (LLMs) for the automatic generation of program specifications has emerged as a promising avenue for enhancing verification efficiency. However, existing research has been predominantly confined to generating specifications based on basic syntactic constructs, falling short of meeting the demands for high-level abstraction in complex program verification. Consequently, we propose incorporating logical constructs into existing LLM-based specification generation framework. Nevertheless, there remains a lack of systematic investigation into whether LLMs can effectively generate such complex constructs. To this end, we conduct an empirical study aimed at exploring the impact of various types of syntactic constructs on specification generation framework. Specifically, we define four syntactic configurations with varying levels of abstraction and perform extensive evaluations on mainstream program verification datasets, employing a diverse set of representative LLMs. Experimental results first confirm that LLMs are capable of generating valid logical constructs. Further analysis reveals that the synergistic use of logical constructs and basic syntactic constructs leads to improvements in both verification capability and robustness, without significantly increasing verification overhead. Additionally, we uncover the distinct advantages of two refinement paradigms. To the best of our knowledge, this is the first systematic work exploring the feasibility of utilizing LLMs for generating high-level logical constructs, providing an empirical basis and guidance for the future construction of automated program verification framework with enhanced abstraction capabilities.

</details>


### [194] [Can Vision-Language Models Handle Long-Context Code? An Empirical Study on Visual Compression](https://arxiv.org/abs/2602.00746)
*Jianping Zhong,Guochang Li,Chen Zhi,Junxiao Han,Zhen Qin,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: 为解决长代码上下文限制，LongCodeOCR通过视觉压缩代码为图像序列，使模型能更好地保持全局依赖，显著提升任务性能，且减少延迟，是现有文本压缩方法的有力补充。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文代码时受限于窗口大小，现有的文本代码压缩方法通过选择性过滤解决这一问题，但往往导致依赖中断和语义碎片化。

Method: 提出了LongCodeOCR，一种视觉压缩框架，将代码渲染为压缩的二维图像序列，供视觉语言模型处理，以保持全局视角，避免依赖中断。

Result: 在四个基准测试中，LongCodeOCR以约1.7倍压缩率在代码总结任务中优于现有方法LongCodeZip 36.85分；在1百万令牌的上下文长度下，使用专用9B视觉语言模型Glyph，LongCodeOCR在约4倍压缩率下保持更高准确率；同时大幅降低压缩阶段延迟，从约4.3小时降至约1分钟。

Conclusion: 视觉代码压缩为需要全局理解的任务提供了一种有效替代方案，虽然在精度关键任务中存在保真度瓶颈，但能更好地保持全局依赖；而文本代码压缩牺牲结构覆盖以保留符号精度。

Abstract: Large Language Models (LLMs) struggle with long-context code due to window limitations. Existing textual code compression methods mitigate this via selective filtering but often disrupt dependency closure, causing semantic fragmentation. To address this, we introduce LongCodeOCR, a visual compression framework that renders code into compressed two-dimensional image sequences for Vision-Language Models (VLMs). By preserving a global view, this approach avoids the dependency breakage inherent in filtering. We systematically evaluate LongCodeOCR against the state-of-the-art LongCodeZip across four benchmarks spanning code summarization, code question answering, and code completion.
  Our results demonstrate that visual code compression serves as a viable alternative for tasks requiring global understanding. At comparable compression ratios ($\sim$1.7$\times$), LongCodeOCR improves CompScore on Long Module Summarization by 36.85 points over LongCodeZip. At a 1M-token context length with Glyph (a specialized 9B VLM), LongCodeOCR maintains higher accuracy than LongCodeZip while operating at about 4$\times$ higher compression. Moreover, compared with LongCodeZip, LongCodeOCR drastically reduces compression-stage overhead (reducing latency from $\sim$4.3 hours to $\sim$1 minute at 1M tokens). Finally, our results characterize a fundamental coverage--fidelity trade-off: visual code compression retains broader context coverage to support global dependencies, yet faces fidelity bottlenecks on exactness-critical tasks; by contrast, textual code compression preserves symbol-level precision while sacrificing structural coverage.

</details>


### [195] [ScratchEval : A Multimodal Evaluation Framework for LLMs in Block-Based Programming](https://arxiv.org/abs/2602.00757)
*Yuan Si,Simeng Han,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: 提出ScratchEval基准，针对Scratch程序修复设计多层次评测，提升LLM在块式编程语言中的语义理解与修复能力，推动相关评测和训练方法的发展。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理文本编程语言表现良好，但面对结构复杂、事件驱动、多媒体紧耦合的Scratch程序时，常导致语义错误的修复操作，缺乏有效评测工具。

Method: 通过人机结合的流程筛选包含复杂结构和语义的100个Scratch项目，配套可执行测试用例、错误描述及修正和块级编辑约束，设计三层可执行协议从功能正确性、修复质量及解释质量多维度评价模型性能。

Result: 构建了覆盖Scratch程序理解、调试、分析和修复的执行基准，验证了领域特定微调、训练数据和模型泛化能力对修复性能的影响，实现了针对块状编程任务的可复现性评估框架。

Conclusion: ScratchEval作为首个针对Scratch程序的可执行基准，为评估基于大语言模型（LLM）的程序修复提供了系统性方法，解决了传统LLM在处理基于块的语言时语义理解和修复效果不足的问题。

Abstract: LLMs have achieved strong performance on text-based programming tasks, yet they remain unreliable for block-based languages such as Scratch. Scratch programs exhibit deeply nested, non-linear structures, event-driven concurrency across multiple sprites, and tight coupling between code and multimedia assets, properties that differ fundamentally from textual code. As a result, LLMs often misinterpret Scratch semantics and generate large, invasive edits that are syntactically valid but semantically incorrect when repairing buggy programs.
  We introduce ScratchEval, the first executable benchmark designed to evaluate LLM-based repair for Scratch programs, covering program understanding, debugging, analysis, and repair. The benchmark contains 100 curated Scratch projects from the public repository, selected for structural and semantic complexity. Each project is paired with executable test suites, bug descriptions with corresponding fixes, block-level edit constraints defining minimal semantically correct repairs, and required multimedia assets. The benchmark is constructed through a human-in-the-loop pipeline combining automated project mining with expert validation of trigger-outcome semantics and representative bug patterns, with emphasis on event ordering, concurrency, and state management.
  To enable rigorous and reproducible evaluation, we propose a three-layer executable protocol measuring functional correctness via VM-level execution, repair quality using block-level edit distance and behavioral trajectory comparisons, and explanation quality via structured rubrics assessing alignment between model reasoning and generated patches. Using ScratchEval, we study domain-specific fine-tuning, training data effectiveness, and model generalization to unseen bug types. ScratchEval provides a reproducible foundation for evaluating and post-training LLMs on block-based programming tasks.

</details>


### [196] [Test Behaviors, Not Methods! Detecting Tests Obsessed by Methods](https://arxiv.org/abs/2602.00761)
*Andre Hora,Andy Zaidman*

Main category: cs.SE

TL;DR: 提出了一种基于运行时多路径覆盖检测的测试异味“Test Obsessed by Method”，发现并实证了这一异味在Python标准库测试中的存在并可拆分优化。


<details>
  <summary>Details</summary>
Motivation: 现有基于生产方法调用次数识别测试异味的方法不准确，无法可靠反映测试验证的功能数量，因此需要一种更有效的识别多行为测试的方法。

Method: 基于运行时分析，检测测试方法覆盖了同一生产方法的多个执行路径，以识别测试中验证多种行为的情况。通过对12个Python标准库测试套件中2054个测试进行分析，识别出“Test Obsessed by Method”测试异味。

Result: 在12个测试套件中检测到44个“Test Obsessed by Method”的测试方法，占11个测试套件，平均每个测试涉及两个不同的行为。这些测试可被拆分为118个更聚焦的测试，其中23%的测试代码注释明确识别了测试多个不同的行为。

Conclusion: 本文提出了新的测试异味“Test Obsessed by Method”，并通过对Python标准库中12个测试套件2054个测试的实证研究，发现该异味在大多数测试套件中存在，且表明测试方法验证了多种行为。研究表明这些异味测试可以拆分成多个更聚焦的测试，有助于提升测试质量。

Abstract: Best testing practices state that tests should verify a single functionality or behavior of the system. Tests that verify multiple behaviors are harder to understand, lack focus, and are more coupled to the production code. An attempt to identify this issue is the test smell \emph{Eager Test}, which aims to capture tests that verify too much functionality based on the number of production method calls. Unfortunately, prior research suggests that counting production method calls is an inaccurate measure, as these calls do not reliably serve as a proxy for functionality. We envision a complementary solution based on runtime analysis: we hypothesize that some tests that verify multiple behaviors will likely cover multiple paths of the same production methods. Thus, we propose a novel test smell named \emph{Test Obsessed by Method}, a test method that covers multiple paths of a single production method. We provide an initial empirical study to explore the presence of this smell in 2,054 tests provided by 12 test suites of the Python Standard Library. (1) We detect 44 \emph{Tests Obsessed by Methods} in 11 of the 12 test suites. (2) Each smelly test verifies a median of two behaviors of the production method. (3) The 44 smelly tests could be split into 118 novel tests. (4) 23% of the smelly tests have code comments recognizing that distinct behaviors are being tested. We conclude by discussing benefits, limitations, and further research.

</details>


### [197] [Code Quality Analysis of Translations from C to Rust](https://arxiv.org/abs/2602.00840)
*Biruk Tadesse,Vikram Nitin,Mazin Salah,Baishakhi Ray,Marcelo d'Amorim,Wesley Assunção*

Main category: cs.SE

TL;DR: 本文系统评估了三种C到Rust自动翻译技术与人工翻译的代码质量，发现自动化方法虽提升部分安全性但带来新问题，翻译质量多维且复杂，需综合多种手段提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅关注自动翻译的正确性和安全性，忽视了性能、稳健性和可维护性等其他关键质量指标，且人工翻译仍然面临内部质量问题，因此需全面评估不同自动化技术在多质量维度上的表现。

Method: 通过对三种C到Rust的翻译工具（C2Rust、C2SaferRust、TranslationGym）进行深入的定量与定性分析，利用Clippy静态分析工具、GPT-4o识别潜在问题以及人工复查，评估转换后Rust代码的性能、稳健性和可维护性等质量属性。

Result: 研究发现自动翻译技术虽减少了一些不安全和非惯用代码，但引入了新的问题，且没有一种技术在所有质量维度上均超越人工翻译，人工翻译也存在代码可读性和惯用性等内部质量问题。

Conclusion: 自动化的C到Rust的代码转换技术在提高安全性和减少非惯用模式方面有一定成效，但普遍引入新的问题，且未能全面超越人工翻译的质量水平。翻译质量是一个多维度的挑战，单一方法难以覆盖所有质量维度，需结合系统化的评估和针对性的工具支持。

Abstract: C/C++ is a prevalent programming language. Yet, it suffers from significant memory and thread-safety issues. Recent studies have explored automated translation of C/C++ to safer languages, such as Rust. However, these studies focused mostly on the correctness and safety of the translated code, which are indeed critical, but they left other important quality concerns (e.g., performance, robustness, and maintainability) largely unexplored. This work investigates strengths and weaknesses of three C-to-Rust translators, namely C2Rust (a transpiler), C2SaferRust (an LLM-guided transpiler), and TranslationGym (an LLM-based direct translation). We perform an in-depth quantitative and qualitative analysis of several important quality attributes for the translated Rust code of the popular GNU coreutils, using human-based translation as a baseline. To assess the internal and external quality of the Rust code, we: (i) apply Clippy, a rule-based state-of-the-practice Rust static analysis tool; (ii) investigate the capability of an LLM (GPT-4o) to identify issues potentially overlooked by Clippy; and (iii) perform a manual analysis of the issues reported by Clippy and GPT-4o. Our results show that while newer techniques reduce some unsafe and non-idiomatic patterns, they frequently introduce new issues, revealing systematic trade-offs that are not visible under existing evaluation practices. Notably, none of the automated techniques consistently match or exceed human-written translations across all quality dimensions, yet even human-written Rust code exhibits persistent internal quality issues such as readability and non-idiomatic patterns. Together, these findings show that translation quality remains a multi-dimensional challenge, requiring systematic evaluation and targeted tool support beyond both naive automation and manual rewriting.

</details>


### [198] [MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers](https://arxiv.org/abs/2602.00933)
*Chaithanya Bandi,Ben Hertzberg,Geobio Boo,Tejas Polakam,Jeff Da,Sami Hassaan,Manasi Sharma,Andrew Park,Ernesto Hernandez,Dan Rambado,Ivan Salazar,Rafael Cruz,Chetan Rane,Ben Levin,Brad Kenstler,Bing Liu*

Main category: cs.SE

TL;DR: 本文提出MCP-Atlas基准，采用真实多步骤任务评估大语言模型调用外部工具的能力，发现当前模型仍存在明显不足，发布数据集和工具推动领域进步。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以反映大语言模型在真实复杂场景下调用外部工具的能力。

Method: 提出MCP-Atlas基准，包含36个真实MCP服务器、220个工具和1000个任务，任务要求多步骤调用多个工具，评估采用基于事实的评分标准和详细诊断指标。

Result: 前沿模型在基准测试中的通过率超过50%，主要失败原因是工具使用和任务理解不足。

Conclusion: MCP-Atlas为评估与促进大语言模型工具调用能力提供了真实、多样且细致的评价体系，有助于推动更健壮的工具增强智能体发展。

Abstract: The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or subjective LLM-as-a-judge metrics. We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools. It includes 1,000 tasks designed to assess tool-use competency in realistic, multi-step workflows. Tasks use natural language prompts that avoid naming specific tools or servers, requiring agents to identify and orchestrate 3-6 tool calls across multiple servers. We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the model's final answer, complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency. Evaluation results on frontier models reveal that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding. We release the task schema, containerized harness, and a 500-task public subset of the benchmark dataset to facilitate reproducible comparisons and advance the development of robust, tool-augmented agents.

</details>


### [199] [Cast: Automated Resilience Testing for Production Cloud Service Systems](https://arxiv.org/abs/2602.00972)
*Zhuangbin Chen,Zhiling Deng,Kaiming Zhang,Yang Liu,Cheng Cui,Jinfeng Zhong,Zibin Zheng*

Main category: cs.SE

TL;DR: Cast是一款自动化微服务韧性测试框架，通过生产流量重放和复杂度驱动测试策略，在华为云实测中有效识别和修复韧性漏洞，显著提升微服务系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 微服务架构的分布式特性带来韧性挑战，传统测试依赖手工且测试环境简化，难以捕获生产复杂性，急需自动化高保真韧性测试方法。

Method: 采用重放生产流量并结合应用级故障库，利用复杂度驱动策略有选择地执行测试，自动管理测试生命周期和多维度自动化验证系统韧性。

Result: 在华为云部署八个月，分析四个大规模应用发现137个潜在漏洞，89个被开发者确认；在基准集48个复现缺陷中覆盖率达90%。

Conclusion: Cast框架通过在生产环境中对微服务架构进行自动化端到端的韧性测试，有效发现和修复了大量潜在的韧性漏洞，显著提升了工业微服务系统的可靠性。

Abstract: The distributed nature of microservice architecture introduces significant resilience challenges. Traditional testing methods, limited by extensive manual effort and oversimplified test environments, fail to capture production system complexity. To address these limitations, we present Cast, an automated, end-to-end framework for microservice resilience testing in production. It achieves high test fidelity by replaying production traffic against a comprehensive library of application-level faults to exercise internal error-handling logic. To manage the combinatorial test space, Cast employs a complexity-driven strategy to systematically prune redundant tests and prioritize high-value tests targeting the most critical service execution paths. Cast automates the testing lifecycle through a three-phase pipeline (i.e., startup, fault injection, and recovery) and uses a multi-faceted oracle to automatically verify system resilience against nuanced criteria. Deployed in Huawei Cloud for over eight months, Cast has been adopted by many service teams to proactively address resilience vulnerabilities. Our analysis on four large-scale applications with millions of traces reveals 137 potential vulnerabilities, with 89 confirmed by developers. To further quantify its performance, Cast is evaluated on a benchmark set of 48 reproduced bugs, achieving a high coverage of 90%. The results show that Cast is a practical and effective solution for systematically improving the reliability of industrial microservice systems.

</details>


### [200] [Morphis: SLO-Aware Resource Scheduling for Microservices with Time-Varying Call Graphs](https://arxiv.org/abs/2602.01044)
*Yu Tang,Hailiang Zhao,Rui Shi,Chuansheng Lu,Yifei Zhang,Kingsum Chow,Shuiguang Deng*

Main category: cs.SE

TL;DR: 本文发现微服务调用路径中存在重复模式，提出依赖感知的资源调度框架Morphis，有效降低CPU资源消耗并保证系统性能。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统的运行时调用图结构动态变化，使得现有资源管理方法无法充分利用调用路径中的潜在规律，导致资源调度效率低下。

Method: 该论文引入了结构指纹技术，将调用路径分解为稳定的执行骨架和可解释的偏差子图，并将资源分配问题建模为基于预测模式分布的约束优化问题，实现全局的资源优化配置。

Result: Morphis在TrainTicket基准测试中相比最先进的方法减少了35-38%的CPU消耗，同时保持了98.8%的SLO合规率。

Conclusion: 该论文提出的Morphis框架通过利用微服务系统调用路径中的重复模式，有效降低了CPU资源消耗，同时保证了系统的服务级别目标（SLO）。

Abstract: Modern microservice systems exhibit continuous structural evolution in their runtime call graphs due to workload fluctuations, fault responses, and deployment activities. Despite this complexity, our analysis of over 500,000 production traces from ByteDance reveals a latent regularity: execution paths concentrate around a small set of recurring invocation patterns. However, existing resource management approaches fail to exploit this structure. Industrial autoscalers like Kubernetes HPA ignore inter-service dependencies, while recent academic methods often assume static topologies, rendering them ineffective under dynamic execution contexts. In this work, we propose Morphis, a dependency-aware provisioning framework that unifies pattern-aware trace analysis with global optimization. It introduces structural fingerprinting that decomposes traces into a stable execution backbone and interpretable deviation subgraphs. Then, resource allocation is formulated as a constrained optimization problem over predicted pattern distributions, jointly minimizing aggregate CPU usage while satisfying end-to-end tail-latency SLOs. Our extensive evaluations on the TrainTicket benchmark demonstrate that Morphis reduces CPU consumption by 35-38% compared to state-of-the-art baselines while maintaining 98.8% SLO compliance.

</details>


### [201] [SPELL: Synthesis of Programmatic Edits using LLMs](https://arxiv.org/abs/2602.01107)
*Daniel Ramos,Catarina Gamboa,Inês Lynce,Vasco Manquinho,Ruben Martins,Claire Le Goues*

Main category: cs.SE

TL;DR: 提出一种利用LLM提取示例再生成迁移脚本的自动API迁移方法，无需预先数据，实验证明在Python库迁移中表现出色。


<details>
  <summary>Details</summary>
Motivation: 库迁移是软件开发中常见但易出错的任务，现有自动迁移工具依赖稀缺的真实迁移数据，且未充分利用现代代码转换技术。

Method: 利用大语言模型（LLMs）提取迁移示例，再通过Agent将这些示例泛化为PolyglotPiranha中的可重复使用的迁移脚本，避免依赖已有迁移数据或直接用LLM进行转换。

Result: 系统能够生成多样的迁移示例并合成可泛化到实际代码库的转换脚本，实验验证了方法在Python库中的有效性。

Conclusion: 该方法成功将隐性迁移知识从LLM中提取并转化为结构化、可测试、可重复的迁移逻辑，降低了人工和数据依赖，提升了自动API迁移的实用性和鲁棒性。

Abstract: Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools exist, most rely on mining examples from real-world projects that have already undergone similar migrations. However, these data are scarce, and collecting them for arbitrary pairs of libraries is difficult. Moreover, these migration tools often miss out on leveraging modern code transformation infrastructure.
  In this paper, we present a new approach to automated API migration that sidesteps the limitations described above. Instead of relying on existing migration data or using LLMs directly for transformation, we use LLMs to extract migration examples. Next, we use an Agent to generalize those examples to reusable transformation scripts in PolyglotPiranha, a modern code transformation tool. Our method distills latent migration knowledge from LLMs into structured, testable, and repeatable migration logic, without requiring preexisting corpora or manual engineering effort. Experimental results across Python libraries show that our system can generate diverse migration examples and synthesize transformation scripts that generalize to real-world codebases.

</details>


### [202] [Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation](https://arxiv.org/abs/2602.01187)
*Chengran Yang,Zichao Wei,Heminghao Deng,Jinfeng Jiang,Zhensu Sun,Ting Zhang,Tianyi Wu,Ming Wen,David Lo*

Main category: cs.SE

TL;DR: 本文提出Stream of Revision范式，让代码生成模型能自我回溯修改生成内容，显著提升安全性并降低漏洞，且推理效率高。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码生成方法采用严格单调的线性生成方式，与人类编程中边生成边修改的认知过程不符。此前引入修改的办法存在高延迟或无法充分利用模型内在语义推理的问题。

Method: 提出Stream of Revision（修订流）范式，通过引入特定的动作标记，使模型能在单次前向传播中回溯并编辑历史生成内容，实现动态、自我修正的生成过程，内化修改循环，无需外部工具。

Result: 在安全代码生成任务中，Stream of Revision显著降低了漏洞率，且推理开销极小。

Conclusion: 该方法有效促进代码生成模型自我修正能力，提升代码质量，减少安全漏洞，推动代码生成向更符合人类认知的动态生成方向发展。

Abstract: Large Language Model (LLM) based code generation is predominantly formulated as a strictly monotonic process, appending tokens linearly to an immutable prefix. This formulation contrasts to the cognitive process of programming, which is inherently interleaved with forward generation and on-the-fly revision. While prior works attempt to introduce revision via post-hoc agents or external static tools, they either suffer from high latency or fail to leverage the model's intrinsic semantic reasoning. In this paper, we propose Stream of Revision, a paradigm shift that elevates code generation from a monotonic stream to a dynamic, self-correcting trajectory by leveraging model's intrinsic capabilities. We introduce specific action tokens that enable the model to seamlessly backtrack and edit its own history within a single forward pass. By internalizing the revision loop, our framework Stream of Revision allows the model to activate its latent capabilities just-in-time without external dependencies. Empirical results on secure code generation show that Stream of Revision significantly reduces vulnerabilities with minimal inference overhead.

</details>


### [203] [TraceLLM: Leveraging Large Language Models with Prompt Engineering for Enhanced Requirements Traceability](https://arxiv.org/abs/2602.01253)
*Nouf Alturayeif,Irfan Ahmad,Jameleddine Hassine*

Main category: cs.SE

TL;DR: 本文提出TraceLLM框架，通过提示工程提升需求追踪准确性，在多领域多模型测试中表现优异，推动半自动追踪应用发展。


<details>
  <summary>Details</summary>
Motivation: 传统需求追踪方法劳动强度大、易出错且精度低，现有大语言模型在此方面的系统设计和评估不足。

Method: 提出TraceLLM框架，通过提示工程和示范选择，结合数据集划分、迭代提示优化，以及多模型和多领域评估。

Result: 在四个基准数据集上，TraceLLM实现了领先的F2分数，优于传统方法和之前的大语言模型方法，且标签感知和多样性采样策略效果显著。

Conclusion: 需求追踪的性能不仅依赖模型能力，更依赖高质量提示工程，TraceLLM可支持人机联合的半自动需求追踪流程。

Abstract: Requirements traceability, the process of establishing and maintaining relationships between requirements and various software development artifacts, is paramount for ensuring system integrity and fulfilling requirements throughout the Software Development Life Cycle (SDLC). Traditional methods, including manual and information retrieval models, are labor-intensive, error-prone, and limited by low precision. Recently, Large Language Models (LLMs) have demonstrated potential for supporting software engineering tasks through advanced language comprehension. However, a substantial gap exists in the systematic design and evaluation of prompts tailored to extract accurate trace links. This paper introduces TraceLLM, a systematic framework for enhancing requirements traceability through prompt engineering and demonstration selection. Our approach incorporates rigorous dataset splitting, iterative prompt refinement, enrichment with contextual roles and domain knowledge, and evaluation across zero- and few-shot settings. We assess prompt generalization and robustness using eight state-of-the-art LLMs on four benchmark datasets representing diverse domains (aerospace, healthcare) and artifact types (requirements, design elements, test cases, regulations). TraceLLM achieves state-of-the-art F2 scores, outperforming traditional IR baselines, fine-tuned models, and prior LLM-based methods. We also explore the impact of demonstration selection strategies, identifying label-aware, diversity-based sampling as particularly effective. Overall, our findings highlight that traceability performance depends not only on model capacity but also critically on the quality of prompt engineering. In addition, the achieved performance suggests that TraceLLM can support semi-automated traceability workflows in which candidate links are reviewed and validated by human analysts.

</details>


### [204] [Evaluating Workflow Automation Efficiency Using n8n: A Small-Scale Business Case Study](https://arxiv.org/abs/2602.01311)
*Ahmed Raza Amir,Syed Muhammad Atif*

Main category: cs.SE

TL;DR: 本研究通过小规模案例验证使用n8n低代码平台自动化工作流程在效率和准确性上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 希望验证低代码平台在小型组织和个人中应用工作流自动化以提升运营效率的实用性和效果。

Method: 通过n8n实现代表性的潜在客户处理工作流程，自动化数据存储、邮件确认和实时通知；并在控制条件下对比20次手动与25次自动执行的性能。

Result: 自动化执行时间平均为1.23秒，比手动的185.35秒减少约151倍；手动错误率为5%，自动化则为零错误。

Conclusion: 低代码自动化显著提升了小规模工作流程的效率和可靠性，显著减少执行时间和错误率。

Abstract: Workflow automation has become increasingly accessible through low-code platforms, enabling small organizations and individuals to improve operational efficiency without extensive software development expertise. This study evaluates the performance impact of workflow automation using n8n through a small-scale business case study. A representative lead-processing workflow was implemented to automatically store data, send email confirmations, and generate real-time notifications. Experimental benchmarking was conducted by comparing 20 manual executions with 25 automated executions under controlled conditions. The results demonstrate a significant reduction in the average execution time from 185.35 seconds (manual) to 1.23 seconds (automated), corresponding to an approximately 151 times reduction in execution time. Additionally, manual execution exhibited an error rate of 5%, while automated execution achieved zero observed errors. The findings highlight the effectiveness of low-code automation in improving efficiency, reliability, and operational consistency for small-scale workflows.

</details>


### [205] [AdNanny: One Reasoning LLM for All Offline Ads Recommendation Tasks](https://arxiv.org/abs/2602.01563)
*Nan Hu,Han Li,Jimeng Sun,Lu Wang,Fangkai Yang,Bo Qiao,Pu Zhao,David Dai,Mengyu Liu,Yuefeng Zhan,Jianjin Zhang,Weihao Han,Allen Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Denvy Deng,Feng Sun,Qi Zhang*

Main category: cs.SE

TL;DR: AdNanny是一种统一的推理中心大型语言模型，通过多任务微调和强化学习优化广告相关任务，实现在必应广告系统中的成功应用，提升准确率并降低维护成本。


<details>
  <summary>Details</summary>
Motivation: 由于在线广告系统对延迟要求极高，难以直接部署大型语言模型（LLMs），因此需要离线利用LLMs提升检索、排序和推荐模型效果。现有方案通常为单个任务单独微调模型，导致模型冗余、维护成本高且性能提升有限。

Method: 提出AdNanny，一个统一的推理中心大型语言模型，基于671亿参数DeepSeek-R1，通过混合稠密-MoE并行训练系统进行微调。构建结合结构化监督和步骤化自然语言解释的推理增强语料，采用多任务有监督微调和自适应重加权，配合强化学习优化在线广告指标。

Result: AdNanny在必应广告系统中部署，显著减少人工标注工作，提升多项离线任务的准确率。通过整合多个任务专用模型为单一推理核心模型，实现了大规模广告系统的高效、低成本解决方案。

Conclusion: AdNanny展示了通过统一推理中心大型语言模型提升广告离线任务性能的可行性和效果，为广告系统提供了可扩展且经济高效的基础模型方案。

Abstract: Large Language Models (LLMs) have shown strong capabilities in Natural Language Understanding and Generation, but deploying them directly in online advertising systems is often impractical due to strict millisecond-level latency constraints. This has motivated the use of LLMs offline to improve retrieval, ranking, and recommendation models. Existing solutions typically fine-tune separate LLMs for individual tasks such as query-ad relevance labeling, keyword-based query generation, and user profiling. This results in redundant models, high maintenance cost, and limited performance gains despite substantial overlap in domain knowledge and reasoning patterns. We introduce AdNanny, a unified reasoning-centric LLM that serves as a shared backbone for offline advertising tasks. AdNanny is obtained by fine-tuning a public 671B-parameter DeepSeek-R1 checkpoint using a scalable training system that supports hybrid dense-MoE parallelism. We construct reasoning-augmented corpora that pair structured supervision with step-by-step natural language explanations. A multi-task supervised fine-tuning stage with adaptive reweighting enables AdNanny to handle diverse labeling and generation tasks in a consistent reasoning format. This is followed by reinforcement learning using downstream advertising metrics to align model behavior with online retrieval and ranking objectives. AdNanny is deployed in production within Bing Ads, where it significantly reduces manual labeling effort and improves accuracy across multiple offline tasks. By consolidating many task-specific models into a single reasoning-centric foundation model, AdNanny provides a scalable and cost-effective solution for large-scale advertising systems.

</details>


### [206] [Role of CI Adoption in Mobile App Success: An Empirical Study of Open-Source Android Projects](https://arxiv.org/abs/2602.01957)
*Xiaoxin Zhou,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 本研究通过分析开源Android应用，发现CI采用显著提升了移动应用的开发效率、发布频率及用户参与度，增强了应用在应用商店的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管CI在通用软件领域已被广泛研究，但其对移动应用开发的影响，特别是用户体验和应用商店表现，尚未充分探索。

Method: 通过分析开源Android应用，比较CI采用者与非采用者，使用活动和缺陷指标来描绘采用模式，并评估采用前后的变化及用户影响。

Result: CI采用者通常规模更大、活动更频繁，发布更快速且规律。CI集中在集成和可靠性要求高的类别中，如金融和生产力工具，并带来更高的应用商店下载和评价数量。

Conclusion: CI的采用有助于提升移动应用的开发活跃度、发布速度及用户参与度，且不会降低用户评分。

Abstract: Mobile apps face strong pressure for fast and reliable updates. Continuous Integration (CI) helps automate builds, tests, and releases, but its impact on mobile development remains underexplored. Despite the widespread use of CI, little is known about how it affects development activity, release speed, and user-facing outcomes in mobile projects. Existing studies mostly focus on CI adoption in general-purpose software, providing limited insight into mobile-specific dynamics, such as app store visibility and user engagement. In this paper, we analyze open-source Android apps to (1) compare CI adopters and non-adopters, (2) characterize adoption patterns using activity and bug metrics, and (3) assess pre/post adoption changes and user-facing outcomes. We observe that CI adopters are larger and more active, with faster and more regular releases. CI adoption is concentrated in integration- and reliability-intensive categories (e.g., finance and productivity) and is associated with higher Google Play Store engagement (more downloads and reviews) without lower ratings. Overall, CI adoption aligns with practices that support sustained delivery, higher project visibility, and stronger user engagement in mobile ecosystems.

</details>


### [207] [CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems](https://arxiv.org/abs/2602.02138)
*Lyu Zongyi,Ji Zhenlan,Chen Songqiang,Wang Liwen,Huang Yuheng,Wang Shuai,Cheung Shing-Chi*

Main category: cs.SE

TL;DR: 提出首个基于因果分析的多代理代码生成系统中间特征重要性评估框架CAM，揭示关键特征及混合后端优势，推动系统优化和性能提升。


<details>
  <summary>Details</summary>
Motivation: 多代理代码生成系统架构复杂，产生大量中间输出，个别中间输出对系统正确性的重要性不明确，阻碍了针对性优化。

Method: 通过系统分类中间输出，对中间特征进行现实错误模拟，量化其对系统正确性的影响，并进行广泛的实证分析验证方法有效性。

Result: 发现上下文依赖特征和混合后端架构能显著提升性能，实现了73.3%故障修复成功率及最大66.8%中间特征剪枝，保持生成性能不变。

Conclusion: CAM框架通过因果分析系统地量化了多代理代码生成系统(MACGS)中不同中间特征对系统正确性的贡献，揭示了上下文依赖的重要特征及混合后端架构的优势，有效提升了系统性能和优化效率。

Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \textbf{C}ausality-based \textbf{A}nalysis framework for \textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.
  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.

</details>


### [208] [Agent-Based Software Artifact Evaluation](https://arxiv.org/abs/2602.02235)
*Zhaonan Wu,Yanjie Zhao,Zhenpeng Chen,Zheng Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: 面向软件工程制品评估的自动化框架ArtifactCopilot，通过构建稳定执行环境及依赖命令图，实现高准确率、低成本且大幅减少人工参与的评估流程。


<details>
  <summary>Details</summary>
Motivation: 现有制品评估依赖人工执行和调试，随着论文提交量上升，人力成本难以承受，亟需自动化解决方案。

Method: 通过执行规范化策略和构建依赖感知的命令图，实现自动环境构建、指令执行和错误恢复的端到端代理框架。

Result: 在48个真实制品测试中，ArtifactCopilot的表现接近人工评估，准确率达85.42%，且平均成本极低，且大部分无须人工干预。

Conclusion: ArtifactCopilot有效提升了软件工程中制品评估的自动化水平，在准确率和成本控制上均优于现有方法。

Abstract: Artifact evaluation has been adopted in the Software Engineering (SE) research community for 15 years, substantially improving research reproducibility across major SE conferences. However, this success has introduced a growing scalability challenge, as artifact evaluation relies heavily on reviewers' manual execution and debugging, leading to escalating human effort amid rapidly increasing paper submissions. To address this problem, we investigate automated artifact evaluation. We first conduct a preliminary study on artifacts from top-tier SE conferences and identify three key challenges: perceiving execution states, maintaining stable execution environments, and recovering from execution errors. Inspired by these findings, we propose ArtifactCopilot, the first end-to-end agent-based framework for automated artifact evaluation. ArtifactCopilot automates environment construction, instruction execution, and error recovery by combining an execution normalization strategy to ensure environment stability with an artifact evaluation graph that transforms README documents into dependency-aware command graphs, enabling structured execution planning, execution-state tracking, and error recovery. Evaluation on 48 real-world artifacts shows that ArtifactCopilot matches human artifact evaluation outcomes for 85.42% of the artifacts, outperforming Claude Code by 52.09 percentage points, while costing only \$0.091 per artifact on average and requiring zero human intervention for 45 out of 48 artifacts.

</details>


### [209] [OmniCode: A Benchmark for Evaluating Software Engineering Agents](https://arxiv.org/abs/2602.02262)
*Atharv Sonwane,Eng-Shen Tu,Wei-Chung Lu,Claas Beger,Carter Larsen,Debjit Dhar,Rachel Chen,Ronit Pattanayak,Tuan Anh Dang,Guohao Chen,Gloria Geng,Kevin Ellis,Saikat Dutta*

Main category: cs.SE

TL;DR: OmniCode是一个面向多语言、多任务的软件工程基准，旨在促进更全面的编码智能体研发。现有智能体在部分任务和语言上表现有限，展现出未来改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的编码基准如HumanEval和SWE-Bench任务范围狭窄，不能全面评估编码智能体在真实软件工程中的多样化任务能力，亟需一个更全面、更真实的基准。

Method: 设计并构建了包含1794个任务的OmniCode基准，涵盖Python、Java和C++三种编程语言以及四种关键任务类别（缺陷修复、测试生成、代码审查修复和风格修复），任务经过人工验证且结合合成与真实数据生成，避免了数据泄漏。

Result: 通过在OmniCode上评估主流智能体框架（如SWE-Agent），发现其在Python缺陷修复表现较好，但在测试生成任务以及Java和C++语言上表现不足，最高准确率仅为20.9%。

Conclusion: OmniCode作为一个更广泛且多样化的软件工程基准，能够有效评估编码智能体在真实软件开发中不同任务的表现，推动编码智能体的发展。现有智能体在某些任务和语言上的表现仍有限。

Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.

</details>


### [210] [RACA: Representation-Aware Coverage Criteria for LLM Safety Testing](https://arxiv.org/abs/2602.02280)
*Zeming Wei,Zhixin Zhang,Chengcan Wu,Yihao Zhang,Xiaokun Luan,Meng Sun*

Main category: cs.SE

TL;DR: 本文提出RACA框架，通过安全关键概念的覆盖准则，提升大型语言模型安全测试的有效性与系统性，成功识别高危绕过攻击，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全测试依赖静态数据集，缺乏系统化的质量和充分性评估准则，而现有覆盖准则难以直接应用于规模庞大且目标不同的LLM。

Method: 通过表示工程技术，选取安全关键概念，以专家校准的绕过提示集合为基础计算概念激活分数，并结合六个子准则评估测试集的覆盖率。

Result: 实验证明RACA能够有效识别高质量绕过攻击提示，优于传统方法，且在实际场景（如测试集优先级和攻击提示采样）中表现出良好效果，且方法具有良好的泛化性和鲁棒性。

Conclusion: RACA提出了一种针对大型语言模型安全测试的新覆盖准则，能够高效识别和评估绕过攻击提示，优于传统神经元水平准则，并具备良好的泛化能力和实际应用价值。

Abstract: Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.

</details>


### [211] [Before Autonomy Takes Control: Software Testing in Robotics](https://arxiv.org/abs/2602.02293)
*Nils Chur,Thiago Santos de Moura,Argentina Ortega,Sven Peldszus,Thorsten Berger,Nico Hochgeschwender,Yannic Noller*

Main category: cs.SE

TL;DR: 本文针对机器人软件测试的复杂性，结合软件测试理论对247篇相关论文进行了映射研究，分析现状与挑战，并总结了关键问题与经验，指导机器人和软件工程社区更好地开展测试研究。


<details>
  <summary>Details</summary>
Motivation: 机器人系统复杂且安全关键，测试难度大且环境不确定，亟需系统总结现有研究。

Method: 通过对247篇机器人测试论文进行映射研究，关联软件测试理论进行分析。

Result: 厘清了机器人软件测试的现状、挑战，并通过示例说明，提出开放问题和经验教训。

Conclusion: 机器人软件测试具有挑战性，需要结合软件测试理论进行系统性研究。

Abstract: Robotic systems are complex and safety-critical software systems. As such, they need to be tested thoroughly. Unfortunately, robot software is intrinsically hard to test compared to traditional software, mainly since the software needs to closely interact with hardware, account for uncertainty in its operational environment, handle disturbances, and act highly autonomously. However, given the large space in which robots operate, anticipating possible failures when designing tests is challenging. This paper presents a mapping study by considering robotics testing papers and relating them to the software testing theory. We consider 247 robotics testing papers and map them to software testing, discussing the state-of-the-art software testing in robotics with an illustrated example, and discuss current challenges. Forming the basis to introduce both the robotics and software engineering communities to software testing challenges. Finally, we identify open questions and lessons learned.

</details>


### [212] [Understanding and Detecting Flaky Builds in GitHub Actions](https://arxiv.org/abs/2602.02307)
*Wenhao Ge,Chen Zhang*

Main category: cs.SE

TL;DR: 本文基于大规模数据分析了GitHub Actions中flaky构建问题，定义失效类型，并用机器学习方法显著提升检测效果，增强CI的可信度和效率。


<details>
  <summary>Details</summary>
Motivation: CI构建结果时常不可靠，间歇性失败影响开发者对CI的信任，浪费计算资源并威胁CI相关实证研究的有效性。

Method: 基于1,960个Java开源项目的重运行数据进行大规模实证分析，识别15类flaky失败，并提出基于机器学习的flaky失败检测方法。

Result: 3.2%的构建被重运行，其中67.73%的重运行构建表现出flaky行为，涉及51.28%的项目。提出的机器学习检测方法将F1分数提高了最多20.3%。

Conclusion: GitHub Actions中的CI构建存在较高比例的flaky失败，主要包括测试不稳定、网络问题和依赖解析问题，影响了超过半数的项目。

Abstract: Continuous Integration (CI) is widely used to provide rapid feedback on code changes; however, CI build outcomes are not always reliable. Builds may fail intermittently due to non-deterministic factors, leading to flaky builds that undermine developers' trust in CI, waste computational resources, and threaten the validity of CI-related empirical studies. In this paper, we present a large-scale empirical study of flaky builds in GitHub Actions based on rerun data from 1,960 open-source Java projects. Our results show that 3.2% of builds are rerun, and 67.73% of these rerun builds exhibit flaky behavior, affecting 1,055 (51.28%) of the projects. Through an in-depth failure analysis, we identify 15 distinct categories of flaky failures, among which flaky tests, network issues, and dependency resolution issues are the most prevalent. Building on these findings, we propose a machine learning-based approach for detecting flaky failures at the job level. Compared with a state-of-the-art baseline, our approach improves the F1-score by up to 20.3%.

</details>


### [213] [A Task-Level Evaluation of AI Agents in Open-Source Projects](https://arxiv.org/abs/2602.02345)
*Shojibur Rahman,Md Fazle Rabbi,Minhaz Zibran*

Main category: cs.SE

TL;DR: 通过评估五个AI编码代理在真实开源项目中的拉取请求接受率、评审讨论量和提交信息质量，发现各代理在不同指标上各有优势，为优化AI辅助软件开发提供了实证参考。


<details>
  <summary>Details</summary>
Motivation: 探究不同自主编码代理在开源软件协作环境中的实际表现差异，帮助选择和改进AI代理以更高效地支持软件工程协作。

Method: 利用公开的AIDev-pop数据集，覆盖多个流行开源库中数千条AI生成的拉取请求，基于拉取请求生命周期的三个维度（接受率、评审讨论量、提交信息质量）进行定量评估。

Result: Codex在大多数任务类别中表现出较高的拉取请求接受率；Copilot生成的拉取请求激发了最多的人类和自动评审讨论；Claude和Cursor在多个任务类型上提供了更高比例的高质量提交信息，而Codex在提交信息质量方面表现较低。

Conclusion: 本研究通过AIDev-pop数据集对五个自主编码代理进行了比较分析，发现不同代理在拉取请求的接受率、评审讨论数量和提交信息质量方面表现各异，Codex接受率最高，Copilot引发最多讨论，Claude和Cursor在提交信息质量上表现优异。

Abstract: In this paper, we present a comparative study of five autonomous coding agents using AIDev-pop, which is a public dataset containing thousands of AI-generated pull requests (PRs) across popular open-source repositories. We evaluate agents' performance along three task-aware dimensions spanning the PR lifecycle: (1) PR acceptance rate, (2) review discussion volume, and (3) commit message quality. Our quantitative analysis finds that Codex consistently achieves high PR acceptance rates across most task categories, while Copilot's PRs trigger the highest volume of both human and automated review discussions. In contrast, commit-level quality varies independently of acceptance outcomes. Claude and Cursor produce higher proportions of high-quality commit messages across several task types, and Codex exhibiting comparatively lower commit quality despite strong integration outcomes. Our findings inform selection and improvements of AI agents for their effective integration to collaborative software engineering.

</details>


### [214] [SWE-Universe: Scale Real-World Verifiable Environments to Millions](https://arxiv.org/abs/2602.02361)
*Mouxiang Chen,Lei Zhang,Yunlong Feng,Xuwu Wang,Wenting Zhao,Ruisheng Cao,Jiaxi Yang,Jiawei Chen,Mingze Li,Zeyao Ma,Hao Ge,Zongmeng Zhang,Zeyu Cui,Dayiheng Liu,Jingren Zhou,Jianling Sun,Junyang Lin,Binyuan Hui*

Main category: cs.SE

TL;DR: 本文提出了SWE-Universe，一种从GitHub PR自动构建大规模高质量软件工程验证环境的高效框架，有效提升了环境构建质量和规模，并推动了编码智能体性能的提升。


<details>
  <summary>Details</summary>
Motivation: 当前自动构建真实世界软件工程验证环境面临产量低、验证能力弱和成本高昂等诸多挑战，亟需一种高效且可扩展的方法来生成大量可靠的SWE环境供智能体训练使用。

Method: SWE-Universe框架利用一个高效的定制训练模型驱动的构建代理，通过迭代自我验证和循环黑客检测机制，实现对任务的可靠生成，从而解决自动构建中产量低、验证薄弱及成本高昂的问题。

Result: 基于该方法，成功构建了超过80万（807,693个）多语言软件工程环境，并通过大规模智能体中期训练和强化学习验证了环境的价值，最终在SWE-Bench Verified测试中实现75.3%的高分。

Conclusion: 本文提出的SWE-Universe框架成功实现了从GitHub拉取请求中自动构建大规模、高保真、可验证的软件工程环境，显著提升了自动构建的生产效率和验证可靠性，并在提升编码智能体性能方面表现出重要价值。

Abstract: We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.

</details>
