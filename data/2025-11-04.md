<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [cs.MA](#cs.MA) [Total: 10]
- [cs.SE](#cs.SE) [Total: 42]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了PlotCraft基准测试及相关数据集和模型，用于评估和提升大型语言模型在复杂数据可视化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成方面表现优异，但在处理复杂、结构化数据可视化方面尚未被充分评估和发展。

Method: 设计了包含1000个复杂可视化任务的PlotCraft基准，涵盖7类任务和48种图表类型，系统评测23个主流模型。构建SynthVis-30K大规模高质量数据集，基于协作代理框架生成复杂可视化代码。开发小型高效的代码生成模型PlotCraftor。

Result: PlotCraft基准测试显示现有模型在复杂任务上表现不足，PlotCraftor模型在多个测试集上达到领先水平，特别在高难度任务上性能提升超过50%。

Conclusion: 通过数据集和新模型的引入，显著提升了大型语言模型处理复杂数据可视化任务的能力，推动该领域进一步发展。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: 提出ProtoMBTI，一个基于原型理论和大语言模型的MBTI人格识别框架，通过多维语料扩增和LoRA微调提升分类准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的人格识别通常采用硬标签分类，忽视了人格判断的原型性质；作者希望通过原型理论更好地捕捉人格认知特征。

Method: 构建多维度增强的高质量语料库，利用LoRA微调轻量编码器学习判别性嵌入和人格原型库；推理时通过检索、投票、修正和保留机制，实现动态原型库完善。

Result: 在多个基准数据集上，ProtoMBTI在MBTI四个维度及16类型任务中均优于基线方法，且表现出良好的跨数据集泛化能力。

Conclusion: 将推理过程与心理学原型理论对齐，能提升文本人格预测的准确性、可解释性和迁移能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 本文提出了一种残差流解码器框架，用于探测语言模型中的激活信息，特别是段落和文档级别的长远规划信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能够处理更长时间跨度的任务，现有的激活理解方法仍局限于特定概念或词汇，难以解析更长远的规划信息。

Method: 开发残差流解码器框架，测试不同方法来解码激活中的潜在信息，尤其是针对段落和文档尺度的长远计划。

Result: 发现可以从小规模模型的激活中解码出相当于5个以上未来词的上下文信息。

Conclusion: 该方法为更好地监控语言模型和理解其如何编码长远规划信息提供了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出通过训练过程中预测信息丰富的目标词，优化大型语言模型训练性能，超越传统的下一个词预测方法。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型训练多采用下一个词预测方法，可能未充分利用训练过程中目标词的信息量，限制了模型性能提升。

Method: 提出一种预测信息丰富目标词的训练策略，并在算术、多标签文本分类及自然语言生成三种任务中评估其效果。

Result: 新策略显著提升模型在不同任务中的表现，体现出更有效的训练效率和模型性能。

Conclusion: 通过选择信息丰富的目标词进行训练，有望优化大型语言模型的训练过程，提升性能并深化对目标词选择策略的理论理解。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架，通过自动指标评估并提升大语言模型（LLM）在模拟角色对话中的人格一致性，采用多轮强化学习微调模型，提高角色扮演的连贯性和忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在模拟人类角色时常常出现人格偏离、前后矛盾和失去角色行为的问题，影响交互体验的真实性和有效性。

Method: 定义三种自动化一致性指标（提示与回复一致性、回复间一致性、问答一致性），利用这些指标作为奖励信号，通过多轮强化学习微调LLM，使其在不同角色（患者、学生、社交伙伴）中保持人格连贯。

Result: 方法显著减少了超过55%的人格不一致问题，使仿真用户表现得更连贯和忠实于所扮演角色。

Conclusion: 引入的指标和强化学习框架有效提升了LLM的角色人格一致性，有助于更真实可靠的人机交互模拟，促进基于LLM的互动应用发展。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: 该论文介绍了一种基于浏览器的网络安全桌面演练新系统AgentBnB，结合大语言模型和增强式辅导，为学习者提供按需提示和渐进式辅助。试点测试显示，该系统比传统物理卡片更具可扩展性并提升了使用意愿。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全桌面演练虽然训练效果好，但存在剧本化、资源密集和难以扩展等问题。

Method: 开发AgentBnB系统，利用大语言模型和检索增强的辅助工具，提供事实、概念、程序及元认知提示，辅以渐隐式脚手架辅助策略。

Result: 四名研究生的单人试点表明，用户更倾向使用AgentBnB系统而非实体卡片，认为其更易扩展，尽管知识测验表现存在天花板效应。

Conclusion: 大语言模型增强的桌面演练可作为轻量、可重复的训练方式，减少传统方法的后勤负担，未来计划扩展多玩家模式和更大规模验证。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了IL-PCR语料库，用于统一检索法规和判例两个相关法律任务，开发了多种基线模型及基于大型语言模型（LLM）的重排名方法，取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 目前法规检索和判例检索通常被独立处理，但两者高度相关，应当联合利用，提高检索效果。

Method: 构建IL-PCR数据集，设计基线模型（词汇、语义、图神经网络），并提出基于大型语言模型的重排名方法，联合利用两个任务的相关性。

Result: 基线模型和LLM重排名方法均进行了大量实验，LLM重排名方法获得了最佳性能，证明了联合利用两任务依赖性的有效性。

Conclusion: IL-PCR语料库为法规和判例检索任务提供统一测试平台，联合模型和LLM重排名方法显著提升检索性能，表明两任务的联合建模具有潜力。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于模板生成句子预训练的新方法POSESTITCH-SLT，用于提升手语翻译效果。


<details>
  <summary>Details</summary>
Motivation: 手语翻译面临大型、句子对齐数据集稀缺的问题，现有方法主要改进特征提取和网络结构。

Method: 提出了POSESTITCH-SLT，一种受语言模板启发的预训练方案，结合模板生成的句子对进行训练，在Transformer编码器-解码器架构基础上实现翻译。

Result: 在两大手语数据集How2Sign和iSign上，BLEU-4分数分别从1.97提升到4.56和0.55提升到3.43，超越了基于姿态的无词汇翻译先前成果。

Conclusion: 模板驱动的合成监督在数据稀缺的手语翻译场景中效果显著，促进了模型性能提升。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [9] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出了一种名为Factorization Memory的新型高效RNN结构，在短上下文语言建模任务中性能可比肩Transformer模型，并在长上下文场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在短上下文任务表现优异，但在长上下文下的泛化能力有限，且计算资源需求较高，亟需一种既高效又能处理长上下文的模型。

Method: 基于Mamba-2模型，设计Factorization Memory，使其在训练阶段支持并行计算，推理阶段保持恒定的计算和内存复杂度；引入稀疏版本仅更新部分状态，提升计算效率，同时保证性能。

Result: Factorization Memory在短上下文语言建模任务中表现与Transformer相当，在长上下文任务中表现更优，且稀疏版本有效降低计算量而不损失性能。

Conclusion: Factorization Memory首次将稀疏记忆激活与优秀的短长上下文性能结合，展示了其作为高效RNN架构的潜力，并在实验中优于相关架构。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [10] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 本文指出因果语言模型的预训练目标具有反转不变性，即模型对正序和逆序文本的预测损失相同，导致模型对文本方向不敏感。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型预训练目标忽视了语言的时间非对称特性，不能有效捕捉语言中的方向依赖，如语音、形态和因果关系。

Method: 作者形式化了因果语言模型目标的反转不变性特性，并分析了这一对称性带来的限制，提出应从时间非对称性的角度重新设计预训练目标和模型架构。

Result: 模型在正序和逆序文本上表现相近，说明现有目标函数对文本方向不敏感，这揭示了预训练目标的一种根本限制。

Conclusion: 现有的预训练目标因对方向的忽视而存在局限，应开发具备时间箭头建模能力的新损失函数和架构，以更好地捕获语言的方向性依赖。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [11] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: 本文提出了LingGym基准，用于评估大语言模型（LLM）在元语言学推理方面的能力，重点考察其对低资源语言和新结构的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定下游任务，缺乏对LLM跨语言类型推理能力的系统评估，因此提出了一个新的基准来弥补这一空白。

Method: 设计了基于跨18种类型语言的IGT和语法描述的Word-Gloss推理任务，要求模型根据上下文推断缺失单词和词汇解释，利用多层次语言学信息进行控制评估。

Result: 结果表明，加入结构化语言线索可以显著提升各模型的推理性能，证明了语言信息对提升泛化能力的重要性。

Conclusion: 该研究展示了LLM在类型学语言分析和低资源语言文献记录中的潜力，同时指出目前模型在处理复杂语言推理时仍存在局限。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [12] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本文提出了基于引导推理轨迹（Reasoning Trajectory, RT）的苏格拉底式调试任务，通过大规模语言模型生成调试对话，有效提升了调试过程的准确性和有效性。


<details>
  <summary>Details</summary>
Motivation: 大多数初学者编程错误源于对编程概念的误解，传统直接给出错误修复的方式难以促进学生深刻理解和认知更新，苏格拉底式调试通过引导学生自主发现并修正错误，激发认知矛盾促使其修正错误信念。

Method: 本文引入推理轨迹生成任务，并构建了带有手工注释推理轨迹的调试问题数据集，基于大规模语言模型(Large Language Models, LLM)设计生成推理轨迹及基于轨迹的苏格拉底式对话的方案。

Result: 大规模语言模型作为评判者的大规模评测结果显示，先进模型生成的推理轨迹正确率达91%，对话轮次有效率达98.7%，证明方法具备较高的实用性和准确性。

Conclusion: 基于引导推理轨迹的苏格拉底式调试方法能够有效帮助学生理解和修正编程错误，且利用大规模语言模型生成调试对话具有良好表现，具备推广应用的潜力。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [13] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 本论文研究了AI文本检测器对于迭代式改写文本失效的问题，提出了针对这一漏洞的PADBen基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成文本检测器虽然对直接生成文本准确率高，但在面对经过多次改写的文本时效果急剧下降，导致检测失效。

Method: 通过分析迭代改写的内部机制，发现改写文本形成了带有语义位移但保留生成模式的中间洗白区，提出两种攻击类别，并设计了包含五种文本类型和五个检测任务的PADBen基准测试，评估11个现有检测器的性能。

Result: 检测器能够有效识别抄袭规避场景，但对于作者身份混淆场景表现差，显示出现有检测方法对中间洗白区无效。

Conclusion: 现有检测架构需突破语义和风格区分方法的限制，根本改进才能有效应对迭代改写文本的检测挑战。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [14] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: 本文介绍了MedRECT，一个跨语言（日语/英语）的医疗错误检测与纠正基准，评价了9个大型语言模型，重点提升了推理模型的表现并实现了超越人类专家的纠正能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在医疗领域的错误检测和纠正能力尚未充分评估，特别是非英语语境下的表现亟需研究，以保证其在临床中的安全应用。

Method: 构建了涵盖日语和英语的MedRECT基准数据集，包含错误检测、错误定位和错误纠正三个子任务，采用自动化构建流程并引入多种类型的语言模型进行测试，同时通过LoRA微调提升模型表现。

Result: 推理模型在错误检测和句子提取任务上优于传统模型，跨语言评测显示日语表现下降但推理模型差距较小，LoRA微调在两种语言中带来不同程度提升，并且微调后模型在结构化医疗错误纠正任务上超过了人类专家水平。

Conclusion: MedRECT作为首个跨语言医疗错误纠正基准，为安全医疗大型语言模型的跨语种开发提供了标准化的数据和评测框架，推动了医疗语言模型在多语言环境下的应用进展。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [15] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的插件式方法G2，利用双引导机制提升大语言模型的输出多样性，且保持生成质量，实现多样性与质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成多样性方面存在严重限制，常生成高度相似的内容，影响创意写作与推理等任务的效果，现有方法提高多样性往往牺牲质量。

Method: G2方法结合基础生成器与两个引导器，通过解码干预引导生成过程，促进基于原始查询的多样输出，无需额外训练。

Result: 实验表明G2显著提升了输出多样性，同时保持了生成质量的最佳平衡，改善了生成内容的丰富性。

Conclusion: G2是一种有效的训练-free策略，能在保证生成质量的前提下显著提升大语言模型输出的多样性，适用于多种自然语言处理任务。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [16] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本论文研究大语言模型(LLMs)的记忆机制对学术合著网络的影响，揭示了模型记忆可能带来的公平性和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术搜索和推荐领域的应用日益增加，其基于记忆生成的结果可能放大科研合作网络中的现有偏见，影响信息生态的公正性。

Method: 评估三种主流LLMs（DeepSeek R1、Llama 4 Scout和Mixtral 8x7B）在不同学科和地区上的记忆影响，分析其输出结果的公平性和偏见表现。

Result: 全球范围内，模型普遍偏向引用量高的研究者，但在临床医学等学科和非洲部分地区的表现较为平衡，体现了训练数据中存在差异。

Conclusion: LLMs在学术发现中的应用既带来风险，也存在促进公平的可能，需警惕记忆偏见，优化训练数据以提升信息的公正性。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [17] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文介绍了全球首个也是最大规模的Bhili-印地语-英语三语并行语料库，包含11万句，旨在促进低资源的部落语言机器翻译研究。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性带来了机器翻译的挑战，尤其是像Bhili这样资源极少的部落语言缺乏高质量语料。

Method: 构建了由专家翻译人员精心校对的三语并行语料库BHEPC，并评测多种多语言大模型在Bhili与印地语、英语之间的翻译性能，尤其针对NLLB-200模型进行了微调和评测，同时考察了多语言大模型的生成翻译能力和泛化能力。

Result: 微调后的NLLB-200 600M模型优于其他模型，表明多语言模型在低资源语言翻译中潜力巨大。

Conclusion: 该工作极大弥补了Bhili语言资源的空缺，推动了边缘和低资源语言的自然语言处理技术发展，促进了语言平等与包容。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [18] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次引入数据集规模对差分隐私文本改写机制效果的影响，设计了基于大规模数据集的实用性和隐私测试，揭示数据规模对隐私-实用性权衡的重要作用。


<details>
  <summary>Details</summary>
Motivation: 目前差分隐私自然语言处理中的文本改写机制评估中，常忽略数据集规模对机制效果的影响。

Method: 设计动态数据拆分比例的大规模数据集实用性和隐私测试，在不同规模数据集（最多百万条文本）上运行测试。

Result: 发现数据集规模显著影响差分隐私文本改写的隐私与实用性权衡，表明规模是机制评估的重要因素。

Conclusion: 呼吁差分隐私自然语言处理领域采用更严格的评估流程，关注数据规模对机制效果的实际影响，为未来大规模应用提供参考。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [19] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出了一种基于树形结构的长上下文推理框架ToM，通过层级语义解析构建文档树，采用类似MapReduce的方法进行递归推理，实现了较好的逻辑连贯性和长距离依赖捕获。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型因上下文窗口限制，在长文本推理时性能下降，现有方法如基于检索生成和分块推理虽有优势但存在逻辑一致性差和长距离依赖处理不足的问题。

Method: ToM框架利用文档的层级结构，构建DocTree，采用树形的MapReduce策略：在Map阶段子节点生成推理理由，在Reduce阶段同级节点间整合推理理由以解决冲突或达成共识，实现递归推理。

Result: 在70B+参数的大模型上，ToM显著优于现有的分块推理和基于检索的生成方法，在逻辑连贯性和长距离上下文推理能力上表现更佳。

Conclusion: 利用文档的层级结构结合树形MapReduce推理，ToM有效提升了长上下文推理的逻辑一致性和性能，为大语言模型处理长文本推理提供了新思路。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [20] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 该论文提出Zero-RAG方法，通过剪枝外部知识库中的冗余知识，提升大语言模型（LLMs）知识利用效率，减少检索工作负担，同时提升检索增强生成（RAG）的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型内部知识的增加，外部语料库与LLMs之间存在大量冗余知识，导致密集检索开销大且冗余信息影响模型回答能力。

Method: 提出Mastery-Score指标用于识别并剪枝外部知识库中的冗余知识，利用Query Router和Noise-Tolerant Tuning提升模型对内部知识的利用率。

Result: Zero-RAG剪枝了维基百科语料库30%，检索速度提升22%，且未损害RAG模型性能。

Conclusion: 通过剪枝冗余知识和优化检索策略，Zero-RAG有效提升了RAG方法的效率和效果，充分利用了大语言模型的内部知识。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [21] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 本研究通过微调离线运行的轻量级对话模型DialoGPT，基于合成的农村尼泊尔常见疾病数据集，实现了医疗对话系统在无互联网环境下的应用。


<details>
  <summary>Details</summary>
Motivation: 农村地区互联网接入受限，传统依赖云端的大规模对话模型难以应用，需要开发离线、轻量且适应当地疾病特点的医疗对话系统。

Method: 使用合成构建的医生-患者对话数据集（涵盖10种常见疾病）微调DialoGPT模型，使其能生成符合医疗语境的对话内容。

Result: 微调后模型能够生成连贯且医学适当的响应，表现出对症状、疾病背景及同理心交流的理解。

Conclusion: 轻量离线对话模型结合针对性数据集可有效适应低资源医疗环境，具备为农村医疗对话AI提供解决方案的潜力。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [22] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了基于Transformer的语言模型中的性别偏见问题，提出了新的偏见度量方法，并通过持续预训练与反事实数据增强有效降低了偏见。


<details>
  <summary>Details</summary>
Motivation: 当前的编码器Transformer模型在语言任务中表现优异，但其训练数据中存在的性别偏见被模型继承，亟需量化和缓解这种偏见。

Method: 引入了基于模型填空概率的性别偏见新度量指标MALoR，采用反事实数据增强生成性别平衡的数据集，进行持续预训练以减轻偏见。

Result: 在BERT-base和其他变体上，性别偏见指数显著降低，如“he-she”偏见从1.27降至0.08，同时模型在下游任务的表现未受影响。

Conclusion: 通过反事实数据增强进行持续预训练的方法有效减少了Transformer模型中的性别偏见，同时保持了模型性能。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [23] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 该论文提出了WordSaladChopper (WSC)，一种检测并剪切大型推理模型（LRMs）生成的无用自我重复“word salad”词汇的方法，以节省解码预算并保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成输出时，存在大量无用的重复词汇（word salad），浪费了解码预算且无实际价值。

Method: 通过分析隐藏状态，利用单层线性分类器实时检测word salad行为；一旦检测到，通过剪切和重新生成提示来减少冗余词汇。

Result: WordSaladChopper有效节省了序列长度，降低成本，同时对输出质量影响极小。

Conclusion: WSC是一种轻量且侵入性小的组件，适合所有需要高效、优质用户体验的大型推理模型应用，强烈建议集成使用。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [24] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 本文提出了基于预训练语言模型的CISEA-MRFE框架，通过上下文指令、语义增强和多重特征提取，提升情感分析在细微情绪、领域变化和不平衡情感分布场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析方法在处理细微情绪线索、领域迁移和情感类别不平衡时表现不佳，主要由于语义基础不足、对多样语言模式泛化能力差及偏向主导情感类别。

Method: 提出CISEA-MRFE框架，包括上下文指令（CI）注入领域感知指导、语义增强数据扩充（SEA）提升稳健性，以及多重特征提取（MRFE）结合尺度自适应编码器（SADE）和情感评估上下文编码器（EECE）实现多尺度特征专化和情感感知序列建模。

Result: 在IMDb、Yelp、Twitter和Amazon四个基准数据集上，CISEA-MRFE相较于强基线模型分别提升准确率4.6%、6.5%、30.3%和4.1%。

Conclusion: CISEA-MRFE框架有效提升了情感分类的准确性和泛化能力，尤其在多领域和情感分布不均等复杂场景表现优异。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [25] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为ISA的意图转换攻击方法，通过最小修改原始请求，伪装成无害信息，使大语言模型(LLMs)误判攻击意图，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然强大，但仍易被越狱攻击利用，现有攻击方法多通过添加上下文或对抗性词汇，未改变核心恶意意图，防御困难。通过混淆攻击意图，可以更有效地突破模型安全机制。

Method: 设计意图转换的分类体系，利用最小编辑生成自然且易读的提示，欺骗模型误以为是无害请求。该方法不依赖复杂词汇或长文本。并通过实验对比ISA和传统攻击效果。

Result: ISA在开源和商用模型上攻击成功率提高70%以上，且通过仅用ISA模板生成的无害数据微调模型后，攻击成功率接近100%。测试显示现有防御手段对ISA效果不足。

Conclusion: 意图推断是大语言模型安全防护的根本挑战，现有防御尚不充分，未来需开发更有效的防御策略。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [26] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: 本文提出了FlashEVA，一种高效的EVA注意力机制实现，通过微调Transformer模型，实现推理阶段显著降低内存占用和提升吞吐率。


<details>
  <summary>Details</summary>
Motivation: Transformer模型性能强大但对内存要求高，尤其是维护全上下文时，导致推理阶段效率低，亟需高效解决方案。

Method: 提出FlashEVA实现EVA注意力，通过微调仅用1.5B tokens，使模型适配FlashEVA；提供超参数调整以权衡吞吐量和准确率。

Result: 推理时吞吐量提升最多6.7倍，峰值GPU内存使用降低5倍；保持多任务效果，检索任务表现有限。

Conclusion: FlashEVA显著提升Transformer推理的效率和适应性，是实现高效可调Transformer模型的重要进展。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [27] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: 本论文提出了OpenSIR，一种通过自我对弈实现开源式自我提升的推理框架，不依赖外部监督。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大语言模型推理方法依赖带注释的数据集，限制了模型超越人类水平的能力。自我对弈虽有潜力，但依赖外部验证器或无法实现开放式学习。

Method: OpenSIR通过让LLM交替扮演教师和学生角色，自生成并解决新问题。其问题生成优化难度与多样性，鼓励挑战性和探索不同概念，实现开源的数学发现。

Result: 从单一简单起始问题开始，OpenSIR显著提升模型表现：Llama-3.2-3B-Instruct在GSM8K和College Math上的成绩分别提升至78.3和34.4，Gemma-2-2B-Instruct在GSM8K上的成绩提升至58.7。

Conclusion: OpenSIR通过教师-学生角色的协同进化，自适应调整难度驱动多样探索，实现了从基础到高级数学的自主开放式学习。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [28] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: 本文提出了一种名为SpecDiff-2的推测性解码框架，通过结合离散扩散非自回归起草与自回归验证，实现了语言模型推理加速和精度无损。


<details>
  <summary>Details</summary>
Motivation: 当前推测性解码受限于自回归起草的串行依赖和起草与验证模型误差导致的令牌拒绝，限制了加速效果。

Method: SpecDiff-2利用离散扩散作为非自回归起草器，提升并行度，同时设计校准技术协调起草器与验证器，减少误拒。

Result: 在推理、编程及数学基准测试中，SpecDiff-2相较于现有方法平均提升55%处理速度，较标准解码快5.5倍，且无准确度损失。

Conclusion: SpecDiff-2有效突破了推测性解码的关键瓶颈，显著提升了大语言模型的推理效率，为高速准确的模型推断提供了新方案。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [29] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 研究了大型语言模型的确定性与理论概率分布的对齐性，发现尽管模型能准确回应概率题，但其输出的概率分布与理论概率存在偏差。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型在决策支持等关键应用中的可靠不确定性量化，提升模型的可信度。

Method: 使用GPT-4.1和DeepSeek-Chat对10个涉及概率的提示进行测试，测量模型响应的有效性及其生成概率与理论概率分布的对齐度。

Result: 两个模型在所有提示场景下都能达到完美的响应准确性，但输出的token级概率和熵值与理论概率分布持续存在偏差。

Conclusion: 尽管大型语言模型能正确回应概率题，其内部概率估计不完全符合理论概率，说明当前的不确定性量化方法还需改进以确保决策支持应用的可靠性。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [30] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 通过计算分析，本文研究了法国侦探小说中侦探原型在150年间的演变，揭示其从次要角色成为故事核心，并在二战后变得复杂。


<details>
  <summary>Details</summary>
Motivation: 探讨法国侦探小说中侦探形象如何随着时间演变及其文学意义。

Method: 采用定量方法和角色级嵌入，通过监督模型分析150年的侦探形象变化。

Result: 模型成功捕捉了侦探原型的统一性和演变过程；侦探由次要角色转变为核心推理者，二战后形象更加复杂。

Conclusion: 侦探原型随着历史和社会背景变化而演进，反映出侦探小说对社会暴力和道德模糊性的回应。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [31] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了一个新的多语言问答基准XNationQA，涵盖9个国家的地理、文化和历史问题，评估多语言大型语言模型对区域文化信息的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多语言问答基准偏西方中心，缺乏区域文化多样性的公平评估，导致不能全面反映模型对全球文化事实的理解。

Method: 构建包含49280个问题、涉及9个国家7种语言的XNationQA数据集，使用两个新的跨语言迁移评估指标，对8个多语言大型语言模型进行测试。

Result: 发现模型对文化特定信息的访问存在显著差异，通常在英语表现更好，而非该文化的主导语言；模型在西方语言表现优异，但并不代表对西方国家文化了解更深入；模型知识跨语言迁移能力较弱，开源模型表现尤为明显。

Conclusion: 多语言大型语言模型存在文化信息理解和跨语言知识迁移的显著短板，尤其在非西方语言和文化中表现不足，提示未来研究需更多关注全球文化多样性和跨语言能力提升。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [32] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次系统评估了大型语言模型（LLMs）绕过安全机制的多语言越狱攻击及防御效果，发现攻击成功率和防御效果在不同语言间差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示LLMs的安全对齐机制可以被绕过，但多语言环境下绕过攻击的跨语言适用性与防御效果尚未充分研究。

Method: 在十种语言（涵盖高、中、低资源语言）和六个LLM模型上，使用HarmBench和AdvBench测试平台，系统评估两类越狱攻击（基于逻辑表达式和对抗性提示）及相应的防御方法。

Result: 结果显示高资源语言在标准查询下更安全，但更易受对抗性攻击，防御效果受语言和模型影响显著。简单防御措施在某些语言和模型中有效。

Conclusion: LLMs的安全评估需要考虑语言特性，呼吁建立多语言安全基准以提升跨语言防护能力。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [33] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 本文对Native Sparse Attention (NSA)进行系统分析，提出改进方案以提升长文本建模效果。


<details>
  <summary>Details</summary>
Motivation: 固定的局部和全局注意力模式限制了长距离依赖信息的有效传播，影响长序列任务表现。

Method: 通过层间交替使用局部滑动窗口注意力和全局压缩选择性注意力，同时引入多头潜在注意力(MLA)和组头潜在注意力(GLA)优化NSA分支，减少内存占用。

Result: 新方法在340M到1.3B参数规模模型上，训练15B到100B词元，KV缓存内存减少50%，常识推理和长文本理解能力显著提升，性能优于完全注意力和NSA。

Conclusion: 交替局部与全局注意力结合潜在注意力技术，有效增强了长距离依赖传播和模型性能，提升了长序列任务表现。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [34] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: 提出了一种新的对比学习框架TriCon-Fair以消除大语言模型中的社会偏见，实现减少歧视性输出并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前的去偏方法忽视偏见和无偏样本之间的相互关系，导致去偏效果受限，社会偏见仍然存在。

Method: 引入TriCon-Fair框架，该框架通过三元组和语言建模损失的解耦组合，分配每个锚点一个明确的有偏负样本和无偏正样本，避免正负样本耦合，联合优化语言模型目标。

Result: 实验表明TriCon-Fair在减少歧视输出方面优于现有去偏基线，同时保持较强的下游任务表现。

Conclusion: TriCon-Fair为敏感NLP应用提供了一个有效且符合伦理的去偏解决方案。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [35] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文提出了一套创新的评估体系，用于系统性地检测大型语言模型在中间推理过程中的知识基础准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型通过逐步推理解决复杂任务，但缺乏有效方法验证其推理是否准确基于知识。

Method: 构建了一个包含基础知识的大型知识库，并基于此设计知识基础的评估指标，利用轻量级评估模型计算指标，实现对模型推理知识调用的测量。

Result: 该评估体系能够有效识别模型推理中缺失或错误应用的知识元素，帮助发现模型推理的根本缺陷。

Conclusion: 知识基础的评价不仅提升了推理准确性的检测，还能被整合入偏好优化，拓展了语言模型评估与优化的应用范围。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [36] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一种专为多模态文档检索设计的模型，通过创新的预训练目标和评分机制显著提升了检索性能，在ViDoRe V2基准测试中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态文档检索方法多沿用文本检索技术，忽视了多模态文档的视觉和结构特性，限制了检索效果的提升。

Method: 提出ColMate模型，采用基于OCR的新型预训练目标、自监督掩码对比学习以及适合多模态文档的晚期交互打分机制。

Result: ColMate在ViDoRe V2基准测试上比现有模型提升了3.61%，且表现出了更强的跨领域泛化能力。

Conclusion: 通过结合多模态特征的预训练和评分策略，ColMate有效提升了多模态文档检索的准确性和泛化能力。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [37] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在医疗诊断沟通中的理解性和同理心表现，发现其内容复杂且存在情感偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在辅助医生进行诊断沟通，生成患者解释和指导方面表现出潜力，但其输出的理解性和同理心尚不明确。

Method: 通过对两个主流大型语言模型在医疗诊断场景中的表现进行评估，使用可读性指标衡量理解性，通过模型作为评判者的评分与人类评价比较同理心。

Result: 模型能根据社会人口变量和患者状况调整解释内容，但生成过于复杂的文本，且情感同理心存在偏差，导致其支持和可访问性不均衡。

Conclusion: 需要对大型语言模型进行系统的校准，以确保其在患者沟通中的公平性和有效性。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [38] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在印地语等七种印度语言中的文化推理能力，评估了五种模型在解谜及自我评估上的表现，发现高准确率模型自信过度，低准确率模型反而更能识别错误，揭示多语言推理存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在多种非英语语言环境下进行文化背景推理的能力，特别是在印度主要语言中的表现，以填补相关研究空白。

Method: 构建多语言谜语数据集，结合传统谜语和上下文重构版本，使用七种提示策略评估Gemini 2.5 Pro等五个模型的解谜能力和自我评估能力。

Result: Gemini 2.5 Pro整体表现最佳，但几次示例提示提升有限；语言间准确率差异明显；自我评估实验显示模型准确率与其识别自身错误能力负相关，高准确率模型过于自信。

Conclusion: 当前大型语言模型在多语言文化推理中存在显著不足，尤其是模型自身识别错误的能力有限，未来需开发既能有效推理又能识别自身局限性的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [39] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 本文研究机器生成文本的检测方法，指出传统方法存在标签模糊问题，提出一个由易到难的增强框架以提供更可靠的监督，显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法默认标签为绝对标准，忽视了标签的边界模糊性和人类认知及检测器的限制，导致训练存在不精确性。

Method: 提出一个由易到难的增强框架，利用较简单的长文本检测任务作为监督者，通过结构嵌入检测器，建立监督者的性能下界以间接优化检测器，实现对“黄金”标签的近似。

Result: 在跨大模型、跨领域、混合文本及释义攻击等多种实际场景中进行了大量实验，验证了该框架在提升机器生成文本检测效果上的显著性能。

Conclusion: 该框架有效解决了标签不精确问题，通过易监督者辅助难检测器的方式提升了检测性能，具备广泛应用潜力，并公开了代码为后续研究提供支持。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [40] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL提出了一个多智能体框架，结合任务分解与交互式强化学习，实现复杂自然语言到SQL的转换，自我纠正性能显著提升，取得最新执行准确率。


<details>
  <summary>Details</summary>
Motivation: 复杂查询的自然语言转SQL任务难度大，需环境交互及自我纠正能力以提高准确率。

Method: 设计地基体、生成和验证三个智能体；生成智能体通过多轮ReAct风格强化学习与数据库交互自我修正；验证智能体基于生成概率选最优解。

Result: 在BIRD和Spider数据集上分别达成77.84%和89.75%的执行准确率，表现优于现有方法。

Conclusion: MARS-SQL有效结合了分工明确的多智能体和交互式RL，显著提升了复杂查询的SQL生成质量。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [41] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出了IF-CRITIC，一种高效且可靠的指令遵循评估模型，通过生成约束清单和多阶段筛选机制训练，实现了性能超越现有强基线的方法，降低了计算成本，同时显著提升了指令遵循优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的指令遵循评估模型存在高成本和评估不可靠的问题，限制了大语言模型在指令遵循能力提升上的效果。

Method: 提出IF-CRITIC，通过开发指令分解及约束清单生成器，采用多阶段筛选机制收集高质量训练数据，结合约束级别偏好优化方法训练评估模型。

Result: IF-CRITIC在评估性能上超过Deepseek-R1和o4-mini等强基线，且提供的奖励信号使得指令遵循优化性能显著提升，计算开销更低。

Conclusion: IF-CRITIC为大语言模型指令遵循的评估与优化提供了一种高效、可靠且低成本的解决方案，推动了相关能力的提升。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [42] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 提出了一种结合小型和大型语言模型的强化学习框架Prompt-R1，通过代替用户交互生成更准确的提示以提升大型语言模型对复杂问题的解决能力。


<details>
  <summary>Details</summary>
Motivation: 用户往往无法针对复杂问题提供准确有效的提示，限制了大型语言模型的性能发挥。

Method: 设计Prompt-R1，一个端到端强化学习框架，利用小型语言模型生成多轮提示，与大型语言模型协作解决问题。采用双重约束奖励优化输出的正确性、生成质量和推理准确性。

Result: 在多个公开数据集上，Prompt-R1显著优于基线模型。

Conclusion: Prompt-R1框架有效改善了大型语言模型在复杂任务中的表现，支持多种大规模语言模型的推理和训练，代码已开源。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [43] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 本文提出了MicroRemed，首个用于评估大语言模型在微服务系统自动修复中的基准，及ThinkRemed多智能体框架提升修复性能。


<details>
  <summary>Details</summary>
Motivation: 当前微服务系统自动修复依赖人工设计提示，大语言模型仅将文本转为可执行代码，缺乏端到端自动修复能力。

Method: 引入MicroRemed基准评测模型从诊断报告直接生成可执行的Ansible剧本，并设计多智能体框架ThinkRemed模拟SRE的反思与感知推理。

Result: 实验表明MicroRemed对现有大语言模型具有较大挑战，ThinkRemed通过迭代推理和系统反思显著提升了修复效果。

Conclusion: MicroRemed为微服务自动修复领域提供了首个评测标准，ThinkRemed展现了多智能体推理提升端到端修复的潜力。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [44] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一种结合自然语言处理和实时海洋数据的对话式AI平台，能基于NOAA权威数据提供准确且可验证的科学回答。


<details>
  <summary>Details</summary>
Motivation: 一般对话式AI经常产生未经验证的错误信息，影响科学严谨性，迫切需要结合权威数据提升准确性和可信度。

Method: OceanAI调用NOAA的实时API，解析并综合相关海洋数据，通过自然语言回答并生成数据可视化，支持多种海洋科学应用。

Result: 在与三个主流AI聊天产品的盲测中，只有OceanAI能提供带有原始数据引用的权威答案，其他系统要么拒答要么给出无支持的数据。

Conclusion: OceanAI通过结合权威数据实现输出的透明性、可复现性和可信赖性，推动了海洋领域AI辅助决策的应用与扩展。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [45] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: 提出了VayuChat，一个结合空气质量、气象和政策数据的对话系统，利用自然语言交互辅助环保决策。


<details>
  <summary>Details</summary>
Motivation: 印度空气污染致死率高，但决策者难以从分散数据中快速获取有用信息，现有工具专业门槛高且静态，无法满足政策制定需求。

Method: 开发基于大型语言模型的VayuChat对话系统，集成多个数据源，支持自然语言问题回答，输出Python代码和交互式可视化。

Result: VayuChat能通过简便对话完成复杂环境数据分析，提高数据科学可及性，支持政策制定和公众参与。该平台已公开部署，方便用户使用。

Conclusion: VayuChat展示了将AI与环境数据结合，促进环保政策制定的信息技术新方向，实现了简易交互与复杂分析的有机结合。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [46] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 提出了一个基于公开医疗指南的验证数据集，评估大语言模型在临床推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在医疗领域应用增多，但缺乏统一的基于指南的临床推理评估标准。

Method: 利用GPT协助构建包含真实患者场景和临床问题的数据集，并对多种流行LLMs进行基准测试。

Result: 验证数据集有效，能够系统评估LLMs的临床实用性和指南遵守情况。

Conclusion: 所提框架可推动LLMs在医疗领域的标准化评估和应用。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [47] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 提出了一个涵盖近200种语言、包含约30万亿标注丰富的文本数据集，用于大型语言模型的多语言预训练，配备开放源码的数据处理流水线，并提供多语言评测基准和多种单语及平行语料模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大型、优质且覆盖多种语言的公开多语言预训练数据集，限制了多语言大型语言模型的发展。

Method: 通过多来源网页抓取构建大规模多语言语料，结合开源数据处理流水线实现文档选择、语言识别、去重和多维度注释，同时开展多语言基准测试和单语模型训练。

Result: 获得了涵盖200种语言的30万亿标注文本数据，验证了数据质量和多语言模型性能，并发布了多语言评测基准和多语种、单语种模型成果。

Conclusion: 该工作显著推动了多语言大型语言模型的训练数据与评测资源建设，为多语言自然语言处理研究提供了重要基础设施和工具。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [48] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文研究了罗马尼亚语预训练语料的数据质量，利用多任务模型对数据进行多层次过滤以提升大语言模型的预训练效果。


<details>
  <summary>Details</summary>
Motivation: 高质量数据对大语言模型训练至关重要，特别是对资源稀缺的语言如罗马尼亚语，因此需要研究和优化其预训练语料。

Method: 利用轻量级多任务模型对经大语言模型注释的罗马尼亚语文本进行多层次过滤，包括教育价值、主题和格式等。

Result: 发现罗马尼亚语和英语语料在主题上存在显著差异，同时多层过滤有效提升了大语言模型在多个基准测试上的表现。

Conclusion: 通过多任务模型的多层过滤方法，可以生成高质量的罗马尼亚语预训练数据，有助于提升大语言模型在低资源语言上的训练效果。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [49] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出了TSVer数据集，专注于时间序列的时间和数值推理事实核查，数据集包含真实世界声明和标注的时间序列证据，并通过多步骤注释保证质量。基线模型在该数据集上的表现有限，显示时间序列推理的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统在处理时间序列证据时，受限于数据集缺乏结构化证据、不充分的判决理由或依赖合成声明，影响了评估效果。

Method: 构建TSVer数据集，包含287条真实声明和400个多领域时间序列证据，采用LLM辅助的多步骤注释方法提升标注质量，并设计基线模型用于验证时间序列事实核查。

Result: TSVer数据集的标注间一致性达到kappa=0.745，基线模型（如Gemini-2.5-Pro）在判决准确率为63.37，判决理由准确度Ev2R为48.63，显示时间序列推理极具挑战。

Conclusion: TSVer提供了一个高质量、结构化的事实核查时间序列数据集，当前最先进模型在此任务上的表现仍有较大提升空间，未来研究需进一步攻克时间序列事实核查难题。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [50] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 本文研究销售人员在高频外呼销售中决定何时结束对话的问题，提出了一种基于生成式语言模型的动态停止决策代理，显著提升了销售效率。


<details>
  <summary>Details</summary>
Motivation: 销售人员在对话中面临是否继续或放弃当前潜在客户的动态筛选决策，然而相关决策效率及改进方法尚不清楚，需要有效模型辅助优化时间与销售成果的权衡。

Method: 将动态筛选决策形式化为最优停止问题，开发了一种基于生成语言模型的顺序决策代理，该代理通过模仿回溯推断的最优停止策略来学习何时终止对话，支持高维文本状态并兼容开源及专有语言模型。

Result: 在一家欧洲大型电信公司的通话数据上，停止代理将无效通话时间减少了54%，几乎保持所有销售额不变，节省的时间重新分配后销售额提升了最多37%。研究发现销售人员倾向过度依赖少量明显表示消费者不感兴趣的语言线索，导致对通话失败风险的误判。

Conclusion: 人工智能算法能纠正受认知限制的人工决策，提升销售效率，具备显著应用潜力。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [51] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: 该论文构建了一个多语种辩论式偏见评估基准DebateBias-8K，揭示大语言模型在多语言、开放式生成中的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估多依赖英语分类任务，缺乏多语言、真实生成环境中的偏见测评工具。

Method: 设计包含四大敏感领域、七种语言的辩论式提示，利用四款大型模型生成并自动分类超10万条回答。

Result: 所有模型均表现出根深蒂固的刻板印象，尤其在低资源语言中偏见更明显，多数偏见与训练语言偏差相关。

Conclusion: 现有模型调优虽减少明确有害内容，却未能消除开放式多语言环境中的偏见，亟需多语种公平性评估及文化包容的模型校准方法。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [52] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ZoFia的两阶段零样本假新闻检测框架，通过多视角多模型协作及外部动态证据检索实现对假新闻的准确识别。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型因知识时效性限制和幻觉生成问题，在处理快速变化的新闻时表现不佳；静态数据训练模型也难以泛化到新兴新闻主题，导致假新闻检测的效果受限。

Method: 该方法首先通过层次显著性度量新闻实体的重要性，并设计SC-MMR算法选择多样且信息丰富的关键词，用于检索最新的外部证据。其次，构建多大语言模型交互系统，各模型扮演不同角色，进行协同分析与对抗辩论，最终给出可解释且鲁棒的判断结果。

Result: 在两个公开数据集上的全面实验表明，ZoFia显著优于现有的零样本基线方法及多数少样本方法。

Conclusion: 该研究展示了结合动态外部证据检索和多模型协作机制的零样本假新闻检测框架的有效性，提升了假新闻识别的准确性和解释性，有助于相关社区的研究与应用发展。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [53] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: 提出了Self-Harmony框架，通过在原始问题及其改写版本中保持答案稳定性，以无标签的方式在推理时自适应模型，避免了传统投票法的陷阱。


<details>
  <summary>Details</summary>
Motivation: 现有测试时强化学习方法依赖于可靠的学习信号，但多数投票法常陷入表面流行但错误的答案，从而影响自适应效果。

Method: Self-Harmony利用单一模型在两个角色（解答者与改写者）间切换，通过计算原始问题和改写问题答案频率的调和平均值来生成伪标签，选择对改写具有稳定性的答案，无需人工监督或辅助模型。

Result: 在多个推理基准测试中，Self-Harmony获得了无标签测试时最先进的结果，在30个设置中排名第一达28次，表现出极高的准确性和鲁棒性，所有实验均未出现训练失败。

Conclusion: Self-Harmony是一种稳定可靠的无监督测试时自适应方法，通过保持答案在改写间一致，有效避免了视角依赖的错误答案，实现了性能和稳定性的双重提升。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [54] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 本文提出了一种名为DEER的两阶段解耦混合专家架构，用于提升机器生成文本检测的跨域性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法在领域迁移时性能显著下降，需要解决不同领域间的特征转移问题。

Method: 设计了解耦混合专家模块，区分领域专家和共享专家，用以捕获领域特定和通用特征；并利用基于强化学习的路由机制动态选择专家，解决推理时缺少领域标签的问题。

Result: 在五个域内和五个域外数据集上，DEER在F1分数和准确率方面均显著优于现有最先进方法，域外提升尤为明显。消融实验验证了解耦专家和自适应路由的关键作用。

Conclusion: DEER框架有效提升了机器生成文本检测的跨域能力，为应对领域不确定性提供了实用方案。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [55] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文提出了最大规模的阿拉伯语金融新闻摘要数据集AraFinNews，并利用该数据集评估了多种基于transformer的语言模型在金融领域摘要生成的表现。


<details>
  <summary>Details</summary>
Motivation: 研究领域特定预训练对阿拉伯语金融新闻抽象摘要质量的影响，提高摘要的事实准确性和叙事流畅性。

Method: 收集并构建包含212,500条阿拉伯语金融新闻及其标题的数据集AraFinNews，基于该数据集评估mT5、AraT5及领域适应模型FinAraT5的摘要生成质量。

Result: 领域适应的FinAraT5模型在生成摘要时，在事实准确性、数字可靠性及风格一致性方面表现更佳，特别是在处理定量信息和实体相关内容时效果显著。

Conclusion: 领域特定的预训练能显著提升阿拉伯语金融新闻摘要的事实一致性和叙述流畅性，强调了金融领域适应的重要性。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [56] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出了ReSpec，一种基于检索增强的推测解码框架，通过自适应触发、反馈驱动候选选择和源感知松弛验证策略，有效提高了大语言模型推理的速度，同时保证输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法受限于草稿模型的有效性，模型基方法准确但成本高，而检索增强方法依赖启发式切换，常触发不必要的检索，影响效率。

Method: ReSpec通过熵引导的自适应触发减少不必要检索，利用历史反馈组织多个高质量候选并行验证，且采用针对不同草稿源采用严格或松弛的验证策略，平衡准确性和效率。

Result: 在Spec-Bench大规模实验中，ReSpec在保持输出质量的同时，推理加速分别超过了EAGLE-2和SAM-Decoding 33%和25%。

Conclusion: ReSpec框架有效提升推测解码的加速效果，且能平衡推理速度与输出质量，优于现有先进方法。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [57] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文系统研究了针对AI辅助科学论文评审的注入型攻击，提出静态和迭代两类攻击方法，均能显著操控AI评分，且攻击在多种环境下均具鲁棒性。提出简单检测防御方法，但攻击者可部分绕过。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科研评审中的广泛应用，存在通过隐藏注入提示操控AI评审评分的潜在威胁，亟需系统研究与防范。

Method: 提出静态注入攻击和迭代注入攻击，前者使用固定注入提示，后者在模拟评审模型上优化注入提示以最大化攻击效果。并设计检测机制进行防御。

Result: 两类攻击均能诱导AI评审给出极高评分，且攻击效果在不同设置中表现鲁棒。检测防御能显著降低攻击成功率，但复杂攻击仍可部分绕过。

Conclusion: 提示注入攻击对AI辅助论文评审构成严重威胁，需加强关注和严格防护措施，确保评审系统的公正性和可靠性。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [58] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 该论文介绍了FirstAidQA数据集，这是一个包含5500条急救问答对的合成数据集，旨在支持轻量级语言模型在紧急救援环境中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型计算资源需求高，不适合急救现场常用的低端设备，且缺乏高质量的急救领域数据集制约了此类轻量模型的发展。

Method: 采用ChatGPT-4o-mini模型基于上下文学习生成初始问答对，利用《重要急救手册》文本进行训练，随后通过文本清洗、上下文切分、过滤及人工验证保证数据质量和安全性。

Result: 生成了5500条高质量急救问答对数据集FirstAidQA，该数据集覆盖广泛急救和应急响应场景，支持模型的指令微调及离线运行，提高模型响应速度和可靠性。

Conclusion: FirstAidQA数据集有效填补了急救领域轻量级语言模型训练数据的空白，有助于推动紧急情况下资源受限的AI应用的研究和发展，并已公开发布以供社区使用。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [59] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: 本文提出了DeepSpecs，一个增强结构和时间推理能力的检索增强生成系统，用于解答复杂的5G规格问题。


<details>
  <summary>Details</summary>
Motivation: 5G标准内容庞杂且跨版本演变，现有基于语义相似性的检索生成方法难以准确解析跨引用和规范演进问题。

Method: DeepSpecs利用三个元数据丰富的数据库（规格文本、版本差异和会议文档），通过递归检索和元数据查找显式解析跨引用，并追踪版本演进以及关联变更请求。

Result: 构建了两套5G问答数据集，实验证明DeepSpecs在多个大语言模型基础上显著优于基线及现有电信领域RAG系统，删除关键模块会降低答案质量。

Conclusion: 通过结合结构化和时间信息，DeepSpecs显著提升了5G标准问题回答能力，验证了显式建模标准结构和演进的重要性。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [60] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 提出了DeepAmbigQA数据集，评估语言模型在含歧义和多跳推理的复杂问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽有强大问答能力，但难以处理涉及名称歧义和多跳推理的复杂问题，且现有基准很少同时评估这两类挑战。

Method: 开发了DeepAmbigQAGen自动数据生成流程，结合文本语料和知识图谱自动生成包含名称歧义和多步推理的自然且可验证问题；基于此构建了3600个问题的DeepAmbigQA数据集。

Result: 在DeepAmbigQA测试中，最先进的GPT-5在含歧义问题上准确匹配率仅为0.13，非歧义问题为0.21，表明回答不完整。

Conclusion: 当前问答系统在处理名称歧义和多步推理任务时表现不足，亟需开发更多关注信息获取和答案完整性的鲁棒模型。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [61] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文提出了四类专为工业应用设计的DistilQwen蒸馏模型，兼顾推理性能和推理速度，支持云平台训练和推理。


<details>
  <summary>Details</summary>
Motivation: 为了满足实际应用中对小型高效推理模型的需求，提升推理性能与速度的平衡。

Method: 基于Qwen模型，设计四种蒸馏模型系列：慢思考模型、高准确性推理；两种自适应思考模型，动态调整推理策略；蒸馏奖励模型，支持强化学习。

Result: 多基准测试显示模型具有高推理效率和良好推理性能，蒸馏奖励模型在实际中表现良好。

Conclusion: 该系列模型适用于工业需求，提供可扩展的训练与推理功能，支持阿里云PAI平台应用。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [62] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文提出了一种用于自然语言推理（NLI）的新模型MiniTruePrefixes，专门检测文本前缀中的事实不一致性，从而提升大语言模型（LLM）生成的内容的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 目前NLI模型通常在完整句子上检测事实一致性，但在自回归生成过程中需要在逐步生成的文本前缀上判断事实一致性，现有方法难以满足这一需求。

Method: 作者扩展了蕴涵检测任务，应用于任意文本前缀，构建了相应的训练和评估数据集，训练了MiniTruePrefixes模型以在文本前缀层面更好地检测事实不一致。

Result: MiniTruePrefixes在文本前缀级别比传统NLI模型提高了5-14 F1分，且将其整合进控制解码框架后，在抽象摘要任务中显著提升了事实一致性。

Conclusion: 通过在生成过程中结合MiniTruePrefixes，引导生成的模型在内存占用减半的情况下，达到了与更大规模模型相媲美的事实准确率和推理速度。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [63] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 本文提出了一个针对癌症相关误区的多语言指标评估基准Cancer-Myth-Indic，涵盖五种南亚次大陆语言，并评估了多个大型语言模型在这些语言上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医疗领域大型语言模型的评估几乎全部集中在英语，缺乏多语言特别是南亚语言的评估基准，导致该区域用户的医疗咨询效果难以验证。

Method: 基于Cancer-Myth数据集，翻译了其中500条项目到五种南亚语言，共计2500条，由母语翻译人员按照保持隐含前提的风格指南完成，同时设计包含癌症相关错误前提的问题，检测模型在这种压力下的表现。

Result: 多个流行的大型语言模型在这些南亚语言的隐含前提测试中表现被评估，具体结果未详述，但表明了模型在多语环境下的有效性和存在的挑战。

Conclusion: Cancer-Myth-Indic补充了多语言医疗问答评估基准，促进了对南亚语言大型语言模型医疗建议能力的理解和提升。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [64] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 本文探讨了大规模语言模型和推理模型在基准测试中的表现趋于饱和的现象，并分析了三大家族模型的推理能力随时间的演变。


<details>
  <summary>Details</summary>
Motivation: 随着模型能力提升及训练数据包含现有测试集，现有基准测试结果趋于饱和，需要评估是否真正反映了推理能力。

Method: 对OpenAI、Anthropic和Google三大家族的模型在不同基准测试上的表现进行纵向分析，探讨推理任务的性能趋势和基准测试的现状。

Result: 发现模型在多项推理任务中的表现逐年变化，基准测试面临饱和和挑战，推动了新挑战的需求。

Conclusion: 基准测试结果可能并不完全反映模型推理能力，需谨慎设计和选择基准测试。该研究为未来推理能力评估和模型开发提供了综述和参考。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [65] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文探讨了语言形态学特征对分词和语言建模的影响，并指出以往研究结论不一是由于实验设计中的混杂因素所致。


<details>
  <summary>Details</summary>
Motivation: 明确个体语言形态学特征是否以及如何影响语言模型性能，解决过去研究结果冲突的问题。

Method: 识别并控制分析中的混杂因素，重新评估三种关于聚合语和屈折语建模难易的假设，引入基于词元二元组的度量方法作为形态学复杂性的梯度预测指标。

Result: 指出现有假设均包含混杂因素，词元二元组度量能无需专家注释有效预测语言建模难度。

Conclusion: 强调合理实验设计是可靠评估形态学与语言建模关系的前提，提出新度量工具为该领域研究提供帮助。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [66] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: 本文提出了RAGSmith框架，通过遗传算法在多种技术组合中优化检索增强生成（RAG）系统，显著提升了多领域的问答性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统的性能依赖于检索、排序、增强、提示和生成等多个模块的联合作用，单独优化容易导致脆弱。

Method: 设计了一个涵盖九大技术类别、46080种管线配置的端到端架构搜索框架，使用遗传算法联合优化检索和生成指标。

Result: 在六个维基百科领域的100个问题集上，RAGSmith平均提升性能3.8%，检索提升最高达12.5%，生成提升最高达7.5%，且发现了通用的有效结构和领域相关配置。

Conclusion: RAGSmith为构建高效、领域感知的RAG系统提供了实用指导，验证了进化搜索在全流程优化中的有效性。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [67] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: 本文提出了LiveSearchBench，一种基于最新知识更新自动构建检索依赖型问答基准的方法，促进对大语言模型动态知识检索与推理能力的评估。


<details>
  <summary>Details</summary>
Motivation: 现有的静态问答基准过于依赖记忆，忽视了检索的作用，不能反映世界知识的动态变化。

Method: 通过计算连续时间点的Wikidata快照差异、筛选高质量三元组，并自动生成保证唯一且可验证答案的自然语言问题，构建动态问答基准。

Result: 实验表明模型在处理超出预训练时间的事实时性能显著下降，特别是在多跳推理任务中，检索增强和更大规模模型虽有提升，但未弥补时间敏感性的差距。

Conclusion: LiveSearchBench有效推动评估从静态记忆转向基于最新知识检索与推理，为长期动态知识评估大语言模型提供基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [68] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 本文提出了一种面向拉丁语的基于草稿改进的翻译流程，利用开源大语言模型实现接近顶级专有系统的性能。


<details>
  <summary>Details</summary>
Motivation: 拉丁语作为形态丰富且资源稀缺的语言，翻译难度大，因此需要有效方法提升翻译质量。

Method: 首先用微调的NLLB-1.3B模型生成结构准确的初稿，再用零样本大语言模型（Llama-3.3或Qwen3）对初稿进行润色，同时通过检索增强生成（RAG）技术加入上下文示例以提升表现。

Result: 该方法在标准内领域测试集和挑战性的12世纪拉丁书信外领域测试集上均表现优异，达到与GPT-5基线统计上相当的性能，无需针对任务微调大语言模型。

Conclusion: 该研究展示了基于草稿改进和检索增强的开源大语言模型翻译流程在低资源形态语言中的有效性，并公开了数据集及代码，促进后续研究。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [69] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 本文提出了BARD框架，通过用户指定的计算预算信号，实现对推理长度的精细控制，提升小型语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有长链条思维蒸馏方法存在推理冗余和计算预算不可控问题，导致资源利用低效。

Method: BARD采用两阶段训练：第一阶段用教师生成的不同预算级别长链条数据进行有监督微调，第二阶段结合强化学习，从推理性能和预算准确性两个方面优化模型策略。

Result: 在多个挑战性推理基准测试（如AIME24, AIME25, GPQA）上，8B参数的学生模型表现出强推理能力，同时能根据预算动态调整推理长度。

Conclusion: BARD成功实现了推理能力的蒸馏和推理长度的精准控制，提升了推理效率，有效平衡了性能和计算资源消耗。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [70] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型（LLMs）作为认知扭曲检测任务中一致且可靠的标注工具，通过多次独立运行揭示稳定的标注模式，采用基于Cohen's kappa的无数据集依赖评估框架，实现跨数据集和跨研究的公平比较。结果表明GPT-4标注表现稳定，训练模型性能优于人类标注数据。


<details>
  <summary>Details</summary>
Motivation: 认知扭曲检测任务的主观性导致人类专家标注一致性低，标注不可靠，亟需一种更一致和可靠的标注方法。

Method: 利用大型语言模型进行多次独立标注以揭示稳定标注模式，提出基于Cohen’s kappa的无数据集依赖评估框架，以公平比较不同数据集和研究中的模型表现。

Result: GPT-4在认知扭曲检测任务上实现了较高的一致性（Fleiss's Kappa=0.78），基于其产生的标注训练的模型在测试集上表现优于基于人类标注训练的模型。

Conclusion: LLMs作为一致且可靠的标注者，为主观性强的NLP任务提供了一种可扩展且内部一致的训练数据生成方案，从而提升下游模型性能。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [71] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究合成数据源多样性对微调大语言模型行为的影响。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，了解其对模型行为的影响变得至关重要。

Method: 从分布塌缩、对抗鲁棒性和自我偏好偏差三方面考察合成数据来源多样性对模型微调的影响。

Result: 多源合成数据能缓解分布塌缩，保持输出分布广度和文本多样性；合成人类数据均可移除安全保护，但合成数据输出质量更高；微调能减少自我偏好偏差，人类数据最有效，其次是多源合成数据。

Conclusion: 利用多源合成数据微调模型可提升输出多样性和质量，同时减少偏见，但需注意潜在风险。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [72] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文针对孟加拉语文本中的有害语言，提出了一种结合Pareto优化大语言模型和Chain-of-Thought提示的新型文本去毒化方法，并构建了较大的人工并行语料库BanglaNirTox用于模型微调。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语在线环境中的有害语言普遍存在，但由于资源有限，文本去毒化研究不足，因此需要有效的去毒方法和数据集支持。

Method: 提出结合Pareto类别优化的大语言模型和Chain-of-Thought提示的管线，通过生成带类别标签、推理和去毒化同义句的并行语料库BanglaNirTox，随后用该语料库微调模型提升去毒效果。

Result: 基于Pareto优化的大语言模型及CoT提示生成的BanglaNirTox语料库，微调模型后显著提升了孟加拉语文本去毒的质量和一致性。

Conclusion: 结合Pareto优化的大语言模型和Chain-of-Thought提示是提高孟加拉语文本去毒化性能的有效方法，所构建的BanglaNirTox数据集为该领域研究提供了有力支持。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [73] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种通过数据增强和多任务学习生成可控难度的干扰项的新框架，解决了干扰项质量和难度标注数据集缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 生成高质量且难度可控的干扰项具有挑战性，现有方法缺乏适应性和难度控制，同时缺少难度标注数据集。

Method: 通过两步干扰项生成过程创建多样且可信的干扰项候选，再用集成问答系统进行筛选与难度分类，利用新数据集通过多任务学习训练难度可控的生成模型，并设计辅助任务提升模型语义理解和难度估计能力。

Result: 实验证明该方法能生成多难度、高质量的干扰项，且在干扰项难度与人类感知的一致性方面显著优于GPT-4o。

Conclusion: 提出的框架有效解决了干扰项生成中难度控制和质量提升的问题，具有较好的实用价值和应用前景。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [74] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究利用认知模型探索心理学大学生的数学焦虑及其对概念认知的影响，通过人类和GPT模型对比揭示情绪和语义结构差异。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑对心理学学生的职业选择和心理健康造成显著影响，理解其认知和情绪结构有助于干预和管理。

Method: 通过四个实验，构建个体及群体的行为认知网络，使用人类学生和GPT模拟学生的数据，分析数学-焦虑相关概念的情绪评价和关联结构，结合心理计量学量表预测焦虑水平。

Result: 在真实学生中，焦虑的正面情绪评分及网络连接度高与数学的负面评分共同预测较高的数学焦虑，但GPT数据模型未能有效预测，表明模拟人群在认知网络结构与焦虑感知上的差异。高焦虑学生在概念框架上表现出情绪极化，科学被正面评价但数学被负面看待。

Conclusion: 理解学生对数学与焦虑相关概念的情绪和语义结构，有助于更有效地管理和干预数学焦虑，促进学生职业发展和心理健康。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [75] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 本文提出了一种基于熵控制的动态调节生成步骤中属性控制强度的解码方法ECO解码，有效提升了对话生成的可控性与流畅度。


<details>
  <summary>Details</summary>
Motivation: 现有CDG方法中，使用固定常数值调节属性概率偏差，难以兼顾可控性和语言流畅性。

Method: 提出ECO解码，依据语言模型和属性分类概率分布的熵动态调整控制强度。

Result: 在DailyDialog和MultiWOZ数据集上，ECO解码提升了可控性，同时保持了流畅性和语法正确性，且优于先前解码方法。

Conclusion: ECO解码有效缓解多属性生成中的概率插值问题，在单属性和多属性生成场景均表现出色。

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [76] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: 本文构建了基于标准学术抄本和时间标签的青铜铭文数据集BIRD，提出了结合字形网络的全形态掩码语言模型，以提升铭文修复和年代断代的效果。


<details>
  <summary>Details</summary>
Motivation: 早期中国青铜铭文存在碎片化且难以准确断代的问题，亟需有效数据集和方法进行修复与断代研究。

Method: 构建了BIRD数据集，提出结合字形网络（Glyph Net，GN）的全形态掩码语言模型，并通过领域和任务自适应预训练来提升模型表现。

Result: 实验表明字形网络能显著提升铭文修复准确率，采用字形偏置采样策略有助于提高断代效果。

Conclusion: 引入字形信息的语言模型和定制采样方法有效促进了青铜铭文的自动修复与断代，验证了BIRD数据集和方法的实用价值。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [77] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 该研究通过分析西班牙母语者的语言错误，评估大语言模型对这些错误的理解和纠正能力。


<details>
  <summary>Details</summary>
Motivation: 语言错误反映了语言的认知结构，同时也揭示了人工智能系统的局限性，研究旨在通过错误分析促进对语言和AI系统的深刻理解。

Method: 结合理论语言学、神经语言学和自然语言处理，利用一个包含500多个真实错误的西班牙语语料库，测试GPT和Gemini等大语言模型对语言错误的解释及泛化能力。

Result: 该研究预计揭示大语言模型在处理真实语言错误时的表现及其认知相关性，促进更具认知性的自然语言处理系统开发。

Conclusion: 该项目不仅深化了对西班牙语语言错误的理解，也推动了认知导向的自然语言处理系统的发展，使其更好地处理人类语言的多样性和不确定性。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [78] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech包含四种斯拉夫语的议会语料库，共6千小时，通过自动注释大幅提升了语料的研究价值。


<details>
  <summary>Details</summary>
Motivation: 建立大规模且多语言的议会语料库，丰富注释层以支持多学科研究。

Method: 自动从ParlaMint转录和元数据构建语料库，添加语言学注释、情感预测、语音停顿标注及词汇对齐和重音位置注释。

Result: 构建了包含丰富注释、涵盖四种语言的6千小时语料库，并展示了情感声学相关分析的应用。

Conclusion: 通过自动注释增强的多语言大规模议会语料库极大提升了其学术研究价值，已公开发布供下载和检索。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [79] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型和图谱增强生成的能源效率问答系统，通过自动构建知识图谱并进行多语言推理，达到约75%的准确率。


<details>
  <summary>Details</summary>
Motivation: 能源效率领域信息丰富但复杂，需自动化系统准确回答用户提问，且支持多语言。

Method: 从能源指导和法规文件中自动抽取知识图谱，使用基于图的检索增强生成（RAG）架构结合大语言模型进行多语言问答。

Result: 系统在101个问答对验证集上准确率为75.2%，在更一般性的能源效率问题上准确率提升至81.0%，多语言问答准确率仅降低4.4%。

Conclusion: 该方法有效提升能源效率问答的准确率和多语言支持，展示了基于图的RAG架构结合LLM的潜力及改进空间。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [80] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 该研究提出了一个融合Bloom分类法与检索增强生成的认知基准评测框架，用于评估大型语言模型处理特定文化知识的能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前大型语言模型在处理和应用特定文化知识方面缺乏系统评估框架的问题。

Method: 结合Bloom的六个认知层级（记忆、理解、应用、分析、评价、创造）和检索增强生成技术，在台湾客家文化数字档案上进行测试，评估模型回答的语义准确性和文化相关性。

Result: 该框架有效地测量了大型语言模型在不同认知层级下对文化知识的处理表现。

Conclusion: 提出的方法为分析和提升大型语言模型处理特定文化知识的能力提供了系统且分层的评估工具。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [81] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: 提出了EngChain基准，用于评估大语言模型在工程领域多步骤推理的能力，包含90个多样化问题，采用两阶段评估方法验证推理步骤的数值和语义有效性，并利用自动判定系统分类推理错误。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试无法有效衡量大语言模型在工程领域复杂的集成推理能力，需要一个专门针对工程多步骤问题解决的评价体系。

Method: 设计EngChain基准，包含多分支、多个领域的多步骤工程问题，问题通过符号模板随机生成以保证多样性和无污染。评估采用两阶段，先量化验证推理步骤的数值与语义有效性，再由LLM-As-A-Judge自动系统对推理错误进行定性分类。

Result: 提出的EngChain基准及评估方法能够细致验证和分类模型在工程推理过程中的错误，帮助更全面评估大语言模型的工程应用能力。

Conclusion: EngChain填补了现有评测的空白，为大语言模型在工程高风险领域的多步骤推理能力提供了系统、可验证、自动化的评估工具。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [82] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个针对东南亚多语言（印尼语、泰语、越南语）及英语、中文的大型音频语言模型，支持多模态多任务，表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对东南亚多语言环境，开发一个能够处理多语言、多模态、多任务的音频语言模型，推动该地区音频理解和语音交互技术发展。

Method: 训练了一个大型音频语料库，设计支持5种语言（id、th、vi、en、zh），并支持音频、文本及两者结合输入，覆盖音频字幕生成、自动语音识别、语音翻译、情感识别、问答及摘要等任务。

Result: SeaLLMs-Audio在多项任务与东南亚语言上表现出竞争力，同时开发了SeaBench-Audio评测基准，促进自动化评估。

Conclusion: SeaLLMs-Audio为推动东南亚音频语言模型研究和产业应用提供了重要工具，具备良好多语言、多模态、多任务能力，预示其在区域内广泛应用前景。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [83] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文首次公开实现了利用合宪AI和合成内省数据进行角色训练的方法，用以有效塑造聊天机器人角色个性，提升生成内容的连贯性和抗对抗性，同时对模型通用能力影响甚微。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的“AI助手”角色影响行为和价值观，进而影响交互质量及对智能性的认知，但角色训练作为后训练关键环节，却鲜有学术研究。

Method: 引入基于合宪AI的新数据管道，通过合成内省数据微调三款流行开源模型，塑造11种示范角色（如幽默、关怀、恶意等），并设计揭示偏好分析方法追踪角色变化。

Result: 所提出的角色训练方法对抗对抗性提示更为鲁棒，生成内容更加连贯真实，且对通用能力影响微乎其微。

Conclusion: 该研究提供了一种高效且可控的角色训练方案，提升了模型个性化表达与交互质量，开源代码利于业界及学术界进一步应用与研究。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [84] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了一种新的rank-2投影子空间方法，更准确地区分大语言模型中参数知识和上下文知识的贡献，实现了对多步自然语言解释中知识交互的系统分析。


<details>
  <summary>Details</summary>
Motivation: 以前的研究只关注单步生成和参数知识与上下文知识的二元互动，忽视了丰富的知识交互形式，这限制了对NLE解释真实性的理解。

Method: 提出rank-2投影子空间来分离参数知识和上下文知识贡献，进行多步自然语言解释序列的分析，并在多个问答数据集和开源模型上验证。

Result: 实验表明，rank-1子空间无法有效代表多样的知识交互，rank-2方法更准确捕捉知识互动；虚构解释偏向参数知识，忠实解释在两者之间平衡，思路链提示减少对参数知识的依赖，增强上下文知识的作用。

Conclusion: rank-2子空间方法为系统研究大语言模型多步知识交互提供了首个框架，促进对模型解释可信度和知识依赖性的深入理解。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [85] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 本文提出了一种基于Qwen3模型和LoRA适配器的多专家系统，用于生成能够自然对话和执行情境动作的NPC，系统效率高，在2025年常识人格对话挑战赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 提升NPC在交互环境中的对话自然性和动作执行的上下文关联能力，同时保证计算效率。

Method: 采用Qwen3模型结合LoRA适配器，构建包括工具调用、工具响应解释和直接对话三个专家模块的多专家系统。

Result: 系统在L40S GPU上资源占用适中且响应迅速，在2025年Commonsense Persona-Grounded Dialogue Challenge中获得第二名。

Conclusion: 多专家系统有效提升了NPC的对话和动作执行表现，兼顾了计算效率，具有实用价值和推广前景。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [86] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文研究了语言模型在长期对话和文本处理过程中，其信念和行为可能发生的变化，揭示了模型信念变动的风险。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型记忆和上下文容量提升，模型在无用户显式干预下会积累大量上下文，这可能导致其信念和行为发生潜移默化的变化，带来不一致和偏离原始目标的风险。

Method: 通过设计涉及道德困境、安全以及政治问题的多轮对话和文本阅读任务，分别测试GPT-5和Grok 4模型的信念偏移情况；并设计需要工具使用的任务来检测行为上的隐含信念转变。

Result: GPT-5在10轮关于道德和安全相关话题的讨论后，其声明的信念变化高达54.7%；Grok 4在阅读对立政治立场文本后信念变化为27.2%；行为测试结果表明信念转变与实际行为选择一致。

Conclusion: 长期交互和阅读会导致语言模型的信念和行为显著变化，存在信念迁移风险，影响模型的可靠性和一致性。模型的意见和行为可能变得不稳定，需关注这一隐性风险。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [87] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 本文提出一种基于提示工程的长度控制方法，无需模型再训练，即可实现大语言模型输出长度的精确控制，特别适用于文档摘要任务。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在长度控制方面存在挑战，现有方法通常需要昂贵的模型再训练或复杂的推理工具，限制了实用性。

Method: 通过在提示中引入结构化规划和计数机制，指导模型跟踪并遵守指定的长度限制，实现精确的长度控制。

Result: 在六个主流大语言模型上进行的评测显示，该方法在长度遵守度上较标准提示提升显著，部分模型提升高达37.6%，且质量未受影响甚至有所提升。

Conclusion: 该方法为长度控制提供了一种无需再训练即可立即部署的解决方案，尤其适合模型再训练成本高或不切实际的生产环境。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [88] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: KVTC是一种轻量级转换编码器，通过PCA特征去相关、自适应量化和熵编码，有效压缩大型语言模型的键值缓存，提高GPU内存利用率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务需要高效管理键值缓存以节省宝贵的GPU内存，减少缓存过时导致的重计算和内存溢出问题。

Method: KVTC结合PCA特征去相关、自适应量化和熵编码技术，经过简短校准，无需更改模型参数，实现缓存的压缩和存储优化。

Result: KVTC实现了最高20倍压缩，特定场景甚至可达40倍，同时保持模型推理和长上下文的准确性，优于传统的Token驱逐、量化和SVD方法。

Conclusion: KVTC作为一个实用的模块，显著提升了大型语言模型使用中键值缓存的内存效率，支持可重用缓存的高效服务。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [89] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: 提出了IMO-Bench，一套针对国际数学奥林匹克水平的高级数学推理评估基准，包括短答案和证明写作两个部分，支持自动评分，推动基础模型数学推理能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评测过于简单或仅聚焦短答案，难以全面衡量基础模型的数学推理能力。

Method: 设计IMO-Bench，包括IMO-AnswerBench（400道短答案题）和IMO-Proof Bench（基础及高级证明题及评分细则），并结合基于Gemini Deep Think模型的自动评分器。

Result: Gemini模型在IMO-AnswerBench和IMO-Proof Bench上分别达到80.0%和65.7%的成绩，明显超过其他模型，自动评分与人工评分高度相关。

Conclusion: IMO-Bench为数学推理能力评估提供了权威、全面的基准，促进了自动长答案评估的发展，有助于推动数学推理基础模型的进步。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [90] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文提出了一种名为Tool-to-Agent Retrieval的统一检索框架，通过将工具及其所属代理嵌入同一向量空间并利用元数据关系连接，实现了细粒度的工具级和代理级检索，显著提升了代理选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法往往仅匹配查询与代理级描述，忽视了工具的细粒度功能，导致代理选择效果不佳。

Method: 提出Tool-to-Agent Retrieval框架，将工具和代理共同嵌入向量空间，利用元数据关系连接，从而支持细粒度的工具或代理检索，避免了将多个工具合并带来的上下文稀释问题。

Result: 在八个嵌入模型上进行评估，该方法在LiveMCPBench基准测试中在Recall@5和nDCG@5指标上分别比最先进方法提升了19.4%和17.7%。

Conclusion: Tool-to-Agent Retrieval有效提升了多代理系统中代理选择的精度，为细粒度工具功能的检索提供了新思路。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [91] [On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.00034)
*Aditya Akella*

Main category: cs.MA

TL;DR: 本文研究了在多智能体强化学习中分散式可学习奖励塑形的效果，提出了DMARL-RSA系统并发现其性能远不及集中式方法，揭示了局部优化与全局性能之间的协调矛盾。


<details>
  <summary>Details</summary>
Motivation: 尽管单智能体中可学习奖励塑形取得了进展，但其在合作多智能体分散式环境中的效果和挑战仍不明确。

Method: 提出了DMARL-RSA，一种完全分散式的奖励塑形方法，每个智能体独立学习奖励函数，并在简单的合作导航任务（simple_spread_v3环境）中进行评估。

Result: DMARL-RSA在平均奖励表现上显著劣于集中式训练的MAPPO，与简单独立学习（IPPO）表现相近。分散式方法在地标覆盖率上较高，但整体性能较差。分析指出存在非平稳性、信用分配复杂性及奖励与全局目标不一致三大难题。

Conclusion: 分散式可学习奖励塑形受制于多智能体协同的根本性限制，集中式协调在有效合作中依然不可或缺。

Abstract: Recent advances in learnable reward shaping have shown promise in
single-agent reinforcement learning by automatically discovering effective
feedback signals. However, the effectiveness of decentralized learnable reward
shaping in cooperative multi-agent settings remains poorly understood. We
propose DMARL-RSA, a fully decentralized system where each agent learns
individual reward shaping, and evaluate it on cooperative navigation tasks in
the simple_spread_v3 environment. Despite sophisticated reward learning,
DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with
centralized training at 1.92 +/- 0.87--a 26.12-point gap. DMARL-RSA performs
similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating
that advanced reward shaping cannot overcome fundamental decentralized
coordination limitations. Interestingly, decentralized methods achieve higher
landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out
of 3 total) but worse overall performance than centralized MAPPO (0.273 +/-
0.008 landmark coverage)--revealing a coordination paradox between local
optimization and global performance. Analysis identifies three critical
barriers: (1) non-stationarity from concurrent policy updates, (2) exponential
credit assignment complexity, and (3) misalignment between individual reward
optimization and global objectives. These results establish empirical limits
for decentralized reward learning and underscore the necessity of centralized
coordination for effective multi-agent cooperation.

</details>


### [92] [Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System](https://arxiv.org/abs/2511.00096)
*Shangyu Lou*

Main category: cs.MA

TL;DR: Urban-MAS 是一个基于大语言模型的多智能体系统框架，用于零样本人类中心城市预测，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂城市系统的异构数据时表现不足，亟需一种系统提升具体领域任务的性能。

Method: 提出三类智能体（预测因子引导、信息提取和多源推理智能体）协同工作，通过优先关键因子、验证信息一致性及多源融合，实现准确的城市预测。

Result: 在东京、米兰、西雅图的跑动量预测和城市感知任务中，Urban-MAS相较单一LLM基线显著降低预测误差，验证了其有效性。

Conclusion: 预测因子引导智能体为提升性能的关键，Urban-MAS为人类中心城市AI预测提供了可扩展的新范式。

Abstract: Urban Artificial Intelligence (Urban AI) has advanced human-centered urban
tasks such as perception prediction and human dynamics. Large Language Models
(LLMs) can integrate multimodal inputs to address heterogeneous data in complex
urban systems but often underperform on domain-specific tasks. Urban-MAS, an
LLM-based Multi-Agent System (MAS) framework, is introduced for human- centered
urban prediction under zero-shot settings. It includes three agent types:
Predictive Factor Guidance Agents, which prioritize key predictive factors to
guide knowledge extraction and enhance the effectiveness of compressed urban
knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve
robustness by com- paring multiple outputs, validating consistency, and
re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which
integrate extracted multi-source information across dimensions for prediction.
Experiments on running-amount prediction and ur- ban perception across Tokyo,
Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors
compared to single-LLM baselines. Ablation studies indicate that Predictive
Factor Guidance Agents are most critical for enhancing predictive performance,
po- sitioning Urban-MAS as a scalable paradigm for human-centered urban AI
prediction. Code is available on the project
website:https://github.com/THETUREHOOHA/UrbanMAS

</details>


### [93] [Sherlock: Reliable and Efficient Agentic Workflow Execution](https://arxiv.org/abs/2511.00330)
*Yeonju Ro,Haoran Qiu,Íñigo Goiri,Rodrigo Fonseca,Ricardo Bianchini,Aditya Akella,Zhangyang Wang,Mattan Erez,Esha Choukse*

Main category: cs.MA

TL;DR: 提出Sherlock方法通过反事实分析选择性验证大语言模型工作流中的错误易发节点，实现准确率提升和成本减少。


<details>
  <summary>Details</summary>
Motivation: 多步骤的大语言模型工作流错误易传播，全面验证代价高，需要找到高风险节点进行选择性验证以平衡效率和准确性。

Method: Sherlock利用反事实分析识别高风险节点，选择最合适的验证器，推测执行下游任务以降低延迟，验证失败时回滚执行。

Result: Sherlock使准确率平均提升18.3%，执行时间比非推测执行快48.7%，验证成本较Monte Carlo方法降低26.0%。

Conclusion: 基于反事实的选择性验证策略能有效提升复杂大语言模型工作流的准确性和效率，兼顾可靠性和成本。

Abstract: With the increasing adoption of large language models (LLM), agentic
workflows, which compose multiple LLM calls with tools, retrieval, and
reasoning steps, are increasingly replacing traditional applications. However,
such workflows are inherently error-prone: incorrect or partially correct
output at one step can propagate or even amplify through subsequent stages,
compounding the impact on the final output. Recent work proposes integrating
verifiers that validate LLM output or actions, such as self-reflection, debate,
or LLM-as-a-judge mechanisms. Yet, verifying every step introduces significant
latency and cost overheads.
  In this work, we seek to answer three key questions: which nodes in a
workflow are most error-prone and thus deserve costly verification, how to
select the most appropriate verifier for each node, and how to use verification
with minimal impact to latency? Our solution, Sherlock, addresses these using
counterfactual analysis on agentic workflows to identify error-prone nodes and
selectively attaching cost-optimal verifiers only where necessary. At runtime,
Sherlock speculatively executes downstream tasks to reduce latency overhead,
while verification runs in the background. If verification fails, execution is
rolled back to the last verified output. Compared to the non-verifying
baseline, Sherlock delivers an 18.3% accuracy gain on average across
benchmarks. Sherlock reduces workflow execution time by up to 48.7% over
non-speculative execution and lowers verification cost by 26.0% compared to the
Monte Carlo search-based method, demonstrating that principled, fault-aware
verification effectively balances efficiency and reliability in agentic
workflows.

</details>


### [94] [Spatial Crowdsourcing-based Task Allocation for UAV-assisted Maritime Data Collection](https://arxiv.org/abs/2511.00387)
*Xiaoling Han,Bin Lin,Zhenyu Na,Bowen Li,Chaoyue Zhang,Ran Zhang*

Main category: cs.MA

TL;DR: 本文提出基于空间众包的无人机辅助海洋数据采集任务分配算法，提升任务完成效率和节能效果。


<details>
  <summary>Details</summary>
Motivation: 随着海洋服务的发展，无人机辅助的海洋数据采集任务日益复杂和个性化，亟需有效的任务分配方法。

Method: 结合空间众包理念，设计基于质量估计和逆向拍卖机制的任务分配算法（SC-MDC-TA），根据任务时空需求和无人机移动性进行优化分配。

Result: 仿真表明，SC-MDC-TA算法在多场景下能够有效分配任务，减少任务完成时间并降低无人机能耗。

Conclusion: 所提算法能够满足多变海洋服务场景的任务分配需求，提升无人机任务执行效率与能源利用率。

Abstract: Driven by the unceasing development of maritime services, tasks of unmanned
aerial vehicle (UAV)-assisted maritime data collection (MDC) are becoming
increasingly diverse, complex and personalized. As a result, effective task
allocation for MDC is becoming increasingly critical. In this work, integrating
the concept of spatial crowdsourcing (SC), we develop an SC-based MDC network
model and investigate the task allocation problem for UAV-assisted MDC. In
variable maritime service scenarios, tasks are allocated to UAVs based on the
spatial and temporal requirements of the tasks, as well as the mobility of the
UAVs. To address this problem, we design an SC-based task allocation algorithm
for the MDC (SC-MDC-TA). The quality estimation is utilized to assess and
regulate task execution quality by evaluating signal to interference plus noise
ratio and the UAV energy consumption. The reverse auction is employed to
potentially reduce the task waiting time as much as possible while ensuring
timely completion. Additionally, we establish typical task allocation scenarios
based on maritime service requirements indicated by electronic navigational
charts. Simulation results demonstrate that the proposed SC-MDC-TA algorithm
effectively allocates tasks for various MDC scenarios. Furthermore, compared to
the benchmark, the SC-MDC-TA algorithm can also reduce the task completion time
and lower the UAV energy consumption.

</details>


### [95] [AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems](https://arxiv.org/abs/2511.00628)
*Yang Li,Siqi Ping,Xiyu Chen,Xiaojian Qi,Zigan Wang,Ye Luo,Xiaowei Zhang*

Main category: cs.MA

TL;DR: 本文提出了AgentGit框架，为多智能体系统（MAS）引入了类似Git的回滚和分支机制，提升了系统的可靠性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统框架在复杂任务中面临可靠性和可扩展性不足的问题。

Method: 基于LangGraph构建AgentGit，支持状态提交、回滚和分支，使智能体能高效遍历、比较和探索多条轨迹。

Result: 与LangGraph、AutoGen和Agno三大基线比较，AgentGit显著减少了冗余计算，降低了运行时间和令牌使用，支持多分支并行探索。

Conclusion: AgentGit改善了多智能体系统开发的可靠性和可扩展性，支持错误恢复、安全探索、迭代调试和A/B测试，推动了协作式AI系统的设计。

Abstract: With the rapid progress of large language models (LLMs), LLM-powered
multi-agent systems (MAS) are drawing increasing interest across academia and
industry. However, many current MAS frameworks struggle with reliability and
scalability, especially on complex tasks. We present AgentGit, a framework that
brings Git-like rollback and branching to MAS workflows. Built as an
infrastructure layer on top of LangGraph, AgentGit supports state commit,
revert, and branching, allowing agents to traverse, compare, and explore
multiple trajectories efficiently. To evaluate AgentGit, we designed an
experiment that optimizes target agents by selecting better prompts. We ran a
multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno --
on a real-world task: retrieving and analyzing paper abstracts. Results show
that AgentGit significantly reduces redundant computation, lowers runtime and
token usage, and supports parallel exploration across multiple branches,
enhancing both reliability and scalability in MAS development. This work offers
a practical path to more robust MAS design and enables error recovery, safe
exploration, iterative debugging, and A/B testing in collaborative AI systems.

</details>


### [96] [Predictive Auxiliary Learning for Belief-based Multi-Agent Systems](https://arxiv.org/abs/2511.01078)
*Qinwei Huang,Stefan Wang,Simon Khan,Garrett Katz,Qinru Qiu*

Main category: cs.MA

TL;DR: 本文提出了一种基于信念的预测辅助学习（BEPAL）框架，通过引入辅助预测任务，提升多智能体强化学习在部分可观测环境中的效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习主要依赖奖励信号进行策略训练，效果受限且训练过程不够稳定。引入辅助预测任务有望改善训练效率和稳定性。

Method: BEPAL框架在集中训练、分散执行的范式下，每个智能体学习一个信念模型，用以预测无法观测的状态信息（如其他智能体的奖励和移动方向），同时训练策略模型，通过丰富隐藏状态表示，辅助策略优化。

Result: 在捕食者-猎物环境和谷歌研究足球环境的实验中，BEPAL相比基线方法，性能指标平均提升约16%，且训练过程更稳定。

Conclusion: 通过引入辅助预测任务，BEPAL有效增强了多智能体强化学习的训练稳定性和性能，证明了辅助任务在MARL中的重要性和应用潜力。

Abstract: The performance of multi-agent reinforcement learning (MARL) in partially
observable environments depends on effectively aggregating information from
observations, communications, and reward signals. While most existing
multi-agent systems primarily rely on rewards as the only feedback for policy
training, our research shows that introducing auxiliary predictive tasks can
significantly enhance learning efficiency and stability. We propose
Belief-based Predictive Auxiliary Learning (BEPAL), a framework that
incorporates auxiliary training objectives to support policy optimization.
BEPAL follows the centralized training with decentralized execution paradigm.
Each agent learns a belief model that predicts unobservable state information,
such as other agents' rewards or motion directions, alongside its policy model.
By enriching hidden state representations with information that does not
directly contribute to immediate reward maximization, this auxiliary learning
process stabilizes MARL training and improves overall performance. We evaluate
BEPAL in the predator-prey environment and Google Research Football, where it
achieves an average improvement of about 16 percent in performance metrics and
demonstrates more stable convergence compared to baseline methods.

</details>


### [97] [Credit Network Modeling and Analysis via Large Language Models](https://arxiv.org/abs/2511.01136)
*Enbo Sun,Yongzhao Wang,Hao Zhou*

Main category: cs.MA

TL;DR: 利用大型语言模型（LLMs）将企业财务报表转换为信用网络，进而分析并优化整体金融系统的信用网络结构。


<details>
  <summary>Details</summary>
Motivation: 通过自动构建和分析信用网络，揭示和提升金融系统的运行效率和稳定性。

Method: 使用LLMs将单个企业的财务报表转换为信用网络，再聚合成整体金融系统网络；在自动检测财务报表不一致性的基础上，结合人类干预；利用LLMs分析网络并设计优化策略，如投资组合压缩和债务清除。

Result: 该方法能有效处理不同结构的信用网络，LLMs展现出良好的推理能力，能提供合理建议以执行金融操作，从而提升网络资产总额。

Conclusion: LLMs在信用网络构建和分析方面表现出强大能力，能辅助实现复杂的金融系统优化，具有广泛应用前景。

Abstract: We investigate the application of large language models (LLMs) to construct
credit networks from firms' textual financial statements and to analyze the
resulting network structures. We start with using LLMs to translate each firm's
financial statement into a credit network that pertains solely to that firm.
These networks are then aggregated to form a comprehensive credit network
representing the whole financial system. During this process, the
inconsistencies in financial statements are automatically detected and human
intervention is involved. We demonstrate that this translation process is
effective across financial statements corresponding to credit networks with
diverse topological structures. We further investigate the reasoning
capabilities of LLMs in analyzing credit networks and determining optimal
strategies for executing financial operations to maximize network performance
measured by the total assets of firms, which is an inherently combinatorial
optimization challenge. To demonstrate this capability, we focus on two
financial operations: portfolio compression and debt removal, applying them to
both synthetic and real-world datasets. Our findings show that LLMs can
generate coherent reasoning and recommend effective executions of these
operations to enhance overall network performance.

</details>


### [98] [From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models](https://arxiv.org/abs/2511.01310)
*Sureyya Akin,Kavita Srivastava,Prateek B. Kapoor,Pradeep G. Sethi,Sunita Q. Patel,Rahu Srivastava*

Main category: cs.MA

TL;DR: 提出了一种基于共享多模态生成世界模型的多智能体强化学习框架，通过融合多源感知信息学习环境动态，在潜在空间中进行策略训练，大幅提升样本效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型无关的多智能体强化学习在处理高维多模态感知输入时存在样本效率低、表示学习和策略学习难以同步的问题。

Method: 设计一个基于注意力机制的多模态生成世界模型，融合来自所有智能体的多模态观测数据，学习环境动态的潜在表示。然后利用该潜在世界模型作为快速的“想象模拟器”，在潜在空间内训练多智能体强化学习策略，解耦表示学习与策略学习。

Result: 在基于3D物理模拟器的新多模态多智能体基准测试中，所提框架相比最先进的模型无关多智能体基线方法，样本效率提升几个数量级。多模态融合提高了在感知不对称环境中的任务成功率，同时架构对传感器掉线的鲁棒性也优于其他方法。

Conclusion: 共享多模态生成世界模型有效解决了多智能体强化学习中样本效率低和多模态信息融合难题，提升了多智能体系统在复杂感知环境中的性能和实际部署的鲁棒性。

Abstract: Learning cooperative multi-agent policies directly from high-dimensional,
multimodal sensory inputs like pixels and audio (from pixels) is notoriously
sample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)
algorithms struggle with the joint challenge of representation learning,
partial observability, and credit assignment. To address this, we propose a
novel framework based on a shared, generative Multimodal World Model (MWM). Our
MWM is trained to learn a compressed latent representation of the environment's
dynamics by fusing distributed, multimodal observations from all agents using a
scalable attention-based mechanism. Subsequently, we leverage this learned MWM
as a fast, "imagined" simulator to train cooperative MARL policies (e.g.,
MAPPO) entirely within its latent space, decoupling representation learning
from policy learning. We introduce a new set of challenging multimodal,
multi-agent benchmarks built on a 3D physics simulator. Our experiments
demonstrate that our MWM-MARL framework achieves orders-of-magnitude greater
sample efficiency compared to state-of-the-art model-free MARL baselines. We
further show that our proposed multimodal fusion is essential for task success
in environments with sensory asymmetry and that our architecture provides
superior robustness to sensor-dropout, a critical feature for real-world
deployment.

</details>


### [99] [An Explanation-oriented Inquiry Dialogue Game for Expert Collaborative Recommendations](https://arxiv.org/abs/2511.01489)
*Qurat-ul-ain Shaheen,Katarzyna Budzynska,Carles Sierra*

Main category: cs.MA

TL;DR: 本文提出了一种基于需求分析的医疗专家协作对话游戏，用于多智能体系统设计中的可解释性支持，且通过用户研究验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 医疗专家在协作推荐过程中需要透明且易于追溯的推理过程，提升系统的可解释性和合作效率。

Method: 设计了基于解释性言语行为的查询对话游戏，允许拥有不同知识库的专家协作生成推荐及推理轨迹，开发了网页版原型并进行形成性用户研究。

Result: 用户研究结果表明该对话游戏满足医疗专家协作需求，能有效支持专家间的沟通和推理共享。

Conclusion: 基于对话的交互工具在医疗专家协作中有实际价值，有助于提高多智能体系统的可解释性和协作效率。

Abstract: This work presents a requirement analysis for collaborative dialogues among
medical experts and an inquiry dialogue game based on this analysis for
incorporating explainability into multiagent system design. The game allows
experts with different knowledge bases to collaboratively make recommendations
while generating rich traces of the reasoning process through combining
explanation-based illocutionary forces in an inquiry dialogue. The dialogue
game was implemented as a prototype web-application and evaluated against the
specification through a formative user study. The user study confirms that the
dialogue game meets the needs for collaboration among medical experts. It also
provides insights on the real-life value of dialogue-based communication tools
for the medical community.

</details>


### [100] [Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning](https://arxiv.org/abs/2511.01554)
*Aditya Kapoor,Yash Bhisikar,Benjamin Freed,Jan Peters,Mingfei Sun*

Main category: cs.MA

TL;DR: 本文提出了一种扩展的可微分离散通信学习(DDCL)框架，用于多智能体强化学习中优化通信的精度和带宽，成功降低带宽消耗同时提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的有效通信受带宽限制，现有方法仅能决定是否通信，不能精确控制通信的位级精度，而细化到位的离散化使梯度流断裂，带来优化难题。

Method: 通过推广DDCL框架，支持无界信号，实现端到端离散消息的优化，形成一个通用可插拔层，适用于任何MARL架构；并将该方法集成进四种先进MARL算法中。

Result: 实验表明智能体能够动态调节消息精度以满足任务信息需求，带宽降低一个数量级以上，同时任务性能保持或提升；更简单的基于Transformer策略利用DDCL可匹配复杂架构性能。

Conclusion: 本文证明了通过位级精度调控的DDCL方法能有效提升MARL通信效率和性能，表明复杂专用通信设计并非必要，简单通用方法同样优秀。

Abstract: Effective communication in multi-agent reinforcement learning (MARL) is
critical for success but constrained by bandwidth, yet past approaches have
been limited to complex gating mechanisms that only decide \textit{whether} to
communicate, not \textit{how precisely}. Learning to optimize message precision
at the bit-level is fundamentally harder, as the required discretization step
breaks gradient flow. We address this by generalizing Differentiable Discrete
Communication Learning (DDCL), a framework for end-to-end optimization of
discrete messages. Our primary contribution is an extension of DDCL to support
unbounded signals, transforming it into a universal, plug-and-play layer for
any MARL architecture. We verify our approach with three key results. First,
through a qualitative analysis in a controlled environment, we demonstrate
\textit{how} agents learn to dynamically modulate message precision according
to the informational needs of the task. Second, we integrate our variant of
DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth
by over an order of magnitude while matching or exceeding task performance.
Finally, we provide direct evidence for the \enquote{Bitter Lesson} in MARL
communication: a simple Transformer-based policy leveraging DDCL matches the
performance of complex, specialized architectures, questioning the necessity of
bespoke communication designs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [101] [ScaleCall - Agentic Tool Calling at Scale for Fintech: Challenges, Methods, and Deployment Insights](https://arxiv.org/abs/2511.00074)
*Richard Osuagwu,Thomas Cook,Maraim Masoud,Koustav Ghosal,Riccardo Mattivi*

Main category: cs.SE

TL;DR: 本文研究了企业环境下大语言模型工具调用的检索方法，通过在万事达卡部署的ScaleCall框架验证了不同方法各自优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在企业尤其是受监管的金融科技环境中部署工具调用面临本地部署、合规和工具功能重叠等挑战，亟需高效准确的工具检索方法。

Method: 设计并评估了基于嵌入的检索、基于提示的列表排序和混合方法，通过企业数据构建的基准测试进行系统比较，并将成果集成到ScaleCall框架中。

Result: 嵌入方法在大规模工具库中延迟较低，列表排序对功能重叠的工具有更好区分效果，混合方法在特定场景表现出潜力。

Conclusion: 工具检索方法的有效性依赖特定领域因素，权衡检索准确性、计算效率和运营需求是设计企业工具调用系统的关键。ScaleCall验证了框架实用性，为受监管行业提供了参考。

Abstract: While Large Language Models (LLMs) excel at tool calling, deploying these
capabilities in regulated enterprise environments such as fintech presents
unique challenges due to on-premises constraints, regulatory compliance
requirements, and the need to disambiguate large, functionally overlapping
toolsets. In this paper, we present a comprehensive study of tool retrieval
methods for enterprise environments through the development and deployment of
ScaleCall, a prototype tool-calling framework within Mastercard designed for
orchestrating internal APIs and automating data engineering workflows. We
systematically evaluate embedding-based retrieval, prompt-based listwise
ranking, and hybrid approaches, revealing that method effectiveness depends
heavily on domain-specific factors rather than inherent algorithmic
superiority. Through empirical investigation on enterprise-derived benchmarks,
we find that embedding-based methods offer superior latency for large tool
repositories, while listwise ranking provides better disambiguation for
overlapping functionalities, with hybrid approaches showing promise in specific
contexts. We integrate our findings into ScaleCall's flexible architecture and
validate the framework through real-world deployment in Mastercard's regulated
environment. Our work provides practical insights into the trade-offs between
retrieval accuracy, computational efficiency, and operational requirements,
contributing to the understanding of tool-calling system design for enterprise
applications in regulated industries.

</details>


### [102] [Adding New Capability in Existing Scientific Application with LLM Assistance](https://arxiv.org/abs/2511.00087)
*Anshu Dubey,Akash Dhruv*

Main category: cs.SE

TL;DR: 本文提出利用大型语言模型（LLM）辅助从零开始为新算法编写代码的方法，并改进了之前开发的代码翻译工具Code-Scribe以实现新代码生成。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型在生成新算法代码时表现不足，因为训练数据中不包含类似代码的例子，故需要探索新的自动编码方法。

Method: 提出一种利用LLM辅助从零写出新算法代码的新方法，并改进了之前的代码翻译工具Code-Scribe以支持新代码生成。

Result: 成功增强了Code-Scribe工具，实现了针对新算法的代码生成能力。

Conclusion: LLM结合改进的代码翻译工具能有效提升对全新算法自动编码的能力，拓展了自动编码的应用范围。

Abstract: With the emergence and rapid evolution of large language models (LLM),
automating coding tasks has become an im- portant research topic. Many efforts
are underway and liter- ature abounds about the efficacy of models and their
ability to generate code. A less explored aspect of code generation is for new
algorithms, where the training data-set would not have included any previous
example of similar code. In this paper we propose a new methodology for writing
code from scratch for a new algorithm using LLM assistance, and describe
enhancement of a previously developed code- translation tool, Code-Scribe, for
new code generation.

</details>


### [103] [Inferring multiple helper Dafny assertions with LLMs](https://arxiv.org/abs/2511.00125)
*Álvaro Silva,Alexandra Mendes,Ruben Martins*

Main category: cs.SE

TL;DR: 该论文提出了一种利用大型语言模型自动推断Dafny程序中缺失的辅助断言的方法，通过结合语言模型预测和错误信息启发式的混合故障定位技术，在单断言和多断言缺失情况下实现程序验证，显著降低了人工编写辅助断言的负担。


<details>
  <summary>Details</summary>
Motivation: Dafny验证器虽然能提供强健的正确性保障，但需手动编写大量辅助断言，限制了其普及和应用。作者希望通过自动推断断言来减轻这一负担。

Method: 作者扩展了DafnyBench基准数据集，构造断言缺失案例并分类断言类型，提出结合大型语言模型预测结果和错误消息启发式的混合故障定位方法，设计实现了DAISY工具用于辅助断言自动推断。

Result: DAISY在单断言缺失情况下能验证63.4%的程序，多断言缺失情况下验证率达31.7%。且许多程序经过补断言后比原始断言更少也能通过验证，说明自动推断具有灵活修复策略。

Conclusion: 该工作表明自动断言推断可极大减少人工证明工作，提高形式验证的可扩展性和易用性，是向自动化形式验证迈进的重要一步。

Abstract: The Dafny verifier provides strong correctness guarantees but often requires
numerous manual helper assertions, creating a significant barrier to adoption.
We investigate the use of Large Language Models (LLMs) to automatically infer
missing helper assertions in Dafny programs, with a primary focus on cases
involving multiple missing assertions. To support this study, we extend the
DafnyBench benchmark with curated datasets where one, two, or all assertions
are removed, and we introduce a taxonomy of assertion types to analyze
inference difficulty. Our approach refines fault localization through a hybrid
method that combines LLM predictions with error-message heuristics. We
implement this approach in a new tool called DAISY (Dafny Assertion Inference
SYstem). While our focus is on multiple missing assertions, we also evaluate
DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one
missing assertion and 31.7% with multiple missing assertions. Notably, many
programs can be verified with fewer assertions than originally present,
highlighting that proofs often admit multiple valid repair strategies and that
recovering every original assertion is unnecessary. These results demonstrate
that automated assertion inference can substantially reduce proof engineering
effort and represent a step toward more scalable and accessible formal
verification.

</details>


### [104] [What a diff makes: automating code migration with large language models](https://arxiv.org/abs/2511.00160)
*Katherine A. Rosenfeld,Cliff C. Kerr,Jessica Lundin*

Main category: cs.SE

TL;DR: 该论文探讨了利用大语言模型(LLMs)进行代码迁移以维持依赖项兼容性的问题，提出了包含差异上下文的方法，显著提升了迁移效果，并发布了相关数据集和开源工具。


<details>
  <summary>Details</summary>
Motivation: 软件栈频繁变更可能导致依赖破坏，需求有效手段支持依赖升级时的代码迁移。

Method: 通过向LLMs提供含版本差异的上下文增强数据，比较测试覆盖率和变更对比指标，验证其在代码迁移中的性能提升。

Result: 在TYPHOIDSIM与STARSIM版本迁移的实际案例中，AIMigrate工具在单次运行中准确识别65%的必要变更，多次运行提高至80%，47%变更生成完全正确。

Conclusion: 基于差异上下文的LLM辅助代码迁移有效提升了迁移质量与效率，发布的数据集和工具为后续研究提供支持。

Abstract: Modern software programs are built on stacks that are often undergoing
changes that introduce updates and improvements, but may also break any project
that depends upon them. In this paper we explore the use of Large Language
Models (LLMs) for code migration, specifically the problem of maintaining
compatibility with a dependency as it undergoes major and minor semantic
version changes. We demonstrate, using metrics such as test coverage and change
comparisons, that contexts containing diffs can significantly improve
performance against out of the box LLMs and, in some cases, perform better than
using code. We provide a dataset to assist in further development of this
problem area, as well as an open-source Python package, AIMigrate, that can be
used to assist with migrating code bases. In a real-world migration of
TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of
required changes in a single run, increasing to 80% with multiple runs, with
47% of changes generated perfectly.

</details>


### [105] [Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories](https://arxiv.org/abs/2511.00197)
*Oorja Majgaonkar,Zhiwei Fei,Xiang Li,Federica Sarro,He Ye*

Main category: cs.SE

TL;DR: 本文通过分析三种先进代码代理在解决软件问题时的执行轨迹，揭示了其不同策略和失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型代理在软件工程中的决策过程不透明，需深入理解其问题解决行为。

Method: 收集并分析OpenHands、SWE-agent和Prometheus三种代理在SWE-Bench基准上的成功与失败轨迹，研究问题解决策略与故障定位。

Result: 发现不同策略如防御性编程和上下文收集促进成功；失败轨迹更长且差异大；大多数失败仍能定位错误文件，但成功依赖于近似修改。

Conclusion: 轨迹分析有助于理解代理行为，为构建更稳健、可解释的自动化软件工程系统奠定基础。

Abstract: The increasing deployment of Large Language Model (LLM) agents for complex
software engineering tasks has created a need to understand their
problem-solving behaviours beyond simple success metrics. While these agents
demonstrate impressive capabilities in automated issue resolution, their
decision-making processes remain largely opaque. This paper presents an
empirical study of agent trajectories, namely the execution traces capturing
the steps agents take when attempting to resolve software issues. We analyse
trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and
Prometheus) on the SWE-Bench benchmark, examining both successful and failed
attempts. Our investigation reveals several key insights into agent behaviour.
First, we identify how distinct problem-solving strategies, such as defensive
programming and context gathering, enable success in different scenarios.
Second, we find that failed trajectories are consistently longer and exhibit
higher variance than successful ones, with failure patterns differing
significantly between agents. Third, our fault localisation analysis shows that
while most trajectories correctly identify problematic files (72-81\% even in
failures), success depends more on achieving approximate rather than exact code
modifications. These and other findings unveiled by our study, provide a
foundation for understanding agent behaviour through trajectory analysis,
contributing to the development of more robust and interpretable autonomous
software engineering systems.

</details>


### [106] [Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification](https://arxiv.org/abs/2511.00202)
*Jacqueline Mitchell,Yasser Shaaban*

Main category: cs.SE

TL;DR: Vibe编码通过与大型语言模型迭代对话开发软件，但存在技术负债、安全问题和代码频繁变动。


<details>
  <summary>Details</summary>
Motivation: 支持开发者克服vibe编码中因LLM无法协调累积约束而引发的问题，如技术债务和安全隐患。

Method: 提出一个侧车系统自动形式化规范，验证目标，提供可操作反馈，并允许开发者直观影响规范。

Result: 该方法可提升vibe编码的可靠性，减少代码不一致和安全问题。

Conclusion: 将形式化方法与LLM集成，通过侧车系统辅助vibe编码，可有效缓解关键缺陷，使开发过程更健壮。

Abstract: ``Vibe coding'' -- the practice of developing software through iteratively
conversing with a large language model (LLM) -- has exploded in popularity
within the last year. However, developers report key limitations including the
accumulation of technical debt, security issues, and code churn to achieve
satisfactory results. We argue that these pitfalls result from LLMs' inability
to reconcile accumulating human-imposed constraints during vibe coding, with
developers inadvertently failing to resolve contradictions because LLMs
prioritize user commands over code consistency. Given LLMs' receptiveness to
verification-based feedback, we argue that formal methods can mitigate these
pitfalls, making vibe coding more reliable. However, we posit that integrating
formal methods must transcend existing approaches that combine formal methods
and LLMs. We advocate for a side-car system throughout the vibe coding process
which: (1) \emph{Autoformalizes} specifications (2) Validates against targets,
(3) Delivers \emph{actionable} feedback to the LLM, and (4) Allows intuitive
developer influence on specifications.

</details>


### [107] [DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies](https://arxiv.org/abs/2511.00215)
*Xiaomeng Xu,Zahin Wahab,Reid Holmes,Caroline Lemieux*

Main category: cs.SE

TL;DR: DocPrism是一种多语言代码-文档不一致检测工具，利用大语言模型（LLM）分析和解释不一致，但避免了高误报率，通过引入LCEF方法显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 代码与文档之间的不一致会导致开发误解和软件缺陷，现有方法误报率高，亟需有效降低误报的检测工具。

Method: 提出Local Categorization, External Filtering（LCEF）方法，利用大语言模型的局部完成能力，减少对长期推理的依赖，从而降低误报率。

Result: LCEF将DocPrism的不一致标记率从98%降低到14%，准确率从14%提升到94%；在Python、TypeScript、C++和Java上的测试显示，标记率维持在15%，精准率达0.62，无需额外微调。

Conclusion: DocPrism结合LCEF方法有效提升了多语言代码与文档不一致检测的准确性和实用性，显著降低了误报率。

Abstract: Code-documentation inconsistencies are common and undesirable: they can lead
to developer misunderstandings and software defects. This paper introduces
DocPrism, a multi-language, code-documentation inconsistency detection tool.
DocPrism uses a standard large language model (LLM) to analyze and explain
inconsistencies. Plain use of LLMs for this task yield unacceptably high false
positive rates: LLMs identify natural gaps between high-level documentation and
detailed code implementations as inconsistencies. We introduce and apply the
Local Categorization, External Filtering (LCEF) methodology to reduce false
positives. LCEF relies on the LLM's local completion skills rather than its
long-term reasoning skills. In our ablation study, LCEF reduces DocPrism's
inconsistency flag rate from 98% to 14%, and increases accuracy from 14% to
94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism
maintains a low flag rate of 15%, and achieves a precision of 0.62 without
performing any fine-tuning.

</details>


### [108] [LLM-Driven Cost-Effective Requirements Change Impact Analysis](https://arxiv.org/abs/2511.00262)
*Romina Etezadi,Sallam Abualhaija,Chetan Arora,Lionel Briand*

Main category: cs.SE

TL;DR: 提出了ProReFiCIA，一种基于大型语言模型的自动识别需求变更影响的工具，显著提高了准确率并减少了人工审查工作量。


<details>
  <summary>Details</summary>
Motivation: 需求在软件开发过程中频繁变更，手动识别变更影响既耗时又易出错，可能导致重要需求被忽视，引发后续任务问题。

Method: 基于大型语言模型（LLM）开发ProReFiCIA，通过多种模型和提示词组合进行评估，自动识别被变更需求影响的其他需求。

Result: 最佳模型和提示组合在基准数据集达到了93.3%的召回率，在工业数据集上达到95.8%，显著提高识别效果，且仅需审查生成结果的2.1%-8.5%。

Conclusion: ProReFiCIA有效利用LLM自动识别需求变更影响，大幅提高准确率并降低工程师工作量，具备良好实用价值。

Abstract: Requirements are inherently subject to changes throughout the software
development lifecycle. Within the limited budget available to requirements
engineers, manually identifying the impact of such changes on other
requirements is both error-prone and effort-intensive. That might lead to
overlooked impacted requirements, which, if not properly managed, can cause
serious issues in the downstream tasks. Inspired by the growing potential of
large language models (LLMs) across diverse domains, we propose ProReFiCIA, an
LLM-driven approach for automatically identifying the impacted requirements
when changes occur. We conduct an extensive evaluation of ProReFiCIA using
several LLMs and prompts variants tailored to this task. Using the best
combination of an LLM and a prompt variant, ProReFiCIA achieves a recall of
93.3% on a benchmark dataset and 95.8% on a newly created industry dataset,
demonstrating its strong effectiveness in identifying impacted requirements.
Further, the cost of applying ProReFiCIA remains small, as the engineer only
needs to review the generated results, which represent between 2.1% and 8.5% of
the entire set of requirements.

</details>


### [109] [Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework](https://arxiv.org/abs/2511.00417)
*Marcel Valovy*

Main category: cs.SE

TL;DR: 本论文基于自我决定理论和个性心理学，提出ROMA框架以优化人机协作中的编程角色分配，有助于提升动力和团队协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能改变软件开发，如何实现开发者与AI系统的高效协作成为关键问题，需基于个性特征优化角色分配以提升动力和满意度。

Method: 通过五个设计科学研究循环，结合200名实验参与者和46名访谈对象，实证验证个性特质、编程角色偏好与协作结果间的关系，定义五种个性原型及对应角色。

Result: 个性驱动的角色优化显著提升自我决定感和团队动力，专业人士动力提升23%，本科生最高提升65%；提出五种个性原型与对应的三种编程角色偏好及关键的角色分配模式。

Conclusion: 论文贡献包括建立个性特质与角色偏好及动力结果的实证框架，个性化AI协作模式的分类，以及基于ISO/IEC 29110标准支持小型组织实施个性化角色优化的方法。

Abstract: As artificial intelligence transforms software development, a critical
question emerges: how can developers and AI systems collaborate most
effectively? This dissertation optimizes human-AI programming roles through
self-determination theory and personality psychology, introducing the Role
Optimization Motivation Alignment (ROMA) framework.
  Through Design Science Research spanning five cycles, this work establishes
empirically-validated connections between personality traits, programming role
preferences, and collaborative outcomes, engaging 200 experimental participants
and 46 interview respondents.
  Key findings demonstrate that personality-driven role optimization
significantly enhances self-determination and team dynamics, yielding 23%
average motivation increases among professionals and up to 65% among
undergraduates. Five distinct personality archetypes emerge: The Explorer (high
Openness/low Agreeableness), The Orchestrator (high
Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low
Extraversion), The Architect (high Conscientiousness), and The Adapter
(balanced profile). Each exhibits distinct preferences for programming roles
(Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for
satisfaction.
  The dissertation contributes: (1) an empirically-validated framework linking
personality traits to role preferences and self-determination outcomes; (2) a
taxonomy of AI collaboration modalities mapped to personality profiles while
preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small
Entities to implement personality-driven role optimization within established
standards.
  Keywords: artificial intelligence, human-computer interaction, behavioral
software engineering, self-determination theory, personality psychology,
phenomenology, intrinsic motivation, pair programming, design science research,
ISO/IEC 29110

</details>


### [110] [SmartDoc: A Context-Aware Agentic Method Comment Generation Plugin](https://arxiv.org/abs/2511.00450)
*Vahid Etemadi,Gregorio Robles*

Main category: cs.SE

TL;DR: 本文提出了SmartDoc，一个基于AI的IntelliJ IDEA插件，通过利用方法及其嵌套调用的上下文信息，生成精准且实时更新的Java方法注释。


<details>
  <summary>Details</summary>
Motivation: 程序维护阶段需要程序理解，而阅读完整方法代码较难，且需要准确及时的注释来辅助理解。

Method: SmartDoc将目标方法及其嵌套调用构成调用图，通过深度优先搜索遍历，从而为大型语言模型生成丰富的上下文提示，以生成方法注释。

Result: 该插件可在IntelliJ IDEA中使用，支持并发注释更新，且通过BERTScore等指标评测生成注释与真实注释的相似度，准确率在0.80至0.90间。

Conclusion: SmartDoc有效提升了基于上下文的Java方法注释生成质量，辅助开发者更好理解和维护代码。

Abstract: Context: The software maintenance phase involves many activities such as code
refactoring, bug fixing, code review or testing. Program comprehension is key
to all these activities, as it demands developers to grasp the knowledge (e.g.,
implementation details) required to modify the codebase. Methods as main
building blocks in a program can offer developers this knowledge source for
code comprehension. However, reading entire method statements can be
challenging, which necessitates precise and up-to-date comments. Objective: We
propose a solution as an IntelliJ IDEA plugin, named SmartDoc, that assists
developers in generating context-aware method comments. Method: This plugin
acts as an Artificial Intelligence (AI) agent that has its own memory and is
augmented by target methods' context. When a request is initiated by the
end-user, the method content and all its nested method calls are used in the
comment generation. At the beginning, these nested methods are visited and a
call graph is generated. This graph is then traversed using depth-first search
(DFS), enabling the provision of full-context to enrich Large Language Model
(LLM) prompts. Result: The product is a software, as a plugin, developed for
Java codebase and installable on IntelliJ IDEA. This plugin can serve
concurrently for methods whose comments are being updated , and it shares
memory across all flows to avoid redundant calls. o measure the accuracy of
this solution, a dedicated test case is run to record SmartDoc generated
comments and their corresponding ground truth. For each collected result-set,
three metrics are computed, BERTScore, BLEU and ROUGE-1. These metrics will
determine how accurate the generated comments are in comparison to the ground
truth. Result: The obtained accuracy, in terms of the precision, recall and F1,
is promising, and lies in the range of 0.80 to 0.90 for BERTScore.

</details>


### [111] [A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements](https://arxiv.org/abs/2511.00467)
*Liu Wang,Dong Wang,Shidong Pan,Zheng Jiang,Haoyu Wang,Yi Wang*

Main category: cs.SE

TL;DR: 本文研究了iOS 15.2引入的App隐私报告功能，评估其实际隐私保护效果并提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用普及，用户隐私数据访问透明性的需求日益增长，而苹果的新功能尚未被评估其实际影响。

Method: 通过系统化评估、LLM辅助和多技术综合增强，以及结构化焦点小组访谈，分析用户体验和功能缺陷。

Result: 发现应用隐私报告的实用性受限，主要问题是访问目的和域描述不够清晰。提出了目的推断框架和域澄清流程，并验证了这些改进的有效性。

Conclusion: 该研究为提升用户隐私透明度提供了实用洞见，推动未来隐私保护功能的优化与研究。

Abstract: The prevalent engagement with mobile apps underscores the importance of
understanding their data practices. Transparency plays a crucial role in this
context, ensuring users to be informed and give consent before any data access
occurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to
inform users about detailed insights into apps' data access and sharing. This
feature continues Apple's trend of privacy-focused innovations (following
Privacy Nutrition Labels), and has been marketed as a big step forward in user
privacy. However, its real-world impacts on user privacy and control remain
unexamined. We thus proposed an end-to-end study involving systematic
assessment of the App Privacy Report's real-world benefits and limitations,
LLM-enabled and multi-technique synthesized enhancements, and comprehensive
evaluation from both system and user perspectives. Through a structured focus
group study with twelve everyday iOS users, we explored their experiences,
understanding, and perceptions of the feature, suggesting its limited practical
impact resulting from missing important details. We identified two primary user
concerns: the clarity of data access purpose and domain description. In
response, we proposed enhancements including a purpose inference framework and
domain clarification pipeline. We demonstrated the effectiveness and benefits
of such enhancements for mobile app users. This work provides practical
insights that could help enhance user privacy transparency and discusses areas
for future research.

</details>


### [112] [Issue-Oriented Agent-Based Framework for Automated Review Comment Generation](https://arxiv.org/abs/2511.00517)
*Shuochuan Li,Dong Wang,Patanamon Thongtanunam,Zan Wang,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: RevAgent是一种基于代理的分问题代码评审自动评论生成框架，显著提升了评论质量和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自动评论生成方法依赖单一模型，难以处理多样化且特定于问题的代码改动，导致评论信息量不足，尤其是复杂场景如BUG修复时表现不佳。

Method: 提出RevAgent框架，将任务分为生成阶段（五个针对不同问题类别的代理生成评论）、判别阶段（批判代理选择最合适的评论）和训练阶段（在分类数据上微调各代理以增强专业化）。

Result: 在多项自动评价指标（BLEU、ROUGE-L、METEOR、SBERT）上，RevAgent较现有最先进方法分别提升12.90%、10.87%、6.32%和8.57%，并在问题类别识别准确性上表现更优，且在人类评估中表现出评论的准确性、可读性和上下文相关性。

Conclusion: RevAgent有效提升了代码评审自动评论的专业性与准确性，实现了性能与效率的良好平衡，适用于复杂代码变更场景。

Abstract: Code review (CR) is a crucial practice for ensuring software quality. Various
automated review comment generation techniques have been proposed to streamline
the labor-intensive process. However, existing approaches heavily rely on a
single model to identify various issues within the code, limiting the model's
ability to handle the diverse, issue-specific nature of code changes and
leading to non-informative comments, especially in complex scenarios such as
bug fixes. To address these limitations, we propose RevAgent, a novel
agent-based issue-oriented framework, decomposes the task into three stages:
(1) Generation Stage, where five category-specific commentator agents analyze
code changes from distinct issue perspectives and generate candidate comments;
(2) Discrimination Stage, where a critic agent selects the most appropriate
issue-comment pair; and (3) Training Stage, where all agents are fine-tuned on
curated, category-specific data to enhance task specialization. Evaluation
results show that RevAgent significantly outperforms state-of-the-art PLM- and
LLM-based baselines, with improvements of 12.90\%, 10.87\%, 6.32\%, and 8.57\%
on BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively
higher accuracy in issue-category identification, particularly for challenging
scenarios. Human evaluations further validate the practicality of RevAgent in
generating accurate, readable, and context-aware review comments. Moreover,
RevAgent delivers a favorable trade-off between performance and efficiency.

</details>


### [113] [HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models](https://arxiv.org/abs/2511.00527)
*Robab Aghazadeh-Chakherlou,Qing Guo,Siddartha Khastgir,Peter Popov,Xiaoge Zhang,Xingyu Zhao*

Main category: cs.SE

TL;DR: 本文提出了HIP-LLM，一种基于分层不精确概率的LLM可靠性评估框架，能够更准确地刻画模型在真实操作条件下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试仅提供模型准确率的描述性统计，难以反映LLM在实际应用中的概率行为及可靠性。

Method: HIP-LLM基于软件可靠性工程，定义LLM可靠性为在特定操作配置下无故障运行的概率，采用分层结构表示子域依赖，融合不精确先验和操作配置，推导后验可靠性区间。

Result: 实验证明，HIP-LLM在多个基准数据集上相比现有基准和最新方法，能够更准确和标准化地描述模型可靠性。

Conclusion: HIP-LLM为LLM的可靠性评估提供了一种系统且有效的方法，弥补了现有评估手段的不足，且已开源供社区使用。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
domains, raising the need for rigorous reliability assessment methods. Existing
benchmark-based evaluations primarily offer descriptive statistics of model
accuracy over datasets, providing limited insight into the probabilistic
behavior of LLMs under real operational conditions. This paper introduces
HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and
inferring LLM reliability. Building upon the foundations of software
reliability engineering, HIP-LLM defines LLM reliability as the probability of
failure-free operation over a specified number of future tasks under a given
Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains
hierarchically, enabling multi-level inference from subdomain to system-level
reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty
and incorporates OPs to reflect usage contexts. It derives posterior
reliability envelopes that quantify uncertainty across priors and data.
Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a
more accurate and standardized reliability characterization than existing
benchmark and state-of-the-art approaches. A publicly accessible repository of
HIP-LLM is provided.

</details>


### [114] [Employee Performance when Implementing Agile Practices in an IT Workforce](https://arxiv.org/abs/2511.00528)
*Muhammad Hamid Raza Mookadam,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 本研究探讨了南非IT行业采用敏捷实践对员工绩效的影响，发现敏捷方法显著提升了团队合作、沟通、规划及员工发展等方面，但也存在采纳和领导力等障碍。


<details>
  <summary>Details</summary>
Motivation: 非洲尤其是南非在IT行业中关于敏捷实践对员工绩效影响的研究较少，本文填补了这一空白。

Method: 采用解释主义单一方法的定性研究，通过对17名敏捷从业者进行半结构式访谈收集数据。

Result: 敏捷实践改善了团队动态、协作、效率、风险管理及员工身心健康，同时存在采纳难度、团队参与度和领导能力等挑战。

Conclusion: 通过解决敏捷实施中遇到的挑战并增强支持，可显著提升IT行业员工绩效。

Abstract: Adoption of agile practices has increased in IT workforces. However, there is
a lack of comprehensive studies in the African context on employee performance
when implementing agile practices. This study addresses this gap by exploring
employee performance in agile environments for IT workforces in South Africa.
An interpretivist mono-method qualitative approach was used, with the use of
interviews as a research strategy. Seventeen semi-structured interviews were
conducted with agile practitioners from various roles. Our results indicated
that agile practices influence employee performance significantly, with
participants reporting on aspects which included planning, communication,
employee development and well-being, collaboration, team culture and progress.
Additionally, our results reported obstacles when using agile practices that
included adoption, team engagement, leadership and instilling an agile mindset.
Agile practices influence employee performance in IT workforces by fostering
improved team dynamics, enhanced collaboration, improved efficiencies, risk
management, planning, continuous improvement, learning, personal development
and well-being. Conclusively, our findings suggest that if agile challenges are
addressed and additional support is provided, employee performance can be
significantly improved.

</details>


### [115] [GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android](https://arxiv.org/abs/2511.00619)
*Huaijin Ran,Haoyi Zhang,Xunzhu Tang*

Main category: cs.SE

TL;DR: 本文介绍了GDPR-Bench-Android，这是首个针对Android应用中GDPR违规检测多方法评估的全面基准数据集，包含1951个手动标注违规实例，涵盖23条GDPR条款。通过11种方法的测试，发现不同方法在不同任务表现各异。


<details>
  <summary>Details</summary>
Motivation: 自动检测源代码中违反欧盟GDPR的行为是重要且未充分研究的问题，需要提供一套全面基准以评估不同自动方法的性能。

Method: 构建了GDPR-Bench-Android数据集，设计多粒度违规定位和多标签分类两项任务，提出Formal-AST作为形式化基线，评估8个大型语言模型（LLMs）、检索增强法（RAG）、代理法（ReAct）等共11种方法。

Result: ReAct在文件级违规定位上表现最好，Qwen2.5-72B在线级定位表现领先，Formal-AST较弱。多标签任务中，Claude-Sonnet-4.5获得最高Macro-F1，RAG取得最高Macro-Precision。各方法有不同优势，表现任务相关。

Conclusion: 不同自动化检测方法各有优劣，没有单一方法能全方位领先，GDPR-Bench-Android基准对于精准诊断方法能力具有重要价值。所有资源均公开。

Abstract: Automating the detection of EU General Data Protection Regulation (GDPR)
violations in source code is a critical but underexplored challenge. We
introduce \textbf{GDPR-Bench-Android}, the first comprehensive benchmark for
evaluating diverse automated methods for GDPR compliance detection in Android
applications. It contains \textbf{1951} manually annotated violation instances
from \textbf{15} open-source repositories, covering 23 GDPR articles at file-,
module-, and line-level granularities. To enable a multi-paradigm evaluation,
we contribute \textbf{Formal-AST}, a novel, source-code-native formal method
that serves as a deterministic baseline. We define two tasks: (1)
\emph{multi-granularity violation localization}, evaluated via
Accuracy@\textit{k}; and (2) \emph{snippet-level multi-label classification},
assessed by macro-F1 and other classification metrics. We benchmark 11 methods,
including eight state-of-the-art LLMs, our Formal-AST analyzer, a
retrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings
reveal that no single paradigm excels across all tasks. For Task 1, the ReAct
agent achieves the highest file-level Accuracy@1 (17.38%), while the
Qwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the
Formal-AST method's 1.86%. For the difficult multi-label Task 2, the
Claude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method
yields the highest Macro-Precision (7.10%). These results highlight the
task-dependent strengths of different automated approaches and underscore the
value of our benchmark in diagnosing their capabilities. All resources are
available at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.

</details>


### [116] [Can Large Language Models Detect Real-World Android Software Compliance Violations?](https://arxiv.org/abs/2511.00624)
*Haoyi Zhang,Huaijin Ran,Xunzhu Tang*

Main category: cs.SE

TL;DR: 本文提出了CompliBench评估框架，用于检测LLM在Android应用合规性违规识别中的表现，涵盖多法律框架并引入稳定性综合指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在不同法律法规框架下识别Android应用合规性违规存在困难，传统评估指标不足以全面衡量模型性能。

Method: 设计了两个任务（检索与定位、多标签判断），模拟审计流程，引入稳定性感知综合指标（SGS, RCS, CRGS, OCS）来提升评估准确性。

Result: 通过测试六种模型，发现CompliBench能显著提升合规检测效果，其中Claude-3.5-sonnet-20241022表现最佳。

Conclusion: CompliBench为提升LLM在合规检测任务中的表现提供了有效评估手段，有助于推动数据保护标准相关工具的发展。

Abstract: The rapid development of Large Language Models (LLMs) has transformed
software engineering, showing promise in tasks like code generation, bug
detection, and compliance checking. However, current models struggle to detect
compliance violations in Android applications across diverse legal frameworks.
We propose \emph{CompliBench}, a novel evaluation framework for assessing LLMs'
ability to detect compliance violations under regulations like LGPD, PDPA, and
PIPEDA. The framework defines two tasks: Task 1 evaluates \emph{retrieval and
localization} at file, module, and line granularities, and Task 2 assesses
\emph{multi-label judgment} for code snippets. These tasks mirror the audit
process, where auditors locate problematic code and determine implicated
provisions. Traditional metrics fail to capture important aspects like
cross-granularity stability and jurisdictional consistency. Thus, we introduce
stability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive
assessment. Experiments with six models, including GPT-4O and Claude-3.5, show
\emph{CompliBench} improves compliance detection, with
Claude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and
Gemini-2.5-pro the lowest (0.0538). This work demonstrates \emph{CompliBench}'s
potential for improving LLM performance in compliance tasks and provides a
foundation for future tools aligned with data protection standards. Our project
is available at https://github.com/Haoyi-Zhang/CompliBench.

</details>


### [117] [Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare](https://arxiv.org/abs/2511.00658)
*Guilherme H. Travassos,Sabrina Rocha,Rodrigo Feitosa,Felipe Assis,Patricia Goncalves,Andre Gheventer,Larissa Galeno,Arthur Sasse,Julio Cesar Guimaraes,Carlos Brito,Joao Pedro Wieland*

Main category: cs.SE

TL;DR: 本文报告了在软件开发过程中使用生成式人工智能技术的经验，尤其是在临床试验相关系统的开发中，涵盖项目管理、需求、设计、开发和质量保证等环节。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在软件工程中的应用尚处于初期，技术成熟度有限，且研究结果尚不确定，作者团队希望探索其在实际开发中的潜力，推动软件质量和生产力的提升。

Method: 团队基于生成式AI技术，开发用于胸部疾病临床试验的网络软件系统，并记录整个开发过程中的学习与实践经历，重点观察项目管理、需求定义、设计、编码及质量保障环节的应用情况。

Result: 虽然尚无显著技术证据表明开发流程有重大改进，但积累的经验和分享的实践建议为其他软件组织采纳生成式AI提供了有价值的参考。

Conclusion: 生成式AI有望促进软件开发过程的创新和质量提升，尽管目前仍处早期阶段，本文经验对推动软件工程实践的发展具有启示意义。

Abstract: The advances and availability of technologies involving Generative Artificial
Intelligence (AI) are evolving clearly and explicitly, driving immediate
changes in various work activities. Software Engineering (SE) is no exception
and stands to benefit from these new technologies, enhancing productivity and
quality in its software development processes. However, although the use of
Generative AI in SE practices is still in its early stages, considering the
lack of conclusive results from ongoing research and the limited technological
maturity, we have chosen to incorporate these technologies in the development
of a web-based software system to be used in clinical trials by a thoracic
diseases research group at our university. For this reason, we decided to share
this experience report documenting our development team's learning journey in
using Generative AI during the software development process. Project
management, requirements specification, design, development, and quality
assurance activities form the scope of observation. Although we do not yet have
definitive technological evidence to evolve our development process
significantly, the results obtained and the suggestions shared here represent
valuable insights for software organizations seeking to innovate their
development practices to achieve software quality with generative AI.

</details>


### [118] [Repairing Responsive Layout Failures Using Retrieval Augmented Generation](https://arxiv.org/abs/2511.00678)
*Tasmia Zerin,Moumita Asad,B. M. Mainul Hossain,Kazi Sakib*

Main category: cs.SE

TL;DR: 本文提出了ReDeFix，一种结合大语言模型和领域知识自动修复响应式布局失败（RLFs）的工具，利用Stack Overflow讨论指导CSS修复，修复准确率达88%。


<details>
  <summary>Details</summary>
Motivation: 响应式网站在特定屏幕尺寸下常出现布局失真（RLFs），手动修复耗时费力。

Method: 提出基于检索增强生成（RAG）的方法ReDeFix，结合Stack Overflow相关讨论和RLF上下文生成CSS修补代码。

Result: 评估显示ReDeFix在修复RLF方面达成88%的准确率，软件工程师评估确认修复的布局视觉效果正确且美观。

Conclusion: ReDeFix有效利用领域知识辅助大语言模型自动修复响应式布局问题，提高了修复效率和效果。

Abstract: Responsive websites frequently experience distorted layouts at specific
screen sizes, called Responsive Layout Failures (RLFs). Manually repairing
these RLFs involves tedious trial-and-error adjustments of HTML elements and
CSS properties. In this study, an automated repair approach, leveraging LLM
combined with domain-specific knowledge is proposed. The approach is named
ReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes
Stack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting
relevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that
is sent to the LLM to generate CSS patches. Evaluation demonstrates that our
approach achieves an 88\% accuracy in repairing RLFs. Furthermore, a study from
software engineers reveals that generated repairs produce visually correct
layouts while maintaining aesthetics.

</details>


### [119] [An Empirical Investigation of the Experiences of Dyslexic Software Engineers](https://arxiv.org/abs/2511.00706)
*Marcos Vinicius Cruz,Pragya Verma,Grischa Liebel*

Main category: cs.SE

TL;DR: 本文通过定性研究探讨了阅读障碍（失读症）软件工程师的经历，发现他们在编程学习阶段遇到较大困难，但掌握技能后能在软件工程任务中表现出色，且具备视觉思维和创造力等优势。


<details>
  <summary>Details</summary>
Motivation: 尽管失读症影响阅读和写作能力，但其在软件工程领域的影响及潜在优势尚未被充分研究，需要了解失读症工程师的实际经历及挑战。

Method: 采用社会技术根基理论的基础阶段，通过10次失读症软件工程师访谈、3篇博客和153条Reddit社交平台帖子进行数据收集和分析。

Result: 失读症软件工程师在学习编程时面临显著挑战，但代码自动补全、代码检查工具等支持工具能有效缓解困难。一旦掌握编程，表现不逊色且在视觉思维和创造力方面有优势。

Conclusion: 失读症软件工程师有独特的优势和挑战，研究结果对软件工程实践有启示，未来研究可针对如何提高代码对失读症者的可理解性展开。

Abstract: Dyslexia is a common learning disorder that primarily impairs an individual's
reading and writing abilities. In adults, dyslexia can affect both professional
and personal lives, often leading to mental challenges and difficulties
acquiring and keeping work. In Software Engineering (SE), reading and writing
difficulties appear to pose substantial challenges for core tasks such as
programming. However, initial studies indicate that these challenges may not
significantly affect their performance compared to non-dyslexic colleagues.
Conversely, strengths associated with dyslexia could be particularly valuable
in areas like programming and design. However, there is currently no work that
explores the experiences of dyslexic software engineers, and puts their
strengths into relation with their difficulties. To address this, we present a
qualitative study of the experiences of dyslexic individuals in SE. We followed
the basic stage of the Socio-Technical Grounded Theory method and base our
findings on data collected through 10 interviews with dyslexic software
engineers, 3 blog posts and 153 posts on the social media platform Reddit. We
find that dyslexic software engineers especially struggle at the programming
learning stage, but can succeed and indeed excel at many SE tasks once they
master this step. Common SE-specific support tools, such as code completion and
linters are especially useful to these individuals and mitigate many of the
experienced difficulties. Finally, dyslexic software engineers exhibit
strengths in areas such as visual thinking and creativity. Our findings have
implications to SE practice and motivate several areas of future research in
SE, such as investigating what makes code less/more understandable to dyslexic
individuals.

</details>


### [120] [A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI](https://arxiv.org/abs/2511.00776)
*Cuiyun Gao,Guodong Fan,Chun Yong Chong,Shizhan Chen,Chao Liu,David Lo,Zibin Zheng,Qing Liao*

Main category: cs.SE

TL;DR: 本文系统综述了代码相关大语言模型中的幻觉现象，涵盖定义、成因、缓解策略、代码领域特有挑战及评估基准。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程任务中的广泛应用，理解并缓解代码生成中的幻觉问题变得尤为关键。

Method: 通过回顾60篇相关文献，分析幻觉的定义及成因，综述广泛领域的幻觉缓解方法，聚焦代码智能中的特有挑战与利用代码智能任务的检测缓解策略，最后总结相关评估基准。

Result: 总结了数据噪声、暴露偏差、语义基础薄弱等幻觉原因，介绍知识增强生成、受限解码和后期编辑等缓解技术，指出代码领域语法敏感、强类型系统和外部库依赖加剧幻觉，同时展示了使用程序分析、符号执行和单元测试等任务作为检测手段。

Conclusion: 本文强调了代码生成幻觉问题的复杂性和紧迫性，提出了针对性的缓解策略和评估需求，为未来研究提供了系统指导和方向。

Abstract: Model hallucination is one of the most critical challenges faced by Large
Language Models (LLMs), especially in high-stakes code intelligence tasks. As
LLMs become increasingly integrated into software engineering tasks,
understanding and mitigating hallucination in code becomes essential. In this
survey, we provide a systematic review of hallucination phenomena in
code-oriented LLMs from four key perspectives. First, we begin by surveying 60
papers to define hallucination in the context of code and summarize its primary
causes, such as data noise, exposure bias, and insufficient semantic grounding,
while also tracing recent trends in literature across natural language
processing (NLP) and software engineering communities. Second, we review model
hallucination surveys in a broader span and summarize representative
hallucination mitigation strategies, such as knowledge-enhanced generation,
constrained decoding, and post-editing. Third, we review approaches targeted
for code intelligence and highlight code-specific challenges that aggravate
hallucination, including syntax sensitivity, strict type systems, and
dependence on external libraries. Meanwhile, we analyze how emerging code
intelligence tasks, e.g., program analysis, symbolic execution, and unit
testing, are utilized to detect and mitigate hallucinations. Fourth, we
summarize current evaluation benchmarks, ranging from static metrics to dynamic
checks, e.g., compilation and execution correctness, and emphasize the need for
hallucination-oriented benchmarks.

</details>


### [121] [Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems](https://arxiv.org/abs/2511.00780)
*Chenyu Zhao,Shenglin Zhang,Zeshun Huang,Weilin Jin,Yongqian Sun,Dan Pei,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Minghua Ma*

Main category: cs.SE

TL;DR: 本文提出了Build-bench，一个评估大型语言模型在跨指令集架构软件迁移中修复构建失败能力的基准。


<details>
  <summary>Details</summary>
Motivation: 现有很少有基准能够评估大型语言模型在处理跨指令集架构（如x86_64到aarch64）软件迁移时修复构建失败的能力，而这类迁移需要应对复杂依赖关系、异构工具链和长构建日志。

Method: Build-bench收集了268个真实构建失败的软件包，集成了结构提取、文件内容提取、内容修改和构建验证等辅助工具，构建了一个迭代式修复流程，使模型在失败后能根据更新的构建日志和前次修复结果不断优化。

Result: 通过对六个代表性大型语言模型的比较评估，Build-bench表明当前模型的最大构建成功率为63%，且各模型的工具使用模式差异显著。

Conclusion: Build-bench首次结合真实构建环境与可验证结果，建立了针对跨架构软件构建与修复的首个架构感知基准，有助于推进相关领域的研究。

Abstract: Large language models (LLMs) have shown growing potential in software
engineering, yet few benchmarks evaluate their ability to repair software
during migration across instruction set architectures (ISAs). Cross-ISA
migration, such as between x86_64 and aarch64, requires handling complex
dependencies, heterogeneous toolchains, and long build logs while ensuring
executable verification. To address this challenge, we present Build-bench, an
end-to-end benchmark that systematically evaluates the capability of LLMs to
repair build failures in cross-ISA settings. Build-bench collects 268
real-world failed packages and integrates auxiliary tools including Structure
Extraction, File Content Extraction, Content Modification, and Build
Verification to support autonomous, tool-augmented reasoning. The repair
process operates in an iterative loop where, upon failure, the model receives
updated build logs and previous repair outcomes to refine subsequent attempts.
Through a comparative evaluation of six representative LLMs, Build-bench
reveals that current models achieve a maximum build success rate of 63% and
tool usage patterns differ significantly across models. By coupling real build
environments with verifiable outcomes, Build-bench establishes the first
architecture-aware benchmark for studying LLM-based software build and repair.

</details>


### [122] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: 该论文提出了一个基于大型语言模型(LLM)的自动优化框架GrowthHacker，用于提升离线A/B测试(即off-policy evaluation，OPE)的性能，通过迭代代码优化实现高效、可靠的评估。


<details>
  <summary>Details</summary>
Motivation: 传统的在线A/B测试资源消耗大、用户影响风险高且周期长，因此离线A/B测试（OPE）成为重要替代手段。然而，如何利用LLM优化OPE代码，提高评估效率和准确性尚未被充分研究。

Method: 作者设计了GrowthHacker基准框架，结合LLM和基于代理的自动化方法，在大规模真实数据集上反复优化和评估OPE相关代码，提出了two_agent框架以简化系统复杂度且保持优化效果。

Result: 实验在Open Bandit Pipeline和Scope-RL数据集上显示two_agent框架达到100%可靠性和106.7%的平均性能提升，并且success rate达到45%，优于其他对照方法。

Conclusion: LLM基代理能够作为自动化"增长黑客"有效提升离线A/B测试系统性能，促进面向生产环境的数据驱动决策的规模化应用。

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [123] [CodeClash: Benchmarking Goal-Oriented Software Engineering](https://arxiv.org/abs/2511.00839)
*John Yang,Kilian Lieret,Joyce Yang,Carlos E. Jimenez,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: 本文提出了CodeClash基准测试，评估语言模型在开放式、多轮迭代的软件开发任务中的表现，结果显示现有模型在战略推理和长期代码维护上存在显著不足，远不及人类专家。


<details>
  <summary>Details</summary>
Motivation: 当前评测多集中在具体、明确的任务，但现实软件开发是围绕高层目标进行的，如何让模型自动、迭代式地实现开放目标尚未解决。

Method: 设计CodeClash多轮锦标赛基准，模型通过代码编辑和互相竞争的双阶段循环，针对开放目标提升代码库表现，在6个竞技场中进行1680场比赛。

Result: 模型表现出多样化开发风格，但在战略推理和代码长期维护上均有明显缺陷，代码库逐渐杂乱冗余，且顶尖模型每轮均不敌人类专家。

Conclusion: CodeClash基准揭示了语言模型在自主、目标导向代码开发中的核心局限，推动该领域未来研究。

Abstract: Current benchmarks for coding evaluate language models (LMs) on concrete,
well-specified tasks such as fixing specific bugs or writing targeted tests.
However, human programmers do not spend all day incessantly addressing isolated
tasks. Instead, real-world software development is grounded in the pursuit of
high-level goals, like improving user retention or reducing costs. Evaluating
whether LMs can also iteratively develop code to better accomplish open-ended
objectives without any explicit guidance remains an open challenge. To address
this, we introduce CodeClash, a benchmark where LMs compete in multi-round
tournaments to build the best codebase for achieving a competitive objective.
Each round proceeds in two phases: agents edit their code, then their codebases
compete head-to-head in a code arena that determines winners based on
objectives like score maximization, resource acquisition, or survival. Whether
it's writing notes, scrutinizing documentation, analyzing competition logs, or
creating test suites, models must decide for themselves how to improve their
codebases both absolutely and against their opponents. We run 1680 tournaments
(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal
that while models exhibit diverse development styles, they share fundamental
limitations in strategic reasoning. Models also struggle with long-term
codebase maintenance, as repositories become progressively messy and redundant.
These limitations are stark: top models lose every round against expert human
programmers. We open-source CodeClash to advance the study of autonomous,
goal-oriented code development.

</details>


### [124] [A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks](https://arxiv.org/abs/2511.00872)
*Zhuowen Yin,Cuifeng Gao,Chunsong Fan,Wenzhang Yang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: 本文通过实证研究评估了七种通用代理框架在软件开发、漏洞检测和程序修复三个代码中心任务中的表现，揭示了各框架在有效性、效率和资源消耗上的差异和折中。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于特定任务或单一方面，未能全面反映代理在软件工程中实际能力，故需进行系统而全面的评估。

Method: 采用标准且广泛认可的基准测试对七个通用代理框架执行三类代码相关任务，综合评估其有效性、效率和开销，并深入分析效果与效率的关系。

Result: 不同代理展现出不同能力模式及取舍，AgentOrchestra效率较低但尝试多，OpenHands推理能力较强，GPTswarm成本最低，整体表现中等。软件开发任务成本最高。

Conclusion: 研究结果为实际应用和未来高效软件工程代理的研发提供了指导，强调需平衡有效性与效率以提升代理性能。

Abstract: Unlike traditional automation tools or static LLM-based systems, agents
combine decision-making and tool utilization to accomplish complex tasks,
showing great potential in software engineering. However, existing studies
largely focus on specific tasks or isolated aspects, providing an incomplete
picture of agents' practical capabilities. To address this, we conduct a
comprehensive empirical study evaluating seven general-purpose agent frameworks
across three representative code-centric tasks: software development,
vulnerability detection, and program repair. Each task is assessed using
standard, widely adopted benchmarks to ensure objective and comparable
evaluation. Agent performance is systematically analyzed from three
complementary perspectives: effectiveness (task success), efficiency (execution
process), and overhead (token consumption). Our findings reveal distinct
capability patterns and trade-offs among the evaluated frameworks. In terms of
effectiveness, agents achieve moderate overall performance. Regarding
efficiency, AgentOrchestra tends to exhibit the longest trajectories and the
most correction attempts due to coordination overhead, whereas OpenHands
demonstrate stronger reflective reasoning abilities. For overhead, software
development incurs the highest monetary cost, while GPTswarm remains the most
cost-efficient. Furthermore, we conduct an in-depth cross-analysis of the
relationship between effectiveness and efficiency, exploring the underlying
reasons behind their interplay. These findings guide both practical adoption
and future research toward more efficient software engineering agents.

</details>


### [125] [Sustainability of Machine Learning-Enabled Systems: The Machine Learning Practitioner's Perspective](https://arxiv.org/abs/2511.00901)
*Vincenzo De Martino,Stefano Lambiase,Fabiano Pecorelli,Willem-Jan van den Heuvel,Filomena Ferrucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 本研究通过定性访谈和大规模问卷调查，探讨了机器学习系统中可持续性管理的现状与挑战，发现实际实施与意识存在显著脱节。


<details>
  <summary>Details</summary>
Motivation: 虽然软件可持续性是重要的非功能需求，但在机器学习系统开发中，如何实践可持续性仍缺乏实证研究，尤其是涵盖环境、社会和经济多个维度的全面探讨。

Method: 通过对8名资深机器学习工程师的访谈进行定性分析，结合对203名机器学习从业者的大规模问卷调查，分析可持续性的认知、实践方法及面临的挑战。

Result: 研究发现机器学习工程师对可持续性有认知但在实际操作中缺乏系统实施，存在对指导方针、衡量框架及政策支持的强烈需求。

Conclusion: 需要制定更系统的指导和评价体系，同时推动监管政策，以促进机器学习系统中可持续性的有效实施。

Abstract: Software sustainability is a key multifaceted non-functional requirement that
encompasses environmental, social, and economic concerns, yet its integration
into the development of Machine Learning (ML)-enabled systems remains an open
challenge. While previous research has explored high-level sustainability
principles and policy recommendations, limited empirical evidence exists on how
sustainability is practically managed in ML workflows. Existing studies
predominantly focus on environmental sustainability, e.g., carbon footprint
reduction, while missing the broader spectrum of sustainability dimensions and
the challenges practitioners face in real-world settings. To address this gap,
we conduct an empirical study to characterize sustainability in ML-enabled
systems from a practitioner's perspective. We investigate (1) how ML engineers
perceive and describe sustainability, (2) the software engineering practices
they adopt to support it, and (3) the key challenges hindering its adoption. We
first perform a qualitative analysis based on interviews with eight experienced
ML engineers, followed by a large-scale quantitative survey with 203 ML
practitioners. Our key findings reveal a significant disconnection between
sustainability awareness and its systematic implementation, highlighting the
need for more structured guidelines, measurement frameworks, and regulatory
support.

</details>


### [126] [Empirical Derivations from an Evolving Test Suite](https://arxiv.org/abs/2511.00915)
*Jukka Ruohonen,Abhishek Tiwari*

Main category: cs.SE

TL;DR: 本文对NetBSD操作系统从2010年代初至2025年的自动化测试套件进行纵向实证分析，覆盖超过一万测试用例，发现失败率整体稳定，代码变更与内核修改对失败的影响较小。


<details>
  <summary>Details</summary>
Motivation: 旨在通过长期数据分析深入理解大规模且持续演进的软件测试套件的性能与稳定性。

Method: 对NetBSD的自动化、虚拟化测试套件从2010年代初到2025年进行纵向数据收集与统计分析，分析测试失败、构建失败、安装失败等多种失败类型及其与代码变更的关系。

Result: 测试套件持续增长，覆盖超过一万测试用例；测试失败率整体稳定，存在短期失败高发期；构建失败、测试未完成和安装失败表现类似；代码变动和内核修改对失败率影响不大，平均作用较小。

Conclusion: 大规模、持续演进的软件测试套件的失败模式具有一定稳定性，代码与内核变动对测试失败影响有限，实证结果有助于未来从长时间跨度分析软件测试套件表现。

Abstract: The paper presents a longitudinal empirical analysis of the automated,
continuous, and virtualization-based software test suite of the NetBSD
operating system. The longitudinal period observed spans from the initial roll
out of the test suite in the early 2010s to late 2025. According to the
results, the test suite has grown continuously, currently covering over ten
thousand individual test cases. Failed test cases exhibit overall stability,
although there have been shorter periods marked with more frequent failures. A
similar observation applies to build failures, failures of the test suite to
complete, and installation failures, all of which are also captured by the
NetBSD's testing framework. Finally, code churn and kernel modifications do not
provide longitudinally consistent statistical explanations for the failures.
Although some periods exhibit larger effects, including particularly with
respect to the kernel modifications, the effects are small on average. Even
though only in an exploratory manner, these empirical observations contribute
to efforts to draw conclusions from large-scale and evolving software test
suites.

</details>


### [127] [DPO-F+: Aligning Code Repair Feedback with Developers' Preferences](https://arxiv.org/abs/2511.01043)
*Zihan Fang,Yifan Zhang,Yueke Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: DPO-f+ 框架优化了大语言模型在代码修复中的反馈，使其更符合开发者需求，提升了代码准确率和反馈质量，促进了人机协作。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代码修复多关注代码结果，缺乏让开发者理解和迭代改进的自然语言反馈，限制了人机协作效率。

Method: 提出DPO-f+框架，定义开发者画像相关反馈指标，自动构建偏好数据集，基于改进的直接偏好优化（DPO）微调模型，并设计自动反馈评估机制。

Result: DPO-f+在新手编程任务和SWE-bench Lite基准测试中均显著提升修复准确率和反馈对齐度，相较基线和标准DPO均有明显优势。

Conclusion: 通过更好地对齐反馈与开发者需求，DPO-f+将语言模型辅助代码修复转变为协作式的理解过程，增强了代码理解能力和人机协作效果。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
tasks, especially code repair. However, developers often struggle to interpret
model outputs, limiting effective human-AI teaming. Prior work largely
optimizes repaired code while under-addressing the natural-language feedback
that enables comprehension and iterative improvement. We present DPO-f+, a
novel framework that aligns code-repair feedback with developer needs and
profiles. It (1) formalizes developer-profiled, domain-specific metrics for
feedback alignment; (2) automatically constructs pairwise preference datasets
from code-repair tasks; (3) fine-tunes using Direct Preference Optimization
(DPO) augmented with a lightweight margin signal; and (4) provides an automated
feedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline
and standard DPO on generated-code accuracy and overall feedback alignment. On
novice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage
points (pp) over the baseline and by 3.30 pp over DPO. On the more challenging
SWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp
over DPO and by 4.67 pp over the baseline. It also achieves the largest
improvement in feedback alignment, outperforming DPO and the baseline. By
aligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted
repair from one-shot outputs into a collaborative sensemaking workflow,
providing a practical approach to enhancing code comprehension and fostering
more effective human-AI teaming in software engineering.

</details>


### [128] [HAFixAgent: History-Aware Automated Program Repair Agent](https://arxiv.org/abs/2511.01047)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出了一种名为HAFixAgent的基于历史感知的自动程序修复代理系统，通过利用代码仓库历史信息显著提升复杂多处修改（multi-hunk）缺陷的修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型和代理的自动程序修复系统大多依赖局部快照上下文，忽视了仓库的历史信息，然而之前的研究表明历史信息对单行缺陷的修复有帮助，因此探究历史信息能否提升复杂缺陷的修复效果。

Method: HAFixAgent通过注入基于缺陷责任认定（blame）的历史启发式信息进入修复循环，借助版本控制中的差异历史及多种互补的历史启发方法，实现历史感知的修复。

Result: 在Defects4J数据集上，HAFixAgent在修复效果上分别较基线系统提升212.3%（代理基线）和29.9%（多hunk基线），修复步骤和token成本无明显增加，复杂缺陷的成本显著降低，多种历史启发结合能修复更多缺陷。

Conclusion: HAFixAgent验证了利用版本控制历史信息来改进代理式自动程序修复方法的有效性和实用性，提出了以历史信息为基础、优先考虑差异历史上下文并结合多种启发式的实用修复策略。

Abstract: Automated program repair (APR) has recently shifted toward large language
models and agent-based systems, yet most systems rely on local snapshot
context, overlooking repository history. Prior work shows that repository
history helps repair single-line bugs, since the last commit touching the buggy
line is often the bug-introducing one. In this paper, we investigate whether
repository history can also improve agentic APR systems at scale, especially
for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing
Agent that injects blame-derived repository heuristics into its repair loop. A
preliminary study of all 854 real-world bugs from Defects4J motivates our
design, showing that bug-relevant history is both widely available and highly
concentrated. Empirical comparison of HAFixAgent with two state-of-the-art
baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the
agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)
Efficiency: history does not significantly increase agent steps and keeps token
costs comparable, with notably lower median costs for complex
multi-file-multi-hunk bugs. (3) Practicality: combining different historical
heuristics repairs more bugs, offering a clear cost-benefit trade-off.
HAFixAgent offers a practical recipe for history-aware agentic APR: ground the
agent in version control history, prioritize diff-based historical context, and
integrate complementary heuristics when needed.

</details>


### [129] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: 本文提出了HarnessLLM，一种利用大型语言模型生成测试框架代码的两阶段训练方法，提升了测试多样性和调试信息丰富性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动测试生成方法仅生成输入输出对，测试多样性有限且调试信息不足。

Method: 提出HarnessLLM，使用SFT和定制奖励设计的RLVR两阶段训练，使LLM能生成输入合成及输出验证的测试框架代码。

Result: 实验表明，HarnessLLM在发现BUG和测试策略多样性方面优于传统输入输出对测试，同时提升了代码生成性能。

Conclusion: HarnessLLM通过生成复杂测试用例和灵活的输出验证，显著提升了自动测试的效果与代码生成能力。

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [130] [An Empirical Study of LLM-Based Code Clone Detection](https://arxiv.org/abs/2511.01176)
*Wenqing Zhu,Norihiro Yoshida,Eunjong Choi,Yutaka Matsubara,Hiroaki Takada*

Main category: cs.SE

TL;DR: 本文评估了大型语言模型(LLMs)在代码克隆检测任务中的跨数据集性能和响应一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽证明LLMs在代码克隆检测中有效，但未解决不同数据集性能一致性和模型响应稳定性的问题。

Method: 构建七个代码克隆数据集，采用四种提示语对五个LLMs进行评估，数据集取自CodeNet和BigCloneBench，样本通过Levenshtein比率采样。

Result: LLMs在CodeNet数据集上的表现优异，最高F1达到0.943，但在BigCloneBench数据集表现显著下降。大多数模型响应一致性高，超过90%的判断保持一致，F1得分波动小于0.03。

Conclusion: LLMs对不同数据集的适应性仍有限，但具有较高的响应稳定性，提示未来需提升模型的跨数据集泛化能力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various software engineering tasks, such as code generation and debugging,
because of their ability to translate between programming languages and natural
languages. Existing studies have demonstrated the effectiveness of LLMs in code
clone detection. However, two crucial issues remain unaddressed: the ability of
LLMs to achieve comparable performance across different datasets and the
consistency of LLMs' responses in code clone detection. To address these
issues, we constructed seven code clone datasets and then evaluated five LLMs
in four existing prompts with these datasets. The datasets were created by
sampling code pairs using their Levenshtein ratio from two different code
collections, CodeNet and BigCloneBench. Our evaluation revealed that although
LLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943
F1 score, their performance significantly decreased in BigCloneBench-related
datasets. Most models achieved a high response consistency, with over 90\% of
judgments remaining consistent across all five submissions. The fluctuations of
the F1 score affected by inconsistency are also tiny; their variations are less
than 0.03.

</details>


### [131] [Lares: LLM-driven Code Slice Semantic Search for Patch Presence Testing](https://arxiv.org/abs/2511.01252)
*Siyuan Li,Yaowen Zheng,Hong Li,Jingdong Guo,Chaopeng Dong,Chunpeng Yan,Weijie Wang,Yimo Ren,Limin Sun,Hongsong Zhu*

Main category: cs.SE

TL;DR: 本文提出了一种名为Lares的方法，旨在准确检测目标二进制文件中补丁的存在，克服现有方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有检测漏洞函数是否被修补的方法依赖编译过程，操作复杂且准确率不高，难以区分补丁改动与编译差异。

Method: Lares通过直接从补丁源代码提取特征，利用大语言模型和SMT求解器进行语义分析，识别目标二进制伪代码中的等价代码片段，避免依赖编译过程。

Result: 实验表明Lares在精确率、召回率和可用性方面表现优异，是首个在不同优化级别、架构和编译器上评估补丁检测的方法。

Conclusion: Lares提升了漏洞补丁检测的准确性与可用性，为软件安全分析提供了有效工具。

Abstract: In modern software ecosystems, 1-day vulnerabilities pose significant
security risks due to extensive code reuse. Identifying vulnerable functions in
target binaries alone is insufficient; it is also crucial to determine whether
these functions have been patched. Existing methods, however, suffer from
limited usability and accuracy. They often depend on the compilation process to
extract features, requiring substantial manual effort and failing for certain
software. Moreover, they cannot reliably differentiate between code changes
caused by patches or compilation variations. To overcome these limitations, we
propose Lares, a scalable and accurate method for patch presence testing. Lares
introduces Code Slice Semantic Search, which directly extracts features from
the patch source code and identifies semantically equivalent code slices in the
pseudocode of the target binary. By eliminating the need for the compilation
process, Lares improves usability, while leveraging large language models
(LLMs) for code analysis and SMT solvers for logical reasoning to enhance
accuracy. Experimental results show that Lares achieves superior precision,
recall, and usability. Furthermore, it is the first work to evaluate patch
presence testing across optimization levels, architectures, and compilers. The
datasets and source code used in this article are available at
https://github.com/Siyuan-Li201/Lares.

</details>


### [132] [Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation](https://arxiv.org/abs/2511.01316)
*Chong Wang,Chen Zhang,Jiajun Wu,Wunan Guo,Jianfeng Qu,Yewen Tian,Yang Liu*

Main category: cs.SE

TL;DR: 本文研究了基于大型语言模型（LLM）进行持续集成（CI）配置迁移的技术，以Travis CI到GitHub Actions的迁移为例，分析迁移过程中的难点和优化策略。


<details>
  <summary>Details</summary>
Motivation: CI平台迁移常见但复杂，涉及配置语义差异和平台差异，现有迁移工具效率不高，LLM有潜力提升迁移质量和效率。

Method: 基于811条真实迁移记录，分析开发者迁移努力；对4种LLM生成的迁移结果进行错误分类；设计并评估结合指导提示和迭代优化的增强策略。

Result: 发现迁移中开发者平均需阅读38行旧配置、编写58行新配置，近半需多次提交；LLM生成配置存在逻辑不一致、平台差异等四类问题；通过增强策略，Build成功率提升至75.5%，显著优于基础GPT-4o。

Conclusion: 结合提示指导和迭代优化的LLM方法显著提升CI配置迁移效果，展示了LLM在软件工程领域具体应用的潜力。

Abstract: Continuous Integration (CI) is a cornerstone of modern collaborative software
development, and numerous CI platforms are available. Differences in
maintenance overhead, reliability, and integration depth with code-hosting
platforms make migration between CI platforms a common practice. A central step
in migration is translating CI configurations, which is challenging due to the
intrinsic complexity of CI configurations and the need to understand semantic
differences and relationships across CI platforms.
  With the advent of large language models (LLMs), recent advances in software
engineering highlight their potential for CI configuration translation. In this
paper, we present a study on LLM-based CI configuration translation, focusing
on the migration from Travis CI to GitHub Actions. First, using 811 migration
records, we quantify the effort involved and find that developers read an
average of 38 lines of Travis configuration and write 58 lines of GitHub
Actions configuration, with nearly half of the migrations requiring multiple
commits. We further analyze translations produced by each of the four LLMs and
identify 1,121 issues grouped into four categories: logic inconsistencies
(38%), platform discrepancies (32%), environment errors (25%), and syntax
errors (5%). Finally, we evaluate three enhancement strategies and show that
combining guideline-based prompting with iterative refinement achieves the best
performance, reaching a Build Success Rate of 75.5%-nearly a threefold
improvement over GPT-4o with a basic prompt.

</details>


### [133] [AI for Requirements Engineering: Industry adoption and Practitioner perspectives](https://arxiv.org/abs/2511.01324)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 本文调查了人工智能在需求工程中应用的现状，发现大多数从业者积极采用AI，且更倾向于将其作为人机协作伙伴而非完全自动化工具。


<details>
  <summary>Details</summary>
Motivation: 需求工程作为软件工程的基础，其人工智能的应用研究较少，本文旨在填补这一空白，理解AI在需求工程不同阶段的应用及其影响。

Method: 通过对55位软件从业者进行调查，分析AI在需求工程四个阶段（引导、分析、规格说明、验证）以及四种决策方式（全人类决策、AI验证、人机协作、全AI自动化）中的使用情况和用户感知。

Result: 58.2%的受访者已经在使用AI，69.1%认为AI影响积极。人机协作占比54.4%，全自动化仅5.4%，被动AI验证更少，表明从业者更重视AI的主动支持作用。

Conclusion: AI在需求工程中最有效的角色是协作伙伴而非替代者，未来需要构建专门的人机协作框架和健全的AI治理机制以促进AI的健康应用。

Abstract: The integration of AI for Requirements Engineering (RE) presents significant
benefits but also poses real challenges.Although RE is fundamental to software
engineering, limited research has examined AI adoption in RE.We surveyed 55
software practitioners to map AI usage across four RE phases:Elicitation,
Analysis, Specification, and Validation, and four approaches for decision
making: human only decisions, AI validation, Human AI Collaboration (HAIC), and
full AI automation.Participants also shared their perceptions, challenges, and
opportunities when applying AI for RE tasks.Our data show that 58.2% of
respondents already use AI in RE, and 69.1% view its impact as positive or very
positive.HAIC dominates practice, accounting for 54.4% of all RE techniques,
while full AI automation remains minimal at 5.4%.Passive AI validation (4.4 to
6.2%) lags even further behind, indicating that practitioners value AI's active
support over passive oversight.These findings suggest that AI is most effective
when positioned as a collaborative partner rather than a replacement for human
expertise.It also highlights the need for RE specific HAIC frameworks along
with robust and responsible AI governance as AI adoption in RE grows.

</details>


### [134] [The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project](https://arxiv.org/abs/2511.01348)
*Robin Gröpler,Steffen Klepke,Jack Johns,Andreas Dreschinski,Klaus Schmid,Benedikt Dornauer,Eray Tüzün,Joost Noppen,Mohammad Reza Mousavi,Yongjian Tang,Johannes Viehmann,Selin Şirin Aslangül,Beum Seuk Lee,Adam Ziolkowski,Eric Zie*

Main category: cs.SE

TL;DR: 本文探讨了生成式人工智能（GenAI）在软件工程全生命周期中的应用现状、挑战与未来发展，基于GENIUS项目的视角，提出了面向未来五年的技术和方法愿景。


<details>
  <summary>Details</summary>
Motivation: 虽然GenAI在编码任务中展现出巨大潜力，但其在软件开发全生命周期的广泛应用尚未充分探索，且存在可靠性、责任、安全和数据隐私等关键问题，亟需深入研究与协调应对。

Method: 通过GENIUS项目汇聚欧洲多方产业与学术伙伴，结合跨领域对话和文献综述，系统分析GenAI在SDLC中的挑战，构建未来技术发展与人才需求的愿景，并推动创新工具研发和工业验证。

Result: 提出了四个核心内容：当前GenAI采纳的挑战综述、未来五年技术与方法进展展望、软件专业人员的角色与技能转变预测，以及GENIUS项目通过实际工具和工业验证促进转型的贡献。

Conclusion: 该论文为GenAI在软件工程中的可靠、可扩展和产业化应用奠定基础，协调技术创新与业务需求，指导后续研究和工业策略，助力软件团队实现GenAI转型。

Abstract: Generative AI (GenAI) has recently emerged as a groundbreaking force in
Software Engineering, capable of generating code, suggesting fixes, and
supporting quality assurance. While its use in coding tasks shows considerable
promise, applying GenAI across the entire Software Development Life Cycle
(SDLC) has not yet been fully explored. Critical uncertainties in areas such as
reliability, accountability, security, and data privacy demand deeper
investigation and coordinated action. The GENIUS project, comprising over 30
European industrial and academic partners, aims to address these challenges by
advancing AI integration across all SDLC phases. It focuses on GenAI's
potential, the development of innovative tools, and emerging research
challenges, actively shaping the future of software engineering. This vision
paper presents a shared perspective on the future of GenAI-based software
engineering, grounded in cross-sector dialogue and experience within the GENIUS
consortium, supported by an exploratory literature review. The paper explores
four central elements: (1) a structured overview of current challenges in GenAI
adoption across the SDLC; (2) a forward-looking vision outlining key
technological and methodological advances expected over the next five years;
(3) anticipated shifts in the roles and required skill sets of software
professionals; and (4) the contribution of GENIUS in realizing this
transformation through practical tools and industrial validation. By aligning
technical innovation with business relevance, this paper aims to inform both
research agendas and industrial strategies, providing a foundation for
reliable, scalable, and industry-ready GenAI solutions for software engineering
teams.

</details>


### [135] [Characterizing Build Compromises Through Vulnerability Disclosure Analysis](https://arxiv.org/abs/2511.01395)
*Maimouna Tamah Diao,Moustapha Awwalou Diouf,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Gervais Mendy,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文通过大规模CVE挖掘和供应链攻击案例分析，系统分类了针对软件构建过程的攻击向量，揭示了构建阶段的安全威胁及其主要表现形式。


<details>
  <summary>Details</summary>
Motivation: 软件构建过程是将源代码转化为部署产物的关键阶段，安全性复杂且易受攻击，尤其是多组件系统复杂性、编译期间的入侵难以检测和构建过程中的随机性掩盖恶意修改，然而安全社区缺乏对构建攻击向量的系统理解，阻碍有效防御措施的设计。

Method: 通过对621个CVE漏洞披露的挖掘，构建了基于注入点的构建攻击向量分类法，涵盖从源代码操作到编译器被攻击的各个环节。并通过分析168起软件供应链攻击案例验证该分类法，其中识别出40起明确针对构建阶段的攻击事件。

Result: 分析发现23.8%的供应链攻击利用了构建漏洞，依赖混淆和构建脚本注入是最常见的攻击方式。研究提供了构建攻击向量的详细分类和对应的攻击案例支持。

Conclusion: 该研究填补了软件构建安全领域的知识空白，系统分类了构建攻击向量，强调构建阶段安全防护的重要性，为设计更有效的防御机制提供了数据支持和理论基础。

Abstract: The software build process transforms source code into deployable artifacts,
representing a critical yet vulnerable stage in software development. Build
infrastructure security poses unique challenges: the complexity of
multi-component systems (source code, dependencies, build tools), the
difficulty of detecting intrusions during compilation, and prevalent build
non-determinism that masks malicious modifications. Despite these risks, the
security community lacks a systematic understanding of build-specific attack
vectors, hindering effective defense design.
  This paper presents an empirically-derived taxonomy of attack vectors
targeting the build process, constructed through a large-scale CVE mining (of
621 vulnerability disclosures from the NVD database). We categorize attack
vectors by their injection points across the build pipeline, from source code
manipulation to compiler compromise. To validate our taxonomy, we analyzed 168
documented software supply chain attacks, identifying 40 incidents specifically
targeting build phases. Our analysis reveals that 23.8\% of supply chain
attacks exploit build vulnerabilities, with dependency confusion and build
script injection representing the most prevalent vectors.
  Dataset available at:
https://anonymous.4open.science/r/Taxonomizing-Build-Attacks-8BB0.

</details>


### [136] [VeriODD: From YAML to SMT-LIB - Automating Verification of Operational Design Domains](https://arxiv.org/abs/2511.01417)
*Bassel Rafie,Christian Schindler,Andreas Rausch*

Main category: cs.SE

TL;DR: 本文介绍了VeriODD工具，实现了从YAML格式的操作设计域（ODD）和当前操作域（COD）规范自动翻译为形式逻辑表达和SMT-LIB格式，支持自动一致性检查和符合性验证。


<details>
  <summary>Details</summary>
Motivation: 当前ODD和COD的规范虽然用YAML表达方便，但不适合形式验证，且手动转译为SMT-LIB繁琐且易错，影响自动驾驶系统安全保障。

Method: 基于ANTLR的编译器技术，VeriODD将YAML的ODD/COD规范转换为命题逻辑和SMT-LIB格式，结合Z3求解器进行自动一致性检查和符合验证，并提供图形界面便于编辑和验证。

Result: VeriODD实现了从人类易读的YAML到适合自动化验证的SMT-LIB的无缝转换，支持快速且自动化的ODD规范一致性检验和COD合规性验证，提升了自动驾驶系统的安全保障效率。

Conclusion: VeriODD弥合了面向利益相关者的规范表达与形式验证间的鸿沟，使得自动驾驶操作边界的安全保障更加自动化、规模化和高效。

Abstract: Operational Design Domains (ODDs) define the conditions under which an
Automated Driving System (ADS) is allowed to operate, while Current Operational
Domains (CODs) capture the actual runtime situation. Ensuring that a COD
instance lies within the ODD is a crucial step in safety assurance. Today, ODD
and COD specifications are frequently expressed in YAML to remain accessible
for stakeholders, but such descriptions are not directly suitable for
solver-based verification. Manual translation into formal languages such as
SMT-LIB is slow and error-prone. We present VeriODD, a tool that automates this
translation. VeriODD uses ANTLR-based compiler technology to transform
YAML-based ODD/COD specifications into both human-readable propositional logic,
for lightweight review on a simple basis, and solver-ready SMT-LIB. The tool
integrates with SMT solvers such as Z3 to provide automated consistency checks
of ODD specifications and verification of COD conformance. A graphical user
interface supports editing specifications, inspecting generated formulas, and
performing verification with a single click. VeriODD thereby closes the gap
between stakeholder-friendly ODD/COD notations and formal verification,
enabling scalable and automated assurance of operational boundaries in
autonomous driving. Video demonstration: https://youtu.be/odRacNoL_Pk Tool
available at: https://github.com/BasselRafie/VeriODD

</details>


### [137] [LLM-Assisted Tool for Joint Generation of Formulas and Functions in Rule-Based Verification of Map Transformations](https://arxiv.org/abs/2511.01423)
*Ruidi He,Yu Zhang,Meng Zhang,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出了一种基于大语言模型(LLM)辅助的自动驾驶高精地图转换验证方法，实现了语义正确性自动校验，减少了手工编码劳动。


<details>
  <summary>Details</summary>
Motivation: 现有地图转换验证规则依赖人工编写的公式和领域函数，难以扩展，且保证语义正确性具有挑战性。

Method: 提出了一个LLM辅助的管线，联合生成形式逻辑(FOL)中的逻辑公式和可执行谓词，扩展了CommonRoad场景设计器中的地图验证器，并支持地形高程。

Result: 在合成的桥梁和坡度情景中进行了原型评估，显示降低了手工工程量且保持了验证正确性。

Conclusion: 该方法实现了一个可扩展的半自动人机协同地图转换验证方案，验证了利用LLM辅助生成逻辑规则的可行性。

Abstract: High-definition map transformations are essential in autonomous driving
systems, enabling interoperability across tools. Ensuring their semantic
correctness is challenging, since existing rule-based frameworks rely on
manually written formulas and domain-specific functions, limiting scalability.
  In this paper, We present an LLM-assisted pipeline that jointly generates
logical formulas and corresponding executable predicates within a computational
FOL framework, extending the map verifier in CommonRoad scenario designer with
elevation support. The pipeline leverages prompt-based LLM generation to
produce grammar-compliant rules and predicates that integrate directly into the
existing system.
  We implemented a prototype and evaluated it on synthetic bridge and slope
scenarios. The results indicate reduced manual engineering effort while
preserving correctness, demonstrating the feasibility of a scalable,
semi-automated human-in-the-loop approach to map-transformation verification.

</details>


### [138] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: 本研究通过分析9000多个Java开源仓库中的自承技术债（SATD）注释，揭示了SATD主要出现在定义、条件语句和异常处理的嵌入代码中。


<details>
  <summary>Details</summary>
Motivation: 前人研究虽关注SATD的检测与优先级排序，但对受SATD影响的源代码部分关注较少。本研究旨在将SATD注释与其周围的代码构造关联起来。

Method: 利用PENTACET大规模SATD数据集，定量推断SATD的常见位置及影响的代码构造或语句类型。

Result: 通过链接22.5万个SATD注释与周围代码，发现SATD主要出现在内联代码中靠近定义、条件语句和异常处理的位置。

Conclusion: SATD反映开发者在面对不确定性和权衡时的有意识别，是变更过程中的一种主动信号，而非简单的疏忽。

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>


### [139] [From Pre-labeling to Production: Engineering Lessons from a Machine Learning Pipeline in the Public Sector](https://arxiv.org/abs/2511.01545)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 公共部门在运用机器学习时面临技术和组织双重挑战，成功依赖于透明可追溯的数据基础设施，而非单纯模型准确率的突破。


<details>
  <summary>Details</summary>
Motivation: 公共部门在部署机器学习系统时遇到技术难题和组织障碍，影响系统的准确性、可审计性和可持续运行。

Method: 研究了巴西参与平台，分析了常用工程手段（如LLM预标签、模型分路分类器、合成数据生成）在加快开发的同时带来的风险。

Result: 发现若缺乏严格的数据治理和人工校验，这些工程手段会引入可追溯性、可靠性和成本风险。

Conclusion: 公共部门的机器学习是制度工程问题，ML管道应视为公共基础设施，成功依赖于透明、可重现和负责任的数据基础设施建设。

Abstract: Machine learning is increasingly being embedded into government digital
platforms, but public-sector constraints make it difficult to build ML systems
that are accurate, auditable, and operationally sustainable. In practice, teams
face not only technical issues like extreme class imbalance and data drift, but
also organizational barriers such as bureaucratic data access, lack of
versioned datasets, and incomplete governance over provenance and monitoring.
Our study of the Brasil Participativo (BP) platform shows that common
engineering choices -- like using LLMs for pre-labeling, splitting models into
routed classifiers, and generating synthetic data -- can speed development but
also introduce new traceability, reliability, and cost risks if not paired with
disciplined data governance and human validation. This means that, in the
public sector, responsible ML is not just a modeling problem but an
institutional engineering problem, and ML pipelines must be treated as civic
infrastructure. Ultimately, this study shows that the success of machine
learning in the public sector will depend less on breakthroughs in model
accuracy and more on the ability of institutions to engineer transparent,
reproducible, and accountable data infrastructures that citizens can trust.

</details>


### [140] [Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy](https://arxiv.org/abs/2511.01757)
*Shamse Tasnim Cynthia,Banani Roy*

Main category: cs.SE

TL;DR: 本文提出了一种结合密集向量检索和大型语言模型重排序的Galaxy科学工作流程检索框架，提高了检索的语义理解和准确率。


<details>
  <summary>Details</summary>
Motivation: 当前Galaxy基于关键词的检索系统缺乏语义查询理解能力，难以有效检索相关工作流程。

Method: 采用两阶段检索框架，先用嵌入模型检索候选工作流程，再用指令调优的生成式大型语言模型（如GPT-4o，Mistral-7B）进行基于任务语义的重排序，并构建标注数据集和真实查询进行评估。

Result: 该方法在多种信息检索指标上显著提升了Top-k准确率和相关性，特别是在长查询或模糊查询场景中表现优异。

Conclusion: 集成的大型语言模型增强的检索系统有效提升了科学工作流程的可用性和可访问性，特别惠及新手和跨学科研究者。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become
essential infrastructure in bioinformatics, supporting the design, execution,
and sharing of complex multi-step analyses. Despite hosting hundreds of
reusable workflows across domains, Galaxy's current keyword-based retrieval
system offers limited support for semantic query interpretation and often fails
to surface relevant workflows when exact term matches are absent. To address
this gap, we propose a task-aware, two-stage retrieval framework that
integrates dense vector search with large language model (LLM)-based reranking.
Our system first retrieves candidate workflows using state-of-the-art embedding
models and then reranks them using instruction-tuned generative LLMs (GPT-4o,
Mistral-7B) based on semantic task alignment. To support robust evaluation, we
construct a benchmark dataset of Galaxy workflows annotated with semantic
topics via BERTopic and synthesize realistic task-oriented queries using LLMs.
We conduct a comprehensive comparison of lexical, dense, and reranking models
using standard IR metrics, presenting the first systematic evaluation of
retrieval performance in the Galaxy ecosystem. Results show that our approach
significantly improves top-k accuracy and relevance, particularly for long or
under-specified queries. We further integrate our system as a prototype tool
within Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.
This work advances the usability and accessibility of scientific workflows,
especially for novice users and interdisciplinary researchers.

</details>


### [141] [Context-Guided Decompilation: A Step Towards Re-executability](https://arxiv.org/abs/2511.01763)
*Xiaohan Wang,Yuxin Hu,Kevin Leach*

Main category: cs.SE

TL;DR: 本文提出了ICL4Decomp，这是一种利用大语言模型（LLM）结合上下文学习（ICL）指导生成可重新执行源代码的混合二进制反编译框架，实现了比现有技术约40%的可重执行性提升。


<details>
  <summary>Details</summary>
Motivation: 现有反编译技术难以生成可重新编译和执行的源代码，特别是在优化后的二进制文件中，且现有神经方法生成的代码多为语义合理但不可执行，受限于编译器优化和语义线索丢失。

Method: 提出ICL4Decomp混合框架，通过上下文学习引导大语言模型生成可重新执行的源代码，结合多数据集、多优化级别及编译器进行评估。

Result: 在多个数据集和不同优化级别、编译器环境下，ICL4Decomp在可重执行性方面较最先进反编译方法提升约40%，且保持了鲁棒性。

Conclusion: 利用上下文学习指导大语言模型进行反编译，可以显著提升生成代码的实际可重执行性，增强反编译技术的实用价值。

Abstract: Binary decompilation plays an important role in software security analysis,
reverse engineering, and malware understanding when source code is unavailable.
However, existing decompilation techniques often fail to produce source code
that can be successfully recompiled and re-executed, particularly for optimized
binaries. Recent advances in large language models (LLMs) have enabled neural
approaches to decompilation, but the generated code is typically only
semantically plausible rather than truly executable, limiting their practical
reliability. These shortcomings arise from compiler optimizations and the loss
of semantic cues in compiled code, which LLMs struggle to recover without
contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid
decompilation framework that leverages in-context learning (ICL) to guide LLMs
toward generating re-executable source code. We evaluate our method across
multiple datasets, optimization levels, and compilers, demonstrating around
40\% improvement in re-executability over state-of-the-art decompilation
methods while maintaining robustness.

</details>


### [142] [SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring](https://arxiv.org/abs/2511.01850)
*Jiawei Jin,Yingxin Su,Xiaotong Zhu*

Main category: cs.SE

TL;DR: 本研究提出了一种集成大语言模型（LLM）助手和自动化MLOps管道的智能IDE，实现了模型开发、部署和监控的一体化，显著提升了配置效率、实验可重复性和漂移检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统IDE主要支持代码编写，缺乏对完整机器学习生命周期的智能支持，现有MLOps平台与编码工作流程分离，难以实现一体化管理和自动化操作。

Method: 设计并实现了一个集成LLM助手（支持代码生成、调试建议及自动配置管道）的智能IDE。后端集成自动数据校验、特征存储、漂移检测及再训练触发机制，搭建CI/CD部署系统。构建原型SmartMLOps Studio并在UCI Adult和M5数据集上评测。

Result: SmartMLOps Studio相比传统工作流，配置时间减少61%，实验可重复性提升45%，漂移检测准确率提升14%。

Conclusion: 该研究通过结合智能代码辅助和自动化运维管道，创新性地将IDE转变为具备生命周期感知的智能平台，为可扩展高效的模型开发奠定了基础。

Abstract: The rapid expansion of artificial intelligence and machine learning (ML)
applications has intensified the demand for integrated environments that unify
model development, deployment, and monitoring. Traditional Integrated
Development Environments (IDEs) focus primarily on code authoring, lacking
intelligent support for the full ML lifecycle, while existing MLOps platforms
remain detached from the coding workflow. To address this gap, this study
proposes the design of an LLM-Integrated IDE with automated MLOps pipelines
that enables continuous model development and monitoring within a single
environment. The proposed system embeds a Large Language Model (LLM) assistant
capable of code generation, debugging recommendation, and automatic pipeline
configuration. The backend incorporates automated data validation, feature
storage, drift detection, retraining triggers, and CI/CD deployment
orchestration. This framework was implemented in a prototype named SmartMLOps
Studio and evaluated using classification and forecasting tasks on the UCI
Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio
reduces pipeline configuration time by 61%, improves experiment reproducibility
by 45%, and increases drift detection accuracy by 14% compared to traditional
workflows. By bridging intelligent code assistance and automated operational
pipelines, this research establishes a novel paradigm for AI engineering -
transforming the IDE from a static coding tool into a dynamic, lifecycle-aware
intelligent platform for scalable and efficient model development.

</details>
