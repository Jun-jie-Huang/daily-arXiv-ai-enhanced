<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text](https://arxiv.org/abs/2601.19913)
*Shinwoo Park,Yo-Sub Han*

Main category: cs.CL

TL;DR: 研究通过结构化校准提高韩语语言学专业人员区分人类与LLM生成文本的能力，准确率由60%提升至100%。


<details>
  <summary>Details</summary>
Motivation: 当前即使是受过语言训练的读者，也难以区分人类书写与流畅的LLM输出，容易对表面文本的完美性过度信任。

Method: 引入LREAD量表，基于韩国国家写作标准，针对标点、空格、语域等微观语言特征，开展三阶段盲测——直觉判断、分项评分与解释、领域专注掌握训练。

Result: 随着训练阶段推进，多数投票准确率从60%增加到100%，标注者一致性显著提升（Fleiss' kappa从-0.09到0.82）。经过校准的人类检测更依赖微观语言诊断，而非粗略的篇章特征。

Conclusion: 基于量表的专家判断可作为非英语语言文本自动检测器的可解释补充，文中发布了完整量表及检测特征分类。

Abstract: Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.

</details>


### [2] [Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments](https://arxiv.org/abs/2601.19914)
*Maxwell Crouse,Ibrahim Abdelaziz,Kshitij Fadnis,Siva Sankalp Patel,Kinjal Basu,Chulaka Gunasekara,Sadhana Kumaravel,Asim Munawar,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据生成方法DiGiT-TC，用于生成不依赖状态执行环境的多轮工具调用对话数据，实现了在实际无状态环境中对工具调用的有效建模，提升了工具调用任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多轮工具调用数据生成方法大多假设执行环境是有状态的，这种假设在许多实际应用场景中（如企业环境、跨源工具规范等）不成立，导致模型训练效果受限。

Method: 提出了DiGiT-TC方法，通过一种新颖的生成模式，隐式地在用户请求中表达特定的工具调用，实现了在无状态环境下生成具有搜索特征的对话数据。

Result: 在标准工具调用基准测试上验证了该方法，结果显示即使在有状态问题环境下，DiGiT-TC也能显著提升性能。

Conclusion: DiGiT-TC有效弥补了当前合成工具调用数据在无状态环境中的不足，为实际工具调用场景提供了更可靠的数据支持和性能提升。

Abstract: Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.

</details>


### [3] [Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication](https://arxiv.org/abs/2601.19915)
*Paul Tarau*

Main category: cs.CL

TL;DR: 本文提出了基于直觉主义逻辑的Arrow语言模型，将序列预测建模为嵌套的蕴含链和构造性证明。


<details>
  <summary>Details</summary>
Motivation: 通过逻辑推理视角解释和设计神经网络模型，从根本上理解序列预测任务及其结构，探索替代Transformer的模型设计。

Method: 将前缀编码为左嵌套蕴含链，使用modus ponens进行下一个token预测，将序列处理视作Curry--Howard对应下的证明扩展。结合Prolog定理证明器验证模型性质，提出基于乘法RNN等同结构的低秩神经实现。

Result: 揭示了语序非交换与结合多单元预测的关系，生成了一个基于证明理论的多项式RNN等价神经结构，实现了低秩化的实际神经模型。

Conclusion: 利用直觉主义蕴含逻辑从理论和实践层面提出了一种替代Transformer的语言模型架构，具备良好的逻辑解释和模型性能潜力。

Abstract: We introduce the \emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.
  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.

</details>


### [4] [PaperAudit-Bench: Benchmarking Error Detection in Research Papers for Critical Automated Peer Review](https://arxiv.org/abs/2601.19916)
*Songjun Tu,Yiwen Ma,Jiahao Lin,Qichao Zhang,Xiangyuan Lan,Junfeng. Li,Nan Xu,Linjing Li,Dongbin Zhao*

Main category: cs.CL

TL;DR: 本文提出了PaperAudit-Bench，一个包含错误数据集和自动评审框架的新工具，用于提升大型语言模型在论文同行评审中的严格性和辨识度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的同行评审虽流畅，但在发现论文中隐晦且分散的实质性问题时缺乏足够的批判严谨性。

Method: 构建了PaperAudit-Bench，包括PaperAudit-Dataset（涵盖单节和跨节错误的数据集）和PaperAudit-Review（结合结构化错误检测与证据感知的自动评审框架）；通过在长上下文设置下进行评测，验证错误检测和评审生成的有效性。

Result: 模型在不同检测深度下错误识别能力差异大，长上下文环境下错误检测困难。结合显式错误检测的评审方法相较基线更严格、区分度更高。利用该数据集可训练轻量级模型进行有效错误检测，降低计算成本。

Conclusion: 集成结构化错误检测的自动评审系统能提升同行评审的批判性和辨识力，PaperAudit-Bench为长上下文错误检测和评审系统训练提供了有效工具和数据支持。

Abstract: Large language models can generate fluent peer reviews, yet their assessments often lack sufficient critical rigor when substantive issues are subtle and distributed across a paper. In this paper, we introduce PaperAudit-Bench, which consists of two components: (1) PaperAudit-Dataset, an error dataset covering both errors identifiable within individual sections and those requiring cross-section reasoning, designed for controlled evaluation under long-context settings; and (2) PaperAudit-Review, an automated review framework that integrates structured error detection with evidence-aware review generation to support critical assessment. Experiments on PaperAudit-Bench reveal large variability in error detectability across models and detection depths, highlighting the difficulty of identifying such errors under long-context settings. Relative to representative automated reviewing baselines, incorporating explicit error detection into the review workflow produces systematically stricter and more discriminative evaluations, demonstrating its suitability for peer review. Finally, we show that the dataset supports training lightweight LLM detectors via SFT and RL, enabling effective error detection at reduced computational cost.

</details>


### [5] [PILOT: Planning via Internalized Latent Optimization Trajectories for Large Language Models](https://arxiv.org/abs/2601.19917)
*Haoyu Zheng,Yun Zhu,Yuqian Yuan,Bo Yuan,Wenqiao Zhang,Siliang Tang,Jun Xiao*

Main category: cs.CL

TL;DR: 本文提出了PILOT方法，通过内化潜在指导向量，提升小型语言模型的多步骤推理能力，在数学和编码任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型缺乏制定全局策略的能力，导致多步推理时错误传播，而依赖外部指导存在延迟和可用性问题。

Method: PILOT通过轻量级超网络生成查询条件的潜在指导向量，作为模型内部的调控机制，引导推理路径，无需修改基础模型权重。

Result: 在数学和编程基准测试中，PILOT稳定了推理路径，显著超过强基线（如MATH500提升8.9%），且推理延迟几乎无增加。

Conclusion: PILOT成功将大型模型的战略监督内化，提升了小型模型多步骤推理的准确性和稳定性，且具备实用的低延迟优势。

Abstract: Strategic planning is critical for multi-step reasoning, yet compact Large Language Models (LLMs) often lack the capacity to formulate global strategies, leading to error propagation in long-horizon tasks. Our analysis reveals that LLMs possess latent reasoning capabilities that can be unlocked when conditioned on explicit plans from a teacher model; however, runtime reliance on external guidance is often impractical due to latency and availability constraints. To bridge this gap, we propose PILOT (Planning via Internalized Latent Optimization Trajectories), a non-invasive framework designed to internalize the strategic oversight of large models into intrinsic Latent Guidance. Instead of altering backbone weights, PILOT employs a lightweight Hyper-Network to synthesize a query-conditioned Latent Guidance vector. This vector acts as an internal steering mechanism, guiding the model's representations toward optimal reasoning paths. Extensive experiments on mathematical and coding benchmarks demonstrate that PILOT effectively stabilizes reasoning trajectories, consistently outperforming strong baselines (e.g., +8.9% on MATH500) with negligible inference latency.

</details>


### [6] [A Dialectic Pipeline for Improving LLM Robustness](https://arxiv.org/abs/2601.20659)
*Sara Candussio*

Main category: cs.CL

TL;DR: 本文提出了一种利用自我对话的辩证流程，提升大语言模型回答质量并减少幻觉问题，实验显示该方法优于传统回答和仅链式思维提示。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在幻觉问题，且领域微调或专门验证器训练成本高且适用范围受限，亟需低资源且通用性强的方法提升输出质量。

Method: 设计一种辩证流程，让模型通过自我对话反思和纠正初步错误答案，利用oracle-RAG环境增强上下文，并开展摘要与筛选的影响研究。

Result: 实验证明，该辩证流程在不同模型和数据集上均显著优于标准模型答案及仅链式思维提示，提升了回答准确性和质量。

Conclusion: 该辩证自对话流程有效保留模型的泛化能力，同时改进回答质量，是减少幻觉、提升大语言模型输出质量的可行方案。

Abstract: Assessing ways in which Language Models can reduce their hallucinations and improve the outputs' quality is crucial to ensure their large-scale use.
  However, methods such as fine-tuning on domain-specific data or the training of a separate \textit{ad hoc} verifier require demanding computational resources (not feasible for many user applications) and constrain the models to specific fields of knowledge.
  In this thesis, we propose a dialectic pipeline that preserves LLMs' generalization abilities while improving the quality of its answer via self-dialogue, enabling it to reflect upon and correct tentative wrong answers.
  We experimented with different pipeline settings, testing our proposed method on different datasets and on different families of models. All the pipeline stages are enriched with the relevant context (in an oracle-RAG setting) and a study on the impact of its summarization or its filtering is conducted.
  We find that our proposed dialectic pipeline is able to outperform by significative margins the standard model answers and that it consistently achieves higher performances than Chain-of-Thought only prompting.

</details>


### [7] [Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs](https://arxiv.org/abs/2601.19918)
*Yitong Qiao,Licheng Pan,Yu Mi,Lei Liu,Yue Shen,Fei Sun,Zhixuan Chu*

Main category: cs.CL

TL;DR: 提出了一种名为Lowest Span Confidence（LSC）的高效零样本幻觉检测指标，能够在资源受限的情况下准确识别大语言模型中的不实内容。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法依赖昂贵的多次采样或需要访问模型内部状态，难以在基于API的常见场景中应用。

Method: LSC通过滑动窗口机制评估语义连贯片段的联合概率，识别不同长度n-gram中置信度最低的区域，从而捕捉与事实不一致强相关的局部不确定性模式。

Result: LSC在多个最先进大语言模型和多样化基准测试中均表现优异，显著超越了现有零样本检测基线，且在资源受限环境下依然具备强大的检测能力。

Conclusion: LSC为幻觉检测提供了一种资源高效且稳健的解决方案，适用于实际API场景中的大语言模型可靠性提升。

Abstract: Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.

</details>


### [8] [FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition](https://arxiv.org/abs/2601.19919)
*Junseok Lee,Nahoon Kim,Sangyong Lee,Chang-Jae Chun*

Main category: cs.CL

TL;DR: 本文提出了一种自适应自知识蒸馏方法ASKD，通过动态减少对教师模型的依赖，提升学生模型的泛化能力，并成功压缩了Whisper模型，生成FastWhisper，实现更低错误率和更快推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法中，学生模型可能继承教师模型的缺陷，导致泛化能力下降。为了解决这一问题，研究者希望改进蒸馏过程，提高学生模型的自训练能力和泛化性能。

Method: 提出了自适应自知识蒸馏算法（ASKD），该算法动态降低对教师模型的依赖，增强学生模型的自我训练能力，并通过自知识蒸馏提升学生模型的泛化能力。

Result: 将Whisper模型蒸馏成更小的FastWhisper模型，后者在后训练设置下的字错误率比教师模型低1.07%，推理速度提升5倍。

Conclusion: ASKD有效提升了学生模型的泛化能力和推理效率，证明了自适应减少教师依赖的蒸馏策略在模型压缩中的优势。

Abstract: Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.

</details>


### [9] [Demystifying Multi-Agent Debate: The Role of Confidence and Diversity](https://arxiv.org/abs/2601.19921)
*Xiaochen Zhu,Caiqi Zhang,Yizhou Chi,Tom Stafford,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文针对多智能体辩论（MAD）在提升大语言模型性能中存在的不足，提出基于观点多样性和置信度调节的改进方法，显著提升了辩论效果。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论由于缺乏观点多样性和有效的置信度交流，无法稳定提升模型性能，且计算成本较高。

Method: 设计了两项轻量级改进：一是引入多样化初始化，确保辩论初期有更多正确假设；二是采用置信度调节的辩论协议，使模型基于他人表现出的置信度调整自身观点。

Result: 理论证明多样化初始化提升成功概率，置信度调节引导观点向正确假设偏移；实验证明在六个推理问答基准上均优于传统MAD和简单多数投票。

Conclusion: 结合人类决策机制的简单原则性改动能够显著增强多智能体辩论的有效性，提高大语言模型的推理表现。

Abstract: Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.

</details>


### [10] [HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue](https://arxiv.org/abs/2601.19922)
*Laya Iyer,Kriti Aggarwal,Sanmi Koyejo,Gail Heyman,Desmond C. Ong,Subhabrata Mukherjee*

Main category: cs.CL

TL;DR: 本文提出HEART框架，首次在多轮情感支持对话中对人类与大型语言模型（LLM）进行直接比较，评估其在人际交流五个维度的表现。结果显示部分模型在同理心和一致性上达到或超过人类水平，但在人类适应性重构、张力识别和细微语调调整等方面仍有优势。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型发展迅速，但缺乏对其在人际情感支持能力方面与人类相比的清晰理解，亟需一个统一的评价框架进行比较。

Method: 设计HEART框架，通过多轮情感支持对话将人类和模型回应配对，采用盲测人类评分者和大型语言模型评审团结合的方式，根据五个人际交流维度进行评估。

Result: 部分前沿模型在同理心和一致性方面表现接近或超越平均人类水平，但在调整语气、命名紧张情绪和灵活应对对抗性话语方面仍落后于人类。评审标准在人类和模型评审间约有80%的共识，显示评价标准趋同。

Conclusion: HEART框架使人类和模型在情感支持对话能力上处于平等比较地位，强调情感支持作为独立于语言流畅度和一般推理的能力轴，有助于理解模型与人类的异同及能力提升路径。

Abstract: Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.

</details>


### [11] [Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation](https://arxiv.org/abs/2601.19923)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Zelin Cao,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.CL

TL;DR: 本文提出Table-BiEval，一种基于无人工干预、自监督评估框架的方法，用于量化评估大型语言模型在结构化格式转换中的性能，特别是在表格和分层结构上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型成为自主代理，准确将自然语言转换为结构化数据以调用工具以及处理复杂表格信息变得非常重要，但现有评估方法难以有效量化结构准确性，且依赖昂贵的人力。

Method: 提出Table-BiEval框架，利用确定性中间表示计算内容语义准确性和归一化树编辑距离，分离内容与结构，基于无人工干预的自监督方式评估语言模型。

Result: 实证评估了15个顶尖大型语言模型，发现中型模型在结构效率上表现优于部分更大模型，且深度递归嵌套是当前架构的主要瓶颈。

Conclusion: Table-BiEval有效地量化了大型语言模型在复杂结构生成中的表现，揭示模型大小与结构能力间的非线性关系，为改进模型结构处理能力提供了方向。

Abstract: As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.

</details>


### [12] [OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling](https://arxiv.org/abs/2601.19924)
*Yitian Chen,Cheng Cheng,Yinan Sun,Zi Ling,Dongdong Ge*

Main category: cs.CL

TL;DR: 本文提出了OPT-ENGINE框架，评估大型语言模型在不同复杂度优化建模任务中的能力，发现集成外部求解器的推理更稳健，自动约束构建为主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在优化建模中的能力边界尚不明确，尤其是面对复杂真实任务时。

Method: 设计OPT-ENGINE基准，涵盖线性规划和混合整数规划任务，通过该基准系统评测语言模型的推理和求解能力。

Result: 发现集成外部工具的推理方法在任务复杂度提升时表现更稳健，纯文本推理达到瓶颈；自动约束建模是性能主要限制因素。

Conclusion: 提出的发现为开发下一代优化方向的语言模型提供了指导意义，公开代码便于后续研究。

Abstract: Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.

</details>


### [13] [Evaluating Large Language Models for Abstract Evaluation Tasks: An Empirical Study](https://arxiv.org/abs/2601.19925)
*Yinuo Liu,Emre Sezgin,Eric A. Youngstrom*

Main category: cs.CL

TL;DR: 本研究评估了三种大型语言模型（ChatGPT-5、Gemini-3-Pro、Claude-Sonnet-4.5）在批判科学摘要的可靠性和一致性，发现它们在整体质量和客观标准上的评分与人类评审具有中等至良好的一致性，但在主观指标上的表现较弱，表明AI应作为辅助工具，而非完全替代人类评审。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在科学内容评审中应用的可行性及其与人类评审者之间的一致性和可靠性。

Method: 使用统一评分标准对160篇会议摘要进行评分，比较三种大型语言模型之间以及它们与14位人类评审者的评分一致性，计算组内相关系数（ICC）并绘制Bland-Altman图分析偏差和一致性。

Result: 三种模型之间表现良好至优秀一致性（ICC 0.59-0.87）；ChatGPT和Claude与人类评审在整体及具体内容标准上表现中等一致性（ICC约0.45-0.60），但在主观评价指标上表现较弱；Gemini表现较差。所有模型对综合评分的平均差异均可接受。

Conclusion: 大型语言模型能批量处理摘要评分任务，在客观评价维度与人类评审有较好一致性，适合辅助科学评审工作，但因主观评价能力有限，仍需依赖人类专业判断。

Abstract: Introduction: Large language models (LLMs) can process requests and generate texts, but their feasibility for assessing complex academic content needs further investigation. To explore LLM's potential in assisting scientific review, this study examined ChatGPT-5, Gemini-3-Pro, and Claude-Sonnet-4.5's consistency and reliability in evaluating abstracts compared to one another and to human reviewers. Methods: 160 abstracts from a local conference were graded by human reviewers and three LLMs using one rubric. Composite score distributions across three LLMs and fourteen reviewers were examined. Inter-rater reliability was calculated using intraclass correlation coefficients (ICCs) for within-AI reliability and AI-human concordance. Bland-Altman plots were examined for visual agreement patterns and systematic bias. Results: LLMs achieved good-to-excellent agreement with each other (ICCs: 0.59-0.87). ChatGPT and Claude reached moderate agreement with human reviewers on overall quality and content-specific criteria, with ICCs ~.45-.60 for composite, impression, clarity, objective, and results. They exhibited fair agreement on subjective dimensions, with ICC ranging from 0.23-0.38 for impact, engagement, and applicability. Gemini showed fair agreement on half criteria and no reliability on impact and applicability. Three LLMs showed acceptable or negligible mean difference (ChatGPT=0.24, Gemini=0.42, Claude=-0.02) from the human mean composite scores. Discussion: LLMs could process abstracts in batches with moderate agreement with human experts on overall quality and objective criteria. With appropriate process architecture, they can apply a rubric consistently across volumes of abstracts exceeding feasibility for a human rater. The weaker performance on subjective dimensions indicates that AI should serve a complementary role in evaluation, while human expertise remains essential.

</details>


### [14] [The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models](https://arxiv.org/abs/2601.19926)
*Nora Graichen,Iria de-Dios-Flores,Gemma Boleda*

Main category: cs.CL

TL;DR: 本文系统回顾了337篇关于基于Transformer语言模型句法能力的研究，分析了1015个模型结果，发现研究过于集中于英语、BERT模型和易处理的现象，模型对形式导向的句法现象表现较好，但对语义接口相关现象表现较弱。


<details>
  <summary>Details</summary>
Motivation: 为了全面评估Transformer语言模型在句法能力上的表现，并识别当前研究的偏向和不足。

Method: 系统整理并分析337篇论文中1015个模型的实验结果，涵盖多种句法现象和解释方法。

Result: 发现研究中存在语言、模型及句法现象的偏重，模型对部分句法现象表现良好，但在句法-语义接口现象上表现不稳定且较弱。

Conclusion: 建议未来研究应报告完整数据，理论与方法更好对齐，采用更多机械解释方法，拓展语言和句法现象的研究范围。

Abstract: We present a systematic review of 337 articles evaluating the syntactic abilities of Transformer-based language models, reporting on 1,015 model results from a range of syntactic phenomena and interpretability methods. Our analysis shows that the state of the art presents a healthy variety of methods and data, but an over-focus on a single language (English), a single model (BERT), and phenomena that are easy to get at (like part of speech and agreement). Results also suggest that TLMs capture these form-oriented phenomena well, but show more variable and weaker performance on phenomena at the syntax-semantics interface, like binding or filler-gap dependencies. We provide recommendations for future work, in particular reporting complete data, better aligning theoretical constructs and methods across studies, increasing the use of mechanistic methods, and broadening the empirical scope regarding languages and linguistic phenomena.

</details>


### [15] [Attribution Techniques for Mitigating Hallucinated Information in RAG Systems: A Survey](https://arxiv.org/abs/2601.19927)
*Yuqing Zhao,Ziyao Liu,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本文综述了基于归因技术的RAG系统中幻觉问题的解决方法，提出了幻觉类型分类、统一技术流程及比较分析。


<details>
  <summary>Details</summary>
Motivation: 现有LLM问答系统存在幻觉问题，RAG框架虽增强了答案的可靠性，但引入了新的幻觉挑战，缺乏统一的归因技术流程和分类体系。

Method: 本文提出了RAG系统中幻觉类型的分类，构建统一的归因技术流程，回顾了针对不同幻觉类型的归因方法，并进行了优缺点比较与实践指导。

Result: 总结了归因技术在缓解RAG系统幻觉中的应用效果，明确了不同方法的适用场景和限制，填补了该领域系统比较和分类的空白。

Conclusion: 本文为未来研究和实际应用归因技术于RAG系统提供了理论基础和实用指南，促进提升回答的真实性和可靠性。

Abstract: Large Language Models (LLMs)-based question answering (QA) systems play a critical role in modern AI, demonstrating strong performance across various tasks. However, LLM-generated responses often suffer from hallucinations, unfaithful statements lacking reliable references. Retrieval-Augmented Generation (RAG) frameworks enhance LLM responses by incorporating external references but also introduce new forms of hallucination due to complex interactions between the retriever and generator. To address these challenges, researchers have explored attribution-based techniques that ensure responses are verifiably supported by retrieved content. Despite progress, a unified pipeline for these techniques, along with a clear taxonomy and systematic comparison of their strengths and weaknesses, remains lacking. A well-defined taxonomy is essential for identifying specific failure modes within RAG systems, while comparative analysis helps practitioners choose appropriate solutions based on hallucination types and application context. This survey investigates how attribution-based techniques are used within RAG systems to mitigate hallucinations and addresses the gap by: (i) outlining a taxonomy of hallucination types in RAG systems, (ii) presenting a unified pipeline for attribution techniques, (iii) reviewing techniques based on the hallucinations they target, and (iv) discussing strengths and weaknesses with practical guidelines. This work offers insights for future research and practical use of attribution techniques in RAG systems.

</details>


### [16] [Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures](https://arxiv.org/abs/2601.19928)
*Yi Hu,Jiaqi Gu,Ruxin Wang,Zijun Yao,Hao Peng,Xiaobao Wu,Jianhui Chen,Muhan Zhang,Liangming Pan*

Main category: cs.CL

TL;DR: 本文综述了大型推理模型（LRMs）的机制理解，聚焦训练动态、推理机制和非预期行为三大维度，旨在提升模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习推动大型推理模型性能提升，理解其内部机制成为关键研究方向，弥补“黑盒”表现与机制透明之间的差距。

Method: 通过系统整合近期研究成果，本文围绕训练动态、推理机制和非预期行为三个核心维度进行梳理和分析，形成机制理解的全面视角。

Result: 本文总结了LRMs在三个机制维度上的最新发现，揭示其训练过程、推理方式及潜在的非预期表现，并指出目前存在的研究空白。

Conclusion: 文章强调未来机制研究需加强应用可解释性、方法改进和统一理论框架建设，以推动大型推理模型的透明化和可靠性提升。

Abstract: Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.

</details>


### [17] [Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding](https://arxiv.org/abs/2601.19929)
*David Linus Ostby*

Main category: cs.CL

TL;DR: 提出了一种层次树结构压缩方案Stingy Context，实现了18:1的上下文压缩比，在代码任务中高效且保真。


<details>
  <summary>Details</summary>
Motivation: 减少LLM在自动编码任务中的上下文长度需求，同时保持任务性能。

Method: 设计了基于树结构的压缩方案Stingy Context，利用TREEFRAG进行代码片段分解和压缩。

Result: 在239k标记的代码库压缩至11k标记，实测12个模型在40个真实问题上表现优异，成功率94%至97%，成本低于平铺方法。

Conclusion: Stingy Context有效缓解长文上下文中的信息丢失问题，提升自动编码任务性能和效率。

Abstract: We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.

</details>


### [18] [SDUs DAISY: A Benchmark for Danish Culture](https://arxiv.org/abs/2601.19930)
*Jacob Nielsen,Stine L. Beltoft,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: 该论文介绍了一个基于丹麦文化典范的新基准数据集Daisy，通过维基百科和语言模型生成问答，涵盖了丹麦文化遗产的重要内容。


<details>
  <summary>Details</summary>
Motivation: 为了建立一个覆盖丹麦文化遗产多样性且具有代表性的基准，帮助理解和研究丹麦文化相关知识。

Method: 通过丹麦文化典范中的文物对应维基百科页面，利用语言模型随机生成问题，并由人工审核和修正，形成741个封闭式问答对。

Result: 数据集包含从公元前1300年的考古发现，到1700世纪诗歌、音乐作品，再到当代流行音乐和设计建筑的丰富内容。

Conclusion: 该数据集为丹麦文化研究和语言模型在文化理解方面的应用提供了高质量、多样化的资源。

Abstract: We introduce a new benchmark for Danish culture via cultural heritage, Daisy, based on the curated topics from the Danish Culture Canon 2006. For each artifact in the culture canon, we query the corresponding Wikipedia page and have a language model generate random questions. This yields a sampling strategy within each work, with a mix of central of peripheral questions for each work, not only knowledge of mainstream information, but also in-depth cornerstones defining the heritage of Danish Culture, defined by the Canon committee. Each question-answer pair is humanly approved or corrected in the final dataset consisting of 741 close-ended question answer pairs covering topics, from 1300 BC. archaeological findings, 1700 century poems and musicals pieces to contemporary pop music and Danish design and architecture.

</details>


### [19] [CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity](https://arxiv.org/abs/2601.19931)
*Sebastien Kawada,Dylan Holyoak*

Main category: cs.CL

TL;DR: 提出了一种结合神经网络和符号方法的叙事故事相似度分析系统，通过多轮投票和符号性多尺度叙事分析集成实现较高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理叙事故事相似度时对模糊和不确定的案例表现有限，需要引入符号方法提升决策的准确性和可信度。

Method: 设计了多轮神经网络投票机制，采用超多数门槛决定结果；对不确定投票进行多轮复核；在投票完全平局时，利用包含词汇重叠、语义嵌入、故事语法结构、事件链对齐及叙事张力曲线五个符号相似度信号的多尺度叙事分析集成做最终判定。

Result: 该混合神经符号系统在开发集上达到了81%的准确率，表现优异。

Conclusion: 选择性将决策权交付给符号方法能够有效弥补神经网络在复杂叙事相似性判定中的不足，提高整体系统性能。

Abstract: We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.

</details>


### [20] ["Newspaper Eat" Means "Not Tasty": A Taxonomy and Benchmark for Coded Languages in Real-World Chinese Online Reviews](https://arxiv.org/abs/2601.19932)
*Ruyuan Wan,Changye Li,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: 本文介绍了一个包含中文谷歌地图评论的编码语言数据集CodedLang，提出了七类编码策略的分类方法，并评估了模型在编码语言检测、分类及评分预测上的表现，揭示了当前模型在处理编码语言上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型对编码语言处理能力不足，主要受制于缺乏真实数据集和明确的编码策略分类体系。

Method: 构建了包含7744条中文评论的CodedLang数据集，其中900条带有编码语言的精细标注。提出七类编码策略的分类体系，包括语音、拼写及跨语言替换。对语言模型进行编码语言检测、分类和评分预测任务的基准测试，并对编码与解码形式进行了语音学分析。

Result: 实验结果表明，现有强模型仍难准确识别和理解编码语言，尤其是依赖发音策略的编码表达。

Conclusion: 编码语言是现实自然语言处理系统中被低估的重要挑战，需更多研究关注以提升模型的实用能力。

Abstract: Coded language is an important part of human communication. It refers to cases where users intentionally encode meaning so that the surface text differs from the intended meaning and must be decoded to be understood. Current language models handle coded language poorly. Progress has been limited by the lack of real-world datasets and clear taxonomies. This paper introduces CodedLang, a dataset of 7,744 Chinese Google Maps reviews, including 900 reviews with span-level annotations of coded language. We developed a seven-class taxonomy that captures common encoding strategies, including phonetic, orthographic, and cross-lingual substitutions. We benchmarked language models on coded language detection, classification, and review rating prediction. Results show that even strong models can fail to identify or understand coded language. Because many coded expressions rely on pronunciation-based strategies, we further conducted a phonetic analysis of coded and decoded forms. Together, our results highlight coded language as an important and underexplored challenge for real-world NLP systems.

</details>


### [21] [Text-to-State Mapping for Non-Resolution Reasoning: The Contradiction-Preservation Principle](https://arxiv.org/abs/2601.19933)
*Kei Saito*

Main category: cs.CL

TL;DR: 本论文提出了一种将自然语言转换为非解析推理（NRR）框架中的叠加状态的映射函数，实现了对语言歧义的保持和量化。


<details>
  <summary>Details</summary>
Motivation: 现有的NRR框架虽能维持语义歧义，但缺乏将自然语言映射到数学结构的具体方法。

Method: 引入文本到状态的映射函数φ，形式化了矛盾保持原则，利用大型语言模型生成解释，并通过协议提取歧义信息。

Result: 在68个包含词汇、结构和语用歧义的测试句子上，映射函数实现了平均香农熵1.087比特，显著高于单解释方法的0比特。

Conclusion: 该方法弥补了自然语言与NRR状态空间之间的算法桥梁，支持语言模型推理中延迟歧义解析，提升理解的灵活性。

Abstract: Non-Resolution Reasoning (NRR) provides a formal framework for maintaining semantic ambiguity rather than forcing premature interpretation collapse. While the foundational architecture establishes state spaces and operators for ambiguity-preserving computation, the critical question of how natural language maps to these mathematical structures remains open. This paper introduces the text-to-state mapping function φ that transforms linguistic input into superposition states within the NRR framework. We formalize the Contradiction-Preservation Principle, which requires that genuinely ambiguous expressions maintain non-zero entropy in their state representations, and develop extraction protocols using existing Large Language Models as interpretation generators. Empirical validation across 68 test sentences spanning lexical, structural, and pragmatic ambiguity demonstrates that our mapping achieves mean Shannon entropy H(S) = 1.087 bits for ambiguous inputs while baseline single-interpretation approaches yield H(S) = 0.000. The framework provides the missing algorithmic bridge between raw text and the formal state spaces on which NRR operators act, enabling architectural collapse deferment in language model inference.

</details>


### [22] [Quantifying non deterministic drift in large language models](https://arxiv.org/abs/2601.19934)
*Claire Nicholson*

Main category: cs.CL

TL;DR: 该论文通过重复实验系统地量化了大型语言模型在相同提示下输出的行为漂移，揭示模型输出存在一定的非确定性。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，即使使用相同的提示和参数，大型语言模型的输出也会存在波动，了解这种行为漂移对模型的可靠性和应用至关重要。

Method: 实验中对gpt-4o-mini和llama3.1-8b两种模型进行多次运行测试，测量在不同提示类别、温度设定(0.0和0.7)、输入扰动等条件下的输出差异，使用唯一输出比例、词汇相似度和字数统计等指标进行量化分析。

Result: 结果显示即使在温度为0.0情况下，模型的输出仍存在非确定性，且这种漂移表现因模型大小、部署方式和提示类别不同而异。

Conclusion: 本研究为行为漂移提供了一个系统的实证基线，为后续稳定性提升和漂移控制方法的评估奠定了参考基础，同时指出了现有词汇指标的局限性并探讨了语义层面的新方法。

Abstract: Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, and reuse modes at temperatures of 0.0 and 0.7. Drift is measured using unique output fractions, lexical similarity, and word count statistics, enabling direct comparison across models, prompting modes, and deployment types. The results show that nondeterminism persists even at temperature 0.0, with distinct variability patterns by model size, deployment, and prompt type. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches. By establishing a systematic empirical baseline in the absence of stabilisation techniques, this study provides a reference point for evaluating future drift mitigation and control methods.

</details>


### [23] [Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents](https://arxiv.org/abs/2601.19935)
*Yiting Shen,Kun Li,Wei Zhou,Songlin Hu*

Main category: cs.CL

TL;DR: 本文提出了Mem2ActBench基准，用于评估基于大型语言模型的智能体如何主动利用长期记忆执行复杂工具操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试被动检索孤立事实能力，缺乏对智能体主动应用记忆执行任务能力的评估。

Method: 设计Mem2ActBench基准，通过自动整合多种数据源生成2000余会话，采用逆向生成方法构造400个工具使用任务，并以人工验证任务对记忆依赖性。

Result: 实验证明现有七种记忆框架在参数绑定时主动利用记忆能力不足，难以满足实际任务需求。

Conclusion: 提出的基准揭示当前系统内存利用不足，强调需要更有效的评估和改进方法以提升智能体在任务执行中的记忆应用能力。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.

</details>


### [24] [Benchmarking von ASR-Modellen im deutschen medizinischen Kontext: Eine Leistungsanalyse anhand von Anamnesegesprächen](https://arxiv.org/abs/2601.19945)
*Thomas Schuster,Julius Trögele,Nico Döring,Robin Krüger,Matthieu Hoffmann,Holger Friedrich*

Main category: cs.CL

TL;DR: 本文评估了29个自动语音识别模型在德语医疗对话中的表现，发现不同模型之间的错误率差异显著。


<details>
  <summary>Details</summary>
Motivation: 德语医疗领域缺乏针对方言和专业术语的语音识别基准测试，亟需构建相关数据集并进行评估。

Method: 构建了模拟医患对话数据集，采用29个ASR模型（包含开源模型和商业API）进行测试，使用WER、CER、BLEU三种指标进行量化评估，并结合语义分析。

Result: 部分领先模型在词错误率低于3%，表现优异，但其他模型在医疗术语和方言处理方面表现较差。

Conclusion: 不同ASR模型在德语医疗语境下的表现存在显著差异，特别是针对专业术语和方言的识别能力仍需提升。

Abstract: Automatic Speech Recognition (ASR) offers significant potential to reduce the workload of medical personnel, for example, through the automation of documentation tasks. While numerous benchmarks exist for the English language, specific evaluations for the German-speaking medical context are still lacking, particularly regarding the inclusion of dialects. In this article, we present a curated dataset of simulated doctor-patient conversations and evaluate a total of 29 different ASR models. The test field encompasses both open-weights models from the Whisper, Voxtral, and Wav2Vec2 families as well as commercial state-of-the-art APIs (AssemblyAI, Deepgram). For evaluation, we utilize three different metrics (WER, CER, BLEU) and provide an outlook on qualitative semantic analysis. The results demonstrate significant performance differences between the models: while the best systems already achieve very good Word Error Rates (WER) of partly below 3%, the error rates of other models, especially concerning medical terminology or dialect-influenced variations, are considerably higher.

</details>


### [25] [On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text](https://arxiv.org/abs/2601.20006)
*Michał Gromadzki,Anna Wróblewska,Agnieszka Kaliska*

Main category: cs.CL

TL;DR: 该论文构建了大规模人类和AI生成文本语料库，提出了基于大规模语料和新颖训练策略的AI文本检测方法，检测准确率达99.6%。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，AI生成文本与人类写作极为相似，导致真实性验证面临挑战，尤其在教育、出版和数字安全领域，因此需要高效的AI文本检测技术。

Method: 建立了10亿词的人类文本语料库和19亿词的AI生成文本语料库；基于这些数据训练多种检测模型，提出两种新训练范式：针对具体大语言模型（Per LLM）和模型家族（Per LLM family）的微调方法。

Result: 在覆盖21个大语言模型的1亿词基准测试上，最佳微调检测模型达到了99.6%的词级准确率，显著优于现有开源基线。

Conclusion: 本文证明通过大规模多样化语料及针对不同模型的微调训练，能够显著提升AI生成文本检测的准确性，为相关应用的真实性验证提供了强有力的技术支持。

Abstract: The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\%$ token-level accuracy, substantially outperforming existing open-source baselines.

</details>


### [26] [LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?](https://arxiv.org/abs/2601.20009)
*J. Ben Tamo,Daniel Carlander-Reuterfelt,Jonathan Rubin,Dezhi Hong,Mingxian Wang,Oleg Poliannikov*

Main category: cs.CL

TL;DR: 大语言模型在多语言任务中语言控制存在瓶颈，通过层级分析发现语言控制主要在后层实现，提出仅微调最后几层以提升语言一致性，效果显著且计算资源低。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在非英语任务中语言控制能力不足的问题，具体表现为语言选择错误或任务回答错误。

Method: 设计四场景评估协议，通过logit lens分析方法追踪语言概率和隐藏状态的跨语言语义相似性，发现语言控制在模型后层实现，提出仅微调后层以提升语言一致性。

Result: 在Qwen-3-32B和Bloom-7.1B模型上，实现了超过98%的语言一致性，微调参数仅占3-5%，且任务准确率不受影响。

Conclusion: 首次利用语言控制的层级定位实现高效的多语言适应，显著提升语言一致性同时大幅降低计算资源消耗。

Abstract: Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.

</details>


### [27] [Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method](https://arxiv.org/abs/2601.20026)
*Pragatheeswaran Vipulanandan,Kamal Premaratne,Dilip Sarkar*

Main category: cs.CL

TL;DR: 本文提出基于量子张量网络的不确定性量化框架用于检测大语言模型的幻觉，提升输出的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然生成能力强，但易产生不可靠的幻觉输出，亟需有效的检测机制。

Method: 采用量子物理启发的量子张量网络对生成序列的概率中的先验不确定性进行量化，实现语义等价聚类；引入熵最大化策略，优先选择高置信且语义连贯的输出，同时标记潜在不可靠区域提供人工审查指引。

Result: 在多种模型和多个任务数据集上进行116次实验，方法在不同生成长度和量化水平下均表现稳健，显著优于当前最先进基准，AUROC和AURAC指标均有一致提升。

Conclusion: 该框架为大语言模型幻觉检测提供了可解释、可信赖的技术手段，且适用于资源有限的部署环境，具备实用价值与推广潜力。

Abstract: Large language models (LLMs) exhibit strong generative capabilities but remain vulnerable to confabulations, fluent yet unreliable outputs that vary arbitrarily even under identical prompts. Leveraging a quantum tensor network based pipeline, we propose a quantum physics inspired uncertainty quantification framework that accounts for aleatoric uncertainty in token sequence probability for semantic equivalence based clustering of LLM generations. This offers a principled and interpretable scheme for hallucination detection. We further introduce an entropy maximization strategy that prioritizes high certainty, semantically coherent outputs and highlights entropy regions where LLM decisions are likely to be unreliable, offering practical guidelines for when human oversight is warranted. We evaluate the robustness of our scheme under different generation lengths and quantization levels, dimensions overlooked in prior studies, demonstrating that our approach remains reliable even in resource constrained deployments. A total of 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD across multiple architectures including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLaMA-2-13b, and LLaMA-2-7b show consistent improvements in AUROC and AURAC over state of the art baselines.

</details>


### [28] [TAIGR: Towards Modeling Influencer Content on Social Media via Structured, Pragmatic Inference](https://arxiv.org/abs/2601.20032)
*Nishanth Sridhar Nakshatri,Eylon Caplan,Rajkumar Pujari,Dan Goldwasser*

Main category: cs.CL

TL;DR: 本文提出了TAIGR框架，通过三阶段方法结构化分析健康领域影响者的说服性话语，实现对其推荐内容的有效验证。


<details>
  <summary>Details</summary>
Motivation: 健康影响者常以叙事和修辞策略传达内容，使得基于具体事实的验证方法难以准确评估其话语的实际意义。

Method: TAIGR包含三个步骤：第一，识别影响者的核心推荐内容；第二，构建论证图以展示支持该推荐的理由；第三，利用因子图的概率推理来验证推荐的有效性。

Result: 在健康视频转录内容验证任务中，实验表明仅将转录内容视为事实集合无法准确验证，必须建模话语的语用和论证结构。

Conclusion: TAIGR有效捕捉了影响者话语中的论证逻辑和语用信息，提升了内容验证的准确性，展现了结构化分析在复杂说服性话语中的应用潜力。

Abstract: Health influencers play a growing role in shaping public beliefs, yet their content is often conveyed through conversational narratives and rhetorical strategies rather than explicit factual claims. As a result, claim-centric verification methods struggle to capture the pragmatic meaning of influencer discourse. In this paper, we propose TAIGR (Takeaway Argumentation Inference with Grounded References), a structured framework designed to analyze influencer discourse, which operates in three stages: (1) identifying the core influencer recommendation--takeaway; (2) constructing an argumentation graph that captures influencer justification for the takeaway; (3) performing factor graph-based probabilistic inference to validate the takeaway. We evaluate TAIGR on a content validation task over influencer video transcripts on health, showing that accurate validation requires modeling the discourse's pragmatic and argumentative structure rather than treating transcripts as flat collections of claims.

</details>


### [29] [Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents](https://arxiv.org/abs/2601.20412)
*Qihao Wang,Yue Hu,Mingzhe Lu,Jiayue Wu,Yanbing Liu,Yuanmin Tang*

Main category: cs.CL

TL;DR: 本文提出了一个基于认知负荷理论的框架，通过分析内在负荷和无关负荷，利用参数化认知负荷的ToolLoad-Bench基准测试，精准评估大语言模型使用外部工具时的能力边界。


<details>
  <summary>Details</summary>
Motivation: 现有评价标准主要关注最终准确率，无法揭示模型认知瓶颈和能力边界，缺乏诊断模型性能的工具。

Method: 基于认知负荷理论，将任务复杂度分解为内在负荷和无关负荷，提出Tool Interaction Graph形式化内在负荷，设计ToolLoad-Bench作为可调认知负荷的基准测试，从而进行控制实验进行能力边界映射。

Result: 评测结果显示，随着认知负荷增加，模型表现出现明显下降，且框架预测与实测高度吻合，能够准确定位模型能力边界。

Conclusion: 本文框架为诊断大语言模型能力限制提供了原则性方法和实用工具，促进构建更高效的智能系统。

Abstract: The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.

</details>


### [30] [VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.20055)
*Vikash Singh,Darion Cassel,Nathaniel Weir,Nick Feng,Sam Bayless*

Main category: cs.CL

TL;DR: 本文提出了一种神经符号融合框架VERGE，将大型语言模型(LLMs)与SMT求解器结合，通过迭代细化实现逻辑验证，从而提升高风险领域中答案的逻辑正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在语法流畅性上表现优秀，但其在高风险领域确保逻辑正确性仍是一个根本难题，需引入形式验证方法提升可信度。

Method: 提出融合LLMs和SMT求解器的神经符号框架，自动将LLM输出分解为原子命题并形式化为一阶逻辑，结合多模型共识、语义路由和最小校正子方法定位逻辑错误，迭代优化答案直到满足收敛条件。

Result: 该方法在推理基准测试中，相比单次推理方法平均提升18.7%性能，能够在更多场景提供形式保证和一致性验证。

Conclusion: VERGE框架通过神经符号混合及迭代验证策略，显著提升了大型语言模型在逻辑推理任务中的可靠性和表现，推动可信AI的发展。

Abstract: Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.

</details>


### [31] [SERA: Soft-Verified Efficient Repository Agents](https://arxiv.org/abs/2601.20789)
*Ethan Shen,Danny Tormoen,Saurabh Shah,Ali Farhadi,Tim Dettmers*

Main category: cs.CL

TL;DR: 本文提出了一种名为SERA的高效训练编码智能体的方法，通过监督微调实现了对私有代码库的快速专化，性能达到开源和前沿模型水平，成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 开源权重编码智能体理论上能针对私有代码库进行专化，但训练成本和复杂度高使该优势难以实现。

Method: 引入Soft Verified Generation（SVG）方法，通过单一代码库生成大量训练轨迹，结合监督微调训练SERA模型，实现高效且经济的专化训练。

Result: SERA在全开源模型中性能领先，并匹配Devstral-Small-2等前沿模型，训练成本分别比强化学习和合成数据方法低26倍和57倍。

Conclusion: SERA方法显著加速了开源编码智能体的研究，展示了开源模型专化私有代码库的优势，同时开放了全部代码、数据和集成，推动社区研究发展。

Abstract: Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.

</details>


### [32] [Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects](https://arxiv.org/abs/2601.20102)
*Amirhossein Haji Mohammad Rezaei,Zahra Shakeri*

Main category: cs.CL

TL;DR: 本文提出了一种针对医疗语言模型的文化敏感性反事实基准，通过对MedQA测试题插入文化相关信息，评估模型诊断准确性的变化。


<details>
  <summary>Details</summary>
Motivation: 现有医疗语言模型在面对非决定性文化信息时，可能改变临床正确的诊断结果，影响医疗的可持续性与公平性。

Method: 设计反事实基准，将150个MedQA测试题扩展为1650个变体，插入三类文化相关信息（身份标识符、上下文线索及其组合）针对不同文化群体，验证金标准答案是否保持不变，并使用多种模型进行准确率评估。

Result: 文化线索显著影响模型准确率，特别是在身份标识符与上下文共同出现时准确率下降3-7个百分点，中性编辑影响较小且不具系统性。人工验证的评分标准显示，大部分带文化线索的解释最终导致错误答案，将文化关联推理与诊断失败联系起来。

Conclusion: 本文发布了相关提示和数据增强方法，有助于评估和减少因文化因素引发的医疗诊断错误，促进医疗语言模型的公平性与可持续性。

Abstract: Engineering sustainable and equitable healthcare requires medical language models that do not change clinically correct diagnoses when presented with non-decisive cultural information. We introduce a counterfactual benchmark that expands 150 MedQA test items into 1650 variants by inserting culture-related (i) identifier tokens, (ii) contextual cues, or (iii) their combination for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a length-matched neutral control, where a clinician verified that the gold answer remains invariant in all variants. We evaluate GPT-5.2, Llama-3.1-8B, DeepSeek-R1, and MedGemma (4B/27B) under option-only and brief-explanation prompting. Across models, cultural cues significantly affect accuracy (Cochran's Q, $p<10^-14$), with the largest degradation when identifier and context co-occur (up to 3-7 percentage points under option-only prompting), while neutral edits produce smaller, non-systematic changes. A human-validated rubric ($κ=0.76$) applied via an LLM-as-judge shows that more than half of culturally grounded explanations end in an incorrect answer, linking culture-referential reasoning to diagnostic failure. We release prompts and augmentations to support evaluation and mitigation of culturally induced diagnostic errors.

</details>


### [33] [FFE-Hallu:Hallucinations in Fixed Figurative Expressions:Benchmark of Idioms and Proverbs in the Persian Language](https://arxiv.org/abs/2601.20105)
*Faezeh Hosseini,Mohammadali Yousefzadeh,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 本文提出了FFEHallu基准测试，评估大语言模型在波斯语固定比喻表达（FFEs）上的生成和识别能力，发现当前模型在比喻生成和翻译中易出现虚假比喻现象。


<details>
  <summary>Details</summary>
Motivation: 固定比喻表达文化依赖强且难以组合，导致大语言模型易产生听似合理但不存在的虚假比喻（比喻幻觉），需要针对这一现象的评估工具。

Method: 设计了FFEHallu基准，包括600个实例，涵盖FFEs生成、虚假FFEs检测和英波斯FFEs翻译三个任务，评估六款多语言大语言模型的表现。

Result: 发现多数模型难以有效区分真实与虚假FFEs，跨语言翻译时频繁产生比喻幻觉，GPT4.1表现相对较好，但整体存在显著不足。

Conclusion: 当前大语言模型在处理文化依赖强的比喻语言时存在系统性弱点，亟需设计针对性基准以评估和减少比喻幻觉问题。

Abstract: Figurative language, particularly fixed figurative expressions (FFEs) such as idioms and proverbs, poses persistent challenges for large language models (LLMs). Unlike literal phrases, FFEs are culturally grounded, largely non-compositional, and conventionally fixed, making them especially vulnerable to figurative hallucination. We define figurative hallucination as the generation or endorsement of expressions that sound idiomatic and plausible but do not exist as authentic figurative expressions in the target language. We introduce FFEHallu, the first comprehensive benchmark for evaluating figurative hallucination in LLMs, with a focus on Persian, a linguistically rich yet underrepresented language. FFEHallu consists of 600 carefully curated instances spanning three complementary tasks: (i) FFE generation from meaning, (ii) detection of fabricated FFEs across four controlled construction categories, and (iii) FFE to FFE translation from English to Persian. Evaluating six state of the art multilingual LLMs, we find systematic weaknesses in figurative competence and cultural grounding. While models such as GPT4.1 demonstrate relatively strong performance in rejecting fabricated FFEs and retrieving authentic ones, most models struggle to reliably distinguish real expressions from high quality fabrications and frequently hallucinate during cross lingual translation. These findings reveal substantial gaps in current LLMs handling of figurative language and underscore the need for targeted benchmarks to assess and mitigate figurative hallucination.

</details>


### [34] [Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://arxiv.org/abs/2601.20126)
*Abha Jha,Akanksha Mahajan,Ashwath Vaithinathan Aravindan,Praveen Saravanan,Sai Sailaja Policharla,Sonal Chaturbhuj Gehlot*

Main category: cs.CL

TL;DR: 本文研究了使用可验证奖励的强化学习（RLVR）来训练大语言模型，使其在回答问题时能够选择“我不知道”以减少错误回答。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在事实领域常产生虚假或无法验证的内容，影响其可靠性，因此需提升模型的知识谦逊性。

Method: 采用三元奖励结构（奖励为-1、r_abs、1）对Granite和Qwen模型进行微调，结合监督学习策略，探索不同的弃答奖励，促进模型在不确定时选择弃答。

Result: 中等程度的弃答奖励（约-0.25到0.3）有效减少错误响应，同时准确率下降不明显。大模型对弃答奖励更具鲁棒性。开放式问答中探索不足带来局限，监督弃答训练可部分缓解。

Conclusion: 可验证奖励设计是缓解语言模型幻觉问题的可行且灵活的方法，提供了实用的弃答训练框架，有助提升模型在事实领域的可靠性。

Abstract: Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention ("I don't know") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.

</details>


### [35] [BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification](https://arxiv.org/abs/2601.20129)
*Akif Islam,Sujan Kumar Roy,Md. Ekramul Hamid*

Main category: cs.CL

TL;DR: 本文介绍了BengaliSent140，这是一个融合了七个现有孟加拉语文本数据集的大规模二分类情感数据集，包含近14万条数据，类别均衡，覆盖多个领域，适合深度学习模型训练。


<details>
  <summary>Details</summary>
Motivation: 当前孟加拉语情感分析研究受限于大型多样化标注数据集的缺乏，现有数据集多数规模小或局限于单一领域，难以支撑现代深度学习模型的训练。

Method: 作者系统整合七个现有孟加拉语文本数据集，统一标签为二分类（非仇恨和仇恨），形成了包含139,792条文本样本的统一数据集，实现跨领域数据的融合和语义标签的标准化。

Result: 构建的BengaliSent140数据集包含68,548条仇恨类数据和71,244条非仇恨类数据，类别较为均衡，覆盖语言和上下文更广泛。基线实验验证了数据集的实用性。

Conclusion: BengaliSent140数据集为孟加拉语情感分析提供了更大规模、更均衡且多领域覆盖的训练资源，有助于推进基于深度学习的情感分析研究，且数据集已公开，可供社区使用和验证。

Abstract: Sentiment analysis for the Bengali language has attracted increasing research interest in recent years. However, progress remains constrained by the scarcity of large-scale and diverse annotated datasets. Although several Bengali sentiment and hate speech datasets are publicly available, most are limited in size or confined to a single domain, such as social media comments. Consequently, these resources are often insufficient for training modern deep learning based models, which require large volumes of heterogeneous data to learn robust and generalizable representations. In this work, we introduce BengaliSent140, a large-scale Bengali binary sentiment dataset constructed by consolidating seven existing Bengali text datasets into a unified corpus. To ensure consistency across sources, heterogeneous annotation schemes are systematically harmonized into a binary sentiment formulation with two classes: Not Hate (0) and Hate (1). The resulting dataset comprises 139,792 unique text samples, including 68,548 hate and 71,244 not-hate instances, yielding a relatively balanced class distribution. By integrating data from multiple sources and domains, BengaliSent140 offers broader linguistic and contextual coverage than existing Bengali sentiment datasets and provides a strong foundation for training and benchmarking deep learning models. Baseline experimental results are also reported to demonstrate the practical usability of the dataset. The dataset is publicly available at https://www.kaggle.com/datasets/akifislam/bengalisent140/

</details>


### [36] [Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR](https://arxiv.org/abs/2601.20142)
*Zilai Wang,Natarajan Balaji Shankar,Kaiyuan Zhang,Zihan Wang,Abeer Alwan*

Main category: cs.CL

TL;DR: 该论文针对儿童自动语音识别（ASR）中预训练模型与儿童语音数据域不匹配的问题，提出利用微调前后SSL模型嵌入的差异（delta embedding）进行特征融合方法，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 儿童ASR任务数据有限且预训练模型多基于成人语音，导致微调时表征空间发生偏移，影响识别效果。论文假设模型微调后的嵌入与预训练嵌入的差异能编码任务特定信息，为性能提升提供新的特征空间。

Method: 定义并提取delta SSL嵌入（微调模型与预训练模型嵌入差异），在MyST儿童语料上，结合多种SSL模型（如WavLM、HuBERT、W2V2）采用多种融合策略进行评估，并对比微调嵌入直接融合的效果。

Result: 实验表明利用WavLM的delta嵌入与HuBERT融合可将词错误率（WER）相对降低10%，与W2V2融合降低4.4%。融合WavLM与delta W2V2嵌入实现了9.64的WER，创SSL模型在MyST语料上的新纪录。

Conclusion: delta嵌入有效编码任务相关信息，结合特征融合策略能显著提升儿童ASR性能，表明此方法在该领域具有广阔应用前景和发展潜力。

Abstract: Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.

</details>


### [37] [Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents](https://arxiv.org/abs/2601.20144)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Jiri Gesi,Xianfeng Tang,Chen Luo,Yisi Sang,Hanqing Lu,Manling Li,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文提出了Trajectory2Task，一个针对复杂真实用户场景中的工具调用研究的数据生成流程，并基于此评测了七种大型语言模型的表现，发现其在复杂情境下频繁失败。通过成功轨迹微调轻量级模型，取得了一致的性能提升和更好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用代理多聚焦于理想化、固定且明确的任务，而真实应用中用户请求常常存在歧义、多变或因政策限制而无法实现，缺乏覆盖这类复杂交互模式的训练和评估数据。

Method: 提出Trajectory2Task数据生成流程：首先多轮探索生成有效的工具调用轨迹，再将轨迹转为带控制意图变化的面向用户任务，从而获得可验证的任务，用于闭环评估和训练。

Result: 基于生成的复杂用户场景任务，对七个先进大型语言模型进行基准测试，发现其频繁失败。利用任务成功轨迹对轻量级模型微调，显著提升性能，并增强对未见工具使用领域的泛化能力。

Conclusion: Trajectory2Task有效填补了复杂真实场景中工具调用研究的数据与评估空白，轻量级模型通过成功轨迹微调展现出更强的工具调用能力和泛化潜力。

Abstract: Tool-calling agents are increasingly deployed in real-world customer-facing workflows. Yet most studies on tool-calling agents focus on idealized settings with general, fixed, and well-specified tasks. In real-world applications, user requests are often (1) ambiguous, (2) changing over time, or (3) infeasible due to policy constraints, and training and evaluation data that cover these diverse, complex interaction patterns remain under-represented. To bridge the gap, we present Trajectory2Task, a verifiable data generation pipeline for studying tool use at scale under three realistic user scenarios: ambiguous intent, changing intent, and infeasible intents. The pipeline first conducts multi-turn exploration to produce valid tool-call trajectories. It then converts these trajectories into user-facing tasks with controlled intent adaptations. This process yields verifiable task that support closed-loop evaluation and training. We benchmark seven state-of-the-art LLMs on the generated complex user scenario tasks and observe frequent failures. Finally, using successful trajectories obtained from task rollouts, we fine-tune lightweight LLMs and find consistent improvements across all three conditions, along with better generalization to unseen tool-use domains, indicating stronger general tool-calling ability.

</details>


### [38] [Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction](https://arxiv.org/abs/2601.20162)
*Shuoxin Wang,Chang Liu,Gowen Loo,Lifan Zheng,Kaiwen Wei,Xinyi Zeng,Jingyuan Zhang,Yu Tian*

Main category: cs.CL

TL;DR: 本文提出了Me-Agent，一种可学习且有记忆的个性化移动代理，通过两级用户习惯学习提升个性化性能，在含有多种模糊指令的新基准User FingerTip上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的移动代理主要跟随明确指令，忽视个性化需求，导致无法解释模糊指令、缺乏交互历史学习和个性化指令处理能力。

Method: 提出Me-Agent，采用两级用户习惯学习：提示层面通过个人奖励模型提升用户偏好学习，内存层面设计分层偏好记忆，分别存储长期记忆和应用特定记忆。

Result: 在User FingerTip和通用基准测试上，Me-Agent在个性化能力上达到最新水平，同时保持竞争性的指令执行性能。

Conclusion: Me-Agent有效解决移动代理个性化不足问题，实现了更好地理解模糊指令和处理个性化需求，提升了移动代理的实用性和用户体验。

Abstract: Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.

</details>


### [39] [Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling](https://arxiv.org/abs/2601.20185)
*Husein Zolkepli*

Main category: cs.CL

TL;DR: 本文针对X-Codec-2.0音频压缩模型，通过增加池化层和加大解码步幅，将潜伏率从50Hz降至25Hz，输出采样率从16kHz提升至24kHz，提升了效率和语音感知质量。


<details>
  <summary>Details</summary>
Motivation: 原X-Codec-2.0在50Hz潜伏率和16kHz采样率下工作，虽性能优异，但限制了时间效率与音频保真度。

Method: 通过新增池化层和增大解码器跳步尺寸，降低潜伏率并提高采样率，而不改变核心架构。

Result: 在Common Voice 17多语言测试集上，改进方案在UTMOSv2测评中得到MOS值提高0.29，且在25Hz潜伏率的所有编解码器中表现最佳。

Conclusion: 简单结构调整即可显著提升神经音频压缩的效率和感知质量，兼顾多语言表现。

Abstract: X-Codec-2.0 has shown strong performance in neural audio compression and multilingual speech modeling, operating at a 50 Hz latent rate and a 16 kHz sampling rate using frozen HuBERT features. While effective, this configuration limits temporal efficiency and audio fidelity. In this work, we explore a simple and effective modification by introducing additional pooling and increasing the decoder hop size. This reduces the latent rate from 50 Hz to 25 Hz and simultaneously raises the output sampling rate from 16 kHz to 24 kHz, improving efficiency and perceptual quality without altering the core architecture. Evaluated on the multilingual Common Voice 17 test set, the proposed configuration achieves a 0.29 MOS improvement over the original X-Codec-2.0 baseline based on UTMOSv2, and attains the best reported performance among all codecs operating at 25 Hz. The source code, checkpoints, and generation comparisons are released at \href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}.

</details>


### [40] [Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems](https://arxiv.org/abs/2601.20230)
*Haoyuan Yu,Yuxuan Chen,Minjie Cai*

Main category: cs.CL

TL;DR: 本文提出一种全双工语音交互框架，将复杂对话拆解为最小对话单元，实现独立处理和智能切换，基于多模态大语言模型构建，无需训练即可即插即用，实验证明性能优异。


<details>
  <summary>Details</summary>
Motivation: 为了实现更自然的人机语音交互，提升对复杂对话的处理能力，有必要开发能够支持全双工和高效切换的对话系统。

Method: 构建了一个半级联全双工对话系统框架，将对话拆解为最小单元，由多模态大语言模型驱动，辅以声音活动检测和文本转语音模块，实现无训练即插即用的应用。

Result: 在HumDial数据集的实验中表现优秀，在Human-like Spoken Dialogue Systems Challenge中排名第二，展示了框架的有效性和实用性。

Conclusion: 所提出的全双工对话框架有效提升了自然语音交互的性能，具备良好的应用前景和扩展潜力。

Abstract: Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal conversational units, enabling the system to process each unit independently and predict when to transit to the next. This framework is instantiated as a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) synthesis. The resulting system operates in a train-free, plug-and-play manner. Experiments on the HumDial dataset demonstrate the effectiveness of our framework, which ranks second among all teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). Code is available at the GitHub repository https://github.com/yu-haoyuan/fd-badcat.

</details>


### [41] [Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy](https://arxiv.org/abs/2601.20253)
*Si Chen,Le Huy Khiem,Annalisa Szymanski,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 本文提出了一种基于专家指南和布鲁姆认知分类法的自动生成开放式问答基准测试框架，用以评估大模型在实践领域的情境化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型测试依赖于人类考试数据集，而在实践性强的领域如教学、营养学等缺乏合适的测试资源，且知识多为程序性和专业判断，难以通过传统数据衡量模型的推理能力。

Method: 通过将专家实践指导转化为违反情境的场景，并扩展成自动评分的多项选择题和多轮对话，覆盖布鲁姆认知分类的四个认知水平，实现确定性、可复现且可扩展的评估框架。

Result: 应用于教学、营养学和护理三个领域，发现大模型在高阶推理水平（分析）表现相对较好，而在低阶记忆层面表现较差，体现出与人类推理不同的行为特征。

Conclusion: 本文构建了大规模心理测量学指导的基准测试，以揭示和评估大模型在实际情境中的多层次推理能力，为未来模型评估和改进提供了有效工具。

Abstract: Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.

</details>


### [42] [SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility](https://arxiv.org/abs/2601.20256)
*Xuanyu Su,Diana Inkpen,Nathalie Japkowicz*

Main category: cs.CL

TL;DR: 本文提出了SoftHateBench，一个生成软性仇恨言论的基准，旨在弥补现有检测系统对基于推理的隐蔽仇恨识别不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体的仇恨言论从明显的粗俗侮辱到表面合理但通过框架和价值观引导排斥目标群体的软性仇恨，现有审核系统主要依赖表面毒性特征，难以识别后者，且缺乏系统性的测试基准。

Method: 结合论证模型（AMT）和关联理论（RT），构建一个统一框架，将明确的仇恨立场重新表达为表面中立但立场不变的软性仇恨文，保持逻辑连贯，涵盖7个社会文化领域和28个目标群体，生成4745条软性仇恨实例。

Result: 在编码器检测器、通用大语言模型和安全模型上做评估，发现从硬核仇恨到软性仇恨的检测率均显著降低，表明当前系统难以识别推理驱动的隐晦敌意。

Conclusion: SoftHateBench有效揭示了现有仿制系统在软性仇恨检测方面的不足，为改进针对隐晦仇恨言论的识别方法提供了基础。

Abstract: Online hate on social media ranges from overt slurs and threats (\emph{hard hate speech}) to \emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \textbf{\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \emph{Argumentum Model of Topics} (AMT) and \emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \textbf{7} sociocultural domains and \textbf{28} target groups, comprising \textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \textcolor{red}{\textbf{Disclaimer.} Contains offensive examples used solely for research.}

</details>


### [43] [RusLICA: A Russian-Language Platform for Automated Linguistic Inquiry and Category Analysis](https://arxiv.org/abs/2601.20275)
*Elina Sigdel,Anastasia Panfilova*

Main category: cs.CL

TL;DR: 本文提出了一种适用于俄语的心理语言学文本分析工具，基于LIWC方法并结合语言模型预测。


<details>
  <summary>Details</summary>
Motivation: 现有LIWC工具主要针对英语，直接翻译不可完全适应俄语的语法和文化特点。

Method: 通过整合句法、形态、词汇及统计特征，结合预训练语言模型预测结果，构建包含96类别的俄语词典，映射42个心理语言学类别。

Result: 构建了专为俄语设计的词典和心理语言学分类，开发了RusLICA在线分析工具。

Conclusion: 本研究成功地将LIWC方法适配到俄语，提升了俄语文本心理语言学分析的准确性与实用性。

Abstract: Defining psycholinguistic characteristics in written texts is a task gaining increasing attention from researchers. One of the most widely used tools in the current field is Linguistic Inquiry and Word Count (LIWC) that originally was developed to analyze English texts and translated into multiple languages. Our approach offers the adaptation of LIWC methodology for the Russian language, considering its grammatical and cultural specificities. The suggested approach comprises 96 categories, integrating syntactic, morphological, lexical, general statistical features, and results of predictions obtained using pre-trained language models (LMs) for text analysis. Rather than applying direct translation to existing thesauri, we built the dictionary specifically for the Russian language based on the content from several lexicographic resources, semantic dictionaries and corpora. The paper describes the process of mapping lemmas to 42 psycholinguistic categories and the implementation of the analyzer as part of RusLICA web service.

</details>


### [44] [Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale](https://arxiv.org/abs/2601.20276)
*Tianwei Lin,Zuyi Zhou,Xinda Zhao,Chenke Wang,Xiaohong Li,Yu Chen,Chuanrui Hu,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 该论文提出了一个针对长上下文大语言模型在大量环境中准确访问证据的对抗性测试基准EMB-S，揭示了语义区分能力比上下文长度更关键。


<details>
  <summary>Details</summary>
Motivation: 现有的NIAH评估方法主要关注简单的文本定位，忽视了语义干扰下模型准确访问证据的能力，难以真实反映长上下文模型的表现。

Method: 构建了包含3.26亿词Token的记忆库MemoryBank，设计EMB-S测试集，利用碰撞测试的近似难负样本和经过人工筛选及大模型验证的黄金证据，同时提出分离诊断协议分别衡量证据定位和端到端问答质量。

Result: 在不同规模的参考语料下，模型在语义干扰环境中证据访问能力明显下降，即使在上下文窗口范围内表现良好的模型也难以维持高质量的证据定位。

Conclusion: 语义区分能力而非上下文长度是长上下文记忆系统性能的主要瓶颈，未来研究应关注增强语义识别与干扰消解能力。

Abstract: Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.

</details>


### [45] [MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting](https://arxiv.org/abs/2601.20300)
*Jing Xu,Minglin Wu,Xueyuan Chen,Xixin Wu,Helen Meng*

Main category: cs.CL

TL;DR: 本文提出MiLorE-SSL，一种结合LoRA模块与软专家混合机制的轻量级多语言自监督学习框架，实现高效持续训练并减轻遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有多语言自监督学习模型只能处理预训练时遇到的语言，重新训练代价高，顺序训练易遗忘。

Method: 利用LoRA进行低秩适配，结合软专家混合模型促进语言间专家共享，并引入有限重放数据缓解遗忘。

Result: 在ML-SUPERB数据集上，MiLorE-SSL在新增语言和已有语言上均表现出强劲性能，仅用2.14%可训练参数。

Conclusion: MiLorE-SSL有效整合LoRA和软专家机制，实现多语言模型的持续高效训练，显著提升新旧语言表现且减少训练成本。

Abstract: Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.

</details>


### [46] [SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger](https://arxiv.org/abs/2601.20312)
*Kaiyuan Chen,Guangmin Zheng,Jin Wang,Xiaobing Zhou,Xuejie Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为SAPO的自适应过程优化方法，解决了小型语言模型自我演化中推理者与验证者之间的差距问题，提高了计算效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自我演化方法忽视了细粒度推理步骤的影响，导致推理者与验证者之间存在差距，且蒙特卡洛过程监督计算低效，增加了缓解这一差距的难度。

Method: 借鉴错误相关负波（ERN）的启发，SAPO方法通过主动最小化推理者与验证者差距，适应性且高效地引入过程监督信号，避免依赖低效的蒙特卡洛估计。

Result: 大量实验证明，SAPO在数学和代码两个具有挑战性的任务上，性能优于大多数现有自我演化方法。

Conclusion: SAPO有效提升了小型语言模型的自我改进能力，并通过引入两个新的数学和编码任务过程奖励模型基准，进一步促进了验证者性能的研究。

Abstract: Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.

</details>


### [47] [Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning](https://arxiv.org/abs/2601.20326)
*Zeyu Xing,Xing Li,Hui-Ling Zhen,Mingxuan Yuan,Sinno Jialin Pan*

Main category: cs.CL

TL;DR: 本文提出将KV缓存作为轻量级表示，用于下游任务，避免重新计算或存储完整隐藏状态。该方法在链式嵌入和快慢思考切换任务中表现优异，显著减少生成代币数量，维持较高准确率。


<details>
  <summary>Details</summary>
Motivation: KV缓存通常仅用于加速自回归解码，但它们编码的上下文信息可被复用于下游任务，节省计算资源。作者希望利用KV缓存制作轻量级表示，提升推理和采样效率。

Method: 将KV缓存视为轻量级表示，替代传统的全隐藏状态存储。通过两项应用验证：1) 链式嵌入（Chain-of-Embedding），2) 快/慢思考切换（Fast/Slow Thinking Switching），在多个大模型上评测性能和效率。

Result: KV缓存生成的表示虽弱于专门的嵌入，但在Llama-3.1-8B-Instruct和Qwen2-7B-Instruct上实现了竞争或更优性能；在Qwen3-8B和DeepSeek-R1-Distil-Qwen-14B上实现最多5.7倍的生成代币减少，准确率仅有轻微损失。

Conclusion: KV缓存可作为免费且有效的表示基础，用于采样和推理，推动大模型推理阶段的表示重用，具备广泛应用潜力。

Abstract: KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.

</details>


### [48] [CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria](https://arxiv.org/abs/2601.20327)
*Xinyu Hu,Yancheng He,Weixun Wang,Tao Feng,Li Lin,Jiashun Liu,Wenbo Su,Bo Zheng,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文提出了一种基于统一查询标准和两阶段展开方法训练的点式生成奖励模型CE-RM-4B，解决了现有大语言模型评估作为奖励模型在强化学习中表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型作为评判者的自动评估方法虽然在基准测试表现优异，但在强化学习实践中效果有限，主要因为主流方法多为成对评价且优化目标不充分。

Method: 提出CE-RM-4B奖励模型，采用两阶段展开的训练策略，基于统一的查询标准，并利用约5.7K高质量开放数据进行训练。

Result: CE-RM-4B在多个奖励模型基准测试中表现优越，尤其在Best-of-N评估场景中，且在实际强化学习任务中带来更有效的改进。

Conclusion: 通过改进训练策略和评分标准，实现了更准确有效的生成式奖励模型，提升了自动评价在强化学习中的实际应用效果。

Abstract: Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.

</details>


### [49] [PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments](https://arxiv.org/abs/2601.20330)
*Zhuang Chen,Dazhen Wan,Zhangkai Zheng,Guanqun Bi,Xiyao Xiao,Binghang Li,Minlie Huang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Ps的统一框架，通过轨迹锚定的竞赛方式评价大语言模型在心理咨询中的治疗能力，解决了当前评估中缺乏锚定导致的过程漂移和评分不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估范式存在无锚定缺陷，导致咨询过程中的过程漂移和评分不稳定，难以可靠评价大语言模型在心理健康咨询中的治疗能力。

Method: 引入Ps框架，通过在模拟中锚定互动轨迹，由客户精确控制咨询过程；采用动态的瑞士制竞赛进行成对对战，产生稳健的Elo评分，同时将竞赛轨迹转化为奖励信号以支持强化学习提升模型性能。

Result: 大量实验验证了Ps框架的有效性，评价结果与人类专家判断高度一致，展示了该方法在增强LLM治疗能力上的潜力。

Conclusion: Ps框架有效解决了评价过程中存在的锚定问题，提供了一种稳定可靠的治疗能力评估方式，并通过强化学习进一步提升了大语言模型在心理咨询领域的表现。

Abstract: While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.

</details>


### [50] [MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment](https://arxiv.org/abs/2601.20335)
*Qinzhuo Wu,Zhizhuo Yang,Hanhao Li,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: 提出了一个名为MobileBench-OL的在线移动GUI智能体评测基准，包含1080个任务，覆盖任务执行、复杂推理和抗噪声能力，支持稳定重复评测。


<details>
  <summary>Details</summary>
Motivation: 现有在线评测基准主要关注任务执行能力，忽略了智能体的推理、探索能力，且缺乏对现实环境中随机噪声的考虑，导致基准与真实环境存在差距。

Method: 设计包含5个子集的MobileBench-OL基准，涵盖多维度评测指标，并开发了带重置机制的自动评测框架，支持稳定可重复的现实环境测试。

Result: 在MobileBench-OL上评测了12个主流GUI智能体，结果显示这些智能体的性能还有较大提升空间。

Conclusion: MobileBench-OL有效测量了移动GUI智能体在现实环境中的综合能力，数据和代码将在论文接受后公开。

Abstract: Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.

</details>


### [51] [Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space](https://arxiv.org/abs/2601.20339)
*Yangyi Shen,Tianjian Feng,Jiaqi Han,Wen Wang,Tianlang Chen,Chunhua Shen,Jure Leskovec,Stefano Ermon*

Main category: cs.CL

TL;DR: 本文提出了一种名为Order-Token Search的新方法，通过联合搜索生成顺序和词元值，提升扩散语言模型的生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型的解码方法只依赖单一路径，限制了探索生成轨迹空间的能力。

Method: 通过一个似然估计器对去噪动作进行评分，实现了稳定剪枝和高效探索多样化轨迹的Order-Token Search。

Result: 在数学推理和代码生成基准测试（GSM8K、MATH500、Countdown、HumanEval）中，Order-Token Search相比基础模型分别提升了3.1%、3.8%、7.9%和6.8%，并达到或超过了diffu-GRPO后训练的d1-LLaDA性能。

Conclusion: 联合搜索生成顺序和词元值是推动扩散语言模型解码性能提升的关键方法。

Abstract: Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.

</details>


### [52] [SpeechMapper: Speech-to-text Embedding Projector for LLMs](https://arxiv.org/abs/2601.20417)
*Biswesh Mohapatra,Marcely Zanon Boito,Ioan Calapodescu*

Main category: cs.CL

TL;DR: SpeechMapper提出了一种高效且抗过拟合的语音到大语言模型嵌入训练方法，通过预训练和短时指令微调，提升了模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的语音大语言模型训练计算资源消耗大且容易过拟合任务和提示，亟需一种更高效且泛化能力强的训练方案。

Method: SpeechMapper先在无LLM情况下预训练模型，随后通过短时间（1K步）的指令调优快速连接目标LLM，提供任务无关和任务相关两种调优策略。

Result: 在语音翻译和语音问答任务中，任务无关调优的SpeechMapper表现可与最佳语音LLM持平，任务相关调优则在多数据集上表现更优，且使用的数据和计算资源更少。

Conclusion: SpeechMapper为实现高效且泛化强的语音与大语言模型整合提供了实用且可扩展的方案，无需大规模指令调优。

Abstract: Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.

</details>


### [53] [Hopes and Fears -- Emotion Distribution in the Topic Landscape of Finnish Parliamentary Speech 2000-2020](https://arxiv.org/abs/2601.20424)
*Anna Ristilä,Otto Tarkka,Veronika Laippala,Kimmo Elo*

Main category: cs.CL

TL;DR: 本文研究了芬兰议会2000至2020年议会发言中不同议题的情感表达，发现议会发言整体趋于积极且不同议题情感表达存在差异。


<details>
  <summary>Details</summary>
Motivation: 以往研究忽略了议会发言中不同议题可能存在的情感表达差异，缺乏对议题特异性情感的系统分析。

Method: 利用情感分析模型，对芬兰议会2000至2020年的议会发言按议题进行情感表达的同步和历时分析。

Result: 发现议会发言情感逐渐趋于积极，同时在不同议题中情感表达存在明显差异。

Conclusion: 议会辩论中的情感表达不仅随着时间向积极方向发展，不同议题的情感表达具有显著的特异性，丰富了对议会话语情感动态的理解。

Abstract: Existing research often treats parliamentary discourse as a homogeneous whole, overlooking topic-specific patterns. Parliamentary speeches address a wide range of topics, some of which evoke stronger emotions than others. While everyone has intuitive assumptions about what the most emotive topics in a parliament may be, there has been little research into the emotions typically linked to different topics. This paper strives to fill this gap by examining emotion expression among the topics of parliamentary speeches delivered in Eduskunta, the Finnish Parliament, between 2000 and 2020. An emotion analysis model is used to investigate emotion expression in topics, from both synchronic and diachronic perspectives. The results strengthen evidence of increasing positivity in parliamentary speech and provide further insights into topic-specific emotion expression within parliamentary debate.

</details>


### [54] [PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use](https://arxiv.org/abs/2601.20439)
*Qihao Wang,Mingzhe Lu,Jiayue Wu,Yue Hu,Yanbing Liu*

Main category: cs.CL

TL;DR: PEARL框架通过离线学习和在线强化学习显著提升大语言模型在复杂多轮工具调用中的规划和执行能力，实现了优于现有方法的工具使用成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂多轮调用外部工具时存在规划能力不足、工具幻觉、参数错误生成以及交互不稳定等问题。

Method: 提出PEARL框架，分为离线阶段（探索工具使用模式及失败条件）和在线阶段（通过群体相对策略优化训练规划器，设计奖励函数指引规划质量）。

Result: 在ToolHop和T-Eval基准测试中，PEARL显著优于现有方法，在ToolHop上达到56.5%的新最高成功率，且调用错误率低。

Conclusion: PEARL有效解决了大语言模型工具调用中的复杂规划难题，推动了更稳健可靠的LLM智能体发展。

Abstract: Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \textbf{56.5\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.

</details>


### [55] [MuVaC: AVariational Causal Framework for Multimodal Sarcasm Understanding in Dialogues](https://arxiv.org/abs/2601.20451)
*Diandian Guo,Fangfang Yuan,Cong Cao,Xixun Lin,Chuan Zhou,Hao Peng,Yanan Cao,Yanbing Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为MuVaC的多模态讽刺检测与解释联合框架，通过变分因果推理模拟人类认知机制，实现多模态特征融合与优化，提升了讽刺识别和解释的效果。


<details>
  <summary>Details</summary>
Motivation: 多模态对话中的讽刺理解涉及讽刺检测与解释两大关键任务，现有研究多将二者分开处理，忽略了二者之间的因果依赖关系。

Method: 论文采用结构因果模型视角，建立变分因果路径实现讽刺检测和解释的联合优化，设计对齐后融合的多模态特征融合策略，并通过检测结果与解释间的一致性增强推理的可信度。

Result: 公开数据集上的实验结果表明，MuVaC框架在多模态讽刺检测与解释任务上优于现有方法。

Conclusion: MuVaC提供了一种结合变分因果推理的联合多模态讽刺理解方法，开辟了多模态讽刺研究的新视角，增强了检测与解释的协同效果。

Abstract: The prevalence of sarcasm in multimodal dialogues on the social platforms presents a crucial yet challenging task for understanding the true intent behind online content. Comprehensive sarcasm analysis requires two key aspects: Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE). Intuitively, the act of detection is the result of the reasoning process that explains the sarcasm. Current research predominantly focuses on addressing either MSD or MuSE as a single task. Even though some recent work has attempted to integrate these tasks, their inherent causal dependency is often overlooked. To bridge this gap, we propose MuVaC, a variational causal inference framework that mimics human cognitive mechanisms for understanding sarcasm, enabling robust multimodal feature learning to jointly optimize MSD and MuSE. Specifically, we first model MSD and MuSE from the perspective of structural causal models, establishing variational causal pathways to define the objectives for joint optimization. Next, we design an alignment-then-fusion approach to integrate multimodal features, providing robust fusion representations for sarcasm detection and explanation generation. Finally, we enhance the reasoning trustworthiness by ensuring consistency between detection results and explanations. Experimental results demonstrate the superiority of MuVaC in public datasets, offering a new perspective for understanding multimodal sarcasm.

</details>


### [56] [BMAM: Brain-inspired Multi-Agent Memory Framework](https://arxiv.org/abs/2601.20465)
*Yang Li,Jiaxiang Liu,Yusong Wang,Yujie Wu,Mingkun Xu*

Main category: cs.CL

TL;DR: 本文提出了BMAM，一种受大脑启发的多智能体记忆架构，用于解决语言模型代理在长期交互中信息保持和行为一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型在长期交互过程中难以保持时间关联信息和行为一致性，导致所谓的“灵魂侵蚀”问题。

Method: BMAM将代理记忆分解为情景记忆、语义记忆、显著性记忆和控制记忆四个功能子系统，模拟认知记忆系统。情景记忆沿时间线组织以支持长期推理，同时融合多种信号进行证据检索。

Result: 在LoCoMo基准测试中，BMAM在标准的长期评估设置下达到了78.45%的准确率，消融实验表明海马体启发的情景记忆子系统在时间推理中发挥关键作用。

Conclusion: BMAM有效解决了语言模型代理的长期记忆和行为一致性问题，情景记忆在时间相关推理中至关重要，为未来多智能体记忆架构设计提供了新方向。

Abstract: Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.

</details>


### [57] [Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch](https://arxiv.org/abs/2601.20476)
*Evanfiya Logacheva,Arto Hellas,Tsvetomila Mihaylova,Juha Sorva,Ava Heinonen,Juho Leinonen*

Main category: cs.CL

TL;DR: 本研究提出一种基于修辞结构理论（RST）的上下文示例图表代码生成方法，旨在提升生成图表的逻辑性与准确性，并减少AI幻觉现象。计算机科学教育者评估结果显示，该方法能降低事实性错误，提高与上下文的贴合度，但生成的图表质量存在波动。


<details>
  <summary>Details</summary>
Motivation: 当前生成式人工智能在计算教育中广泛应用，但生成材料的质量参差不齐，尤其存在AI幻觉问题，影响教学效果。

Method: 基于修辞结构理论（RST）设计上下文示例的图表代码生成方法，通过大语言模型生成图表，并由教育者对150个图表进行逻辑组织、连通性、布局美学及幻觉现象的评估。

Result: 该方法有效降低了事实性幻觉的发生率，增强了生成图表与提供上下文的一致性，但因大语言模型的随机性，生成质量仍有波动。自动化评价潜力也被初步验证。

Conclusion: 基于RST的上下文示例生成方法能提升生成图表的质量并减少幻觉，但复杂文本背景会增加幻觉风险，且大语言模型难以自我纠错，需进一步研究改进。

Abstract: Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.

</details>


### [58] [Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models](https://arxiv.org/abs/2601.20546)
*Kumiko Nakajima,Jan Zuiderveld,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 该论文针对现有大语言模型创造力评估方法Divergent Association Task (DAT)不足，提出了结合新颖性与适当性的条件发散关联任务(CDAT)，更准确评估模型创造力，发现较小模型通常更具创造力而大型模型更注重适当性。


<details>
  <summary>Details</summary>
Motivation: 现有的创造力评估方法DAT仅关注新颖性，忽视了创造力的另一核心要素——适当性，因此难以准确评估大语言模型的创造能力。

Method: 基于人类创造力理论，提出条件发散关联任务（CDAT），将新颖性评价与上下文适当性结合，区分噪声与创造力，同时保持评估的简洁与客观。

Result: 通过CDAT评测发现，较小模型系列表现出更高的创造力，而更先进模型系列更倾向于产生适当性较高但新颖性较低的输出。

Conclusion: CDAT有效弥补了DAT的缺陷，更合理地评估语言模型的创造力，揭示了训练与校准过程可能使模型输出更适当但创造力降低的趋势，推动未来模型创造力评估方法的发展。

Abstract: Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.

</details>


### [59] [Single-Nodal Spontaneous Symmetry Breaking in NLP Models](https://arxiv.org/abs/2601.20582)
*Shalom Rosner,Ronit D. Gross,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 本文揭示了自然语言处理模型在训练过程中表现出的自发对称破缺现象，即使在确定性动力学和有限架构条件下也存在。


<details>
  <summary>Details</summary>
Motivation: 研究统计力学中自发对称破缺现象在自然语言处理模型中的体现，揭示模型内部节点在预训练与微调过程中如何形成特定的学习能力。

Method: 通过分析BERT-6模型在维基百科数据集上的预训练及FewRel分类任务上的微调过程，考察个别注意力头和节点的学习行为，并利用凸包分析对节点功能进行上界限制。

Result: 发现自发对称破缺不仅在个别节点层级存在，还展现为节点数量增加时学习能力的交叉变化，体现节点之间协作提升整体性能，超越单节点能力之和。

Conclusion: 自然语言处理模型中的节点通过自发对称破缺机制形成特定学习能力，这一机制不同于物理中自旋玻璃系统的行为，且有助于理解和提升模型性能。

Abstract: Spontaneous symmetry breaking in statistical mechanics primarily occurs during phase transitions at the thermodynamic limit where the Hamiltonian preserves inversion symmetry, yet the low-temperature free energy exhibits reduced symmetry. Herein, we demonstrate the emergence of spontaneous symmetry breaking in natural language processing (NLP) models during both pre-training and fine-tuning, even under deterministic dynamics and within a finite training architecture. This phenomenon occurs at the level of individual attention heads and is scaled-down to its small subset of nodes and also valid at a single-nodal level, where nodes acquire the capacity to learn a limited set of tokens after pre-training or labels after fine-tuning for a specific classification task. As the number of nodes increases, a crossover in learning ability occurs, governed by the tradeoff between a decrease following random-guess among increased possible outputs, and enhancement following nodal cooperation, which exceeds the sum of individual nodal capabilities. In contrast to spin-glass systems, where a microscopic state of frozen spins cannot be directly linked to the free-energy minimization goal, each nodal function in this framework contributes explicitly to the global network task and can be upper-bounded using convex hull analysis. Results are demonstrated using BERT-6 architecture pre-trained on Wikipedia dataset and fine-tuned on the FewRel classification task.

</details>


### [60] [A Computational Approach to Language Contact -- A Case Study of Persian](https://arxiv.org/abs/2601.20592)
*Ali Basirat,Danial Namazifard,Navid Baradaran Hemmati*

Main category: cs.CL

TL;DR: 本研究探讨了单语语言模型中语言接触的结构性痕迹，重点分析波斯语模型在接触不同语言时的中间表示，发现句法信息不受接触影响，而形态特征如格和性别则受语言特定结构强烈影响，表明接触效应有选择性且受结构限制。


<details>
  <summary>Details</summary>
Motivation: 研究语言接触如何体现在单语语言模型的内部表示中，特别是在历史接触丰富的波斯语环境下，理解语言接触对模型表示的影响机制。

Method: 通过分析波斯语训练模型的中间表示，量化不同形态句法特征编码的语言信息量，评估这些信息在模型组件中的分布，以及不同语言接触度对信息编码的影响。

Result: 发现普遍句法信息对历史接触不敏感，而形态特征如格和性别强烈受语言特定结构影响，表明语言接触效应在模型中是选择性且结构受限的。

Conclusion: 语言接触效应在单语语言模型内部表现为特定形态特征的变化，而非普遍句法信息的变化，说明接触影响具有选择性和结构限制的特点。

Abstract: We investigate structural traces of language contact in the intermediate representations of a monolingual language model. Focusing on Persian (Farsi) as a historically contact-rich language, we probe the representations of a Persian-trained model when exposed to languages with varying degrees and types of contact with Persian. Our methodology quantifies the amount of linguistic information encoded in intermediate representations and assesses how this information is distributed across model components for different morphosyntactic features. The results show that universal syntactic information is largely insensitive to historical contact, whereas morphological features such as Case and Gender are strongly shaped by language-specific structure, suggesting that contact effects in monolingual language models are selective and structurally constrained.

</details>


### [61] [AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios](https://arxiv.org/abs/2601.20613)
*Kaiyuan Chen,Qimin Wu,Taiyu Hou,Tianhao Tang,Xueyu Hu,Yuchen Hou,Bikun Li,Chengming Qian,Guoyin Wang,Haolin Chen,Haotong Tian,Haoye Zhang,Haoyu Bian,Hongbing Pan,Hongkang Zhang,Hongyi Zhou,Jiaqi Cai,Jiewu Rao,Jiyuan Ren,Keduan Huang,Lucia Zhu Huang,Mingyu Yuan,Naixu Guo,Qicheng Tang,Qinyan Zhang,Shuai Chen,Siheng Chen,Ting Ting Li,Xiaoxing Guo,Yaocheng Zuo,Yaoqi Guo,Yinan Wang,Yinzhou Yu,Yize Wang,Yuan Jiang,Yuan Tian,Yuanshuo Zhang,Yuxuan Liu,Yvette Yan Zeng,Zenyu Shan,Zihan Yin,Xiaobo Hu,Yang Liu,Yixin Ren,Yuan Gong*

Main category: cs.CL

TL;DR: 本文提出了AgentIF-OneDay基准，评估普通用户是否能通过自然语言指令和AI代理完成多样的日常任务，涵盖工作流程执行、隐性指令推断和迭代改进三大类别，并通过实例级精细评分和LLM辅助评判实现高达80.1%的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI能力评估主要聚焦任务难度提升，未充分覆盖普通用户日常生活和工作中多样化的任务需求，导致公众对AI能力的认知有限。

Method: 设计AgentIF-OneDay基准，包含104个任务和767个评分点，分为开放工作流程执行、隐性指令理解、迭代完善三个任务类别，采用实例级评分标准和结合LLM验证与人工判断的评价流程。

Result: 在对四大AI代理进行基准测试时，基于API的代理产品和基于强化学习的ChatGPT代理均表现优异，领先的LLM API和开源模型已具备内在代理能力。

Conclusion: AgentIF-OneDay有效评估了普通用户利用AI代理完成复杂多样日常任务的能力，推动AI产品团队开发具备先进代理能力的应用。

Abstract: The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.

</details>


### [62] [P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering](https://arxiv.org/abs/2601.20649)
*Wenlin Zhong,Chengyuan Liu,Yiquan Wu,Bovin Tan,Changlong Sun,Yi Wang,Xiaozhong Liu,Kun Kuang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的自监督方法P2S，通过在强化学习中对推理过程中的每一步给予概率奖励，解决了通用领域推理任务中缺乏可验证奖励信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的推理方法往往只利用最终结果作为奖励，忽视了对推理过程的细粒度监督，导致奖励信号稀疏，难以提升模型的推理能力。

Method: 提出Probabilistic Process Supervision (P2S)框架，在强化学习过程中合成并筛选高质量的参考推理链（gold-CoT），通过计算每一步推理的路径忠实度奖励（PFR），即基于当前推理前缀生成剩余推理链的条件概率，为推理步骤提供密集的奖励信号。该方法可以与任何基于结果的奖励结合使用。

Result: 在阅读理解和医疗问答基准测试中，P2S显著优于现有强基线，证明其有效提升了模型的推理性能。

Conclusion: P2S通过提供细粒度的推理过程监督，解决了推理任务中奖励稀疏的问题，是提高通用领域强化学习推理能力的有效方法。

Abstract: While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.

</details>


### [63] [Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science](https://arxiv.org/abs/2601.20674)
*Juan Jose Rubio Jan,Jack Wu,Julia Ive*

Main category: cs.CL

TL;DR: 研究应用大规模语言模型在电子健康记录数据科学的结构化查询和非结构化文本信息提取任务，展示了其在临床数据分析和信息提取中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语言模型在处理结构化电子健康记录数据查询和非结构化临床文本信息提取中的能力和可靠性。

Method: 使用RAG流水线结合程序化语言（Python/Pandas）进行结构化数据查询和信息抽取，构建了自动生成合成问答对的灵活评估框架，在MIMIC III数据集多个表格及临床笔记上测试多种LLM实现。

Result: 实验通过精确匹配、语义相似度和人工评判评估，证明LLM能够准确执行数据查询和语义正确的信息提取任务。

Conclusion: 大规模语言模型具备支持临床数据查询和信息提取的潜力，可促进临床工作流程的效率和准确性。

Abstract: This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.

</details>


### [64] [Efficient Multimodal Planning Agent for Visual Question-Answering](https://arxiv.org/abs/2601.20676)
*Zhuo Chen,Xinyu Geng,Xinyu Wang,Yong Jiang,Zhen Zhang,Pengjun Xie,Kewei Tu*

Main category: cs.CL

TL;DR: 该论文提出了一种训练多模态规划代理的方法，动态分解多模态检索增强生成（mRAG）流程，以提升视觉问答（VQA）任务的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态检索增强生成流程在处理知识密集型视觉问答时依赖多阶段流水线，效率较低且存在冗余计算。

Method: 训练一个多模态规划代理，动态判断和调整mRAG的各个步骤，智能确定每一步的必要性，以减少冗余计算，提高效率。

Result: 实验显示，该代理减少了60%以上的搜索时间和昂贵工具调用，同时在六个数据集上的表现优于深度研究代理和基于提示的对比方法。

Conclusion: 该方法有效权衡了视觉问答任务中的效率与效果，显著提升了系统性能，减少了计算资源浪费。

Abstract: Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.

</details>


### [65] [ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code](https://arxiv.org/abs/2601.20679)
*Mingqiao Mo,Yunlong Tan,Hao Zhang,Heng Zhang,Yangfan He*

Main category: cs.CL

TL;DR: 本文提出ShieldedCode框架，利用大规模成对数据和层次依赖建模，提升虚拟机保护代码的鲁棒表示和生成能力，实现对软件保护的学习式防御。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成方面进展显著，但在软件保护领域的潜力尚未充分利用；传统虚拟机保护方法设计复杂且易被自动化分析攻击，亟需新技术提升软件逆向保护效果。

Method: 构建大规模源码与标准化虚拟机实现的配对数据集，采用指令内、前置及指令间层次依赖建模，同时结合功能感知与保护感知对比学习，设计保护效果优化任务，并通过持续预训练与微调提升模型能力。

Result: ShieldedCode方法在L0虚拟机代码生成上达到26.95% Pass@1，优于GPT-4o的22.58%；在二进制相似性检测任务中，Recall@1指标较先进方法提升10%，显示显著鲁棒性提升。

Conclusion: 该框架有效增强了基于学习的软件保护能力，为虚拟机保护代码的表示学习与防御开辟了新方向，推动软件安全领域的研究进展。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, yet their potential for software protection remains largely untapped. Reverse engineering continues to threaten software security, while traditional virtual machine protection (VMP) relies on rigid, rule-based transformations that are costly to design and vulnerable to automated analysis. In this work, we present the first protection-aware framework that learns robust representations of VMP-protected code. Our approach builds large-scale paired datasets of source code and normalized VM implementations, and introduces hierarchical dependency modeling at intra-, preceding-, and inter-instruction levels. We jointly optimize language modeling with functionality-aware and protection-aware contrastive objectives to capture both semantic equivalence and protection strength. To further assess resilience, we propose a protection effectiveness optimization task that quantifies and ranks different VM variants derived from the same source. Coupled with a two-stage continual pre-training and fine-tuning pipeline, our method enables models to generate, compare, and reason over protected code. Extensive experiments show that our framework significantly improves robustness across diverse protection levels, opening a new research direction for learning-based software defense. In this work, we present ShieldedCode, the first protection-aware framework that learns robust representations of VMP-protected code. Our method achieves 26.95% Pass@1 on L0 VM code generation compared to 22.58% for GPT-4o., and improves binary similarity detection Recall@1 by 10% over state of art methods like jTrans.

</details>


### [66] [Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin](https://arxiv.org/abs/2601.20680)
*Ostap Vykhopen,Viktoria Skorik,Maxim Tereschenko,Veronika Solopova*

Main category: cs.CL

TL;DR: 本文研究了传统批处理聚类算法在社交媒体监测中的扩展性问题，探索将离线HDBSCAN替换为在线增量聚类方法，提升实时性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的HDBSCAN算法虽然能够发现层次密度聚类并处理噪声，但其批处理特性导致每个时间窗口都需完全重训练，造成内存限制、计算效率低下，无法实时适应不断变化的叙事内容，难以满足社交媒体监测的需求。

Method: 构建一个三阶段管道（数据收集、建模、仪表盘生成），使用滑动窗口模拟历史数据流，对多种在线聚类算法进行评估，评估指标结合传统聚类指标（轮廓系数、Davies-Bouldin指数）和叙事相关指标（叙事独特性、权变性和方差），以比较算法在真实环境下的权衡。

Result: 在线聚类算法在保持聚类质量的同时，提高了计算效率和内存利用率，更适合实时监测和动态叙事的需求。通过对乌克兰信息空间数据的模拟验证，证实了所提方案在生产环境中的适用性和优势。

Conclusion: 本研究弥补了批处理主题建模框架与社交媒体监测流式处理需求之间的空白，提升了叙事智能系统的实时性能和可扩展性，对计算社会科学、危机信息学和叙事监控系统具有重要意义。

Abstract: Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.

</details>


### [67] [AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts](https://arxiv.org/abs/2601.20730)
*Shicheng Fang,Yuxin Wang,XiaoRan Liu,Jiahao Lu,Chuanyuan Tan,Xinchi Chen,Yining Zheng. Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为AgentLongBench的新基准测试，旨在评估大型语言模型作为自主智能体在动态复杂环境中的表现，尤其是在非线性推理和迭代反馈任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的评测基准过于静态，无法有效模拟智能体与环境的复杂交互，限制了对模型实际工作流程中动态信息处理能力的检验。

Method: 提出AgentLongBench框架，通过基于横向思维谜题的环境模拟来生成严谨的交互轨迹，涵盖知识密集和无知识场景，评估模型在动态信息综合过程中的表现。

Result: 实验显示，尽管最新模型和内存系统在静态检索任务表现良好，但在需要动态信息综合的任务中表现较差，且模型性能下降与解决查询所需的最少token数密切相关。

Conclusion: 动态信息合成是当前大型语言模型面临的关键瓶颈，尤其是在处理大量高信息密度响应时，这对模型能力提出了更大挑战，提示未来研究应关注改进动态信息合成和长上下文管理。

Abstract: The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.

</details>


### [68] [QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks](https://arxiv.org/abs/2601.20731)
*Mae Sosto,Delfina Sol Martinez Pandiani,Laura Hollink*

Main category: cs.CL

TL;DR: 本文研究大型语言模型（LLMs）如何复制社会规范，尤其是异性恋规范，并在文本生成中表现为偏见。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在不同性别和性取向表述下的偏见表现，了解其再现的社会规范。

Method: 分析三类性别/性取向（酷儿标记、非酷儿标记和未标记）在LLMs生成的英语句子中四种维度的差异：情感、尊重、有害性和预测多样性。

Result: 掩码语言模型（MLMs）对酷儿标记表现最差，表现出更负面情感、更高毒性和负面尊重；自回归模型（ARLMs）部分缓解了这些偏见，但闭源ARLM对未标记对象有更多有害输出。

Conclusion: LLMs复制了社会规范及其偏见，具体偏见表现依赖模型特性，可能改变但无法消除代表性伤害。

Abstract: This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized "unmarked" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.

</details>


### [69] [Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts](https://arxiv.org/abs/2601.20747)
*Elham Aghakhani,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本研究通过分析5126条Reddit心理健康社区帖子，探讨了用户对情感支持类大型语言模型的评价和使用态度。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在临床外情感支持和心理健康互动中的应用日益增多，但缺乏关于用户日常使用评价和关系的研究。

Method: 结合技术接受模型和治疗联盟理论，建立理论驱动的标注框架，采用混合LLM-人工方法对帖子中的评估语言、态度和关系匹配进行大规模分析。

Result: 研究发现用户参与主要受叙述的使用结果、信任和回应质量影响，情感纽带影响较小。任务和目标一致性与积极情绪高度相关，而以陪伴为导向的使用则常伴随关系错位和风险。

Conclusion: 本研究展示了如何将理论构建应用于大规模话语分析，强调在敏感的真实环境中理解用户如何解读语言技术的重要性。

Abstract: Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.

</details>


### [70] [Persona Prompting as a Lens on LLM Social Reasoning](https://arxiv.org/abs/2601.20757)
*Jing Yang,Moritz Hechtbauer,Elisabeth Khalilov,Evelyn Luise Brinkmann,Vera Schmitt,Nils Feldhus*

Main category: cs.CL

TL;DR: 本文研究了使用Persona prompting（PP）对大语言模型（LLM）在敏感社会任务（如仇恨言论检测）中生成解释质量和模型偏见的影响。


<details>
  <summary>Details</summary>
Motivation: 在仇恨言论检测等社会敏感任务中，模型解释的质量对用户信任和模型对齐至关重要，但PP对模型推理解释的影响尚未充分探究。

Method: 通过使用带有词级推理标注的数据集，比较不同模拟人口角色（persona）条件下的模型推理与不同人群的人类标注的一致性，评估PP对模型偏见和人类对齐的影响。

Result: PP能提升最具主观性的仇恨言论分类性能，但会降低推理解释质量；模拟人口身份和真实人口身份对齐失败，模型对身份引导抵抗力强；模型持续表现出人口偏见和过度判断有害内容的倾向。

Conclusion: PP在提升敏感任务分类效果的同时，可能损害推理解释质量且未能缓解基本偏见，应用时需谨慎。

Abstract: For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.

</details>


### [71] [Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers](https://arxiv.org/abs/2601.20796)
*Yiran Huang,Karsten Roth,Quentin Bouniot,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: 本文通过对小型Transformer进行控制实验，研究了Transformer如何通过上下文示例实现多模态学习。发现了多模态中存在学习不对称性，即主要模态数据多样性高时，次要模态对数据复杂度的要求较低，即可实现多模态上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer如何通过上下文示例在多模态任务中学习不同模态间的信息关联。

Method: 在合成分类任务上训练小型Transformer，通过精确控制数据统计和模型结构，考察并对比单模态和多模态上下文学习的机制。

Result: 发现Rotary位置编码提升了上下文学习所需的数据复杂度阈值；多模态设置下主要模态数据多样性大时，次要模态数据复杂度可较低；多模态训练扩展并细化了跨模态标签复制的归纳机制。

Conclusion: 本研究为理解现代Transformer的多模态上下文学习机制提供了机械解释基础，并构建了一个可控测试平台，为未来研究提供支持。

Abstract: Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.

</details>


### [72] [Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction](https://arxiv.org/abs/2601.20803)
*Aunabil Chakma,Mihai Surdeanu,Eduardo Blanco*

Main category: cs.CL

TL;DR: 本文提出了一种基于句法-语义结构相似性的例子选择策略，通过结合人工选择和大模型生成的例子，提升了一次性关系抽取的效果。


<details>
  <summary>Details</summary>
Motivation: 现有一次性关系抽取中的上下文学习样本不足，单一获取样本方法存在局限。

Method: 基于句法-语义结构的相似性选择新样本，并与大模型生成样本结合形成混合系统。

Result: 混合策略在多个数据集及不同大模型上表现优异，优于单独方法，达到最新水平。

Conclusion: 结合句法-语义选择和大模型生成的混合策略是提升一次性关系抽取的有效途径，具备良好的泛化性和性能。

Abstract: This paper presents several strategies to automatically obtain additional examples for in-context learning of one-shot relation extraction. Specifically, we introduce a novel strategy for example selection, in which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example. We show that this method results in complementary word choices and sentence structures when compared to LLM-generated examples. When these strategies are combined, the resulting hybrid system achieves a more holistic picture of the relations of interest than either method alone. Our framework transfers well across datasets (FS-TACRED and FS-FewRel) and LLM families (Qwen and Gemma). Overall, our hybrid selection method consistently outperforms alternative strategies and achieves state-of-the-art performance on FS-TACRED and strong gains on a customized FewRel subset.

</details>


### [73] [Linear representations in language models can change dramatically over a conversation](https://arxiv.org/abs/2601.20834)
*Andrew Kyle Lampinen,Yuxuan Li,Eghbal Hosseini,Sangnie Bhardwaj,Murray Shanahan*

Main category: cs.CL

TL;DR: 语言模型中的线性表示会随着对话内容动态变化，影响对信息属性的表达，如事实性辨析。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型表示中线性方向的动态变化，了解模型在对话中如何适应和调整对信息的编码。

Method: 分析不同模型及其层级中的线性表示在模拟对话中的变化，比较真实与复述对话的影响，并检验通过调整表示方向对对话的引导效果。

Result: 线性表示随对话显著变化，尤其是对话相关信息，事实性表示可逆；这种变化在不同模型、层次以及非策略对话中均存在，但与上下文故事框架无直接关系。调节表示方向在对话不同阶段效果差异大。

Conclusion: 模型表示的动态变化挑战了静态特征解释和一致性假设，但为研究模型上下文适应性开辟了新方向。

Abstract: Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.

</details>


### [74] [When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation](https://arxiv.org/abs/2601.20858)
*David Tan,Pinzhen Chen,Josef van Genabith,Koel Dutta Chowdhury*

Main category: cs.CL

TL;DR: 本文通过FLORES-200基准检测7-8B参数的多语言大语言模型，发现Bloomz模型存在基准数据泄露，导致成绩虚高且显示出跨语言迁移的记忆现象。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言翻译任务中可能因基准数据泄露而表现虚假提升，掩盖了模型的记忆行为，亟需识别和分析这种现象。

Method: 选取Bloomz和Llama两款7-8B参数的多语言LLM，利用FLORES-200翻译基准，分析Bloomz的基准泄露及其对跨语言翻译性能的影响，同时通过改变源语言表述（如同义改写、实体替换）检测记忆特征。

Result: Bloomz确实存在基准数据泄露，且记忆能跨方向迁移，导致未见翻译方向表现异常提升。替换命名实体虽然降低了BLEU得分，但回忆记忆参考文献依然较强，表明命名实体替换是检测记忆的重要手段。

Conclusion: 基准数据泄露导致的记忆现象广泛存在且跨语言迁移，命名实体替换等源语言扰动方法有效揭示了模型的记忆能力，提示未来评测需警惕数据污染问题。

Abstract: Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to "uncontaminated" languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz's FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [75] [Game-Theoretic Autonomous Driving: A Graphs of Convex Sets Approach](https://arxiv.org/abs/2601.20054)
*Nikolaj Käfer,Ahmed Khalil,Edward Huynh,Efstathios Bakolas,David Fridovich-Keil*

Main category: cs.MA

TL;DR: 本文提出了IBR-GCS方法，将多车道高速公路驾驶建模为非合作博弈，通过图凸集框架实现车辆的组合机动推理和轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 多车自动驾驶涉及策略交互和混合动作规划，现有方法难以高效融合组合推理和博弈互动。

Method: 基于图凸集，构建车辆依赖于其他车辆策略的特定图，在车辆迭代最优响应更新中求解最短路径的凸松弛问题，无需离散树搜索。

Result: 仿真中IBR-GCS在多车多车道场景下产生安全轨迹和策略一致的交互行为，展现了高效且符合策略博弈特性的规划能力。

Conclusion: IBR-GCS通过迭代最优响应配合图凸集，有效实现了多车高速公路驾驶的策略性互动与轨迹规划，保证收敛至近似纳什均衡。

Abstract: Multi-vehicle autonomous driving couples strategic interaction with hybrid (discrete-continuous) maneuver planning under shared safety constraints. We introduce IBR-GCS, an Iterative Best Response (IBR) planning approach based on the Graphs of Convex Sets (GCS) framework that models highway driving as a generalized noncooperative game. IBR-GCS integrates combinatorial maneuver reasoning, trajectory planning, and game-theoretic interaction within a unified framework. The key novelty is a vehicle-specific, strategy-dependent GCS construction. Specifically, at each best-response update, each vehicle builds its own graph conditioned on the current strategies of the other vehicles, with vertices representing lane-specific, time-varying, convex, collision-free regions and edges encoding dynamically feasible transitions. This yields a shortest-path problem in GCS for each best-response step, which admits an efficient convex relaxation that can be solved using convex optimization tools without exhaustive discrete tree search. We then apply an iterative best-response scheme in which vehicles update their trajectories sequentially and provide conditions under which the resulting inexact updates converge to an approximate generalized Nash equilibrium. Simulation results across multi-lane, multi-vehicle scenarios demonstrate that IBR-GCS produces safe trajectories and strategically consistent interactive behaviors.

</details>


### [76] [Interpreting Emergent Extreme Events in Multi-Agent Systems](https://arxiv.org/abs/2601.20538)
*Ling Tang,Jilin Mei,Dongrui Liu,Chen Qian,Dawei Cheng,Jing Shao,Xia Hu*

Main category: cs.MA

TL;DR: 本文提出了首个用于解释多智能体系统中突发极端事件的框架，通过改进Shapley值方法，追踪极端事件的起源、驱动力和贡献行为，并通过量化不同时间、代理和行为维度的风险贡献，提升系统安全性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统内部复杂交互常导致难以解释的极端事件，理解这些事件的起因对于保证系统安全至关重要。

Method: 本文将Shapley值应用于多智能体系统中，分配每个动作在不同时间步对极端事件的归因分数，随后沿时间、代理和行为维度聚合，量化各维度的风险贡献，并设计指标描述极端事件特征。

Result: 在经济、金融和社会等多种多智能体系统场景中，实验证明该框架有效揭示极端现象的起因和特征，并提供通用见解。

Conclusion: 该框架首次系统性解释多智能体系统中的极端事件，有助于理解系统内复杂交互导致的风险，提升系统安全管理水平。

Abstract: Large language model-powered multi-agent systems have emerged as powerful tools for simulating complex human-like systems. The interactions within these systems often lead to extreme events whose origins remain obscured by the black box of emergence. Interpreting these events is critical for system safety. This paper proposes the first framework for explaining emergent extreme events in multi-agent systems, aiming to answer three fundamental questions: When does the event originate? Who drives it? And what behaviors contribute to it? Specifically, we adapt the Shapley value to faithfully attribute the occurrence of extreme events to each action taken by agents at different time steps, i.e., assigning an attribution score to the action to measure its influence on the event. We then aggregate the attribution scores along the dimensions of time, agent, and behavior to quantify the risk contribution of each dimension. Finally, we design a set of metrics based on these contribution scores to characterize the features of extreme events. Experiments across diverse multi-agent system scenarios (economic, financial, and social) demonstrate the effectiveness of our framework and provide general insights into the emergence of extreme phenomena.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [77] [Achieving Productivity Gains with AI-based IDE features: A Journey at Google](https://arxiv.org/abs/2601.19964)
*Maxim Tabachnyk,Xu Shu,Alexander Frömmgen,Pavel Sychev,Vahid Meimand,Ilia Krets,Stanislav Pyatykh,Abner Araujo,Kristóf Molnár,Satish Chandra*

Main category: cs.SE

TL;DR: 谷歌开发和优化了基于AI的代码补全与自然语言驱动的代码转换工具，通过解决延迟、用户体验和建议质量等挑战，提升了企业环境中的开发者生产力。


<details>
  <summary>Details</summary>
Motivation: 提升开发者的编程效率和体验，解决现有AI开发工具在延迟、用户体验和建议质量上的不足。

Method: 深入优化用户界面、后端架构及AI模型，通过严谨的实验验证改进效果。

Result: 显著提高了代码补全和代码转换功能的响应速度和建议准确度，增强了用户体验。

Conclusion: 多层次改进AI开发工具可以在企业环境中实现实质性的生产力提升。

Abstract: We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.

</details>


### [78] [Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis](https://arxiv.org/abs/2601.20103)
*Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.SE

TL;DR: 本文提出了一个包含54类奖励漏洞的分类体系和一个包含517条测试轨迹的基准数据集TRACE，用于评估代码生成中强化学习模型对奖励篡改的检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）越来越多地作为代码生成强化学习中的评估者，检测奖励篡改的重要性日益突出，但其检测能力尚未充分研究。

Method: 作者设计了一个奖励漏洞的分类体系，构建了包含真实与合成数据的TRACE基准，并采用对比异常检测方法评估模型在奖励篡改检测上的表现。

Result: 实验表明，在对比异常检测设置中，模型比孤立分类更有效地捕捉奖励篡改，GPT-5.2在最高推理模式下检测率提升至63%。同时，模型在语义上下文的奖励篡改上表现较差。

Conclusion: TRACE基准及评估工具的发布为社区提供了检测奖励篡改的标准平台，有助于推动相关模型能力的提升和扩展研究。

Abstract: Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.

</details>


### [79] [Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents](https://arxiv.org/abs/2601.20106)
*Shamse Tasnim Cynthia,Joy Krishan Das,Banani Roy*

Main category: cs.SE

TL;DR: 本文通过对9427个由自主AI代理生成的合并请求（PR）进行实证分析，揭示了核心开发者与边缘开发者在使用、审查、修改和验证AI代理贡献过程中的差异。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理在软件开发中的广泛应用，不同经验背景的开发者如何协同使用AI工具尚不清晰，亟需实证研究揭示其协作机制。

Method: 作者采集并分析了9427个代理生成的PR，结合定性与定量研究方法，比较核心与边缘开发者在代理使用、评审、修改和验证PR的行为差异。

Result: 边缘开发者更频繁使用AI代理，任务分布均衡；核心开发者更关注文档和测试，其代理PR更常被合并。核心开发者更积极参与代码审查并要求持续集成检查。代理PR较少被修改，偶尔会进行重构。

Conclusion: 开发者经验显著影响AI代理贡献的整合方式，理解这些差异有助于核心与边缘开发者更有效地与自主编码代理协作，提高开发效率。

Abstract: Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.

</details>


### [80] [Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests](https://arxiv.org/abs/2601.20109)
*Shamse Tasnim Cynthia,Al Muttakin,Banani Roy*

Main category: cs.SE

TL;DR: 本文分析了AI生成的修复性拉取请求（PR）合并后的代码质量，发现合并成功并不代表代码质量高，强调了系统性质量检查的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI编程代理的普及使得大量自动生成的PR被合并，但其合并后的代码质量尚未得到充分研究。

Method: 基于AIDev数据集中的1,210个Python仓库合并的AI生成的修复性PR，使用SonarQube对比合并前后的代码质量问题，分析问题频率、密度、严重程度及规则级别的分布。

Result: 不同代理生成的PR原始问题数量差异在按代码变动量归一化后消失，代码异味占主导，尤其是严重程度高的类别，漏洞较少但多为严重问题。

Conclusion: PR合并成功不能可靠反映合并后代码质量，强调为AI生成的修复性PR建立系统性的质量检测机制的重要性。

Abstract: The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.

</details>


### [81] [Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study](https://arxiv.org/abs/2601.20112)
*Maja Vukovic,Rangeet Pan,Tin Kam Ho,Rahul Krishna,Raju Pavuluri,Michele Merler*

Main category: cs.SE

TL;DR: 本文调查了57名开发者以及35份用户调查，探讨了AI编程助手和代码大语言模型(CodeLLMs)在真实企业项目中的应用现状及影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型推动软件工程任务自动化，企业中对这些工具在实际项目中的适用性和影响产生疑问。

Method: 通过问卷调查57名不同领域和技能层次的软件开发者，同时回顾分析35份用户调查报告。

Result: 总结了开发者对AI编程助手和CodeLLMs的使用体验、需求和期望，发现其在企业应用中仍有挑战需解决。

Conclusion: 提出了AI编程助手的使用要求，强调需要进一步改进以满足企业级软件开发的实际需要。

Abstract: The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.

</details>


### [82] [Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization](https://arxiv.org/abs/2601.20147)
*Saima Afrin,Zaiyu Cheng,Tushar Sharma,Alexander Serebrenik,Massimiliano Di Penta,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 本文系统评估了系统提示词的细节程度、模型规模、提示策略和编程语言对指令调优语言模型（ILMs）和代码语言模型（CLMs）在代码生成任务中的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管ILMs和CLMs在代码生成方面表现卓越，但系统提示词对其性能的影响尚未被充分研究。

Method: 设计评估框架，涵盖120种模型配置，考察系统提示词细节、模型规模、提示策略（零样本与少样本）和编程语言对模型表现的影响。

Result: 发现系统提示词的影响随着模型规模增大而增强，少样本提示减少了系统提示词的影响，且Java对系统提示词变化更敏感于Python。

Conclusion: 系统提示词是影响代码生成性能的重要因素，其效果受模型规模、提示方式和编程语言影响，未来代码生成模型设计需重视提示词优化。

Abstract: Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.

</details>


### [83] [LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis](https://arxiv.org/abs/2601.20148)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: LogSieve是一种针对持续集成（CI）日志的轻量级日志压缩技术，能够在保留重要语义信息的前提下，大幅减少日志数据量，提高分析效率并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 由于CI日志的数量庞大且冗长，手工检查和自动分析成本高、耗时且环境成本大，现有方法多针对结构化日志，难以有效处理CI中的非结构化、嘈杂且冗长的日志。

Method: 提出LogSieve方法，通过识别和过滤低信息量日志行，同时保留关键语义内容，达到日志压缩的目的。利用嵌入式分类器自动检测相关性，实现高精度语义过滤。

Result: 在20个开源Android项目的CI日志中，LogSieve平均减少42%的日志行和40%的标记，语义损失极小。相较于结构优先的基线方法，LogSieve在语义和类别保真度上明显优越，自动相关性检测准确率达97%。

Conclusion: LogSieve有效降低了LLM推理中的数据处理量，减少计算成本和能耗，实现了日志管理与大规模语言模型推理的结合，为绿色环保且可解释的CI自动化提供了实用方案。

Abstract: Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.
  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.
  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.

</details>


### [84] [Cascaded Vulnerability Attacks in Software Supply Chains](https://arxiv.org/abs/2601.20158)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: 本论文提出了一种基于SBOM的异构图注意力网络（HGAT）方法，用于识别软件组件中的漏洞，并通过多层感知机预测多漏洞级联链，实现对软件供应链安全的更加准确分析。


<details>
  <summary>Details</summary>
Motivation: 当前安全分析工具通常孤立地评估漏洞，忽视依赖组件间的多级漏洞链条，而SBOM生成和分析工具的下游漏洞结果存在较大差异，因此需要一种统一且更准确的分析方法。

Method: 将丰富的SBOM表示为包含组件、依赖关系、已知漏洞和安全弱点的异构图，通过训练HGAT模型预测组件是否存在漏洞，并用多层感知机神经网络解决稀缺的多漏洞级联链发现问题，将其视为CVE对间的链路预测问题。

Result: HGAT组件分类器实现了91.03%的准确率和74.02%的F1分数，验证了该方法在漏洞识别上的有效性。

Conclusion: 基于异构图注意力网络的SBOM驱动安全分析显著提升了对多级漏洞链的识别能力，有助于更全面评估软件供应链的安全风险。

Abstract: Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%.

</details>


### [85] [How do Agents Refactor: An Empirical Study](https://arxiv.org/abs/2601.20160)
*Lukas Ottenhof,Daniel Penner,Abram Hindle,Thibaud Lutellier*

Main category: cs.SE

TL;DR: 本文首次分析了软件开发代理在Java代码重构中的表现，发现代理重构主要集中在注解变更，而开发者则更注重结构性改进。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究评估了代码补全和任务自动化代理的能力，但缺乏关于这些代理在Java重构中的实际表现、变更类型及其对代码质量影响的研究。

Method: 通过分析86个项目中代理和开发者提交的重构pull requests，使用RefactoringMiner和DesigniteJava 3.0检测重构类型和代码气味变化。

Result: 发现代理重构主要为注解相关的变更，而开发者则更注重多样的结构性改进；其中只有Cursor模型的重构导致代码气味显著增加。

Conclusion: 代理和开发者在Java重构方式上存在明显差异，代理主要聚焦注解修改，且部分代理可能增加代码气味，提示未来需提升代理的重构能力和代码质量保障。

Abstract: Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.

</details>


### [86] [Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests](https://arxiv.org/abs/2601.20171)
*Kazuma Yamasaki,Joseph Ayobami Joshua,Tasha Settewong,Mahmoud Alfadel,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: AI代理在软件工程3.0中大量参与文档相关工作，但其贡献的文档审核较少，带来质量保障和人机协作的新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程进入3.0阶段，AI代理越来越多地参与开发任务，特别是文档工作，理解AI贡献及人类审查方式对风险评估十分重要。

Method: 基于AIDev数据集，分析了1997个由AI代理和人类开发者提交的文档相关PR，比较AI和人类在文档工作中的贡献与审查行为。

Result: AI代理提交的文档相关PR数量明显多于人类，且这些文档修改往往很少被人类后续修改或审查。

Conclusion: 虽然AI代理在文档工作中贡献显著，但文档质量保证和人机协作存在隐忧，需要进一步探索改进策略。

Abstract: As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.
  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.

</details>


### [87] [Control Models for In-IDE Code Completion](https://arxiv.org/abs/2601.20223)
*Aral de Moor,Yana Hrynevich,Hleb Badzeika,Vladyslav Furda,Marko Kojic,Artem Savelev,Kostadin Cvejoski,Darya Rovdo,Ekaterina Garanina*

Main category: cs.SE

TL;DR: 本文提出了在JetBrains集成开发环境中使用机器学习分类器控制大语言模型驱动的代码补全，提升补全质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码补全存在触发频繁及建议不精准的问题，影响用户体验和效率。

Method: 设计了基于boosting和Transformer的机器学习分类器来控制补全触发和过滤生成的建议，在离线真实代码补全数据集上进行评估，并在多语言环境及生产环境中进行了效果验证。

Result: boosting模型在多种语法差异语言上的离线分类表现良好，在线A/B测试显示改进了代码补全的效率和质量指标。

Conclusion: 辅助模型可实现更智能的IDE内LLM功能集成，提升用户体验，未来有广阔的研究和优化空间。

Abstract: We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.

</details>


### [88] [Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development](https://arxiv.org/abs/2601.20240)
*Anthony Peruma,Truman Choy,Gerald Lee,Italo De Oliveira Santos*

Main category: cs.SE

TL;DR: 本文通过对75名npm包开发者的调查，分析了他们对安全的认知、实践、障碍及改进建议，揭示了当前安全工具不足和开发者安全意识现状。


<details>
  <summary>Details</summary>
Motivation: 近年来第三方npm包的安全漏洞频发，危及应用安全，研究旨在了解开发者如何看待并应对安全问题，为提升npm生态安全提供依据。

Method: 通过在线问卷调查75名npm包开发者，采用混合方法分析他们的安全认知、实践、障碍及改进建议。

Result: 开发者重视安全但认为包安全性中等，担忧供应链攻击和依赖漏洞；40%满意现有工具，偏好自动化方法；面临时间限制和误报多等问题；建议提升检测工具、文档、账户保护和安全教育。

Conclusion: 研究强调npm开发者面临的安全挑战，促进最佳安全实践讨论，为增强npm生态的安全性和可信度提供参考。

Abstract: Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.

</details>


### [89] [How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective](https://arxiv.org/abs/2601.20382)
*Klara Borowa,Andrzej Zalewski,Lech Madeyski*

Main category: cs.SE

TL;DR: 本文从波兰视角出发，通过社区调查分析，揭示了软件工程领域研究与产业脱节的问题，特别是对小经济体的不利影响，提出了改善建议


<details>
  <summary>Details</summary>
Motivation: 小经济体、特别是非英语国家的软件工程研究者在社区中是有价值的少数群体，他们面临研究与工业之间日益扩大的鸿沟问题

Method: 通过反思性主题分析方法分析ICSE FOSE社区的调查数据

Result: 指出研究与工业之间的差距尤其影响小社区和本地小公司

Conclusion: 提出了一系列针对小经济体促进软件工程研究与工业合作的改进建议

Abstract: The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.

</details>


### [90] [Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments](https://arxiv.org/abs/2601.20394)
*Giovanna Broccia,Maurice H. ter Beek,Walter Cazzola,Luca Favalli,Francesco Bertolotti,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文通过实验评估了语言工作台Neverlang的元语言的可理解性及用户接受度，发现用户对元语言语法有较好理解，认同其有用性且倾向使用，但易用性仍需改进。


<details>
  <summary>Details</summary>
Motivation: 语言工作台作为新兴工具，其元语言的可理解性及用户接受度是影响其采纳的关键因素，但现有研究较少关注这些用户中心的评估维度。

Method: 采用定制版方法评价模型（MEM），通过三轮学术界参与者实验，评估Neverlang元语言及程序的可理解性，测量感知易用性、感知有用性及使用意图，并分析这些维度间的关系。

Result: 用户对Neverlang元语言的语法理解良好，感知其有用性并有使用意愿，但感知易用性存在不足。感知易用性和有用性对使用意图有显著影响，但可理解性与用户接受度无显著相关。

Conclusion: 元语言的高可理解性并不必然带来更高的用户接受度，表明理解与采纳之间存在复杂关系，未来需重视提高工具易用性以促进其广泛应用。

Abstract: Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.

</details>


### [91] [On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents](https://arxiv.org/abs/2601.20404)
*Jai Lal Lulla,Seyedmoein Mohsenimofidi,Matthias Galster,Jie M. Zhang,Sebastian Baltes,Christoph Treude*

Main category: cs.SE

TL;DR: 本文研究了AGENTS.md文件对AI编码代理在GitHub拉取请求上的运行时间和代币消耗的影响，发现该文件能显著提高代理的运行效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理在软件仓库的自主管理应用增加，鲜有研究关注仓库级别的配置文件对其操作效率的影响。

Method: 选取10个仓库和124个拉取请求，在有无AGENTS.md两个条件下执行AI编码代理，测量其执行时间和代币消耗。

Result: AGENTS.md的存在使中位运行时间降低28.64%，输出代币消耗减少16.58%，且任务完成行为相当。

Conclusion: AGENTS.md文件对提高AI编码代理的效率具有显著作用，未来应重视仓库级指令在AI代理行为及集成中的应用与研究。

Abstract: AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS$.$md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS$.$md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS$.$md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.

</details>


### [92] [An Empirical Evaluation of Modern MLOps Frameworks](https://arxiv.org/abs/2601.20415)
*Jon Marcos-Mercadé,Unai Lopez-Novoa,Mikel Egaña Aranguren*

Main category: cs.SE

TL;DR: 本文实证评估了四种常用的MLOps工具（MLflow, Metaflow, Apache Airflow, Kubeflow Pipelines），通过两个机器学习场景测试其安装便利性、配置灵活性、互操作性、代码复杂度、结果可解释性和文档质量，给出加权评分，指导开发者选择合适工具。


<details>
  <summary>Details</summary>
Motivation: 随着AI解决方案在专业环境的广泛采用，开发者需要对现有的工具生态有清晰认识，做出合理选择以便高效管理ML模型生命周期。

Method: 选取MLflow、Metaflow、Apache Airflow和Kubeflow Pipelines四款工具，基于数字识别（MNIST）和情感分类（IMDB+BERT）两个典型场景，评估多个维度指标并加权整合结果。

Result: 通过多维度加权评估，得出各工具在不同使用场景中的优劣表现，为开发者提供实用的参考和工具选择建议。

Conclusion: 针对不同的机器学习任务和应用需求，本文提供了基于实证结果的MLOps工具选型指南，助力开发者更好地管理和部署机器学习模型。

Abstract: Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.

</details>


### [93] [Challenges in Android Data Disclosure: An Empirical Study](https://arxiv.org/abs/2601.20459)
*Mugdha Khedkar,Michael Schlichtig,Mohamed Soliman,Eric Bodden*

Main category: cs.SE

TL;DR: 本文通过调查和分析开发者在填写谷歌Play商店数据安全部分（DSS）表单时的经验，揭示了开发者在隐私数据分类和报告中的困难和信心不足，强调了提供更明确指导和辅助工具的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前法律要求Android开发者准确报告应用收集的数据，但大规模代码库使得报告过程复杂，本文旨在了解开发者在Google Play Store DSS表单填写过程中的体验和挑战。

Method: 作者通过对41名Android开发者的问卷调查，和分析172个在线开发者讨论，收集共计683名开发者的观点，研究开发者如何分类隐私数据及其填写DSS表单的信心和困难。

Result: 结果显示开发者常常手动分类隐私数据，甚至有时遗漏分类，依赖网络资源完成表单，尽管识别数据较有信心，但对符合DSS要求的准确披露信心不足，面临数据识别、表单理解及担忧应用被拒等问题。

Conclusion: 研究表明，当前开发者面临的隐私报告障碍凸显了提供更清晰指导和更便捷工具的必要性，以帮助其更好地履行隐私报告义务。

Abstract: Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.
  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.
  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.
  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.

</details>


### [94] [DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning](https://arxiv.org/abs/2601.20615)
*Yanlin Wang,Jiadong Wu,Tianyue Jiang,Mingwei Liu,Jiachi Chen,Chong Wang,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出了一种针对基于检索增强生成(RAG)的大型语言模型代码生成系统的对抗攻击方法DrainCode，旨在通过污染检索上下文使模型产出更长的代码，从而显著提高计算延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 当前在安全领域中，针对大型语言模型推理阶段计算成本（延迟和能耗）的研究较少，本文旨在填补这一空白，评估和提升这类系统的计算效率风险。

Method: DrainCode通过基于变异的策略对检索上下文进行污染，诱使模型生成更长的输出来增加GPU延迟和能源消耗。

Result: 在多种模型和不同提示策略下，DrainCode使延迟提升最高达85%，能耗增加49%，输出长度则超过3倍，且在多种防御措施下依然有效。

Conclusion: DrainCode是一种有效提升大型语言模型计算开销的攻击方法，有助于在资源受限环境下评估模型安全性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.

</details>


### [95] [Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model](https://arxiv.org/abs/2601.20662)
*Julien Malka,Arnout Engelen*

Main category: cs.SE

TL;DR: 本文提出了Lila，一种针对功能性包管理模型的去中心化再现性监控系统，用于解决大规模软件集合中构建产物的一致性验证问题。


<details>
  <summary>Details</summary>
Motivation: 软件构建产物的完整性保障日益重要，但大规模实现高再现性率和监控基础设施的挑战仍未解决。

Method: 设计Lila系统，实现构建结果的分布式报告和再现性数据库的聚合，支持大规模监控。

Result: Lila系统能够有效支持功能性包管理环境下的再现性评估，促进实务和实证研究。

Conclusion: Lila为构建产物再现性监控提供了可扩展、去中心化的解决方案，推动软件透明度和信任的提升。

Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.

</details>


### [96] [ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler](https://arxiv.org/abs/2601.20755)
*Bohua Zou,Debayan Roy,Dhimankumar Yogesh Airao,Weihao Xu,Binqi Sun,Yutao Liu,Haibo Chen*

Main category: cs.SE

TL;DR: 本文提出了基于eBPF技术的细粒度、非侵入式LLM推理分析框架，实现对推理过程的实时可视化和性能诊断。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统缺乏对操作级别的可见性，导致开发者无法清楚了解推理过程中的资源消耗和性能瓶颈，难以进行优化。

Method: 利用扩展的Berkeley Packet Filter (eBPF)技术，在不修改或重新编译源码的情况下，动态挂载探针采集多层运行时函数的调用数据，生成操作符、图结构、时间线和硬件计数器趋势的丰富可视化信息。

Result: 框架在llama.cpp等推理系统上测试，开销低于4%，且具备高精度的性能分析能力，能揭示密集推理、专家混合路由和操作符卸载的实际行为。

Conclusion: 该分析框架有效提升了LLM推理过程的透明度和可诊断性，转变性能分析为优化、调度和资源感知部署的实用工具。

Abstract: As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.

</details>


### [97] [Context-Augmented Code Generation Using Programming Knowledge Graphs](https://arxiv.org/abs/2601.20810)
*Shahd Seddik,Fahd Seddik,Iman Saberi,Fatemeh Fard,Minh Hieu Huynh,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 本文提出了基于语义表示的编程知识图（PKG）来提升代码和文本的精细检索，结合树剪枝和重新排序机制，显著提升了复杂问题的代码生成准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂代码生成任务中表现不佳，检索增强生成方法（RAG）虽然能引入外部知识，但存在检索遗漏和生成内容虚假的问题。

Method: 通过构建编程知识图实现语义结构化和细粒度检索，利用树剪枝提高检索精度，并通过重新排序机制整合非RAG解法减少虚假生成。

Result: 在HumanEval和MBPP数据集上，PKG方法在pass@1准确率上提升了20%，MBPP数据集上提升了34%，同时对无需RAG的正确解影响极小。

Conclusion: PKG结合重排序机制有效解决了复杂代码生成中的检索与虚假生成问题，显著提升了生成模型的性能和准确性。

Abstract: Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph

</details>
