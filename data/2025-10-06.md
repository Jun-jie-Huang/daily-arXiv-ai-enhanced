<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: 本文提出了一种名为AMANDA的无训练医疗知识增强框架，用于提升医疗多模态大语言模型在低资源环境下的医疗视觉问答能力，解决了内在和外在推理瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型在缺乏丰富标注数据的低资源条件下表现不佳，主要因其推理能力不足：忽视医疗图像细节和缺乏专业医学知识整合。

Method: 提出AMANDA框架，通过无需训练的代理方式增强医疗知识。内部通过问题的粗到细分解实现全面诊断，外部则利用生物医学知识图谱检索增强推理过程。

Result: 在八个医疗视觉问答基准测试中，AMANDA在零样本和少量样本设置下均显著提升了模型表现。

Conclusion: AMANDA有效解决了医疗多模态语言模型在低资源环境中推理能力的瓶颈，显著提升了医疗视觉问答的性能。代码已开源。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [2] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: CASAL提出了一种高效算法，通过将激活引导机制融入模型权重，显著减少大语言模型的幻觉现象，提高模型在短问答任务中的准确率，同时兼具计算和数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在回答中产生幻觉（错误自信回答）的问题，且现有激活引导方法需要实时监控和干预，不适合实际部署。

Method: CASAL通过对单个Transformer层的子模块进行轻量级训练，将激活引导机制内嵌到模型权重中，实现模型自主判断是否应回答问题。

Result: 在多个短问答基准测试中，CASAL减少了30%-40%的幻觉，计算效率提升30倍，数据效率提升20倍，并能有效推广至分布外领域和多模态模型。

Conclusion: CASAL为将可解释性激活引导方法实用化提供了一条有效路径，适用于稠密和Mixture-of-Experts模型，有望推动大规模语言模型在生产环境中的可靠应用。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [3] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY是一种结合有限状态机和大语言模型的临床助手平台，能够实现患者到专科医生的智能分诊和病情评估。


<details>
  <summary>Details</summary>
Motivation: 提高患者分诊准确率和咨询效率，减少依赖人力资源，提升医疗流程的智能化水平。

Method: 构建了一个混合架构平台，结合有限状态机管理对话流程与大语言模型分析症状，采用模块化微服务框架保障系统性能与可扩展性。

Result: 该系统已在全国大型医院IT平台成功部署，两个月内完成超过55000次对话，经过专家标注验证，其首次分诊准确率超过人类，咨询耗时仅为人类的三分之一。

Conclusion: CLARITY证明了AI驱动的临床助手在提升医疗分诊效率和精度方面的巨大潜力，具备推广应用价值。

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [4] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: 本文提出了基于有限状态机控制的GPT研究助手RA-FSM，以提升文献综合的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在文献综合中存在幻觉和引用错误问题，限制了其在专业领域工作的实用性。

Method: RA-FSM采用有限状态控制循环（相关性->置信度->知识），结合向量检索和确定性引用管道，过滤无关查询、评分可回答性、分解问题并触发检索，输出带有置信标签和去重引用的答案。系统通过分层知识库构建涵盖多种文献来源。

Result: 在光子学领域六类任务中，专家盲测结果表明RA-FSM优于强基线模型，表现出更好的边界条件处理和证据使用；覆盖和新颖性分析显示其能探索更广泛内容但带来可调节的延迟和成本。

Conclusion: RA-FSM设计强调透明和有据可查的答案，适合高风险技术工作且可推广至其他科学领域。

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [5] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 本文提出了一种结合实时语音到语音转换模型与大规模语言模型(LLM)的混合架构，实现低延迟同时具备丰富知识的对话回应。


<details>
  <summary>Details</summary>
Motivation: 实时S2S模型响应速度快但缺乏深层语义理解，级联系统知识丰富但延迟高，影响对话流畅性，亟需兼顾两者优点的方案。

Method: 设计一个框架，用户语音首先经过S2S模型生成快速响应，同时将查询传递给后端LLM，LLM的文本响应实时注入S2S模型指导语音生成，从而融合丰富知识且保持低延迟。

Result: 在基于MT-Bench的多轮问答评测中，该方法在响应正确性上显著优于基础S2S模型，接近级联系统表现，且延迟保持在基础模型水平。

Conclusion: 通过混合架构实现了知识丰富与低延迟的平衡，提升了实时语音对话系统的实用性和响应质量。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [6] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: 本文提出了SelfJudge，通过自监督训练判别器，提升了推测解码的速度和准确性，实现了更广泛的自然语言处理任务加速。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法依赖于人工标注或可验证的任务，限制了方法的通用性和应用范围。

Method: SelfJudge通过自监督方式训练判别器，评估替换后的响应是否保持原始语义，从而实现自动化和广泛的验证器训练。

Result: 实验表明，SelfJudge在推理速度和准确性权衡上优于现有的判别解码基线方法。

Conclusion: SelfJudge提供了一种通用有效的加速大语言模型推理的方法，适用于多种自然语言处理任务。

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [7] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: 本文提出了EntropyLong方法，通过基于熵的不确定性评估，构造带有验证长距离依赖的数据集，提升语言模型长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的长上下文语言模型训练方法常用文本拼接或启发式策略，难以保证真实的长距离依赖，影响模型捕捉长程信息的能力。

Method: EntropyLong利用预测熵识别文档中高不确定性位置，从大规模语料检索语义相关上下文，验证这些上下文是否降低预测熵，确保依赖关系的真实性，将原文与验证的上下文结合构造训练样本。

Result: 在FineWebEdu和Cosmopedia上构建了含128K长度序列的验证依赖数据集，训练模型在RULER基准测试中表现显著提升，特别是远距离信息任务。经指令微调后，模型在LongBenchv2中也获得显著进步。

Conclusion: 基于熵的验证机制是构建有效长距离依赖训练数据的关键方法，有助于提升语言模型的长上下文理解能力，验证了EntropyLong方法的有效性和必要性。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [8] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: 本文提出了一种结合行为模拟器和语言模型提示生成用户行为一致的自然对话数据的方法，以解决对话推荐系统训练数据缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 由于公共对话推荐系统数据稀缺，难以对语言模型进行微调以适应对话推荐系统，且现有基于语言模型的用户模拟器生成的对话行为不一致。

Method: 使用行为模拟器结合语言模型提示生成与用户内在状态一致的自然对话，从而得到大型开源数据集，包含偏好引导和示例批评。

Result: 生成的数据集通过评分者评估，表现出较高的一致性、事实准确性和自然性。

Conclusion: 该方法有效提升了生成对话的行为一致性和自然性，为训练基于语言模型的对话推荐系统提供了有价值的数据支持。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [9] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 该论文提出了一种名为look-ahead Sync的新方法，解决了神经语言隐写中因分词歧义带来的解码失败和容量受限问题，显著提升了嵌入容量与安全性。


<details>
  <summary>Details</summary>
Motivation: 现代分词器的分词歧义导致神经语言隐写中解码失败，现有方法SyncPool虽解决了同步问题，但牺牲了嵌入容量。

Method: 提出look-ahead Sync方法，仅对真正无法区分的分词序列进行同步采样，同时保留其他可区分路径以最大化嵌入容量，并提供安全性理论证明。

Result: 在英（Llama 3）中嵌入率提升超过160%，中（Qwen 2.5）中提升超过25%，方法接近理论容量上限且明显优于SyncPool。

Conclusion: 该方法为实现高容量且可证明安全的实际语言隐写技术迈出了重要一步。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [10] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 本文发布了包含语义丰富信息的两个人类轨迹数据集，涵盖两大城市，并结合了合成社交媒体文本，支持多模态和语义移动分析。


<details>
  <summary>Details</summary>
Motivation: 传统的轨迹数据缺乏语义和多模态信息，限制了行为建模和移动性分析的深度。本文旨在提供融合真实轨迹与结构化语义信息的公开数据资源。

Method: 通过公开GPS轨迹数据，结合停留点、交通方式、兴趣点、天气信息及由大型语言模型生成的社交媒体文本，构建多层语义丰富的数据集，支持RDF格式实现语义推理。

Result: 构建了覆盖巴黎和纽约两大城市的开放数据集，含多模态数据及语义信息，且数据集和数据处理流程均开源，支持研究人员自定义和扩展。

Conclusion: 该资源首次结合真实轨迹、结构化语义增强、LLM生成文本和语义Web标准，为多模态语义移动研究提供了可重用的框架和丰富的数据支持。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [11] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于模型激活空间表示及其梯度的新型高效诊断框架，用于分析和追踪大规模语言模型的不良行为，如有害内容生成和知识污染。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在实际应用中容易出现有害内容、事实错误和社会偏见等不良行为，且现有基于参数梯度的归因方法因噪声和计算复杂度问题难以有效诊断模型失败根源。

Method: 提出一种直接在模型激活空间操作的诊断方法，通过分析表示及其梯度，实现与训练数据的语义关联，从而对模型输出的负面行为进行归因和细粒度标注，包括样本级和词元级的因果识别。

Result: 方法在跟踪有害内容、检测后门中毒及知识污染等任务中表现优异，实现了准确的样本级和词元级不良行为归因，揭示了具体训练样本和短语对模型行为的因果影响。

Conclusion: 该研究提供了一种强有力的诊断工具，促进对大规模语言模型风险的理解、审计和缓解，推动其安全可靠应用。代码已开源。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [12] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: 本文提出了FormalML，这是一个基于Lean 4的子目标完成基准，旨在测试大型语言模型在填补复杂数学证明中的缺失步骤的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在形式定理证明中表现突出，但在协助数学家完成复杂证明中的未解决子目标方面的能力尚未充分研究。

Method: 引入FormalML基准，使用一种转换策略将过程性证明转换为声明式形式，提取了4937个涵盖优化和概率不等式的子问题，结合了前提检索和复杂的研究级上下文。

Result: 通过对先进证明器的评估，发现其在准确性和效率上存在持续的限制。

Conclusion: 现有大型语言模型基础的定理证明器在子目标完成任务中仍需提高，FormalML为此提供了有效的评估平台。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [13] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 首次构建库尔德语的语义文本相似度（STS）数据集，包含1万对句子，覆盖正式和非正式语域，并对多模型进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有STS资源主要集中在高资源语言，库尔德语等低资源语言缺乏相关数据和工具，制约其自然语言处理发展。

Method: 构建包含1万对句子的库尔德语STS数据集，涵盖不同语域，进行人工相似度标注；采用Sentence-BERT、多语种BERT及其他基线模型进行评测。

Result: 模型在库尔德语数据上表现具有竞争力，但受到库尔德语形态变化、正字法多样性及混合语言使用等因素挑战。

Conclusion: 所构建数据集和基线模型为库尔德语语义研究和低资源自然语言处理提供了标准化评测平台和研究起点。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [14] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: CRACQ是一个多维度评价框架，用于评估机器生成文本的五个特质，提供更稳定和可解释的自动化评价。


<details>
  <summary>Details</summary>
Motivation: 现有的自动评分多依赖单一分数，缺乏细致特质分析和广泛文本类型的适用性。

Method: CRACQ结合语言学、语义和结构信号，针对五个特质进行综合评分，训练数据包括合成的申请书，并在真实案例中验证。

Result: CRACQ在初步测试中表现出比大型语言模型直接评判更稳定和可解释的特质层面判断。

Conclusion: CRACQ提供了一种多维且可解释的自动化评价方法，尽管存在可靠性和领域适用范围的挑战。

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [15] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 提出了一种集成评估的强化学习框架用于长文本临床记录生成，提升了事实准确性和完整性，减少了训练成本。


<details>
  <summary>Details</summary>
Motivation: 自动化生成临床文档需要严格对齐完整性和事实凭据优先级，传统方法依赖人工参考或独立奖励模型存在局限。

Method: 结合群体相对策略优化（GRPO）和DocLens评价器，通过确定性、基于对话的奖励直接优化事实准确性和完整性，无需训练独立奖励模型。

Result: 方法显著提升临床笔记质量，降低训练成本，GPT-5定性评估显示事实性、完整性和简洁性均优于对比方法，减少遗漏和虚构内容。

Conclusion: 该框架适用于实际场景，可扩展为满足特定目标（如指南遵循、计费偏好），表现出保守的性能提升下界。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [16] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 本文研究了大语言模型不确定性量化（UQ）方法在论证型大语言模型（ArgLLMs）中的应用，发现在索引任务中直接提示的UQ策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型的可靠性需要有效的不确定性量化方法，尤其在解释性模型ArgLLMs的决策中尤为关键。

Method: 通过在ArgLLMs中整合多种LLM的UQ方法，并在声明验证任务中进行对比实验，评估各UQ方法的效果。

Result: 实验结果表明，直接提示作为一种简单的UQ策略，在ArgLLMs中表现优于更复杂的方法。

Conclusion: 直接提示作为UQ方法在论证型大语言模型中具有显著优势，为UQ方法的评价提供了一种新颖的实验范式。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [17] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 本文探讨通过提示机制让大语言模型模拟早期知识截止点，发现其对直接遗忘的事实知识有效，但对间接相关知识遗忘能力不足，提示未来需要更严谨的时间预测模型评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖预训练数据进行时间预测，可能因记忆而非推理导致泛化能力高估，如何让模型模拟早期知识截止以避免数据污染是本研究关注点。

Method: 构建三种评估数据集，分别考察模型对直接事实知识、语义变化和因果相关知识的遗忘能力，采用提示方法模拟早期知识截止。

Result: 提示方法能有效让模型遗忘直接查询的过期信息，但在模型未被直接提问但内容因果相关时，模型难以实现遗忘。

Conclusion: 当前提示模拟知识截止手段存在局限，尤其对于因果相关信息遗忘效果较差，强调需要更严谨的时间预测评估方法。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [18] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: 该论文提出了一种名为DRIFT的方法，利用真实用户的不满意信号优化大语言模型的性能，显著提升任务评分和评测胜率，超过现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 现实中用户的显式满意反馈稀缺，而不满意信号丰富且隐含用户偏好，现有偏好学习方法难以有效利用这些数据。

Method: DRIFT通过以不满意信号为锚，动态采样正向样本，结合迭代训练策略，实现模型优化。

Result: DRIFT在真实和合成数据集上均显著优于基线，并在大规模模型上超越GPT-4o-mini，且保持模型的探索能力和多样性。

Conclusion: DRIFT是一种有效且可扩展的后期训练方法，能充分利用丰富的不满意信号，实现大语言模型性能提升。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [19] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: 该论文提出了SIMPACT框架和BluePrint数据集，用于训练和评估大语言模型在模拟社交媒体用户行为方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准化数据资源以微调和评估大语言模型作为真实社交媒体代理的能力，影响了社交媒体行为模拟的研究。

Method: 开发了SIMPACT框架，用行为驱动的方法构建隐私保护的社交媒体数据集，并将下一步行为预测作为训练和评估任务，设计了群体与个体层面的评估指标。

Result: 发布了基于Bluesky公共数据构建的BluePrint大规模数据集，通过用户行为聚类形成不同角色，涵盖12类社交媒体互动类型，兼顾隐私保护与真实性。

Conclusion: SIMPACT及BluePrint为社交媒体行为模拟提供了标准化的数据和评价方法，有助于推进伦理负责的社交媒体模拟研究，尤其在政治话语的建模和相关领域挑战研究中具有应用价值。

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [20] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于动态专家聚类和结构化压缩的统一框架，有效解决了大规模Mixture-of-Experts语言模型中的负载不均、参数冗余和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 当前MoE大语言模型存在负载不均衡、参数冗余和通信开销高的三难困境，需提出一种能够同步改善这些问题的方法。

Method: 通过在线聚类周期性重新组合专家，利用参数和激活相似度度量稳定专家利用率；在聚类内分解专家权重为共享基矩阵和极低秩适配器实现参数压缩；采用两级层次路由策略减少路由搜索空间和通信；运用异构精度存储和动态离线集群降低内存占用。

Result: 在GLUE和WikiText-103数据集上，框架维持了与标准MoE模型相当的性能，同时参数减少约80%，吞吐量提升10%-20%，专家负载方差降低三倍以上。

Conclusion: 结构性重组为实现可扩展、高效且节省内存的MoE大型语言模型提供了有效策略。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [21] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: 本文研究了利用基于检索增强生成的技术和开源小型语言模型开发AI教学助手，在教育中实现个性化指导。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI和大型语言模型在教育领域的应用尚处于初期阶段，如何构建有效且可持续的AI助教成为挑战。

Method: 通过检索增强生成(RAG)流程，比较包括LLaMA 3.1、IBM Granite 3.3和Gemma 3等八个7-17亿参数的小型语言模型与领先的大型模型GPT-4o的表现。

Result: 结果表明，经过精心设计的提示和检索，小型语言模型在准确性和教学对齐度上可媲美大型模型。并且小模型计算资源和能耗低，支持消费级硬件实时运行。

Conclusion: 小型语言模型作为AI教学助手不仅低成本、保护隐私，还环保节能，适合教育机构实现个性化学习的可持续发展目标。

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [22] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 本文提出了mini-vec2vec，一种高效且稳定的线性文本嵌入对齐方法，在计算效率和结果质量上均优于原有的vec2vec方法。


<details>
  <summary>Details</summary>
Motivation: 原有的vec2vec方法虽能实现近乎完美的文本嵌入空间对齐，但计算成本高且不稳定，限制了其在实际应用中的推广。

Method: mini-vec2vec包括三个步骤：伪平行嵌入向量的初步匹配、线性变换拟合及迭代优化，确保对齐过程的简单与高效。

Result: 该线性方法在效率上比原始vec2vec提升了几个数量级，同时保持甚至超过其对齐效果。

Conclusion: mini-vec2vec稳定且可解释的算法步骤使其易于扩展，具备在更多领域应用的潜力。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [23] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文提出了LLMSQL，一个针对大语言模型时代改进和清洗的WikiSQL数据集版本，旨在解决原数据集的结构和注释问题。


<details>
  <summary>Details</summary>
Motivation: 原WikiSQL数据集存在大小写不一致、数据类型不匹配、语法错误和未回答问题等问题，影响了基于大语言模型的文本到SQL任务的研究和应用。

Method: 系统性分类错误，采用自动化方法进行数据清理和重新注释，使数据集适用于现代大语言模型。

Result: 评估了多个大语言模型在LLMSQL上的表现，证明其更适合大语言模型生成SQL，且数据更干净、规范。

Conclusion: LLMSQL不是简单更新WikiSQL，而是一个专为大语言模型设计的文本到SQL基准数据集，支持生成和评估现代NL2SQL模型。

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [24] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: 本论文研究了大型语言模型在政治言论中评估冒犯性的表现，强调模型需具备推理能力以更准确地反映不同政治和文化视角。


<details>
  <summary>Details</summary>
Motivation: 探讨如何使大型语言模型能够从特定政治和文化视角判断政治言论的冒犯性，提高其个性化和解释性能力。

Method: 利用2020年美国选举推文的多语种MD-Agreement数据子集，评估包括DeepSeek-R1, o4-mini, GPT-4.1-mini等多款大型语言模型，从极右、保守、中间派、进步主义等不同政治角色和英语、波兰语、俄语背景下判断推文冒犯性。

Result: 带有显式推理能力的大型模型在判断冒犯性时表现更一致，对意识形态和文化差异更敏感。较小模型则难以捕捉细微区别。推理能力显著提升了模型个性化和解释性判断。

Conclusion: 推理能力是提升大型语言模型在跨语言、跨意识形态的复杂社会政治文本分类中关键，促进更精准、个性化的冒犯性评估。

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [25] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 本文首次对语音大语言模型中的偏见进行了系统性评估，发现多轮对话和重复负反馈会加剧偏见，封闭源模型偏见较低，推荐任务偏见更显著。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型的偏见已有研究，但涉及语音输入输出的对话模型中的偏见及其特点尚未充分探索，尤其在多轮对话和反复负面反馈情境下的偏见影响。

Method: 通过引入群体不公平评分（GUS）和基于相似性的归一化统计率（SNSR），评估多个开源和闭源语音大语言模型在决策和推荐任务中的偏见表现，并分析多轮对话中偏见的持续性。

Result: 闭源模型偏见较低，开源模型对年龄和性别更敏感，推荐任务加剧跨群体差异，多轮对话中偏见决策可能持续存在。

Conclusion: 本文首次系统研究了端到端语音对话模型的偏见现象，促进了对公平可靠音频交互系统的理解，并发布了相关数据集与评测代码以推动后续研究。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [26] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 本文探讨了利用人工智能和大型语言模型改善塞内加尔司法系统法律文本的访问，通过提取法律文章并构建图数据库，实现法律文本间关系的可视化和知识抽取。


<details>
  <summary>Details</summary>
Motivation: 塞内加尔司法系统中法律文件提取和整理存在困难，迫切需要改善司法信息的获取。

Method: 提取7967篇法律条文，特别关注土地和公共领域法典，建立包含2872个节点和10774条关系的图数据库，采用GPT-4o、GPT-4及Mistral-Large等先进模型进行三元组知识抽取。

Result: 成功构建了详尽的法律文本图数据库和知识抽取系统，提升了法律文本关系的可视化和理解能力。

Conclusion: 通过AI和大型语言模型应用，建立了一个强有力的框架，帮助塞内加尔公民及法律专业人士更有效理解法律权利与义务。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [27] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 语言皮层中存在高度抽象且不依赖形式的意义表征，通过视觉模型和语言模型对句子神经反应的预测验证了这一点。


<details>
  <summary>Details</summary>
Motivation: 探讨语言系统中意义表征的抽象性，并寻求语言皮层中的抽象意义表示。

Method: 利用视觉模型和语言模型对句子对应的神经活动进行建模，通过生成图像及提取视觉嵌入、多句释义的平均嵌入等方法提升预测准确性。

Result: 多图像生成的视觉嵌入和多句释义的平均嵌入能够提高对语言皮层神经反应的预测精度，加入上下文细节的释义甚至超过原句嵌入的表现。

Conclusion: 语言皮层中存在形式无关的高度抽象的意义表征，且语言系统所维持的语义表示比目前语言模型更加丰富和广泛。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [28] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出DiffuSpec，利用扩散语言模型（DLM）进行多令牌草拟，以减少自回归解码的延迟，实现最多3倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型体量增大，准确率提升，但自回归解码带来较大延迟，需寻找更高效的解码方法。

Method: 提出基于预训练扩散语言模型的训练免费草拟框架DiffuSpec，通过单次前向传递生成多词草稿，采用因果一致路径搜索和自适应草拟长度控制来解决多令牌草拟中的路径和长度问题。

Result: DiffuSpec在各项基准测试中实现最多3倍的墙钟时间加速，且与传统自回归验证器兼容。

Conclusion: 基于扩散模型的多令牌草拟是自回归草拟的有效替代方案，显著提升了推理速度，具有广泛应用前景。

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [29] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: 本文介绍了Emission-GPT，一种基于大规模知识库和大语言模型的工具，用于大气排放领域的精准问答和数据分析。


<details>
  <summary>Details</summary>
Motivation: 当前排放数据知识分散且专业性强，查询和整理方法低效，非专业人员难以理解，阻碍了研究和管理。

Method: 基于包含1万余份文档的知识库，利用提示工程和问答完成技术，开发知识增强的大语言模型Emission-GPT，实现自然语言交互和排放数据的查询、可视化与分析。

Result: 在广东省案例中，Emission-GPT能够通过简单指令从原始数据提取关键见解，如点源分布和行业趋势，实现传统手工流程自动化。

Conclusion: Emission-GPT以其模块化和可扩展架构，为下一代排放清单开发和基于场景的评估提供了基础工具。

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [30] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 本文探讨在大型语言模型（LLM）集体中是否会出现类似沉默螺旋（SoS）理论的动态，即少数意见因害怕孤立而不表达，导致多数意见主导。


<details>
  <summary>Details</summary>
Motivation: 沉默螺旋理论来源于人类社会心理学，但LLM作为代理是否会出现类似的舆论动态尚不明确，需评估SoS机制在LLM中的适用性。

Method: 提出一个评估框架，通过四种控制条件（有无历史记录和人格信号）来分析LLM群体的意见动态，使用趋势检验（如Mann-Kendall、Spearman等级）和集中度指标（峰度、四分位距）进行定量评估。

Result: 实验表明：历史和人格信号共存时，出现强烈的多数优势和SoS模式；单独历史信号导致固定锚定效应；单独人格信号则产生多样但无关的观点，显示没有历史锚定时无法产生SoS动态。

Conclusion: 研究揭示了LLM群体中潜在的沉默螺旋效应，强调了在负责任的AI设计中需监控并防止模型间的不良从众行为，促进计算社会学与AI设计的结合。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [31] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: 提出ChunkLLM框架，通过QK Adapter和Chunk Adapter实现对长文本的高效处理，显著加速Transformer推理速度，保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在处理长文本时计算效率低，现有的块选择和压缩方法存在语义不完整或训练推理效率差的问题。

Method: 设计轻量级的QK Adapter和Chunk Adapter，分别用于特征压缩、块关注获取及块边界检测；采用冻结主干网络，仅训练这两个适配器；引入注意力蒸馏提升关键块召回率；推理时仅在块边界触发块选择。

Result: 在多任务长短文本基准测评中，ChunkLLM短文本表现可比，长文本性能保留98.64%，关键值缓存率达48.58%；在120K长文本上最大推理加速达4.48倍。

Conclusion: ChunkLLM有效解决了长文本处理中效率瓶颈，兼顾语义完整性与训练推理效率，具备广泛应用潜力。

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [32] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 本文通过多语言大语言模型回答争议性罗马尼亚历史问题，评估其偏见和回答一致性。


<details>
  <summary>Details</summary>
Motivation: 历史表述常受国家文化和意识形态影响，大语言模型也可能继承这种偏见，影响用户认知，因此需要研究其偏见与一致性。

Method: 选择有争议的历史问题，利用多语言大语言模型分三阶段回答，包括二元回答和数值评分，观察其回答稳定性和语言差异。

Result: 模型的二元回答稳定性较高但不完美，回答在不同语言和格式间经常改变；数值评分与二元回答存在显著差异；最一致的模型并非最准确或中立。

Conclusion: 大语言模型在特定语言和问题情境中容易出现回答不一致，揭示了模型回答中潜在的偏见和不稳定性，提醒用户谨慎对待模型生成内容。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [33] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 本论文发现并解决了大语言模型代理中被忽视的实例级上下文问题，提出了一种任务无关的方法，通过智能探索和轻量循环，生成可复用的高精度上下文文档，提高复杂任务的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要利用环境级手册和任务级指导，但忽视了与具体环境实例相关的可验证、可复用的实例级上下文，导致复杂任务中表现不佳。

Method: 提出实例级上下文学习（ILCL）问题，设计了基于TODO树引导的探索和轻量级计划-执行-提取循环的方法，自动生成高精度、可复用的上下文文档，降低探索成本。

Result: 在TextWorld、ALFWorld和Crafter中实验表明该方法显著提升任务成功率和效率，如TextWorld中ReAct成功率从37%提升至95%，IGE从81%提升至95%。

Conclusion: 通过将一次性探索转化为持久且可复用的知识，所提方法有效补充了现有上下文，提升了LLM代理在复杂任务中的可靠性和效率。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [34] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 本文通过控制训练条件，研究了大语言模型在预训练和上下文知识之间的知识仲裁策略，发现文档内部事实重复及非理想训练数据有助于模型形成更健壮的知识融合能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时面临预训练获得的参数知识和检索获得的上下文知识冲突，目前缺乏系统理解训练条件如何影响知识仲裁策略，导致模型行为欠佳且资源浪费。

Method: 在合成传记语料上，系统控制各种训练条件，训练基于Transformer的语言模型，分析不同训练特性对模型参数和上下文知识利用及仲裁策略的影响。

Result: 发现文档内部事实的重复促进参数和上下文知识能力的发展；训练中包含信息不一致或分布偏斜的语料能促使模型形成稳健的知识利用策略。

Conclusion: 非理想训练数据属性并非需去除的噪声，而是学习健壮知识仲裁策略的重要因素，研究为设计可协调参数与上下文知识的预训练模型提供了经验指导。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [35] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 本文提出了一种结合分层参数化记忆库的小型语言模型架构，通过在预训练和推理时动态访问上下文相关的记忆块，实现了在有限参数规模下性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统大规模语言模型性能提升依赖参数量增长，压缩所有世界知识到参数中既不必要也不适合资源受限边缘设备。因此，寻找高效利用世界知识的方法成为研究动机。

Method: 设计了一种小型语言模型结合分层参数化记忆库的结构，预训练过程中学习将长尾世界知识存储于记忆参数中，模型通过动态获取上下文相关的记忆块增强推理能力。

Result: 在万亿级令牌预训练数据上，160M参数模型结合18M参数记忆块（从4.6B记忆库中获取）达到相当于超过2倍参数规模常规模型的性能。进一步扩展参数规模到21B级别，提出的分层前馈记忆在多种Transformer架构和训练阶段均表现出优越和稳定的性能提升。

Conclusion: 分层参数化记忆结构为小型语言模型提供了有效的知识扩展方式，显著提升模型性能且兼容多种架构和硬件环境，适合资源受限设备的实践应用。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [36] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 提出一种高效方法，通过校准的对数似然分数选择多个大语言模型中最可靠的回答，提升了模型选择性能。


<details>
  <summary>Details</summary>
Motivation: 面对多大语言模型回答选择的挑战，尤其在资源受限情况下，现有方法依赖昂贵的外部验证者或多样本生成，效率低下且效果有限。

Method: 利用校准的对数似然分数，结合不同大语言模型内在的知识和置信度，有效且高效地从多个模型中选择最佳回答。

Result: 在GSM8K、MMLU和ARC数据集上，提出方法分别提升了约4%、3%和5%的性能，适用于多轮对话和多模型选择场景。

Conclusion: 该方法兼顾效果与计算效率，成功提升了多大语言模型系统的回答选择可靠性，展示了其在实际应用中的潜力。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [37] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则驱动的路由框架，实现了在问答系统中动态选择使用文档还是关系数据库以提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成方法主要依赖非结构化文档，忽视了关系型数据库在提供准确及时信息方面的重要作用，且简单结合两者并未显著提升性能。

Method: 通过系统分析发现数据库和文档各有优势，提出基于规则的路由代理根据查询类型选择最佳检索路径，并通过专家代理迭代优化规则，同时利用缓存减少计算成本。

Result: 在三个问答基准测试中，该框架相比静态策略和学习路由基线方法，表现出更高的准确率和适中的计算开销。

Conclusion: 基于规则的路由机制能够有效整合文档与数据库资源，提升领域特定问答的性能和效率，具有良好的适应性和实用价值。

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [38] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出KnowledgeSmith框架，系统性地研究大语言模型的知识更新机制，揭示编辑与遗忘的异同及其对模型知识传播的影响。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型知识更新机制缺乏系统且大规模的评估，无法充分理解编辑与遗忘过程中的行为差异及与人类的异同。

Method: 将编辑和遗忘统一建模为受限优化问题，设计自动化数据集生成器，实现多层次多规模的结构化干预，并通过大量实验研究知识传播、可塑性、稳定性等特性。

Result: 结果表明大语言模型在不同知识层次上的更新行为与人类不同，模型存在一致性与容量的权衡，提供了对知识传播机理的细致洞察。

Conclusion: 研究为更可靠、可扩展的知识更新策略设计提供了理论支持和实践指导。

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [39] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在将自然语言查询转化为SQL时的性能差异，提出了基于数据库级结构化领域陈述的系统框架，通过子字符串匹配检索相关领域信息，提高了查询准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖不切实际的特定查询文本提示，缺乏有效表达领域知识的方法，导致不同数据库间语言模型的翻译性能差异显著。

Method: 提出在数据库级别关联结构化领域陈述，使用子字符串匹配方法检索与用户查询相关的领域陈述。

Result: 在11个包含多样领域的真实数据库模式上，评估了5种大型语言模型，结果显示数据库级结构化领域陈述更实用准确，且子字符串匹配检索显著优于其他方法。

Conclusion: 数据库级结构化领域陈述及基于子字符串匹配的检索方法显著提升了自然语言转SQL的准确率，比传统的临时查询文本提示更有效。

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [40] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 本文研究通过感官提示激活纯文本训练的大型语言模型中的视觉和听觉表征，验证了文本模型内部存在多模态潜在结构。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型仅用文本训练，缺乏直接感知经验，但语言中编码的多模态规律可能隐含在其内部表示中，本文希望通过感官提示激发这些潜在的多模态表示，使文本模型与视觉和音频编码器在表征上更接近。

Method: 设计“看”或“听”的感官提示，促使模型在生成下一个词时模拟基于未实际提供的潜在视觉或听觉证据的条件概率，从而激活特定模态的内部表示。

Result: 实验结果表明，轻量级的提示工程能可靠地激活纯文本训练的语言模型中的模态相关表征，使其在视觉和听觉特征上更接近专业编码器。

Conclusion: 通过显式的感官提示，纯文本训练的大型语言模型可以调动其隐含的多模态潜在结构，增强与感官信息相关的内部表征，证实了文本模型具备潜在的跨模态连接能力。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [41] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 本文介绍了一个新框架，用于研究语言模型如何学习语法，特别是基于概率上下文无关文法（PCFGs）训练的小型模型的学习动态。


<details>
  <summary>Details</summary>
Motivation: 目前对大型语言模型的学习动态了解不足，作者希望通过PCFGs构建的合成语言深入理解模型学习语法的过程。

Method: 作者使用PCFGs生成合成语言，控制语法复杂度和递归深度，通过递归公式推导训练损失和KL散度，并观察模型的学习行为和内部表示。

Result: 发现Transformer模型在所有子语法上同步减小损失，且子语法预训练有助于小型模型性能提升，但模型在处理深递归结构上存在困难，表现出层级语法表示的根本挑战。

Conclusion: 本文首次系统研究了基于PCFG的Transformer学习动态，揭示了模型语法学习的特点和挑战，为未来语言模型学习机制的探索提供了新方向。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [42] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: 本文提出了基于Cobweb层次化框架的神经文档检索方法，通过构建原型树实现多粒度相关性评估和检索路径解释，提升了检索的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的神经文档检索方法多将语料库视为单一向量云，忽视了语料库的层次结构，且缺乏可解释性，导致检索效果受限和透明度不足。

Method: 利用Cobweb框架将句子嵌入组织成原型树，采用粗到细的遍历进行文档排序，提出了广义最佳优先搜索和轻量路径求和排序两种推理方法，结合多种编码器和解码器嵌入进行评估。

Result: 在MS MARCO和QQP数据集上，所提方法在强编码器嵌入下与点积检索性能相当，在嵌入质量下降时表现更稳健，尤其是GPT-2嵌入下点积搜索性能显著下降，而本文方法依然能检索到相关结果。

Conclusion: Cobweb框架的层次化原型树检索方法在保持竞争性效果的同时，提升了检索的鲁棒性、可扩展性和可解释性，为神经文档检索提供了新的路径。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [43] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的评估范式，改进了对检索增强生成（RAG）系统的评价方法。


<details>
  <summary>Details</summary>
Motivation: 现有传统评价指标难以有效评估高流畅性和自然性的LLM生成内容，尤其是在RAG系统中。

Method: 借鉴RAGAS工具，扩展为支持多跳推理和语义社区聚类的知识图谱（KG）评估方法，从而获得更全面的评分指标。

Result: 通过与RAGAS得分比较及人工标注子集验证，证明该方法更敏感于生成内容中细微的语义差异。

Conclusion: 基于KG的评估方法能更深入理解RAG系统表现，且对未来评估研究提供关键挑战和方向。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [44] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: 本文研究了语音语言模型中模态适配器（MA）如何将语音编码器的输出转换为解码器可理解的表示，发现不同模型采用基于英语的中介语或基于英语词的语音表征两种策略。


<details>
  <summary>Details</summary>
Motivation: 语音语言模型依赖模态适配器连接语音编码器和大语言模型，但现有研究对模态适配器如何转换表征知之甚少，亟需深入分析其机制。

Method: 通过分析三种语音语言模型（SALMONN、Qwen2-Audio和Phi-4-Multimodal-Instruct）中模态适配器的输出，寻找最接近的解码器语言模型词汇，揭示其采用的表征策略。

Result: 发现使用Whisper编码器的模型模态适配器采用基于英语的中介语表征，可处理未见过的语言；而Phi-4-Multimodal-Instruct则采用基于英语词汇的语音学表征。

Conclusion: 模态适配器的表征策略取决于语音编码器是仅用于语音识别还是同时用于翻译，这为理解和设计更有效的语音语言模型提供了新视角。

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [45] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 这篇论文提出了首个用于评估语言模型上下文利用的金标准高亮解释框架，系统测试了四种解释方法，发现MechLight方法表现最佳，但所有方法在长上下文和位置偏见方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法准确评价语言模型在生成回答时如何利用上下文信息，且用户难以理解模型具体参考了哪些上下文部分。

Method: 设计控制测试用例构建金标准评估框架，避免现有间接代理指标的不足；基于该框架评估四种高亮解释方法，包括三种传统技术和一种新调整的机械可解释性方法MechLight，在多个语境、数据集和语言模型上进行对比。

Result: MechLight方法在各种上下文场景下均表现最佳，但所有方法在处理较长上下文时准确性下降，且存在位置偏向问题。

Conclusion: 当前高亮解释方法在准确解释语言模型上下文利用方面仍有固有难题，需要开发新方法以实现大规模且可靠的解释。

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [46] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 用户与大型语言模型(LlM)聊天机器人交流时表现出不同的沟通风格，模型需要适应这种变化。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在客户应用中的推广，探究用户与聊天机器人和人类代理的沟通差异成为必要。

Method: 通过实验分析用户语法流畅度、礼貌性和词汇多样性，尝试数据增强和推理时的用户消息重构两种策略以提高模型适应性。

Result: 训练于风格多样化数据集的模型表现优于仅用原始或单一风格数据集训练的模型，推理时消息重构效果较差。

Conclusion: 为提升LLM与用户的交互体验，需使用多样化沟通风格的数据训练模型以适应通信风格的转变。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [47] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: 提出了一种无训练的多步骤转换方法Structured-of-Thought（SoT），提升大语言模型的多语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多语言推理任务中表现受限，尤其是资源匮乏语言，推理能力难以迁移。

Method: SoT通过语言思维转换和结构化知识转换，将特定语言的语义信息转成语言无关的结构化表示，引导模型在跨语言表达时保持一致的推理路径。

Result: 在多种多语言推理基准测试中，SoT优于多个强基线模型，且可与其他无训练方法结合提升效果。

Conclusion: SoT有效提升了多语言推理任务中大语言模型的性能，为资源有限语言的推理问题提供了一种新的解决方案。

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [48] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: 该综述系统总结了多模态大语言模型在自我提升方面的最新进展，从数据采集、组织和模型优化三个角度全面阐述方法，涵盖评价和应用，指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型自我提升技术仍处于初期，但具备利用多样数据资源显著提升模型能力的潜力，亟需系统性综述以促进该领域发展。

Method: 从数据采集、数据组织和模型优化三个视角，系统地整理和分析了现有文献，并总结了常用评估方法和下游应用。

Result: 提出了一个结构化的综述框架，涵盖多模态LLM自我提升的主要技术路径和应用现状，为研究者提供清晰的理解和指导。

Conclusion: 该领域存在诸多开放挑战，未来研究应聚焦提升多模态数据利用效率、优化模型自我提升算法及拓展应用场景。

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [49] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文针对上下文问答任务中的不确定性量化（UQ）问题，提出了一种从理论上量化认知不确定性的通用方法，并通过语义特征差距近似该不确定性，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前UQ研究多集中于闭卷事实问答，缺乏对现实中重要的上下文问答任务不确定性的研究。

Method: 提出基于交叉熵分解的通用token级不确定性度量，隔离认知不确定性，并用理想模型近似真实分布；通过对比模型语义特征差距界定不确定性上界；针对上下文问答抽取上下文依赖、理解和诚实三个特征，并通过少量带标签样本的自上而下解释性方法构建稳健不确定性评分。

Result: 实验显示该方法在多种问答数据集上，在分布内和分布外均显著优于当前无监督和有监督UQ方法，不确定性评估精度提高多达13个百分点，且推理开销极低。

Conclusion: 该研究首次系统量化上下文问答中的认知不确定性，提出的基于语义特征差距的通用框架有效提升了不确定性评估效果，具有广泛的应用前景。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [50] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本文首次采用生存分析方法研究大型语言模型在多轮对话中的鲁棒性，揭示语义漂移对对话失败风险的不同影响。


<details>
  <summary>Details</summary>
Motivation: 当前多轮对话评估多集中于静态单轮测试，难以反映实际对话中的时间动态和渐进性变化。

Method: 利用Cox比例风险模型、加速失效时间模型及随机生存森林对9个先进LLM的36,951轮对话进行时间事件型生存分析。

Result: 发现突发且剧烈的 prompt-to-prompt 语义漂移会极大增加对话失败风险，而渐进性累积漂移则显著降低风险，延长对话时间。AFT模型表现最佳。

Conclusion: 生存分析为评估LLM对话鲁棒性提供新范式，挑战了语义一致性是必要条件的传统看法，并为设计更稳健的对话系统提供实证指导。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [51] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 本文分析了大语言模型(LLM)在低资源旅行领域任务中的表现，发现现有通用基准测试难以反映模型的实际能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM基准测试无法有效评估模型在低资源领域任务中的性能，限制了针对这些领域的解决方案开发。

Method: 收集了14个旅行领域数据集，涵盖7种常见NLP任务，使用匿名真实数据，评估了不同LLM的准确率、扩展性能和推理能力。

Result: 通用基准测试不足以反映低资源任务中模型性能，尽管训练计算力充足，开箱即用的LLM在复杂领域任务中表现受限。推理能力对较小LLM有显著提升效果。

Conclusion: 针对低资源领域，单靠通用基准测试无法全面评估LLM性能，需要关注推理能力提升模型在专门领域的表现。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [52] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于策略梯度生成对抗网络的多模态实体链接方法，旨在通过生成高质量的负样本提升实体链接的表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态实体链接方法忽略了负样本质量对度量学习的重要影响，且缺乏利用生成对抗机制挖掘高质量负样本的研究。

Method: 提出了PGMEL框架，在生成对抗网络中由生成器生成高质量负样本，判别器负责度量学习，利用策略梯度优化生成器以处理离散生成过程。

Result: 在Wiki-MEL、Richpedia-MEL和WikiDiverse数据集上的实验表明，PGMEL能够选出更具挑战性的负样本，学习到更有效的表示，性能优于现有最先进方法。

Conclusion: 引入基于策略梯度的生成对抗网络生成负样本显著提升了多模态实体链接的效果，验证了负样本选择在度量学习中的关键作用。

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [53] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: 本论文针对印度复杂文化背景下大语言模型的偏见问题，提出基于对比学习的编码器评估框架，并构建了包含五个社会维度的印度偏见数据集IndiCASA，揭示了多模型中存在的刻板偏见现象。


<details>
  <summary>Details</summary>
Motivation: 现有基于嵌入的偏见评估方法难以捕捉印度多元文化中的细微刻板印象，且大语言模型在关键领域的应用不断增加，对偏见评估的严格性提出更高要求。

Method: 设计了一种基于对比学习训练的编码器，通过嵌入相似度捕捉细粒度偏见，并构建了涵盖种姓、性别、宗教、残疾和社会经济地位五个维度的IndiCASA数据集，包含2575条人工验证句子。

Result: 评估了多个开放权重大语言模型，发现所有模型均存在一定刻板偏见，尤其残疾相关偏见较为普遍，而宗教偏见较低，可能得益于全球去偏见努力。

Conclusion: 研究强调了在多元文化背景下评估和缓解大语言模型偏见的必要性，推动更公平的模型开发。

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [54] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: 本文提出了一种基于最小数据量训练大型语言模型（LLMs）的方法，通过自我意识机制提升强化学习（RL）的效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习提升LLM推理能力通常需要大量数据制作和标注，数据需求高昂。本文旨在探索如何以最小数据量提升LLM。

Method: 设计两种自我意识机制：1）自我感知难度预测，模型评估任务难度并选择恰当挑战任务；2）自我感知突破限制，模型识别超出能力范围的任务并主动请求外部数据辅助。训练过程中交替让模型提出任务并尝试解决。

Result: 在九个基准测试中，使用不到1.2%的额外数据，模型性能提升了53.8%，验证了自我意识强化学习的有效性。

Conclusion: 自我意识机制能够显著降低数据依赖，促进LLM基于强化学习的自我进化训练，展现出巨大的应用前景。

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [55] [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
*Tien Phat Nguyen,Vu Minh Ngo,Tung Nguyen,Linh Van Ngo,Duc Anh Nguyen,Sang Dinh,Trung Le*

Main category: cs.CL

TL;DR: 本文提出了XTRA，一种结合词袋模型和多语言嵌入的跨语言主题建模框架，实现了更高的主题一致性和跨语言对齐。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题模型在提高主题多样性的同时，难以保证主题的一致性和跨语言的对齐效果。

Method: XTRA引入了表示对齐（通过对比学习对文档-主题分布进行对齐）和主题对齐（将主题-词分布映射到共享语义空间），统一词袋模型和多语言嵌入。

Result: 在多语言语料上的实验表明，XTRA在主题一致性、多样性和对齐质量方面显著优于现有强基线方法。

Conclusion: XTRA有效提升了跨语言主题模型的解释性和跨语言一致性，表现优异且代码已开源。

Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across
languages. Several methods have been proposed to address this problem,
leveraging both traditional and neural approaches. While previous methods have
achieved some improvements in topic diversity, they often struggle to ensure
high topic coherence and consistent alignment across languages. We propose XTRA
(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a
novel framework that unifies Bag-of-Words modeling with multilingual
embeddings. XTRA introduces two core components: (1) representation alignment,
aligning document-topic distributions via contrastive learning in a shared
semantic space; and (2) topic alignment, projecting topic-word distributions
into the same space to enforce crosslingual consistency. This dual mechanism
enables XTRA to learn topics that are interpretable (coherent and diverse) and
well-aligned across languages. Experiments on multilingual corpora confirm that
XTRA significantly outperforms strong baselines in topic coherence, diversity,
and alignment quality. Code and reproducible scripts are available at https:
//github.com/tienphat140205/XTRA.

</details>


### [56] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: 本文提出了两个基于Reddit数据收集的大规模人格数据集MBTI9k和PANDORA，结合多种人格模型与人口统计信息，克服了数据稀缺和标签覆盖不足的问题。并设计了SIMPA框架，通过语义匹配方法实现了可解释且高效的人格自动评估，评估结果与人工相当。


<details>
  <summary>Details</summary>
Motivation: 当前数字化足迹丰富，但缺乏大规模人格标注数据且人格心理学与自然语言处理结合不足，限制了自动人格评估模型的有效性与解释性。

Method: 收集两个大型多样化数据集MBTI9k和PANDORA，整合MBTI和大五人格模型及用户人口数据，提出SIMPA方法通过机器学习和语义相似度将用户文本与问卷条目匹配，实现可解释的人格评估。

Result: 利用新数据集进行实验，发现人口统计变量影响模型有效性，SIMPA框架在保持高解释性的同时，给出与人类评估可比的准确结果。

Conclusion: SIMPA不仅适用于人格评估，其模型无关和分层线索检测特性也使其具备广泛的适用性，适合处理复杂标签体系及各种线索关联的研究与实际应用。

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [57] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为StepChain GraphRAG的新框架，通过结合问题分解和基于广度优先搜索的推理流程，提升多跳问答的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多跳问答方法在整合迭代推理步骤和外部知识检索方面仍面临挑战，尤其是在构建高效且解释性强的推理链方面存在不足。

Method: 构建语料库的全局索引，推理时仅对检索到的段落即时解析成知识图谱，将复杂查询拆分为子问题，并通过基于广度优先搜索的遍历机制动态扩展相关边，组装显式的证据链，避免语言模型被冗余上下文干扰。

Result: 在MuSiQue、2WikiMultiHopQA和HotpotQA数据集上，StepChain GraphRAG在准确率和F1分数上均达到最新最好成绩，平均EM提升2.57%，F1提升2.13%，HotpotQA提升尤为显著。

Conclusion: StepChain GraphRAG不仅提升了多跳问答的性能，还增强了推理过程的可解释性，未来研究需解决其计算开销及大模型潜在的幻觉问题，以进一步提升效率和可靠性。

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [58] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: 本研究评估了五种大型语言模型在21,955个物种的保护状态评估中的表现，发现模型在分类任务上表现优异，但在保护状态推理方面表现较差，且存在生物多样性保护中的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在生物多样性保护中的广泛应用，评估其在物种保护状态判断中的可靠性变得迫切。

Method: 系统验证五种领先模型在四个核心IUCN红色名录评估组成部分：分类学、保护状态、分布和威胁，覆盖21,955个物种。

Result: 模型在分类任务准确率达94.9%，但保护状态评估仅有27.2%的准确率，表现出知识-推理差距，且偏向于具有代表性的脊椎动物。

Conclusion: 大型语言模型适合信息检索但推理能力有限，建议采用人机混合模式，由模型辅助专家，人类专家保持最终判断权。

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [59] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: 本文将Wordle游戏形式化为约束满足问题（CSP），提出了CSP感知信息熵和概率化CSP框架，实现了更高的猜词成功率和更快的速度，并验证了方法的鲁棒性和跨语言适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的Wordle求解器多基于信息论或频率启发式，缺乏对约束的正式处理，限制了性能提升。

Method: 提出CSP感知信息熵，计算经过约束传播后的信息增益；引入结合贝叶斯频率先验和逻辑约束的概率CSP框架。

Result: CSP感知信息熵相比传统方法提高了1.7%的猜测成功率且运行速度提升46%；在10%噪声下仍保持优势，概率CSP在0-20%噪声下成功率100%；跨语言测试表现良好。

Conclusion: 基于正式CSP处理与概率逻辑整合的方法在结构化谜题求解中优于经典的信息论和基于学习的方法，展示了约束满足技术的强大潜力。

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [60] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: 本文提出了一种名为SRGen的轻量级测试时自反生成框架，通过动态熵阈值识别不确定词元，训练纠正向量进行自我反思修正，显著提升大型语言模型(LLMs)在数学推理等复杂任务中的推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在链式思维生成过程中易受前期错误影响，现有的自我反思方法要么依赖完整草稿的修正，要么需要昂贵训练，效率低且被动，迫切需要一种高效主动的自我反思机制。

Method: SRGen在生成过程中通过动态熵阈值检测高不确定性词元，为每个词元训练特定的纠正向量，利用已生成上下文进行概率分布修正，实现生成前的自我反思，从而降低错误发生概率。

Result: 在多个复杂数学推理基准测试及多种LLM模型上，SRGen均表现出显著提升，如AIME2024测试中，DeepSeek-R1-Distill-Qwen-7B模型的Pass@1提升12.0%，Cons@5提升13.3%。

Conclusion: SRGen是一种即插即用的测试时反思生成框架，能够有效集成到现有生成流程，具备运行开销低、兼容性强等优点，为LLM的可靠推理提供了强有力的解决方案。

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [61] [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
*Yohan Lee,Yongwoo Song,Sangyeop Kim*

Main category: cs.CL

TL;DR: 本文提出了首个全面的会话数据检索（CDR）基准测试，包含1.6k查询和9.1k对话，用于评估产品洞察类对话数据检索系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前对话数据检索存在显著挑战，而已有嵌入模型性能不足，缺乏标准化评价基准。

Method: 构建了覆盖五个分析任务的CDR基准，设计实用查询模板并对16个流行嵌入模型进行评估与错误分析。

Result: 评测显示即使是最优模型NDCG@10也仅约0.51，表明对话数据检索能力远不及文档检索。

Conclusion: 会话数据检索存在隐性状态识别、对话动态及上下文引用等独特挑战，提出的CDR基准为推动该领域研究提供标准和工具。

Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

</details>


### [62] [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
*Jingqi Zhang,Ruibo Chen,Yingqing Yang,Peihua Mai,Heng Huang,Yan Pang*

Main category: cs.CL

TL;DR: 本文提出了TRACE，一种在大语言模型微调过程中，通过无失真水印实现版权数据集使用的黑盒检测方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地在小型专业领域数据集上进行微调，这些数据集常涉及版权材料，亟需可靠的未经授权使用检测手段。现有方法要么依赖内部信号，要么依赖手工提示或干净参考数据集，应用受限。

Method: TRACE通过私钥引导无失真水印重写数据集，保证文本质量和下游任务性能。检测时利用微调引起的水印“放射性”效应，采用基于熵门控的选择性高不确定性标记评分策略，显著增强检测能力。

Result: TRACE在多个数据集和模型家族上均显著检测到版权数据（p<0.05），呈现强统计学证据。支持多数据集归属，并在继续预训练非水印语料时仍具鲁棒性。

Conclusion: TRACE提供了一种实用且强大的黑盒版权数据集使用验证手段，兼顾文本质量、任务效用及检测效果，适合实际应用推广。

Abstract: Large Language Models (LLMs) are increasingly fine-tuned on smaller,
domain-specific datasets to improve downstream performance. These datasets
often contain proprietary or copyrighted material, raising the need for
reliable safeguards against unauthorized use. Existing membership inference
attacks (MIAs) and dataset-inference methods typically require access to
internal signals such as logits, while current black-box approaches often rely
on handcrafted prompts or a clean reference dataset for calibration, both of
which limit practical applicability. Watermarking is a promising alternative,
but prior techniques can degrade text quality or reduce task performance. We
propose TRACE, a practical framework for fully black-box detection of
copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets
with distortion-free watermarks guided by a private key, ensuring both text
quality and downstream utility. At detection time, we exploit the radioactivity
effect of fine-tuning on watermarked data and introduce an entropy-gated
procedure that selectively scores high-uncertainty tokens, substantially
amplifying detection power. Across diverse datasets and model families, TRACE
consistently achieves significant detections (p<0.05), often with extremely
strong statistical evidence. Furthermore, it supports multi-dataset attribution
and remains robust even after continued pretraining on large non-watermarked
corpora. These results establish TRACE as a practical route to reliable
black-box verification of copyrighted dataset usage. We will make our code
available at: https://github.com/NusIoraPrivacy/TRACE.

</details>


### [63] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 本文开发并评估了一个基于检索增强生成（RAG）系统，利用大型语言模型查询英国NICE临床指南，提高了信息获取的准确性和时效性。


<details>
  <summary>Details</summary>
Motivation: 临床指南篇幅长且内容庞大，限制了其在医疗系统中的快速应用，急需一个能够高效精准回答自然语言查询的系统。

Method: 设计了一个混合嵌入机制的检索架构，构建包含10,195个文本块的数据库，基于7901条查询进行检索与生成能力评测。

Result: 检索性能表现优异，MRR达0.814，一查召回率81%，前十召回率达99.1%；生成阶段RAG增强模型在可信度上提升64.7个百分点至99.5%，且达成完美上下文精度，显著优于其他医疗领域模型。

Conclusion: RAG系统有效可靠，可扩展地应用于医疗领域，促进医疗指南的普及和成本效益的提升，防止生成虚假信息。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [64] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: 本研究探讨了语音情感识别中的描述性语义与表达性语义的区别及其对情感识别的影响，提出了结合两者的分析方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音情感识别准确度受限于情感细微差别的复杂性，研究希望通过区分不同语义层次改进识别效果。

Method: 通过录制参与者观看带情绪电影片段后的语音描述，采集情绪标签、自评情感及情绪维度分数，分析描述性语义与表达性语义分别与意图情绪与唤起情绪的关联。

Result: 实验表明描述性语义与意图情绪高度对应，而表达性语义则与唤起的实际情绪相关。

Conclusion: 研究结果有助于提升人机交互中的语音情感识别，促进更具上下文感知能力的AI系统发展。

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [65] [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
*Oriol Pareras,Gerard I. Gállego,Federico Costa,Cristina España-Bonet,Javier Hernando*

Main category: cs.CL

TL;DR: 本文比较了基于大型语言模型的语音到文本翻译（S2TT）中链式思维提示（CoT）与直接提示两种策略在不同数据规模下的表现，发现随着数据量增加，直接提示效果提升更显著，未来可能更有效。


<details>
  <summary>Details</summary>
Motivation: 当前S2TT研究多采用CoT提示，通过先转录后翻译的步骤提升性能，但随着更多S2TT数据涌现，需评估CoT与直接提示的效果差异。

Method: 作者通过伪标注将ASR语料的转录文本翻译成六种欧洲语言，利用不同数据规模训练基于大型语言模型的S2TT系统，系统比较了CoT与直接提示的表现。

Result: 实验结果显示，虽然CoT在小规模数据下表现优越，但随着训练数据增加，直接提示改善更明显。

Conclusion: 直接提示在大规模S2TT数据条件下可能成为更有效的方法，提示未来S2TT系统的设计应考虑数据规模对提示策略的影响。

Abstract: Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based
models, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,
where the model is guided to first transcribe the speech and then translate it.
CoT typically outperforms direct prompting primarily because it can exploit
abundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)
datasets to explicitly model its steps. In this paper, we systematically
compare CoT and Direct prompting under increasing amounts of S2TT data. To this
end, we pseudo-label an ASR corpus by translating its transcriptions into six
European languages, and train LLM-based S2TT systems with both prompting
strategies at different data scales. Our results show that Direct improves more
consistently as the amount of data increases, suggesting that it may become a
more effective approach as larger S2TT resources are created.

</details>


### [66] [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
*Beth Pearson,Ahmed Adnan,Zahraa Abdallah*

Main category: cs.CL

TL;DR: 本文提出了一种结合Llama3.1与命名实体识别（NER）的语义相似度评分方法Llama-EntScore，用于评估放射学初步报告与最终报告之间的语义差异，帮助初级放射科医生培训和诊断准确性提升。


<details>
  <summary>Details</summary>
Motivation: 放射学报告评估对放射科医生培训及诊断准确性至关重要，当前大语言模型（LLMs）和传统NER方法在对比报告时均存在准确性不足的问题。

Method: 首先比较多种LLMs在放射报告对比任务中的表现，再评估传统NER方法，最后提出将Llama3.1与NER结合并赋予可调权重的Llama-EntScore评分方法，量化语义相似度并提供解释性反馈。

Result: Llama-EntScore在与放射科医生提供的标准评分对比中，达到67%的完全匹配准确率，93%的±1范围内准确率，优于单独使用LLMs或NER。

Conclusion: 结合LLMs与NER的Llama-EntScore有效提升了放射学报告语义相似度评价的准确性和解释性，可作为放射科医生培训和报告质量控制的有力工具。

Abstract: Radiology report evaluation is a crucial part of radiologists' training and
plays a key role in ensuring diagnostic accuracy. As part of the standard
reporting workflow, a junior radiologist typically prepares a preliminary
report, which is then reviewed and edited by a senior radiologist to produce
the final report. Identifying semantic differences between preliminary and
final reports is essential for junior doctors, both as a training tool and to
help uncover gaps in clinical knowledge. While AI in radiology is a rapidly
growing field, the application of large language models (LLMs) remains
challenging due to the need for specialised domain knowledge. In this paper, we
explore the ability of LLMs to provide explainable and accurate comparisons of
reports in the radiology domain. We begin by comparing the performance of
several LLMs in comparing radiology reports. We then assess a more traditional
approach based on Named-Entity-Recognition (NER). However, both approaches
exhibit limitations in delivering accurate feedback on semantic similarity. To
address this, we propose Llama-EntScore, a semantic similarity scoring method
using a combination of Llama 3.1 and NER with tunable weights to emphasise or
de-emphasise specific types of differences. Our approach generates a
quantitative similarity score for tracking progress and also gives an
interpretation of the score that aims to offer valuable guidance in reviewing
and refining their reporting. We find our method achieves 67% exact-match
accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided
ground truth scores - outperforming both LLMs and NER used independently. Code
is available at:
\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}

</details>


### [67] [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
*Jacobo Romero-Díaz,Gerard I. Gállego,Oriol Pareras,Federico Costa,Javier Hernando,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 当前的语音转文本翻译系统依赖于语音识别和文本翻译两模块，存在错误传递和难以利用语音韵律信息的问题。引入了链式思维提示(CoT)方法，但其主要依赖于文本，未充分利用语音特征。通过简单训练改进可以提升系统的鲁棒性和对语音信息的利用。


<details>
  <summary>Details</summary>
Motivation: 传统的语音转文本翻译系统受限于错误传播及无法利用语音韵律等声学特征，期望通过CoT提示法联合使用语音和转录信息来克服这些问题。

Method: 使用归因方法分析CoT的表现，测试其在转录文本被破坏时的鲁棒性，同时评估其对韵律的感知，进一步通过加入直接的语音翻译数据和噪声转录文本训练策略来提升性能。

Result: 发现CoT方法表现类似于传统的流水线结构，主要依赖转录文本，几乎未利用语音信息。训练改进方法成功增强系统的鲁棒性并提升了对语音属性的利用。

Conclusion: CoT方法并未实现预期的优势，表明未来需要设计能够显式整合声学信息的系统架构以改进语音到文本的翻译效果。

Abstract: Speech-to-Text Translation (S2TT) systems built from Automatic Speech
Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major
limitations: error propagation and the inability to exploit prosodic or other
acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,
with the expectation that jointly accessing speech and transcription will
overcome these issues. Analyzing CoT through attribution methods, robustness
evaluations with corrupted transcripts, and prosody-awareness, we find that it
largely mirrors cascaded behavior, relying mainly on transcripts while barely
leveraging speech. Simple training interventions, such as adding Direct S2TT
data or noisy transcript injection, enhance robustness and increase speech
attribution. These findings challenge the assumed advantages of CoT and
highlight the need for architectures that explicitly integrate acoustic
information into translation.

</details>


### [68] [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
*Zhaojun Sun,Xuzhou Zhu,Xuanhe Zhou,Xin Tong,Shuo Wang,Jie Fu,Guoliang Li,Zhiyuan Liu,Fan Wu*

Main category: cs.CL

TL;DR: 本文提出了一个细粒度的、基于测验的学术综述自动生成评估框架SurveyBench，用于全面评估自动生成综述的质量。


<details>
  <summary>Details</summary>
Motivation: 当前自动学术综述生成方法表现不佳，且缺乏与读者需求对齐的严格评测标准。

Method: 设计SurveyBench，包括从大量arXiv论文及高质量综述中提取主题，构建多层次评价指标体系，并采用内容和测验双模评测协议。

Result: 实验证明SurveyBench能有效揭示现有自动综述方法的不足，自动生成综述内容质量平均比人类低21%。

Conclusion: SurveyBench提供了一个系统且读者导向的评测框架，有助于推动自动综述生成技术的发展。

Abstract: Academic survey writing, which distills vast literature into a coherent and
insightful narrative, remains a labor-intensive and intellectually demanding
task. While recent approaches, such as general DeepResearch agents and
survey-specialized methods, can generate surveys automatically (a.k.a.
LLM4Survey), their outputs often fall short of human standards and there lacks
a rigorous, reader-aligned benchmark for thoroughly revealing their
deficiencies. To fill the gap, we propose a fine-grained, quiz-driven
evaluation framework SurveyBench, featuring (1) typical survey topics source
from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;
(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,
coverage breadth, logical coherence), content quality (e.g., synthesis
granularity, clarity of insights), and non-textual richness; and (3) a
dual-mode evaluation protocol that includes content-based and quiz-based
answerability tests, explicitly aligned with readers' informational needs.
Results show SurveyBench effectively challenges existing LLM4Survey approaches
(e.g., on average 21% lower than human in content-based evaluation).

</details>


### [69] [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
*Ej Zhou,Caiqi Zhang,Tiancheng Hu,Chengzu Li,Nigel Collier,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 多语言大语言模型在置信度校准方面存在问题，非英语语言的校准表现较差。


<details>
  <summary>Details</summary>
Motivation: 探讨多语言环境下大语言模型置信度校准不足的问题，提升模型在多语言中的置信度可靠性。

Method: 对六个模型族和100多种语言进行大规模系统研究，分析模型不同层的置信度信号，提出无需训练的层选择方法（LACE），适配不同语言。

Result: 发现英文中心训练导致最终层表现较差，晚期中间层的置信度信号更可靠，LACE方法提高了多语言模型的置信度校准。

Conclusion: 通过利用晚期中间层并采用语言感知的层集成方法，可以改善多语言大语言模型的置信度校准，推动构建更公平可靠的全球模型。

Abstract: Confidence calibration, the alignment of a model's predicted confidence with
its actual accuracy, is crucial for the reliable deployment of Large Language
Models (LLMs). However, this critical property remains largely under-explored
in multilingual contexts. In this work, we conduct the first large-scale,
systematic studies of multilingual calibration across six model families and
over 100 languages, revealing that non-English languages suffer from
systematically worse calibration. To diagnose this, we investigate the model's
internal representations and find that the final layer, biased by
English-centric training, provides a poor signal for multilingual confidence.
In contrast, our layer-wise analysis uncovers a key insight that
late-intermediate layers consistently offer a more reliable and
better-calibrated signal. Building on this, we introduce a suite of
training-free methods, including Language-Aware Confidence Ensemble (LACE),
which adaptively selects an optimal ensemble of layers for each specific
language. Our study highlights the hidden costs of English-centric alignment
and offer a new path toward building more globally equitable and trustworthy
LLMs by looking beyond the final layer.

</details>


### [70] [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
*Katherine Thai,Bradley Emi,Elyas Masrour,Mohit Iyyer*

Main category: cs.CL

TL;DR: 本文提出了一种基于相似性度量和回归模型EditLens来检测和量化文本中AI编辑的程度，并在区分人工、AI及混合撰写文本上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大量使用大型语言模型进行文本编辑，但目前仅关注全AI生成文本的检测，忽视了AI编辑文本的识别。

Method: 提出轻量级相似性度量对比人类原文和编辑文本，结合人工标注监督训练回归模型EditLens，预测AI编辑程度。

Result: EditLens在二分类和三分类任务中分别实现94.7%和90.4%的F1分数，成功区分人工写作、AI写作及混合文本，能够量化AI的编辑程度。

Conclusion: AI编辑文本不仅可被识别，其编辑程度的检测有助于作者身份归属、教育和政策制定，相关模型和数据集将公开发布，推动后续研究。

Abstract: A significant proportion of queries to large language models ask them to edit
user-provided text, rather than generate new text from scratch. While previous
work focuses on detecting fully AI-generated text, we demonstrate that
AI-edited text is distinguishable from human-written and AI-generated text.
First, we propose using lightweight similarity metrics to quantify the
magnitude of AI editing present in a text given the original human-written text
and validate these metrics with human annotators. Using these similarity
metrics as intermediate supervision, we then train EditLens, a regression model
that predicts the amount of AI editing present within a text. Our model
achieves state-of-the-art performance on both binary (F1=94.7%) and ternary
(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.
Not only do we show that AI-edited text can be detected, but also that the
degree of change made by AI to human writing can be detected, which has
implications for authorship attribution, education, and policy. Finally, as a
case study, we use our model to analyze the effects of AI-edits applied by
Grammarly, a popular writing assistance tool. To encourage further research, we
commit to publicly releasing our models and dataset.

</details>


### [71] [Neural Correlates of Language Models Are Specific to Human Language](https://arxiv.org/abs/2510.03156)
*Iñigo Parra*

Main category: cs.CL

TL;DR: 本研究验证了大型语言模型隐藏状态与大脑fMRI响应之间的相关性，确认了相关性存在并探讨其影响因素。


<details>
  <summary>Details</summary>
Motivation: 此前研究表明大型语言模型隐藏状态与fMRI脑反应存在相关性，但存在维度诅咒、相似性度量等潜在问题。本研究旨在验证这些相关性的稳健性。

Method: 通过降维、新的相似性度量方法，分析模型与大脑的相关性，并比较不同训练数据和位置编码情况下的相关性变化。

Result: 发现相关性在降维后依然存在，新的相似度度量验证了结果，且相关性仅在以人类语言训练的模型中出现，且依赖于位置编码。

Conclusion: 研究结果支持大型语言模型与人脑语言处理机制存在相似性，增强了其生物学合理性和可解释性。

Abstract: Previous work has shown correlations between the hidden states of large
language models and fMRI brain responses, on language tasks. These correlations
have been taken as evidence of the representational similarity of these models
and brain states. This study tests whether these previous results are robust to
several possible concerns. Specifically this study shows: (i) that the previous
results are still found after dimensionality reduction, and thus are not
attributable to the curse of dimensionality; (ii) that previous results are
confirmed when using new measures of similarity; (iii) that correlations
between brain representations and those from models are specific to models
trained on human language; and (iv) that the results are dependent on the
presence of positional encoding in the models. These results confirm and
strengthen the results of previous research and contribute to the debate on the
biological plausibility and interpretability of state-of-the-art large language
models.

</details>


### [72] [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
*Xuan Xu,Haolun Li,Zhongliang Yang,Beilin Chu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CL

TL;DR: 本文探讨了在大语言模型时代下，将主题模型视为长文本生成任务的新范式，并提出了一种基于大语言模型的简单实用主题模型方法。通过零样本提示进行系统比较，检验该新范式是否优于传统神经主题模型。


<details>
  <summary>Details</summary>
Motivation: 传统神经主题模型依赖推断和生成网络学习潜在主题分布，随着大语言模型的发展，探讨能否利用其强大的文本生成能力改进主题模型，有助于提升主题质量并验证当前传统模型的有效性。

Method: 提出了一种基于大语言模型的主题模型方法，包括抽样数据子集、使用预设提示生成主题和代表文本、以及基于关键词匹配进行文本分配。通过零样本提示，实现长文本生成式的主题建模。

Result: 通过系统性实验证明，在主题质量方面，大语言模型在零样本提示下能与传统神经主题模型进行有效竞争，部分情况甚至表现更佳。

Conclusion: 本文证实了基于长文本生成范式的大语言模型方法为主题建模提供了新的思路，并实证部分传统神经主题模型确实已逐渐过时。

Abstract: Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."

</details>


### [73] [Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2510.03202)
*Abteen Ebrahimi,Adam Wiemerslage,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文提出了NN-Rank算法，通过多语言模型的隐藏表示和无标注目标语言数据，为跨语言迁移选择源语言，在词性标注和命名实体识别任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 跨语言迁移中如何有效排序源语言以提升目标语言任务效果，尤其在目标语言数据有限的情况下。

Method: 利用多语言预训练模型的隐藏表示结合无标注的目标语言数据，设计NN-Rank算法进行源语言排序。

Result: 在包含51个源语言和超过50个目标语言的实验中，NN-Rank优于基于词汇和语言特征的现有方法，在词性标注和命名实体识别任务NDCG指标提升显著，且在仅用圣经这类异域领域数据时依旧表现竞争力。

Conclusion: NN-Rank能有效利用少量无标注目标语言数据，实现高质量源语言排序，促进跨语言迁移任务的性能提升。

Abstract: We present NN-Rank, an algorithm for ranking source languages for
cross-lingual transfer, which leverages hidden representations from
multilingual models and unlabeled target-language data. We experiment with two
pretrained multilingual models and two tasks: part-of-speech tagging (POS) and
named entity recognition (NER). We consider 51 source languages and evaluate on
56 and 72 target languages for POS and NER, respectively. When using in-domain
data, NN-Rank beats state-of-the-art baselines that leverage lexical and
linguistic features, with average improvements of up to 35.56 NDCG for POS and
18.14 NDCG for NER. As prior approaches can fall back to language-level
features if target language data is not available, we show that NN-Rank remains
competitive using only the Bible, an out-of-domain corpus available for a large
number of languages. Ablations on the amount of unlabeled target data show
that, for subsets consisting of as few as 25 examples, NN-Rank produces
high-quality rankings which achieve 92.8% of the NDCG achieved using all
available target data for ranking.

</details>


### [74] [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Léo Boisvert,Massimo Caccia,Jérémy Espinas,Alexandre Aussem,Véronique Eglin,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 该论文提出了FocusAgent，一种基于轻量级大语言模型的检索方法，用于从网页内容中提取最相关的信息，减少处理内容，提高效率并降低安全风险。


<details>
  <summary>Details</summary>
Motivation: 处理网页中的海量文本使得基于LLM的网页代理面临上下文限制、计算成本高和安全风险（如提示注入攻击）的问题。现有方法无法有效平衡内容保留与甄别。

Method: FocusAgent利用轻量级LLM检索器，从辅助技术树（AxTree）观察中提取与任务目标最相关的内容，去除无关和噪声信息，从而降低观察内容大小并减少注入攻击风险。

Result: 在WorkArena和WebArena基准测试中，FocusAgent表现与强基线相当，但观察内容减少了50%以上。同时，其变体显著降低了提示注入攻击成功率，在无攻击时保持任务成功率。

Conclusion: 基于任务目标指导的轻量级LLM检索是构建高效、有效且安全的网页代理的实用且稳健的策略。

Abstract: Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

</details>


### [75] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种多大语言模型（LLM）间直接语义通信的新范式Cache-to-Cache（C2C），通过KV-Cache的深度语义融合提升性能和效率，显著优于传统基于文本的通信方式。


<details>
  <summary>Details</summary>
Motivation: 传统多LLM系统通过文本通信，导致语义信息损失和生成延迟，限制了模型性能和效率。

Method: 提出C2C方法，利用神经网络将源模型和目标模型的KV-Cache进行投影和融合，并通过可学习的门控机制选择受益层，实现直接语义转移。

Result: C2C比单一模型准确率提高8.5-10.5%，相比基于文本的通信，准确率提升约3.0-5.0%，延迟加速约2倍。

Conclusion: C2C利用LLM深层语义信息，实现了更高效、更精准的多模型协同，是提升多LLM系统性能的新方向。

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


### [76] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: 提出了Self-Anchor，通过结构化推理步骤引导大语言模型关注关键中间步骤，有效提升复杂推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在长推理链中容易忽略关键步骤和原始提示信息，导致推理错误。

Method: Self-Anchor将推理轨迹分解为结构化计划，自动对齐模型注意力至最相关推理步骤，保持生成过程中的关注度。

Result: 在六个基准测试上，Self-Anchor优于现有最先进提示方法，显著缩小了通用模型与专用推理模型之间的差距。

Conclusion: Self-Anchor有望使大多数大语言模型无需重新训练即可有效应对复杂推理任务。

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


### [77] [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
*Sebastian Gehrmann*

Main category: cs.CL

TL;DR: 奖励模型用于评估大语言模型的输出质量生成训练信号，评价指标监控AI模型表现，两者研究相对独立，本论文主张两者结合以解决常见挑战。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型和评价指标领域相互独立，导致术语冗余及重复困难，如虚假相关性和奖励黑客行为，亟需加强协作。

Method: 本文通过对奖励模型和评价指标领域的广泛调研，展示评价指标在特定任务上优于奖励模型，并讨论两者的共同挑战和改进方向。

Result: 研究指出评价指标在某些任务上表现优于奖励模型，并总结了两领域的主要问题和潜在改进方法。

Conclusion: 加强奖励模型与评价指标的协同研究，促进在偏好采集、避免虚假相关性、奖励黑客、校准感知元评价等方面的改进。

Abstract: The emergence of reinforcement learning in post-training of large language
models has sparked significant interest in reward models. Reward models assess
the quality of sampled model outputs to generate training signals. This task is
also performed by evaluation metrics that monitor the performance of an AI
model. We find that the two research areas are mostly separate, leading to
redundant terminology and repeated pitfalls. Common challenges include
susceptibility to spurious correlations, impact on downstream reward hacking,
methods to improve data quality, and approaches to meta-evaluation. Our
position paper argues that a closer collaboration between the fields can help
overcome these issues. To that end, we show how metrics outperform reward
models on specific tasks and provide an extensive survey of the two areas.
Grounded in this survey, we point to multiple research topics in which closer
alignment can improve reward models and metrics in areas such as preference
elicitation methods, avoidance of spurious correlations and reward hacking, and
calibration-aware meta-evaluation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [78] [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/abs/2510.02387)
*FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estapé,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,François Fleuret,Fabian Gloeckle,Alex Gu,Michael Hassid,Daniel Haziza,Badr Youbi Idrissi,Christian Keller,Rahul Kindi,Hugh Leather,Gallil Maimon,Aram Markosyan,Francisco Massa,Pierre-Emmanuel Mazaré,Vegard Mella,Naila Murray,Keyur Muzumdar,Peter O'Hearn,Matteo Pagliardini,Dmitrii Pedchenko,Tal Remez,Volker Seeker,Marco Selvi,Oren Sultan,Sida Wang,Luca Wehrstedt,Ori Yoran,Lingming Zhang,Taco Cohen,Yossi Adi,Gabriel Synnaeve*

Main category: cs.SE

TL;DR: CWM是一个基于世界模型的开源32亿参数代码生成大模型，通过Python解释器和Docker环境的观察-动作数据中间训练实现跨任务推理，在代码理解和生成方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统代码训练仅依赖静态代码，难以捕捉代码执行和交互动态，影响代码理解和生成效果。作者希望通过引入世界模型和多任务强化学习提升代码生成能力。

Method: 在静态代码预训练基础上，作者用大量Python解释器和agentic Docker环境的观察-动作轨迹数据对CWM进行中间训练，结合多任务推理强化学习，支持长上下文和步骤仿真能力。

Result: CWM拥有131k上下文长度，能执行逐步代码仿真，且在多个代码和数学基准中表现优异（例如SWE-bench Verified 65.8%，LiveCodeBench 68.6%，Math-500 96.6%，AIME 2024 76.0%）。

Conclusion: 基于世界模型的中间训练和多任务强化学习显著提升了代码生成的推理和计划能力，CWM作为强大测试平台推动代码世界模型相关研究。

Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,
to advance research on code generation with world models. To improve code
understanding beyond what can be learned from training on static code alone, we
mid-train CWM on a large amount of observation-action trajectories from Python
interpreter and agentic Docker environments, and perform extensive multi-task
reasoning RL in verifiable coding, math, and multi-turn software engineering
environments. With CWM, we provide a strong testbed for researchers to explore
the opportunities world modeling affords for improving code generation with
reasoning and planning in computational environments. We present first steps of
how world models can benefit agentic coding, enable step-by-step simulation of
Python code execution, and show early results of how reasoning can benefit from
the latter. CWM is a dense, decoder-only LLM trained with a context size of up
to 131k tokens. Independent of its world modeling capabilities, CWM offers
strong performance on general coding and math tasks: it reaches pass@1 scores
of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on
LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further
research on code world modeling, we release model checkpoints after
mid-training, SFT, and RL.

</details>


### [79] [From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization](https://arxiv.org/abs/2510.02389)
*Haoran Xi,Minghao Shao,Brendan Dolan-Gavitt,Muhammad Shafique,Ramesh Karri*

Main category: cs.SE

TL;DR: 该论文提出了T2L-Agent，一个能够从模块级逐渐缩小至精确代码行的漏洞定位框架，并设计了用于评测线级漏洞定位的T2L-ARVO基准。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型漏洞检测方法多局限于孤立代码分析，难以处理长上下文且定位粒度较粗，难以为工程师提供精准的修复指导。

Method: 提出了端到端的项目级框架T2L-Agent，通过多轮反馈和Agentic Trace Analyzer融合运行时证据与AST代码切片，实现多轮迭代细化诊断，从而实现从模块到代码行的漏洞定位。同时设计了支持细粒度定位的T2L-ARVO基准。

Result: 在T2L-ARVO基准上，T2L-Agent分别达到58.0%的漏洞检测率和54.8%的代码行定位准确率，显著优于现有基线方法。

Conclusion: 该工作将基于大语言模型的漏洞检测从粗粒度识别推进到可部署的精确诊断，显著减少误报噪声，促进开源软件的快速修复。

Abstract: Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.

</details>


### [80] [AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization](https://arxiv.org/abs/2510.02393)
*Jianqing Zhang,Wei Xia,Hande Dong,Qiang Lin,Jian Cao*

Main category: cs.SE

TL;DR: 提出了一种名为AP2O-Coder的适应性渐进偏好优化方法，通过构建错误笔记本并逐步优化以减少代码错误，提高大语言模型的代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线偏好优化方法仅利用通过/失败的信号，忽视了失败代码中更细粒度的错误类型，导致代码依然存在编译和运行错误。

Method: 构建失败代码的错误笔记本，按错误类型逐步优化大语言模型，并通过自适应重放错误类型，针对模型训练中不断变化的弱点进行调整。

Result: 在多个代码与通用大语言模型（参数规模从0.5B到34B）上，AP2O-Coder提升了代码生成的通过率最高3%，且所需偏好数据更少。

Conclusion: AP2O-Coder通过细粒度错误类型引导及自适应优化显著提升了代码生成质量，表明该方法有效改善了模型编程能力和代码错误率。

Abstract: LLMs' code generation capabilities have yielded substantial improvements in
the effectiveness of programming tasks. However, LLM-generated code still
suffers from compilation and runtime errors. Existing offline preference
optimization methods primarily focus on enhancing LLMs' coding abilities using
pass/fail signals in the preference data, overlooking the deep-level error
types in the failed codes. To address this, we propose Adaptively Progressive
Preference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that
guides LLMs adaptively and methodically to reduce code errors for code
generation. Specifically, we construct an error notebook from failed codes and
progressively optimize the LLM to correct errors type by type. Furthermore, we
adaptively replay error types to tailor to the LLM's changing weaknesses
throughout the training process. Through extensive experiments on both code and
general LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from
0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in
pass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O

</details>


### [81] [Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions](https://arxiv.org/abs/2510.02404)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.SE

TL;DR: 本文综述了无服务器计算中功能即服务（FaaS）的资源配置问题，提出了影响函数设计与性能的分类体系，分析现有文献，指出研究空白并建议未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前云服务器无服务器模型中，资源配置缺乏透明度，开发者难以基于性能约束进行最优资源配置，特别是在开源平台中独立配置资源带来额外复杂度。

Method: 文章对资源配置技术进行了分类，分析现有文献中的函数配置方法，评估其对设计、成本和性能保障的影响。

Result: 提出了一个涵盖函数设计与资源配置影响因素的分类体系，综合评述了现有研究，揭示了配置优化领域中的不足。

Conclusion: 未来研究需填补资源配置的研究空白，提升函数配置效率和服务器无执行环境能力，促进无服务器计算的广泛应用。

Abstract: The serverless cloud computing model offers a framework where the service
provider abstracts the underlying infrastructure management from developers. In
this serverless model, FaaS provides an event-driven, function-oriented
computing service characterised by fine-grained, usage-based pricing that
eliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,
and Cloud Run Functions require developers to configure their function(s) with
minimum operational resources for its successful execution. This resource
allocation influences both the operational expense and the performance quality
of these functions. However, a noticeable lack of platform transparency forces
developers to rely on expert knowledge or experience-based ad-hoc decisions to
request desired function resources. This makes optimal resource configuration a
non-trivial task while adhering to performance constraints. Furthermore, while
commercial platforms often scale resources like CPU and network bandwidth
proportional to memory, open-source frameworks permit independent configuration
of function resources, introducing additional complexity for developers aiming
to optimise their functions. These complexities have directed researchers to
resolve developer challenges and advance towards an efficient server-less
execution model. In this article, we identify different aspects of resource
configuration techniques in FaaS settings and propose a taxonomy of factors
that influence function design, configuration, run-time cost, and performance
guarantees. We conduct an analysis of existing literature on resource
configuration to present a comprehensive review of current studies on function
configuration. We also identify existing research gaps and suggest future
research directions to enhance function configuration and strengthen the
capabilities of serverless computing environments to drive its broader
adoption.

</details>


### [82] [Product Manager Practices for Delegating Work to Generative AI: "Accountability must not be delegated to non-human actors"](https://arxiv.org/abs/2510.02504)
*Mara Ulloa,Jenna L. Butler,Sankeerti Haniyur,Courtney Miller,Barrett Amos,Advait Sarkar,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 本文研究了生成式人工智能(GenAI)对软件开发团队中产品经理(PM)工作的影响，通过调查、数据分析和访谈，揭示了PMs对GenAI的采用情况、任务委派框架及其适应策略。


<details>
  <summary>Details</summary>
Motivation: 当前大多数软件工程研究聚焦于开发者与GenAI的互动，缺乏对产品经理工作如何因GenAI演变的理解。

Method: 在微软进行混合方法研究，包括对885名产品经理的调查，731名PM的遥测数据分析，以及15名PM的访谈。

Result: 揭示了PMs当前的GenAI采用率、应用案例及其感知的收益和障碍；提出了一个PM评估任务委派给GenAI的框架；总结了PMs整合GenAI的适应实践及角色演变的看法。

Conclusion: 本文为理解GenAI在产品经理工作中的集成提供了理论和实证基础，促进了对未来软件开发角色变革和工作流采纳的讨论。

Abstract: Generative AI (GenAI) is changing the nature of knowledge work, particularly
for Product Managers (PMs) in software development teams. While much software
engineering research has focused on developers' interactions with GenAI, there
is less understanding of how the work of PMs is evolving due to GenAI. To
address this gap, we conducted a mixed-methods study at Microsoft, a large,
multinational software company: surveying 885 PMs, analyzing telemetry data for
a subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:
(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and
barriers and; (2) a framework capturing how PMs assess which tasks to delegate
to GenAI; (3) PMs adaptation practices for integrating GenAI into their roles
and perceptions of how their role is evolving. We end by discussing
implications on the broader GenAI workflow adoption process and software
development roles.

</details>


### [83] [ZeroFalse: Improving Precision in Static Analysis with LLMs](https://arxiv.org/abs/2510.02534)
*Mohsen Iranmanesh,Sina Moradi Sabet,Sina Marefat,Ali Javidi Ghasr,Allison Wilson,Iman Sharafaldin,Mohammad A. Tayebi*

Main category: cs.SE

TL;DR: ZeroFalse框架通过结合静态分析和大型语言模型（LLMs），有效减少静态应用安全测试工具的误报，同时保持高覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统SAST工具存在大量误报，导致开发者信任度低且需要大量手动筛查，限制了其应用效果。

Method: ZeroFalse将静态分析结果视为结构化合约，结合流敏感跟踪、上下文证据和特定CWE知识，再利用LLM进行判定，以保持静态分析的系统覆盖性并利用LLM的推理能力。

Result: 在OWASP Java Benchmark和OpenVuln数据集上，ZeroFalse实现了最高F1分数0.912及0.955，召回率和精确度均超过90%。CWE专用提示和推理导向的LLM表现优于通用提示和其他模型。

Conclusion: ZeroFalse是一种实用且可扩展的方法，显著提升SAST工具的可靠性，有助于其在实际CI/CD流水线中的集成应用。

Abstract: Static Application Security Testing (SAST) tools are integral to modern
software development, yet their adoption is undermined by excessive false
positives that weaken developer trust and demand costly manual triage. We
present ZeroFalse, a framework that integrates static analysis with large
language models (LLMs) to reduce false positives while preserving coverage.
ZeroFalse treats static analyzer outputs as structured contracts, enriching
them with flow-sensitive traces, contextual evidence, and CWE-specific
knowledge before adjudication by an LLM. This design preserves the systematic
reach of static analysis while leveraging the reasoning capabilities of LLMs.
We evaluate ZeroFalse across both benchmarks and real-world projects using ten
state-of-the-art LLMs. Our best-performing models achieve F1-scores of 0.912 on
the OWASP Java Benchmark and 0.955 on the OpenVuln dataset, maintaining recall
and precision above 90%. Results further show that CWE-specialized prompting
consistently outperforms generic prompts, and reasoning-oriented LLMs provide
the most reliable precision-recall balance. These findings position ZeroFalse
as a practical and scalable approach for enhancing the reliability of SAST and
supporting its integration into real-world CI/CD pipelines.

</details>


### [84] [Key Considerations for Auto-Scaling: Lessons from Benchmark Microservices](https://arxiv.org/abs/2510.02585)
*Majid Dashtbani,Ladan Tahvildari*

Main category: cs.SE

TL;DR: 本文研究了微服务架构中自动伸缩存在的挑战，强调了设计、实现和部署阶段的关键考虑因素，并通过实验证明关注这些关键因素可以提升自动伸缩性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动伸缩方法在评估时往往忽略了软件生命周期中的设计、实现和部署实践，导致难以在真实环境中发挥最佳效果。

Method: 本研究分类总结了自动伸缩的关键问题，并在Sock-Shop基准测试中验证了多种自动伸缩策略，包括阈值法、控制理论法、学习法、黑箱优化和依赖感知法。

Result: 结果显示，忽视生命周期关键问题会降低自动伸缩器性能，解决这些问题则能实现更稳定高效的伸缩。

Conclusion: 强调了在微服务自动伸缩中，关注架构、实现及部署的生命周期工程实践对于发挥自动伸缩的最大效能至关重要。

Abstract: Microservices have become the dominant architectural paradigm for building
scalable and modular cloud-native systems. However, achieving effective
auto-scaling in such systems remains a non-trivial challenge, as it depends not
only on advanced scaling techniques but also on sound design, implementation,
and deployment practices. Yet, these foundational aspects are often overlooked
in existing benchmarks, making it difficult to evaluate autoscaling methods
under realistic conditions. In this paper, we identify a set of practical
auto-scaling considerations by applying several state-of-the-art autoscaling
methods to widely used microservice benchmarks. To structure these findings, we
classify the issues based on when they arise during the software lifecycle:
Architecture, Implementation, and Deployment. The Architecture phase covers
high-level decisions such as service decomposition and inter-service
dependencies. The Implementation phase includes aspects like initialization
overhead, metrics instrumentation, and error propagation. The Deployment phase
focuses on runtime configurations such as resource limits and health checks. We
validate these considerations using the Sock-Shop benchmark and evaluate
diverse auto-scaling strategies, including threshold-based, control-theoretic,
learning-based, black-box optimization, and dependency-aware approaches. Our
findings show that overlooking key lifecycle concerns can degrade autoscaler
performance, while addressing them leads to more stable and efficient scaling.
These results underscore the importance of lifecycle-aware engineering for
unlocking the full potential of auto-scaling in microservice-based systems.

</details>


### [85] [RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents](https://arxiv.org/abs/2510.02609)
*Chengquan Guo,Chulin Xie,Yu Yang,Zhaorun Chen,Zinan Lin,Xander Davies,Yarin Gal,Dawn Song,Bo Li*

Main category: cs.SE

TL;DR: 本文提出了RedCodeAgent，一种自动化攻击代理，旨在系统性地发现代码代理中的安全漏洞，提升了攻击成功率和评估效率。


<details>
  <summary>Details</summary>
Motivation: 现有的静态安全基准测试和攻击红队工具难以覆盖边界条件，无法有效发现代码代理中的新兴安全风险。

Method: RedCodeAgent通过自适应记忆模块，利用已有的攻击知识，动态选择最有效的攻击工具组合，并采用模拟沙箱环境评估代码执行结果，避免仅依赖静态代码带来的偏见。

Result: 在多种先进代码代理、多样风险场景和多种编程语言的测试中，RedCodeAgent表现优于现有红队方法，攻击成功率更高，拒绝率更低，效率显著提升。

Conclusion: RedCodeAgent实现了代码代理的自动化和优化红队测试，支持可扩展、适应性强且高效的安全评估，揭示了真实代码助手中的未发现安全风险。

Abstract: Code agents have gained widespread adoption due to their strong code
generation capabilities and integration with code interpreters, enabling
dynamic execution, debugging, and interactive programming capabilities. While
these advancements have streamlined complex workflows, they have also
introduced critical safety and security risks. Current static safety benchmarks
and red-teaming tools are inadequate for identifying emerging real-world risky
scenarios, as they fail to cover certain boundary conditions, such as the
combined effects of different jailbreak tools. In this work, we propose
RedCodeAgent, the first automated red-teaming agent designed to systematically
uncover vulnerabilities in diverse code agents. With an adaptive memory module,
RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the
most effective red-teaming tools and tool combinations in a tailored toolbox
for a given input query, thus identifying vulnerabilities that might otherwise
be overlooked. For reliable evaluation, we develop simulated sandbox
environments to additionally evaluate the execution results of code agents,
mitigating potential biases of LLM-based judges that only rely on static code.
Through extensive evaluations across multiple state-of-the-art code agents,
diverse risky scenarios, and various programming languages, RedCodeAgent
consistently outperforms existing red-teaming methods, achieving higher attack
success rates and lower rejection rates with high efficiency. We further
validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,
exposing previously unidentified security risks. By automating and optimizing
red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective
safety assessments of code agents.

</details>


### [86] [Automatic Building Code Review: A Case Study](https://arxiv.org/abs/2510.02634)
*Hanlong Wan,Weili Xu,Michael Rosenberg,Jian Zhang,Aysha Siddika*

Main category: cs.SE

TL;DR: 本文提出了一种基于智能代理的新框架，结合BIM数据提取与自动化建筑规范审核，利用LLM技术实现高效、可靠的设计文件审查。


<details>
  <summary>Details</summary>
Motivation: 面对设计项目规模和复杂性的增加，建筑官员在资源有限的环境下进行手工审核成本高、易出错，推动自动化代码审核技术的发展。

Method: 引入基于BIM的数据提取和利用LLM驱动的检索增强生成(RAG)与模型上下文协议(MCP)两种智能代理流水线，实现几何、日程和系统属性的自动提取，并通过美国能源部COMcheck引擎API及RAG方式进行建筑规范核查。

Result: 框架在案例测试中成功完成几何属性提取、运行时间解析和照明配额验证。GPT-4o表现出最佳效率与稳定性，MCP代理性能优于RAG推理流水线。

Conclusion: 该方法实现了可扩展、互操作且适合生产环境的自动代码审核，为基于BIM的建筑规范审查提供了坚实技术支撑。

Abstract: Building officials, particularly those in resource-constrained or rural
jurisdictions, face labor-intensive, error-prone, and costly manual reviews of
design documents as projects increase in size and complexity. The growing
adoption of Building Information Modeling (BIM) and Large Language Models
(LLMs) presents opportunities for automated code review (ACR) solutions. This
study introduces a novel agent-driven framework that integrates BIM-based data
extraction with automated verification using both retrieval-augmented
generation (RAG) and Model Context Protocol (MCP) agent pipelines. The
framework employs LLM-enabled agents to extract geometry, schedules, and system
attributes from heterogeneous file types, which are then processed for building
code checking through two complementary mechanisms: (1) direct API calls to the
US Department of Energy COMcheck engine, providing deterministic and
audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling
flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated
extraction of geometric attributes (such as surface area, tilt, and insulation
values), parsing of operational schedules, and validation of lighting
allowances under ASHRAE Standard 90.1-2022. Comparative performance tests
across multiple LLMs showed that GPT-4o achieved the best balance of efficiency
and stability, while smaller models exhibited inconsistencies or failures.
Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in
rigor and reliability. This work advances ACR research by demonstrating a
scalable, interoperable, and production-ready approach that bridges BIM with
authoritative code review tools.

</details>


### [87] [Using Fourier Analysis and Mutant Clustering to Accelerate DNN Mutation Testing](https://arxiv.org/abs/2510.02718)
*Ali Ghanbari,Sasan Tavakkol*

Main category: cs.SE

TL;DR: 本文提出了一种基于傅里叶分析的深度神经网络变异测试加速技术DM#，通过克服变异体数量多导致的测试成本高的问题，实现了测试过程的显著加速。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络变异测试需要对大量变异体进行测试，且涉及大规模数据集，导致测试过程成本极高。

Method: 利用傅里叶分析对DNN输出进行量化，聚类行为相似的变异体，选择代表变异体测试，并将测试结果复用，从而减少需要测试的变异体数量。

Result: 在14个不同规模和数据集的DNN模型上，DM#平均加速变异测试28.38%，平均变异分数误差仅为0.72%，相比随机选择等方法误差显著降低。

Conclusion: DM#有效提升了深度神经网络变异测试的效率和准确性，显著降低了测试成本，具有较高的实用价值。

Abstract: Deep neural network (DNN) mutation analysis is a promising approach to
evaluating test set adequacy. Due to the large number of generated mutants that
must be tested on large datasets, mutation analysis is costly. In this paper,
we present a technique, named DM#, for accelerating DNN mutation testing using
Fourier analysis. The key insight is that DNN outputs are real-valued functions
suitable for Fourier analysis that can be leveraged to quantify mutant behavior
using only a few data points. DM# uses the quantified mutant behavior to
cluster the mutants so that the ones with similar behavior fall into the same
group. A representative from each group is then selected for testing, and the
result of the test, e.g., whether the mutant is killed or survived, is reused
for all other mutants represented by the selected mutant, obviating the need
for testing other mutants. 14 DNN models of sizes ranging from thousands to
millions of parameters, trained on different datasets, are used to evaluate DM#
and compare it to several baseline techniques. Our results provide empirical
evidence on the effectiveness of DM# in accelerating mutation testing by
28.38%, on average, at the average cost of only 0.72% error in mutation score.
Moreover, on average, DM# incurs 11.78, 15.16, and 114.36 times less mutation
score error compared to random mutant selection, boundary sample selection, and
random sample selection techniques, respectively, while generally offering
comparable speed-up.

</details>


### [88] [Automated Repair of OpenID Connect Programs (Extended Version)](https://arxiv.org/abs/2510.02773)
*Tamjid Al Rahat,Yanju Chen,Yu Feng,Yuan Tian*

Main category: cs.SE

TL;DR: 提出了一种基于大模型的自动修复引擎AuthFix，用于修复OpenID Connect中的安全漏洞，显著提高了自动生成补丁的正确率。


<details>
  <summary>Details</summary>
Motivation: OpenID Connect虽广泛使用但存在关键安全漏洞，造成重大损失，迫切需要有效的自动修复方法。

Method: AuthFix结合故障定位、补丁合成和验证，利用新的基于Petri网的模型检查器保证补丁正确性，并通过大模型辅助修复。

Result: 在23个OpenID漏洞修复测试中，AuthFix成功生成了17个正确补丁，准确率达74%，补丁与开发者修复高度语义等价。

Conclusion: AuthFix有效提升了OpenID漏洞自动修复的精度与可靠性，为安全漏洞修复提供了实用工具。

Abstract: OpenID Connect has revolutionized online authentication based on single
sign-on (SSO) by providing a secure and convenient method for accessing
multiple services with a single set of credentials. Despite its widespread
adoption, critical security bugs in OpenID Connect have resulted in significant
financial losses and security breaches, highlighting the need for robust
mitigation strategies. Automated program repair presents a promising solution
for generating candidate patches for OpenID implementations. However,
challenges such as domain-specific complexities and the necessity for precise
fault localization and patch verification must be addressed. We propose
AuthFix, a counterexample-guided repair engine leveraging LLMs for automated
OpenID bug fixing. AuthFix integrates three key components: fault localization,
patch synthesis, and patch verification. By employing a novel Petri-net-based
model checker, AuthFix ensures the correctness of patches by effectively
modeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates
that AuthFix successfully generated correct patches for 17 out of 23 bugs
(74%), with a high proportion of patches semantically equivalent to
developer-written fixes.

</details>


### [89] [C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development](https://arxiv.org/abs/2510.02854)
*Boshuai Ye,Arif Ali Khan,Teemu Pihkakoski,Peng Liang,Muhammad Azeem Akbar,Matti Silveri,Lauri Malmi*

Main category: cs.SE

TL;DR: 本文提出了C2|Q>: 一个硬件无关的量子软件开发框架，将经典代码转化为量子程序，简化了开发流程，提高了完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的量子开发环境复杂，需要处理底层细节，普通软件工程师难以使用。

Method: 将工作流分为编码器、部署模块和解码器三个核心模块，编码器生成量子电路，部署模块推荐硬件，解码器解释结果。

Result: 编码器完成率达93.8%，硬件推荐准确，整体框架处理Python和JSON输入完成率分别为93.8%和100%，实现效率提升40倍。

Conclusion: C2|Q>:有效降低了量子软件开发门槛，提高了开发效率，适合当前NISQ硬件能力的量子程序开发，并已开源。

Abstract: Quantum Software Engineering (QSE) is emerging as a critical discipline to
make quantum computing accessible to a broader developer community; however,
most quantum development environments still require developers to engage with
low-level details across the software stack - including problem encoding,
circuit construction, algorithm configuration, hardware selection, and result
interpretation - making them difficult for classical software engineers to use.
To bridge this gap, we present C2|Q>: a hardware-agnostic quantum software
development framework that translates classical specifications (code) into
quantum-executable programs while preserving methodological rigor. The
framework applies modular software engineering principles by classifying the
workflow into three core modules: an encoder that classifies problems, produces
Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a
deployment module that generates circuits and recommends hardware based on
fidelity, runtime, and cost, and a decoder that interprets quantum outputs into
classical solutions. In evaluation, the encoder module achieved a 93.8%
completion rate, the hardware recommendation module consistently selected the
appropriate quantum devices for workloads scaling up to 56 qubits, and the full
C2|Q>: workflow successfully processed classical specifications (434 Python
snippets and 100 JSON inputs) with completion rates of 93.8% and 100%,
respectively. For case study problems executed on publicly available NISQ
hardware, C2|Q>: reduced the required implementation effort by nearly 40X
compared to manual implementations using low-level quantum software development
kits (SDKs), with empirical runs limited to small- and medium-sized instances
consistent with current NISQ capabilities. The open-source implementation of
C2|Q>: is available at https://github.com/C2-Q/C2Q

</details>


### [90] [GramTrans: A Better Code Representation Approach in Code Generation](https://arxiv.org/abs/2510.02887)
*Zhao Zhang,Qingyuan Liang,Zeyu Sun,Yizhou Chen,Guoqing Wang,Yican Sun,Lu Zhang,Ge Li,Yingfei Xiong*

Main category: cs.SE

TL;DR: 论文研究了代码表示方式对代码生成模型性能的影响，提出代表性解析难度越低，模型表现越好的猜想，并通过实验证实了这一点。


<details>
  <summary>Details</summary>
Motivation: 现有研究对代码表示方法缺乏系统性理解，尤其是解析难度与模型性能的关系未被充分探讨。

Method: 通过形式化语法类（如LL(1)）定义解析难度，设计GramTrans算法将上下文无关语言转换至LL(1)类，提升代码表示的解析简易度。

Result: 在Python和Java的多个基准测试中，GramTrans相比传统表示方式显著提升了三种主流代码生成模型的性能。

Conclusion: 解析难度与模型性能高度相关，易解析的代码表示能有效提升生成效果，GramTrans验证并应用了这一理论，促进了代码生成技术的发展。

Abstract: Code generation has shown great promise in assisting software development. A
fundamental yet underexplored question is how the choice of code representation
affects model performance. While existing studies employ various
representations, such as treating code as plain text, grammar rule sequences,
or syntax tree sequences, they lack a principled understanding of the
relationship between parsing difficulty and model effectiveness. This paper
proposes a conjecture: the easier a representation is to parse, the better
performance the model achieves. We formalize this idea using grammar classes,
where representations in simpler classes (e.g., LL(1)) are easier to parse.
Through a controlled experiment on a Python-based DSL, we show that parsing
difficulty strongly correlates with model performance. Motivated by this
finding, we present GramTrans, a general approach that automatically transforms
a context-free language into a representation within the LL(1) class. GramTrans
introduces a novel hierarchical conflict elimination algorithm, enabling a
flexible trade-off between syntactic simplicity and token efficiency. We
evaluate GramTrans on both Python and Java using three code generation models:
StarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple
benchmarks, GramTrans consistently delivers significant improvements over
baseline representations. Furthermore, our analysis of existing representations
reconfirms the strong alignment between parsing difficulty and model
performance, providing additional support for the conjecture.

</details>


### [91] [Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders](https://arxiv.org/abs/2510.02917)
*Kriz Tahimic,Charibeth Cheng*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型（LLM）中表示的稀疏自编码器，识别与代码正确性相关的方向，发现这些方向能有效预测和纠正代码错误，并揭示了代码生成中关注测试用例的重要性及模型微调对机制的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在软件开发中的广泛应用，理解其内部代码正确性机制对于安全部署至关重要。

Method: 利用稀疏自编码器分解LLM表示，选择预测方向和引导方向，结合注意力分析和权重正交化方法研究其机制特性。

Result: 发现与代码正确性相关的方向能可靠预测错误代码，纠正能力存在修正错误与保留正确代码的权衡，代码生成更依赖测试用例关注而非问题描述，引导方向在微调后仍保持有效性。

Conclusion: 机制洞察提出三大实践应用：提示优先关注测试用例，预测方向可作为错误预警工具，并用预测方向实现选择性引导以避免代码损坏。

Abstract: As Large Language Models become integral to software development, with
substantial portions of AI-suggested code entering production, understanding
their internal correctness mechanisms becomes critical for safe deployment. We
apply sparse autoencoders to decompose LLM representations, identifying
directions that correspond to code correctness. We select predictor directions
using t-statistics and steering directions through separation scores from base
model representations, then analyze their mechanistic properties through
steering, attention analysis, and weight orthogonalization. We find that code
correctness directions in LLMs reliably predict incorrect code, while
correction capabilities, though statistically significant, involve tradeoffs
between fixing errors and preserving correct code. Mechanistically, successful
code generation depends on attending to test cases rather than problem
descriptions. Moreover, directions identified in base models retain their
effectiveness after instruction-tuning, suggesting code correctness mechanisms
learned during pre-training are repurposed during fine-tuning. Our mechanistic
insights suggest three practical applications: prompting strategies should
prioritize test examples over elaborate problem descriptions, predictor
directions can serve as error alarms for developer review, and these same
predictors can guide selective steering, intervening only when errors are
anticipated to prevent the code corruption from constant steering.

</details>


### [92] [Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection](https://arxiv.org/abs/2510.02934)
*Thanh Trong Vu,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.SE

TL;DR: 针对大型语言模型生成代码正确性评估，AUTOPROBE通过动态选择内部表征实现更准确检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定层和固定位置的内部表示，限制了适用性和泛化能力。

Method: AUTOPROBE利用注意力机制动态选择最具信息量的隐藏状态，结合探针分类器多维度评估代码正确性。

Result: 在多个基准测试和模型上，AUTOPROBE在安全性、可编译性和功能性评估上均优于现有方法，安全性提升18%，在复杂代码评估中提高19%-111%。

Conclusion: 动态选择重要内部特征使AUTOPROBE成为一种稳健、泛化性强的代码正确性评估方案，适用于多种大型语言模型生成代码。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.

</details>


### [93] [Tracing and Metrics Design Patterns for Monitoring Cloud-native Applications](https://arxiv.org/abs/2510.02991)
*Carlos Albuquerque,Filipe F. Correia*

Main category: cs.SE

TL;DR: 本文提出了三种用于云原生应用监控的设计模式，分别是分布式追踪、应用指标和基础设施指标，旨在提升系统的可观察性和问题诊断能力。


<details>
  <summary>Details</summary>
Motivation: 随着软件架构日益分布式和动态变化，传统监控手段难以有效诊断系统问题，面临碎片化的可观察性和复杂的根因分析挑战。

Method: 基于先前研究和行业实践，提出三种设计模式：分布式追踪用于跨服务请求流的可见性，应用指标用于实时性能监控和异常检测，基础设施指标用于环境资源利用和运行健康监测。

Result: 通过这三种设计模式，提升了对云原生应用的监控效果，有助于更准确地分析延迟、定位根因及评估系统资源和健康状态。

Conclusion: 本文设计模式为软件从业者提供了实用的指导，助力提升云原生系统的可靠性和可维护性。

Abstract: Observability helps ensure the reliability and maintainability of
cloud-native applications. As software architectures become increasingly
distributed and subject to change, it becomes a greater challenge to diagnose
system issues effectively, often having to deal with fragmented observability
and more difficult root cause analysis. This paper builds upon our previous
work and introduces three design patterns that address key challenges in
monitoring cloud-native applications. Distributed Tracing improves visibility
into request flows across services, aiding in latency analysis and root cause
detection, Application Metrics provides a structured approach to instrumenting
applications with meaningful performance indicators, enabling real-time
monitoring and anomaly detection, and Infrastructure Metrics focuses on
monitoring the environment in which the system is operated, helping teams
assess resource utilization, scalability, and operational health. These
patterns are derived from industry practices and observability frameworks and
aim to offer guidance for software practitioners.

</details>


### [94] [Patterns for Teaching Agile with Student Projects -- Team and Project Setup](https://arxiv.org/abs/2510.03005)
*Daniel Pinho,Petr Pícha,Filipe Correia,Přemek Brada*

Main category: cs.SE

TL;DR: 本文介绍了一套针对高等教育中敏捷软件开发教学的模式语言，聚焦于团队和项目启动阶段，提出了五个具体教学模式。


<details>
  <summary>Details</summary>
Motivation: 随着敏捷软件开发理念在行业中的普及，高等教育中相关课程增多，但现有文献缺乏具体可操作的教学建议。

Method: 基于教育者自身经验，提出一套教学模式语言，涵盖团队和项目设置的关键方面，如团队规模、项目范围及团队自主组建等。

Result: 提出了五个具体模式：限制团队规模、缩小项目范围、非关键业务项目、自组团队及团队自选课题，作为整体模式语言的起点。

Conclusion: 该模式语言为高校中敏捷软件开发教学提供了具体的实践指导，有助于改进课程设计和教学效果。

Abstract: Higher education courses teaching about agile software development (ASD) have
increased in commonality as the ideas behind the Agile Manifesto became more
commonplace in the industry. However, a lot of the literature on how ASD is
applied in the classroom does not provide much actionable advice, focusing on
frameworks or even moving beyond the software development area into teaching in
an agile way. We, therefore, showcase early work on a pattern language that
focuses on teaching ASD practices to university students, which stems from our
own experiences as educators in higher education contexts. We present five
patterns, specifically focused on team and project setup phase: Capping Team
Size, Smaller Project Scope, Business Non-Critical Project, Self-assembling
Teams, and Team Chooses Topic as a starting point for developing the overall
pattern language.

</details>


### [95] [Investigating The Smells of LLM Generated Code](https://arxiv.org/abs/2510.03029)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: 本研究提出了一种基于场景的方法评估大语言模型（LLM）生成代码的质量，主要通过代码气味的测量，发现LLM生成的代码质量明显低于人工代码。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量研究关注LLM生成代码的功能正确性，但对代码质量的研究较少，本研究旨在填补这一空白，识别LLM代码质量最薄弱的使用场景。

Method: 通过测量代码气味并与专业人员编写的参考代码进行对比，将测试数据集按代码主题和任务复杂度划分为不同场景，使用四种先进LLM（Gemini Pro、ChatGPT、Codex、Falcon）生成Java程序进行实验，并开发了自动测试系统。

Result: LLM生成代码的代码气味明显多于参考代码，其中Falcon表现最好，气味增加42.28%，其他模型如Codex增幅高达84.97%。代码气味随任务复杂度和高级主题（如面向对象）而增加。

Conclusion: LLM在不同场景下代码质量与对应人类代码质量高度相关，但总体上LLM生成代码的质量远低于人类代码。

Abstract: Context: Large Language Models (LLMs) are increasingly being used to generate
program code. Much research has been reported on the functional correctness of
generated code, but there is far less on code quality.
  Objectives: In this study, we propose a scenario-based method of evaluating
the quality of LLM-generated code to identify the weakest scenarios in which
the quality of LLM generated code should be improved.
  Methods: The method measures code smells, an important indicator of code
quality, and compares them with a baseline formed from reference solutions of
professionally written code. The test dataset is divided into various subsets
according to the topics of the code and complexity of the coding tasks to
represent different scenarios of using LLMs for code generation. We will also
present an automated test system for this purpose and report experiments with
the Java programs generated in response to prompts given to four
state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.
  Results: We find that LLM-generated code has a higher incidence of code
smells compared to reference solutions. Falcon performed the least badly, with
a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)
and finally Codex (84.97%). The average smell increase across all LLMs was
63.34%, comprising 73.35% for implementation smells and 21.42% for design
smells. We also found that the increase in code smells is greater for more
complex coding tasks and for more advanced topics, such as those involving
object-orientated concepts.
  Conclusion: In terms of code smells, LLM's performances on various coding
task complexities and topics are highly correlated to the quality of human
written code in the corresponding scenarios. However, the quality of LLM
generated code is noticeably poorer than human written code.

</details>


### [96] [Refactoring Towards Microservices: Preparing the Ground for Service Extraction](https://arxiv.org/abs/2510.03050)
*Rita Peixoto,Filipe F. Correia,Thatiane Rosa,Eduardo Guerra,Alfredo Goldman*

Main category: cs.SE

TL;DR: 本文提出了一个包括七种重构方法的目录，专注处理代码级依赖，支持从单体架构向微服务架构的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的微服务迁移策略主要关注架构层面，忽视了开发者在迁移过程中面对的代码级依赖和挑战，迁移过程依旧繁琐且劳动密集。

Method: 归纳并整理文献中已识别的七种重构方法，形成一个系统化的目录，指导开发者如何在代码层面处理依赖，支持微服务架构迁移。

Result: 提供了一个结构化、分步骤的迁移指导目录，简化了迁移过程，并为迁移自动化打下基础。

Conclusion: 通过该目录，开发者能够更高效、有效地进行微服务迁移，实现代码级依赖管理，推动迁移过程的简化和自动化发展。

Abstract: As organizations increasingly transition from monolithic systems to
microservices, they aim to achieve higher availability, automatic scaling,
simplified infrastructure management, enhanced collaboration, and streamlined
deployments. However, this migration process remains largely manual and
labour-intensive. While existing literature offers various strategies for
decomposing monoliths, these approaches primarily focus on architecture-level
guidance, often overlooking the code-level challenges and dependencies that
developers must address during the migration. This article introduces a
catalogue of seven refactorings specifically designed to support the transition
to a microservices architecture with a focus on handling dependencies. The
catalogue provides developers with a systematic guide that consolidates
refactorings identified in the literature and addresses the critical gap in
systematizing the process at the code level. By offering a structured,
step-by-step approach, this work simplifies the migration process and lays the
groundwork for its potential automation, empowering developers to implement
these changes efficiently and effectively.

</details>


### [97] [State Field Coverage: A Metric for Oracle Quality](https://arxiv.org/abs/2510.03071)
*Facundo Molina,Nazareno Aguirre,Alessandra Gorla*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的度量指标“状态字段覆盖率”用于评估测试预言机（oracle）的质量，以提高软件缺陷检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有的测试预言机质量评估指标不够全面，或者仅适用于特定类型的预言机，限制了其通用性和指导改善的能力。

Method: 引入状态字段覆盖率作为衡量指标，静态分析对象状态字段被预言机在测试执行中访问的比例，进而评估预言机的质量。实现了一种静态计算该指标的机制，便于识别未被检查的状态字段以指导改善。

Result: 通过273个表示不变式和249,027个测试断言的实验，结果表明状态字段覆盖率与预言机的缺陷检测能力（通过变异分数衡量）强相关。

Conclusion: 状态字段覆盖率是评估测试预言机质量的有效指标，能够为增强预言机检测能力提供直接指导。

Abstract: The effectiveness of testing in uncovering software defects depends not only
on the characteristics of the test inputs and how thoroughly they exercise the
software, but also on the quality of the oracles used to determine whether the
software behaves as expected. Therefore, assessing the quality of oracles is
crucial to improve the overall effectiveness of the testing process. Existing
metrics have been used for this purpose, but they either fail to provide a
comprehensive basis for guiding oracle improvement, or they are tailored to
specific types of oracles, thus limiting their generality.
  In this paper, we introduce state field coverage, a novel metric for
assessing oracle quality. This metric measures the proportion of an object's
state, as statically defined by its class fields, that an oracle may access
during test execution. The main intuition of our metric is that oracles with a
higher state field coverage are more likely to detect faults in the software
under analysis, as they inspect a larger portion of the object states to
determine whether tests pass or not.
  We implement a mechanism to statically compute the state field coverage
metric. Being statically computed, the metric is efficient and provides direct
guidance for improving test oracles by identifying state fields that remain
unexamined. We evaluate state field coverage through experiments involving 273
representation invariants and 249,027 test assertions. The results show that
state field coverage is a well-suited metric for assessing oracle quality, as
it strongly correlates with the oracles' fault-detection ability, measured by
mutation score.

</details>


### [98] [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
*Cuong Chi Le,Minh V. T. Pham,Cuong Duc Van,Hoang N. Phan,Huy N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型在代码任务中对程序意义的理解，强调代码通过结构语义和命名两种方式传递信息，并提出了去命名通道对任务表现的影响及命名泄露问题，提出了语义保留混淆技术和相应基准，提升了代码理解评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码任务中表现优秀，但其对程序意义的理解机制不清楚，且现有基准可能奖励对命名模式的记忆而非真正的语义推理，影响对模型能力的准确评估。

Method: 提出语义保留的代码混淆技术，系统抑制命名线索但保留程序行为，设计了ClassEval-Obf混淆增强基准来测试模型的代码理解和泛化能力，揭示命名泄露现象。

Result: 去除命名通道显著降低了对意图层面任务（如代码摘要）的表现，且对结构依赖的执行任务也产生影响，表明命名泄露广泛存在；ClassEval-Obf基准有效减少了模型的记忆捷径，缩小了性能差距，提升评价的可靠性。

Conclusion: 代码命名在大型语言模型代码理解中扮演重要角色，当前基准存在命名信息泄露问题，采用语义保留混淆技术和ClassEval-Obf基准能更准确地评估模型的真是语义推理和泛化能力。

Abstract: Large Language Models (LLMs) achieve strong results on code tasks, but how
they derive program meaning remains unclear. We argue that code communicates
through two channels: structural semantics, which define formal behavior, and
human-interpretable naming, which conveys intent. Removing the naming channel
severely degrades intent-level tasks such as summarization, where models
regress to line-by-line descriptions. Surprisingly, we also observe consistent
reductions on execution tasks that should depend only on structure, revealing
that current benchmarks reward memorization of naming patterns rather than
genuine semantic reasoning. To disentangle these effects, we introduce a suite
of semantics-preserving obfuscations and show that they expose identifier
leakage across both summarization and execution. Building on these insights, we
release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically
suppresses naming cues while preserving behavior. Our results demonstrate that
ClassEval-Obf reduces inflated performance gaps, weakens memorization
shortcuts, and provides a more reliable basis for assessing LLMs' code
understanding and generalization.

</details>


### [99] [Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair](https://arxiv.org/abs/2510.03217)
*José Cambronero,Michele Tufano,Sherry Shi,Renyao Wei,Grant Uy,Runxiang Cheng,Chin-Jung Liu,Shiying Pan,Satish Chandra,Pat Rondon*

Main category: cs.SE

TL;DR: 本文提出了两种基于大语言模型的策略（错误弃权和补丁验证）来减少自动程序修复中的噪声，提高补丁成功率。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复生成的补丁需要人工审核，显示不太可能成功的补丁会浪费开发者时间并降低信任度。

Method: 利用两种策略：错误弃权（排除可能无法修复的错误）和补丁验证（拒绝错误补丁），在谷歌代码库中验证。

Result: 在174个人工报告的错误上，策略提升修复成功率最高分别达13%和15%，组合使用最高提升39%。对空指针异常和其他自动报告的错误同样有效。

Conclusion: 这两种策略为工业界大规模、可靠地部署自动程序修复系统提供了可行方案。

Abstract: Agentic Automated Program Repair (APR) is increasingly tackling complex,
repository-level bugs in industry, but ultimately agent-generated patches still
need to be reviewed by a human before committing them to ensure they address
the bug. Showing unlikely patches to developers can lead to substantial noise,
wasting valuable developer time and eroding trust in automated code changes. We
introduce two complementary LLM-based policies to reduce such noise: bug
abstention and patch validation policies. Bug abstention excludes bugs that the
agentic APR system is unlikely to fix. Patch validation rejects patches that
are unlikely to be a good fix for the given bug. We evaluate both policies on
three sets of bugs from Google's codebase, and their candidate patches
generated by an internal agentic APR system. On a set of 174 human-reported
bugs, removing bugs and patch trajectories rejected by our policies can raise
success rates by up to 13 percentage points and 15 percentage points,
respectively, and by up to 39 percentage points in combination. On null pointer
exceptions and sanitizer-reported bugs with machine-generated bug reports,
patch validation also improves average single-sample success rates. This
two-policy approach provides a practical path to the reliable, industrial-scale
deployment of agentic APR systems.

</details>
