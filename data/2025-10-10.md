<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 97]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
*Jingfei Huang,Han Tu*

Main category: cs.CL

TL;DR: 本文通过结合街景图像与社交媒体文本数据，提出新方法揭示城市中人们感知与舆论情绪的不一致性，并分析其变化规律及影响因素，为城市环境管理和更新提供策略依据。


<details>
  <summary>Details</summary>
Motivation: 社会媒体的发展使得对城市环境的情绪分析更加复杂，现有多维度情绪分析方法难以捕捉个体感知与舆论之间的情绪差异和变化。

Method: 构建了包含街景图像与微博文本的大型数据集，开发反应指数，运用目标检测和自然语言处理技术分类北京市2016年与2022年第二环线区域的情绪反应，结合回归分析、图像分割和词频分析揭示情绪变化及其与土地利用的关系。

Result: 感知情绪趋向均匀分布的正面趋势，舆论情绪变化更为剧烈，两者存在显著差异；情绪变化与建筑密度及行人数量等因素相关；疫情前后情绪映射揭示不一致情绪的时空演变。

Conclusion: 研究揭示了城市感知与舆论情绪的显著不一致及其环境关联，为疫情后城市环境管理和更新策略制定提供科学依据和新视角。

Abstract: The ascension of social media platforms has transformed our understanding of
urban environments, giving rise to nuanced variations in sentiment reaction
embedded within human perception and opinion, and challenging existing
multidimensional sentiment analysis approaches in urban studies. This study
presents novel methodologies for identifying and elucidating sentiment
inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent
Street view images to measure perceptions, and 984,024 Weibo social media text
posts to measure opinions. A reaction index is developed, integrating object
detection and natural language processing techniques to classify sentiment in
Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is
analysed and visualized using regression analysis, image segmentation, and word
frequency based on land-use distribution to discern underlying factors. The
perception affective reaction trend map reveals a shift toward more evenly
distributed positive sentiment, while the opinion affective reaction trend map
shows more extreme changes. Our mismatch map indicates significant disparities
between the sentiments of human perception and opinion of urban areas over the
years. Changes in sentiment reactions have significant relationships with
elements such as dense buildings and pedestrian presence. Our inconsistent maps
present perception and opinion sentiments before and after the pandemic and
offer potential explanations and directions for environmental management, in
formulating strategies for urban renewal.

</details>


### [2] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

TL;DR: 该论文提出了HaystackCraft，一个基于维基百科超链接网络的新长上下文语言模型评测基准，用以评估模型在噪声上下文中的鲁棒性，重点关注检索策略多样性和动态代理操作对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文语言模型在合成的“针叶堆”测试中表现优异，但未能覆盖现实中来源于偏见检索和代理流程中噪声上下文的复杂情况，因此需要构建更真实的噪声长上下文环境进行评测。

Method: 作者设计了HaystackCraft，利用维基百科多跳超链接网络，结合稀疏、密集、混合及图检索策略，动态模拟模型自我调整查询和停止时机的代理操作，以系统评估不同检索策略和动态环境下模型表现。

Result: 15个长上下文模型实验显示：密集检索器虽带来更具挑战的干扰信息，但图检索重排序可同时提升检索效果并削弱有害干扰；高级模型如Gemini 2.5 Pro及GPT-5在动态代理测试中仍受困于自生成干扰和停止决策困难。

Conclusion: 研究揭示了当前长上下文代理推理面临的持续挑战，验证了HaystackCraft在真实噪声环境中评测模型鲁棒性的价值，为未来模型改进提供了重要测试平台。

Abstract: Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [3] [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
*Olia Toporkov,Alan Akbik,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在词形还原任务中的表现，特别是在缺乏监督训练数据的情况下，LLMs通过少量示例进行上下文词形还原，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统词形还原依赖监督训练数据，面对无监督或跨语言的情况效果有限。大型语言模型的能力在上下文词形还原任务中的有效性尚未明确。

Method: 比较了基于编码器的监督方法（外域微调）、跨语言方法与LLMs直接进行上下文词形还原的效果。实验涉及12种形态复杂度不同的语言。

Result: LLMs在无监督情况下，通过少量示例直接生成词形，取得了大多数语言的最新最佳结果。编码器方法在有金标准外域微调时仍具备竞争力。

Conclusion: 大型语言模型具备强大的上下文词形还原能力，可在无监督和跨语言场景中超越传统方法，适合现实应用。代码和数据将在论文发布后开放。

Abstract: Lemmatization is the task of transforming all words in a given text to their
dictionary forms. While large language models (LLMs) have demonstrated their
ability to achieve competitive results across a wide range of NLP tasks, there
is no prior evidence of how effective they are in the contextual lemmatization
task. In this paper, we empirically investigate the capacity of the latest
generation of LLMs to perform in-context lemmatization, comparing it to the
traditional fully supervised approach. In particular, we consider the setting
in which supervised training data is not available for a target domain or
language, comparing (i) encoder-only supervised approaches, fine-tuned
out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma
generation with LLMs. Our experimental investigation across 12 languages of
different morphological complexity finds that, while encoders remain
competitive in out-of-domain settings when fine-tuned on gold data, current
LLMs reach state-of-the-art results for most languages by directly generating
lemmas in-context without prior fine-tuning, provided just with a few examples.
Data and code available upon publication:
https://github.com/oltoporkov/lemma-dilemma

</details>


### [4] [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
*Amruta Parulekar,Preethi Jyothi*

Main category: cs.CL

TL;DR: 提出了一种基于大型语言模型（LLM）的ASR评估方法LASER，改进了传统WER指标对形态和句法细微差异的不公平惩罚。


<details>
  <summary>Details</summary>
Motivation: 传统的ASR评价指标如WER对形态和句法细节的差异过于严格，无法准确反映语义层面的错误，导致评价不够合理。

Method: 利用最先进的LLM的上下文学习能力，通过包含详细示例的提示设计评分规则（LASER）；并使用Gemini 2.5 Pro进行评分。此外，将小型LLM（如Llama 3）微调于参考和ASR预测的词对示例，实现错误惩罚的准确预测。

Result: LASER在印地语上的得分与人工标注的相关度高达94%，其印地语示例还能有效用于马拉地语、卡纳达语和马拉雅拉姆语的错误分析。微调后的Llama 3在惩罚预测上也达到了近89%的准确率。

Conclusion: 基于LLM的LASER评分方法能够更准确地评估ASR输出，尤其在处理形态和句法细节上优于传统WER，且对多种印度语言有良好适应性。同时，微调小型LLM可有效辅助错误类型判定。

Abstract: Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly
penalize morphological and syntactic nuances that do not significantly alter
sentence semantics. We introduce an LLM-based scoring rubric LASER that
leverages state-of-the-art LLMs' in-context learning abilities to learn from
prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro
achieved a very high correlation score of 94% with human annotations. Hindi
examples in the prompt were also effective in analyzing errors in other Indian
languages such as Marathi, Kannada and Malayalam. We also demonstrate how a
smaller LLM like Llama 3 can be finetuned on word-pair examples derived from
reference and ASR predictions to predict what kind of penalty should be applied
with close to 89% accuracy.

</details>


### [5] [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
*Zifan Jiang,Colin Leong,Amit Moryossef,Anne Göhring,Annette Rios,Oliver Cory,Maksym Ivashechkin,Neha Tarigopula,Biao Zhang,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文针对手语表达中的人体骨骼姿势提出了全面的评估方法，比较了关键点距离、嵌入和回译等多种指标，通过自动和人工实验验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译和生成系统缺乏统一和有效的姿势评价指标，难以客观衡量模型输出质量。

Method: 研究了基于关键点距离、嵌入向量以及回译的多种评估指标，并通过自动元评价和人类相关性研究验证指标在不同手语和任务中的表现。

Result: 发现不同指标在不同场景下存在权衡，提出了一套实用且可复现的开源姿势评估工具包，便于手语翻译系统的开发和评估。

Conclusion: 本文为手语姿势的评估提供了系统且实用的方法，对推动手语翻译技术的标准化和性能提升具有重要意义。

Abstract: We present a comprehensive study on meaningfully evaluating sign language
utterances in the form of human skeletal poses. The study covers keypoint
distance-based, embedding-based, and back-translation-based metrics. We show
tradeoffs between different metrics in different scenarios through automatic
meta-evaluation of sign-level retrieval and a human correlation study of
text-to-pose translation across different sign languages. Our findings and the
open-source pose-evaluation toolkit provide a practical and reproducible way of
developing and evaluating sign language translation or generation systems.

</details>


### [6] [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
*Eduardo Ryô Tamaki,Yujin J. Jung,Julia Chatterley,Grant Mitchell,Semir Dzebo,Cristóbal Sandoval,Levente Littvay,Kirk A. Hawkins*

Main category: cs.CL

TL;DR: 本文提出了一种基于链式思维的提示策略，通过模仿人工编码者训练过程，利用全球民粹主义数据库，使用大语言模型对政治演讲中的民粹主义程度进行有效分类，实现了与专家人工编码者相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有文本分析方法在测量民粹主义思想内容时成本高、耗时长，且难以跨语言和大规模文本中推广。

Method: 采用基于评分标准和锚点引导的链式思维提示方法，利用全球民粹主义数据库的注释数据，模仿人工编码者的训练过程，引导大语言模型进行推理和分类。

Result: 多种专有及开放权重模型均能复制数据库中的评分，显示该提示策略使模型达到与专家编码者相匹配的分类准确率。

Conclusion: 通过领域特定的提示策略，大语言模型能够有效捕捉民粹主义的细微和语境敏感特征，实现高效且可扩展的民粹主义测量。

Abstract: Measuring the ideational content of populism remains a challenge. Traditional
strategies based on textual analysis have been critical for building the
field's foundations and providing a valid, objective indicator of populist
framing. Yet these approaches are costly, time consuming, and difficult to
scale across languages, contexts, and large corpora. Here we present the
results from a rubric and anchor guided chain of thought (CoT) prompting
approach that mirrors human coder training. By leveraging the Global Populism
Database (GPD), a comprehensive dataset of global leaders' speeches annotated
for degrees of populism, we replicate the process used to train human coders by
prompting the LLM with an adapted version of the same documentation to guide
the model's reasoning. We then test multiple proprietary and open weight models
by replicating scores in the GPD. Our findings reveal that this domain specific
prompting strategy enables the LLM to achieve classification accuracy on par
with expert human coders, demonstrating its ability to navigate the nuanced,
context sensitive aspects of populism.

</details>


### [7] [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
*Zheyuan Zhang,Lin Ge,Hongjiang Li,Weicheng Zhu,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 本文提出MAPRO框架，通过最大后验推断优化多代理系统的提示词设计，实现多代理提示词的自动化优化，提升多代理系统性能。


<details>
  <summary>Details</summary>
Motivation: 多代理系统设计困难，提示词敏感且不稳定，现有自动提示设计方法未充分解决多代理提示词优化挑战，需系统性方法应对复杂搜索空间和模糊的信用分配问题。

Method: 将多代理提示词优化建模为最大后验推断问题，用语言指导的最大乘积置信传播算法求解，结合拓扑感知的迭代更新机制，基于执行反馈和责任归属选择性更新代理提示词，实现协调优化。

Result: MAPRO在多任务基准测试中表现出色，持续超过手工设计和现有自动化方法，取得了最先进的性能。

Conclusion: 基于最大后验推断的MAPRO框架不仅提升多代理系统提示词优化效果，还提出了构建更可靠和原则性多代理系统的通用指导。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, and LLM-based agents further extend these abilities to various
practical workflows. While recent progress shows that multi-agent systems (MAS)
can outperform single agents by coordinating specialized roles, designing
effective MAS remains difficult due to prompt sensitivity and the compounded
instability MAS creates. To cope with the challenge, recent efforts in
automated prompt design have reduced manual effort. However, multi-agent prompt
optimization remains largely unexplored. Challenges like exponentially
expanding search space and ambiguous credit assignment together make systematic
design intractable without principled methods. Therefore, we introduce
M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first
formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference
problem and solves it using a language-guided variant of max-product belief
propagation algorithm. To address credit assignment and updates the system
iteratively, MAPRO employs a topology-aware refinement mechanism that
integrates execution feedback and downstream blames to selectively update agent
prompts. Through this process, MAPRO progressively converges to a coordinated
set of agent-specific prompt policies. Across benchmarks in various tasks,
MAPRO achieves state-of-the-art performance, consistently surpassing manually
engineered baselines and recent automated alternatives. Beyond performance, our
MAP-based formulation also delivers general guidelines for building more
reliable and principled multi-agent systems in the future

</details>


### [8] [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
*Shuqing Luo,Yilin Guan,Pingzhi Li,Hanrui Wang,Tianlong Chen*

Main category: cs.CL

TL;DR: 本文提出了AsyncSpade，一种高效的测试时缩放(TTS)异步框架，通过无缝并行KV缓存筛选和推理，显著提升长链思考(Chain-of-Thought)任务的推理速度和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTS方法由于KV缓存线性增长，导致内存瓶颈和推理效率低下，尤其在高并发和长链推理场景下，串行的页级稀疏解码限制性能提升。

Method: 提出了基于短期查询历史的轻量时序回归模块预测下一个查询状态，结合异步解耦框架实现KV缓存筛选和自回归解码的重叠，从而消除序列依赖并提升效率。

Result: AsyncSpade在A100节点测试环境下实现KV缓存操作与推理流水线的完全重叠，比现有领先方法Quest提升20%以上的时间效率，相较全注意力模型减少至少50%的TPOT，且在多个TTS基准上准确率持平或更优。

Conclusion: AsyncSpade有效解决了TTS中KV缓存和解码的耦合问题，显著提升模型在长链推理和高并发环境下的推理速度和性能，具备广泛应用前景。

Abstract: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),
but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM
decoding. Query-aware page-level sparse decoding can achieve state-of-the-art
performance under constrained FLOPs budgets, but is limited by both
sequential-dependent page filtering and coarse-grained token selection,
hampering serving efficiency and model performance on TTS tasks under high
concurrency and long CoT scenarios (consuming even higher runtime than the
forward pipeline itself). In this paper, we first find that the current-step
query state can be accurately approximated in a unified manner from a short
window of recent queries, enabling training-free query-aware sparsity without
waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework
for efficient TTS built on two core components: (1) a novel light-weight
temporal-regressive module that predicts the next-token query state; (2) an
asynchronous and disaggregated framework that decouples the KV cache filtering
from the auto-regressive decoding loop, overlapping the token-level KV
selection with the forward inference computation through asynchronism. To our
knowledge, AsyncSpade is the first to eliminate the sequential dependence
without sacrificing model performance. We validate the effectiveness of
AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade
fully overlaps KV-cache operations with the inference pipeline, achieving
theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade
delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and
at least 50% TPOT reduction compared to full attention on Qwen3-8B and
Qwen3-32B models, while matching or surpassing their accuracy on various TTS
benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

</details>


### [9] [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
*Rasika Muralidharan,Jaewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 本文提出多智能体框架以研究基于大语言模型代理的团队结构、多样性及交互动态，发现扁平结构团队表现优于层级结构，且多样性影响复杂。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体系统关注较多，然而对团队动态特别是团队科学核心方面的研究较少。

Method: 借鉴人类团队科学，设计多智能体框架，从结构、多样性和交互动态三个维度分析团队表现；通过四个任务（CommonsenseQA、StrategyQA、Social IQa 和 Latent Implicit Hate）评估团队表现，并进行访谈以理解团队协作。

Result: 实验发现扁平团队性能优于层级团队，多样性的影响较为复杂；访谈揭示代理对团队表现过于自信，但任务后反思显示他们认可协作的重要性，同时面临交流协调有限等挑战。

Conclusion: 基于大语言模型的多智能体系统团队结构对表现影响显著，未来需加强团队成员间的交流与协作机制以提升整体表现。

Abstract: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are
gaining attention, yet fewer studies explore their team dynamics. Inspired by
human team science, we propose a multi-agent framework to examine core aspects
of team science: structure, diversity, and interaction dynamics. We evaluate
team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and
Latent Implicit Hate, spanning commonsense and social reasoning. Our results
show that flat teams tend to perform better than hierarchical ones, while
diversity has a nuanced impact. Interviews suggest agents are overconfident
about their team performance, yet post-task reflections reveal both
appreciation for collaboration and challenges in integration, including limited
conversational coordination.

</details>


### [10] [Can Speech LLMs Think while Listening?](https://arxiv.org/abs/2510.07497)
*Yi-Jen Shih,Desh Raj,Chunyang Wu,Wei Zhou,SK Bong,Yashesh Gaur,Jay Mahadeokar,Ozlem Kalinli,Mike Seltzer*

Main category: cs.CL

TL;DR: 本文研究了链式思维微调对多流语音大语言模型的影响，证明了在文本空间推理可显著提升语音模型的准确率，并提出了基于熵的“问题完整性”指标以实现边听边思考，显著降低响应延迟，同时使用直接偏好优化进一步提升准确率与延迟的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型在复杂推理任务中表现较弱，且语音交互中响应延迟影响用户体验，亟需提高推理能力同时降低响应延迟。

Method: 对多流语音大语言模型进行链式思维（CoT）微调，采用基于熵的“问题完整性”指标指导模型在用户提问结束前开始推理，使用拒绝采样生成偏好数据并应用直接偏好优化（DPO）提升准确率-延迟平衡。

Result: CoT微调使语音模型推理准确率平均提升2.4倍，基于“问题完整性”的推理提前策略在相同延迟下提升4%准确率，DPO方法实现了70%延迟降低且准确率无损。

Conclusion: 结合链式思维微调及基于熵的提前推理机制，可有效提升语音大语言模型的推理准确率和响应速度，DPO优化进一步推动准确率与延迟的最优平衡。

Abstract: Recent advances in speech large language models (speech LLMs) have enabled
seamless spoken interactions, but these systems still struggle with complex
reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning
has been to shown to significantly improve the reasoning abilities of
text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for
multi-stream speech LLMs, demonstrating that reasoning in text space improves
the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken
reasoning tasks. Beyond accuracy, the latency of the spoken response is a
crucial factor for interacting with voice-based agents. Inspired by the human
behavior of "thinking while listening," we propose methods to reduce the
additional latency from reasoning by allowing the model to start reasoning
before the user query has ended. To achieve this, we introduce an entropy-based
metric, "question completeness," which acts as an indicator to guide the model
on the optimal time to start reasoning. This method provides greater control
over the accuracy-latency trade-off compared with heuristic-based approaches
and, under equivalent latency conditions, yields a 4% accuracy gain on
ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference
data created using rejection sampling to push the accuracy-latency pareto
frontier further, resulting in a 70% reduction in latency without loss in
accuracy.

</details>


### [11] [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.07499)
*Soyeong Jeong,Taehee Jung,Sung Ju Hwang,Joo-Kyung Kim,Dongyeop Kang*

Main category: cs.CL

TL;DR: 提出了一种利用思维模板（thought templates）增强长上下文语言模型（LCLMs）进行多跳推理的方法，通过结构化证据组合和迭代优化模板，有效提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文语言模型虽然能处理大量文本，但直接加入更多文档未能有效捕捉证据连接方式，限制了多跳推理能力。

Method: 引入可重用的思维模板，基于先前问题解决痕迹重构推理过程，指导多跳推理；通过自然语言反馈不断迭代优化模板；支持检索和非检索两种场景，将优化后的模板蒸馏至小型开源模型。

Result: 在多种基准和不同LCLM模型上，方法均优于强基线，在检索和非检索设置中均表现出一致的性能提升。

Conclusion: 思维模板增强框架ToTAL能有效引导多跳推理，提升长上下文语言模型的推理能力，且支持小型模型的知识迁移和推理复用，具有广泛应用前景。

Abstract: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands
of tokens in a single prompt, enabling new opportunities for
knowledge-intensive multi-hop reasoning by integrating large sets of retrieved
documents or, in some cases, directly all necessary information. However,
simply feeding more documents into the context window fails to capture how
evidence should be connected. We address this gap with thought templates, which
recast reasoning as reusable thought caches, derived from prior problem solving
traces, structuring how evidence is combined and guiding multi-hop inference
with factual documents. To keep these templates effective, we propose an update
strategy that iteratively refines templates derived from training data through
natural-language feedback. Across diverse benchmarks and LCLM families, our
approach delivers consistent gains over strong baselines in both
retrieval-based and retrieval-free settings. Furthermore, we show that
optimized templates can be distilled into smaller open-source models,
demonstrating its broad applicability and transparent reasoning reuse. We refer
to our framework as Thought Template Augmented LCLMs (ToTAL).

</details>


### [12] [ParsTranslit: Truly Versatile Tajik-Farsi Transliteration](https://arxiv.org/abs/2510.07520)
*Rayyan Merchant,Kevin Tang*

Main category: cs.CL

TL;DR: 本文提出了一种针对波斯语和塔吉克语之间两种不同书写标准（波斯-阿拉伯文与塔吉克-西里尔文）转写的序列到序列模型，并发布了两个新的数据集，实现了跨领域的数据训练，达到了当前最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 波斯语因书写系统不同，导致伊朗/阿富汗和塔吉克斯坦之间的文字交流受阻。现有转写模型多限于单一领域的自建数据集，缺乏跨领域适用性，难以满足真实世界的应用需求。

Method: 提出一种新的序列到序列模型，结合所有可用的数据集进行训练，并发布了两个新的转写数据集以拓展训练数据的领域多样性，提升模型的通用性。

Result: 模型在波斯语转塔吉克语任务中达到chrF++ 87.91和归一化CER 0.05，在塔吉克语转波斯语任务中获得chrF++ 92.28和归一化CER 0.04，表现优异，设立了全面且可比的基准。

Conclusion: 通过跨领域大规模数据训练的序列到序列模型显著提升了波斯语与塔吉克语书写转换的效果，为两地语者提供了高效的交流工具，模型和数据公开促进未来研究和应用。

Abstract: As a digraphic language, the Persian language utilizes two written standards:
Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite
the significant similarity between the dialects of each country, script
differences prevent simple one-to-one mapping, hindering written communication
and interaction between Tajikistan and its Persian-speaking ``siblings''. To
overcome this, previously-published efforts have investigated machine
transliteration models to convert between the two scripts. Unfortunately, most
efforts did not use datasets other than those they created, limiting these
models to certain domains of text such as archaic poetry or word lists. A truly
usable transliteration system must be capable of handling varied domains,
meaning that suck models lack the versatility required for real-world usage.
The contrast in domain between data also obscures the task's true difficulty.
We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi
transliteration trained across all available datasets, and present two datasets
of our own. Our results across domains provide clearer understanding of the
task, and set comprehensive comparable leading benchmarks. Overall, our model
achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik
and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available
at https://anonymous.4open.science/r/ParsTranslit-FB30/.

</details>


### [13] [OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs](https://arxiv.org/abs/2510.07535)
*Jaeseong Lee,seung-won hwang,Aurick Qiao,Gabriele Oliaro,Ye Wang,Samyam Rajbhandari*

Main category: cs.CL

TL;DR: 本文针对大语言模型推理速度在长上下文场景下表现差的问题，提出了新的长上下文基准测试LongSpecBench，并设计了新模型OWL，实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有投机解码方法在实际应用中的长上下文环境下性能显著下降，导致推理速度不升反降。

Method: 提出OWL模型，采用基于LSTM的起草器仅依赖最后一个Token状态，利用[SPEC]特殊Token增强校验器表示，以及结合树形和非树形混合解码算法。

Result: OWL在长上下文输入下接受长度较EAGLE3提升约5倍，显著提升生成速度。

Conclusion: OWL及LongSpecBench为长上下文环境下的投机解码提供了有效解决方案，代码和数据集开源，促进后续研究。

Abstract: Speculative decoding promises faster inference for large language models
(LLMs), yet existing methods fail to generalize to real-world settings.
Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical
workloads involve long contexts. We find current approaches degrade severely
with long contexts; for instance, EAGLE3 even slows down the generation speed
by 0.81x. We address these limitations by releasing a new long-context
benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves
about 5x higher acceptance length than EAGLE3 on long-context inputs through
three innovations: (1) an LSTM-based drafter conditioned only on the last-token
state, making it generalize to various lengths, (2) a special token [SPEC] in
the verifier that produces richer representation for drafter, and (3) a hybrid
algorithm combining both tree and non-tree decoding methods. We release all
code and datasets to advance future research.

</details>


### [14] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Mizanur Rahman,Amran Bhuiyan,Israt Jahan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 本文针对小型视觉语言模型在图表理解判评任务中的性能不足，提出了多标准提示和领域自适应迁移学习两种方法，开发了2B参数的ChartJudge模型，实现资源受限环境下的低成本高效评估。


<details>
  <summary>Details</summary>
Motivation: 当前仅有7B参数的大型视觉语言模型在图表判评任务中表现良好，但小于等于2B参数的微型模型表现较差，限制了其在资源受限环境中的应用。

Method: 提出多标准提示，将多个评价标准融合入单一查询；采用领域自适应迁移学习，对2B参数的LVLM在合成图表判决数据上进行微调，形成专门的ChartJudge模型。

Result: 多标准提示揭示了7B模型的稳健性缺陷，导致性能大降；而微型ChartJudge模型有效实现了从一个数据集向另一个数据集的知识迁移，表现出良好的专门化能力。

Conclusion: 通过细粒度分析，平衡模型大小、提示设计和迁移能力，为图表推理任务提供了可扩展、低成本的评估方案，适合资源受限的应用场景。

Abstract: Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.

</details>


### [15] [Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER](https://arxiv.org/abs/2510.07566)
*Junyi Zhu,Savas Ozkan,Andrea Maracani,Sinan Mutlu,Cho Jung Min,Mete Ozay*

Main category: cs.CL

TL;DR: 本文针对移动平台部署的轻量级BERT模型，提出了一种多任务预微调方法以提升模型在命名实体识别和文本分类任务上的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 为了满足移动平台对高效且能够适应多种NLP应用需求的模型的需求，研究如何通过预微调提升轻量级BERT模型的下游任务表现。

Method: 提出基于任务主导的LoRA模块的多任务预微调框架，利用共享的编码器骨干和模块化适配器解决多任务预微调中优化冲突的问题。

Result: 在21个下游任务上，方法分别在命名实体识别和文本分类任务上平均提升了0.8%和8.8%的性能，效果接近于对每个任务单独预微调。

Conclusion: 所提多任务预微调策略有效提升了轻量级BERT模型在移动端多个NLP任务上的表现，兼顾了性能和部署实用性。

Abstract: Deploying natural language processing (NLP) models on mobile platforms
requires models that can adapt across diverse applications while remaining
efficient in memory and computation. We investigate pre-finetuning strategies
to enhance the adaptability of lightweight BERT-like encoders for two
fundamental NLP task families: named entity recognition (NER) and text
classification. While pre-finetuning improves downstream performance for each
task family individually, we find that na\"ive multi-task pre-finetuning
introduces conflicting optimization signals that degrade overall performance.
To address this, we propose a simple yet effective multi-task pre-finetuning
framework based on task-primary LoRA modules, which enables a single shared
encoder backbone with modular adapters. Our approach achieves performance
comparable to individual pre-finetuning while meeting practical deployment
constraint. Experiments on 21 downstream tasks show average improvements of
+0.8% for NER and +8.8% for text classification, demonstrating the
effectiveness of our method for versatile mobile NLP applications.

</details>


### [16] [Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets](https://arxiv.org/abs/2510.07579)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.CL

TL;DR: 该研究通过计算语言学分析疫情相关网络话语，区分健康错误信息与事实性沟通，发现错误信息在可读性较低且使用更多恐惧和说服性语言，表明其采用复杂的修辞风格和情感线索以增强可信度。


<details>
  <summary>Details</summary>
Motivation: 随着数字健康错误信息的传播日益严重，理解其语言特征对于识别和应对此类信息至关重要。

Method: 分析了三组语料库中疫情相关的文本数据，比较可读性、修辞标记和说服性语言的使用差异。

Result: COVID-19错误信息具有明显较低的可读性和更多恐惧及说服性词汇，且情感表达方式与其他素材不同，表明其语言策略旨在提高可信度。

Conclusion: 研究揭示了健康错误信息的语言特征，有助于改进检测方法和公共健康传播策略，同时指出了研究局限与未来方向，如采用更多情感词汇和动态分析方法。

Abstract: This study conducts a computational linguistic analysis of pandemic-related
online discourse to examine how language distinguishes health misinformation
from factual communication. Drawing on three corpora: COVID-19 false narratives
(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts
(n = 5787), we identify significant differences in readability, rhetorical
markers, and persuasive language use. COVID-19 misinformation exhibited
markedly lower readability scores and contained over twice the frequency of
fear-related or persuasive terms compared to the other datasets. It also showed
minimal use of exclamation marks, contrasting with the more emotive style of
Monkeypox content. These patterns suggest that misinformation employs a
deliberately complex rhetorical style embedded with emotional cues, a
combination that may enhance its perceived credibility. Our findings contribute
to the growing body of work on digital health misinformation by highlighting
linguistic indicators that may aid detection efforts. They also inform public
health messaging strategies and theoretical models of crisis communication in
networked media environments. At the same time, the study acknowledges
limitations, including reliance on traditional readability indices, use of a
deliberately narrow persuasive lexicon, and reliance on static aggregate
analysis. Future research should therefore incorporate longitudinal designs,
broader emotion lexicons, and platform-sensitive approaches to strengthen
robustness.

</details>


### [17] [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/abs/2510.07591)
*Chihiro Taguchi,Richard Sproat*

Main category: cs.CL

TL;DR: 本文提出了一个利用大型语言模型（LLMs）辅助构造人造语言的系统，该系统模块化设计，实现从语音学模型、形态句法标注到正字法制定及语法手册编写的全流程。


<details>
  <summary>Details</summary>
Motivation: 旨在开发有趣的工具辅助人造语言创建，同时探究LLMs对语言及语言学概念的理解深度。

Method: 通过代理方法细化目标语音学，英语句子转形态句法标注构建语料库，基于语料库和语音模型构造词典，再用现有文字系统设计正字法，最终生成语法手册并支持句子翻译。

Result: 不同LLMs和语言规格能力存在显著差异，系统对常见语言模式的处理更为有效，对稀有模式挑战较大。尝试将此系统应用于高资源语言向低资源语言的翻译，初步结果不佳但显示出潜在改进空间。

Conclusion: 该系统不仅为人造语言开发提供了有趣实用的工具，也揭示了LLMs在语言理解上的能力差异，为未来提升低资源语言翻译提供了可能的方向。

Abstract: We present a system that uses LLMs as a tool in the development of
Constructed Languages. The system is modular in that one first creates a target
phonology for the language using an agentic approach that refines its output at
each step with commentary feedback on its previous attempt. Next, a set of
sentences is 'translated' from their English original into a morphosyntactic
markup that reflects the word order and morphosyntactic feature specifications
of the desired target language, with affixes represented as morphosyntactic
feature bundles. From this translated corpus, a lexicon is constructed using
the phonological model and the set of morphemes (stems and affixes) extracted
from the 'translated' sentences. The system is then instructed to provide an
orthography for the language, using an existing script such as Latin or
Cyrillic. Finally, the system writes a brief grammatical handbook of the
language. The system can also translate further sentences into the target
language.
  Our goal is twofold. First, we hope that these tools will be fun to use for
creating artificially constructed languages. Second, we are interested in
exploring what LLMs 'know' about language-not what they know about any
particular language or linguistic phenomenon, but how much they know about and
understand language and linguistic concepts. As we shall see, there is a fairly
wide gulf in capabilities both among different LLMs and among different
linguistic specifications, with it being notably easier for systems to deal
with more common patterns than rarer ones. An additional avenue that we explore
is the application of our approach to translating from high-resource into
low-resource languages. While the results so far are mostly negative, we
provide some evidence that an improved version of the present system could
afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC

</details>


### [18] [Vocabulary embeddings organize linguistic structure early in language model training](https://arxiv.org/abs/2510.07613)
*Isabel Papadimitriou,Jacob Prince*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中输入词汇表表示的几何结构及其训练过程中的演变，通过关联输入和输出嵌入与语义、句法和频率特征，揭示了词频和功能词的不同作用。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型输入词汇表示的结构如何形成及训练过程中何时演变，以理解词汇几何结构的发展轨迹。

Method: 采用表征相似性分析，对比两种开源模型（Pythia 12B和OLMo 7B）输入和输出嵌入的几何结构与语义、句法和频率指标的关联，观察训练过程中的变化。

Result: 训练过程中词汇嵌入几何结构迅速收敛，与语义和句法特征高度相关；高频词和功能词的嵌入向最终向量收敛较快，而低频词则保留一定的随机初始化偏差。

Conclusion: 词汇嵌入几何结构的演变反映了语言结构的组织过程，词频和词类在这一过程中扮演不同角色，激发了对词汇几何随训练发展促进模型能力提升机制的深入研究。

Abstract: Large language models (LLMs) work by manipulating the geometry of input
embedding vectors over multiple layers. Here, we ask: how are the input
vocabulary representations of language models structured, and how and when does
this structure evolve over training? To answer this question, we use
representational similarity analysis, running a suite of experiments that
correlate the geometric structure of the input embeddings and output embeddings
of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,
and frequency-based metrics over the course of training. Our key findings are
as follows: 1) During training, the vocabulary embedding geometry quickly
converges to high correlations with a suite of semantic and syntactic features;
2) Embeddings of high-frequency and function words (e.g., "the," "of") converge
to their final vectors faster than lexical and low-frequency words, which
retain some alignment with the bias in their random initializations. These
findings help map the dynamic trajectory by which input embeddings organize
around linguistic structure, revealing distinct roles for word frequency and
function. Our findings motivate a deeper study of how the evolution of
vocabulary geometry may facilitate specific capability gains during model
training.

</details>


### [19] [Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation](https://arxiv.org/abs/2510.07629)
*Zhangdie Yuan,Han-Chin Shing,Mitch Strong,Chaitanya Shivade*

Main category: cs.CL

TL;DR: 本文研究提高医疗临床编码准确性，提出了轻量级方法和临床代码验证任务，并发布了新的门诊临床笔记数据集以提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗临床编码中存在层级错误且评估方法忽略了这一点，同时现有数据集存在偏见和不完整性。

Method: 通过提示工程、小规模微调等轻量级方法提升准确率，提出临床代码验证任务作为独立或流水线组件，同时发布专家双重标注的门诊临床笔记数据集。

Result: 轻量级干预方法有效提升了准确率，临床代码验证显著减少层级近似错误，新的门诊数据集解决了现有数据集中证据不完整和偏见问题。

Conclusion: 临床代码验证是提高基于大语言模型的医疗编码的有效且可靠步骤，结合轻量级优化方法和新数据集，有望推动医疗编码的准确性提升。

Abstract: Accurate clinical coding is essential for healthcare documentation, billing,
and decision-making. While prior work shows that off-the-shelf LLMs struggle
with this task, evaluations based on exact match metrics often overlook errors
where predicted codes are hierarchically close but incorrect. Our analysis
reveals that such hierarchical misalignments account for a substantial portion
of LLM failures. We show that lightweight interventions, including prompt
engineering and small-scale fine-tuning, can improve accuracy without the
computational overhead of search-based methods. To address hierarchically
near-miss errors, we introduce clinical code verification as both a standalone
task and a pipeline component. To mitigate the limitations in existing
datasets, such as incomplete evidence and inpatient bias in MIMIC, we release
an expert double-annotated benchmark of outpatient clinical notes with ICD-10
codes. Our results highlight verification as an effective and reliable step
toward improving LLM-based medical coding.

</details>


### [20] [Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models](https://arxiv.org/abs/2510.07642)
*Đorđe Klisura,Joseph Khoury,Ashish Kundu,Ram Krishnan,Anthony Rios*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在访问控制中的角色条件拒绝行为，通过基于真实PostgreSQL权限策略扩展的数据库问答数据集，评估模型在授权回答和拒绝未授权请求中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常常模糊权限角色边界，导致产生不受限制的回复，影响安全访问控制。本文旨在研究和改进模型遵守访问控制策略的能力。

Method: 文章创建了包含真实基于角色的访问控制（RBAC）策略的新数据集，基于Spider和BIRD数据集，并对比了三种方法：(i) 零或少样本提示，(ii) 两步生成器-验证器架构，通过验证生成的SQL是否符合策略，(iii) 微调模型直接学习权限意识。

Result: 两步验证架构显著提升了拒绝精度并降低了错误授权，微调模型在安全性和执行准确率之间取得了更优平衡。复杂策略会降低所有方法的可靠性。

Conclusion: 显式权限验证和模型微调均能提高大型语言模型的访问控制合规性，但更复杂的策略增加了挑战。作者还公开了相关的扩展数据集和代码以促进后续研究。

Abstract: Access control is a cornerstone of secure computing, yet large language
models often blur role boundaries by producing unrestricted responses. We study
role-conditioned refusals, focusing on the LLM's ability to adhere to access
control policies by answering when authorized and refusing when not. To
evaluate this behavior, we created a novel dataset that extends the Spider and
BIRD text-to-SQL datasets, both of which have been modified with realistic
PostgreSQL role-based policies at the table and column levels. We compare three
designs: (i) zero or few-shot prompting, (ii) a two-step generator-verifier
pipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that
learn permission awareness directly. Across multiple model families, explicit
verification (the two-step framework) improves refusal precision and lowers
false permits. At the same time, fine-tuning achieves a stronger balance
between safety and utility (i.e., when considering execution accuracy). Longer
and more complex policies consistently reduce the reliability of all systems.
We release RBAC-augmented datasets and code.

</details>


### [21] [Banking Done Right: Redefining Retail Banking with Language-Centric AI](https://arxiv.org/abs/2510.07645)
*Xin Jie Chua,Jeraelyn Ming Li Tan,Jia Xuan Tan,Soon Chang Poh,Yi Xian Goh,Debbie Hui Tian Choong,Chee Mun Foong,Sze Jue Yang,Chee Seng Chan*

Main category: cs.CL

TL;DR: 本文介绍了Ryt AI，一个由内部开发的大型语言模型驱动的对话式银行系统，首次获得全球监管机构批准，允许用户通过自然语言完成核心金融交易。


<details>
  <summary>Details</summary>
Motivation: 当前对话式人工智能多用于咨询或支持，尚未作为主要银行业务接口获得监管批准，Ryt AI旨在实现一个合规且安全的自然语言银行交易系统。

Method: Ryt AI基于内部开发的闭源大型语言模型ILMU，通过四个任务专用的LLM智能代理（守护、防范、支付及FAQ）协同工作，结合确定性守护规则、人机确认和无状态审计架构部署在银行内部基础设施中。

Result: 实现了一个由监管机构批准的自然语言银行界面，能够可靠支持核心金融操作，替代了传统多屏流程，实现了单一对话流程的便捷性和安全合规性。

Conclusion: Ryt AI证明了自然语言接口在严格治理下可以安全、合规且高效地完成核心银行业务，开创了全球首个监管批准的对话式银行系统实践。

Abstract: This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt
Bank to enable customers to execute core financial transactions through natural
language conversation. This represents the first global regulator-approved
deployment worldwide where conversational AI functions as the primary banking
interface, in contrast to prior assistants that have been limited to advisory
or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a
closed-source LLM developed internally, and replaces rigid multi-screen
workflows with a single dialogue orchestrated by four LLM-powered agents
(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific
LoRA adapter to ILMU, which is hosted within the bank's infrastructure to
ensure consistent behavior with minimal overhead. Deterministic guardrails,
human-in-the-loop confirmation, and a stateless audit architecture provide
defense-in-depth for security and compliance. The result is Banking Done Right:
demonstrating that regulator-approved natural-language interfaces can reliably
support core financial operations under strict governance.

</details>


### [22] [OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2510.07651)
*Yuzhe Gu,Xiyu Liang,Jiaojiao Zhao,Enmao Diao*

Main category: cs.CL

TL;DR: 提出了一种基于层级结构剪枝的缓存淘汰方法Optimal Brain Cache(OBCache)，通过量化缓存中令牌对注意力输出的实际影响，提升长上下文的模型推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的缓存机制随着序列长度和批次大小线性增长，带来巨大内存开销，且现有缓存淘汰多基于启发式累积注意权重，未能准确评估令牌对输出的真实影响。

Method: 借鉴Optimal Brain Damage理论，将缓存淘汰视为层级结构剪枝问题，设计基于注意力输出扰动的闭式令牌显著性评分，考虑key、value及其联合对输出的贡献，增强淘汰策略的输出感知能力。

Result: 在LLaMA和Qwen模型上实验表明，用OBCache的输出感知评分替换现有遍历不同查询位置的启发式得分，显著提升了长上下文推理的准确率。

Conclusion: OBCache框架通过结构化、输出感知的令牌显著性评估，有效优化了缓存淘汰过程，降低内存开销的同时提升模型长上下文处理性能。

Abstract: Large language models (LLMs) with extended context windows enable powerful
downstream applications but impose significant memory overhead, as caching all
key-value (KV) states scales linearly with sequence length and batch size.
Existing cache eviction methods address this by exploiting attention sparsity,
yet they typically rank tokens heuristically using accumulated attention
weights without considering their true impact on attention outputs. We propose
Optimal Brain Cache (OBCache), a principled framework that formulates cache
eviction as a layer-wise structured pruning problem. Building upon the Optimal
Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the
perturbation in attention outputs induced by pruning tokens, with closed-form
scores derived for isolated keys, isolated values, and joint key-value pairs.
Our scores account not only for attention weights but also for information from
value states and attention outputs, thereby enhancing existing eviction
strategies with output-aware signals. Experiments on LLaMA and Qwen models
demonstrate that replacing the heuristic scores in existing works, which
estimate token saliency across different query positions, with OBCache's
output-aware scores consistently improves long-context accuracy.

</details>


### [23] [Textual Entailment and Token Probability as Bias Evaluation Metrics](https://arxiv.org/abs/2510.07662)
*Virginia K. Felkner,Allison Lim,Jonathan May*

Main category: cs.CL

TL;DR: 该论文比较了基于Token概率（TP）和自然语言推理（NLI）的社会偏见测量方法，发现两者区别显著，建议结合多种指标综合评估语言模型偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型偏见测量多依赖Token概率指标，但其与现实使用场景和实际危害间存在差距，因此探索更现实的度量方法。

Method: 引入自然语言推理（NLI）作为偏见度量指标，比较NLI与TP在偏见检测上的表现差异，分析两者的相关性和敏感性。

Result: 发现NLI和TP偏见指标相关性极低，NLI更易发现“消偏不足”情况但对反典型句式更敏感且脆弱。

Conclusion: TP和NLI各有优缺点，单一指标不足以全面评估偏见，建议结合多种指标及下游任务偏见测试进行综合评估。

Abstract: Measurement of social bias in language models is typically by token
probability (TP) metrics, which are broadly applicable but have been criticized
for their distance from real-world langugage model use cases and harms. In this
work, we test natural language inference (NLI) as a more realistic alternative
bias metric. We show that, curiously, NLI and TP bias evaluation behave
substantially differently, with very low correlation among different NLI
metrics and between NLI and TP metrics. We find that NLI metrics are more
likely to detect "underdebiased" cases. However, NLI metrics seem to be more
brittle and sensitive to wording of counterstereotypical sentences than TP
approaches. We conclude that neither token probability nor natural language
inference is a "better" bias metric in all cases, and we recommend a
combination of TP, NLI, and downstream bias evaluations to ensure comprehensive
evaluation of language models.
  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.

</details>


### [24] [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686)
*Jifan Zhang,Henry Sleight,Andi Peng,John Schulman,Esin Durmus*

Main category: cs.CL

TL;DR: 本文提出了一种系统的方法对大语言模型的行为规范进行压力测试，以发现原则冲突和歧义。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的行为规范存在原则间冲突和覆盖不足的问题，需要系统方法识别和解决。

Method: 通过生成要求模型在价值原则间权衡的情景，测试多个前沿大语言模型的回复，评估其行为分歧并进行定性分析。

Result: 发现超过7万个模型行为高度分歧的案例，揭示模型规范中的矛盾和歧义，以及模型之间的价值优先级差异。

Conclusion: 模型规范存在显著的冲突和模糊点，系统测试有助于发现并改进模型的价值一致性和对齐问题。

Abstract: Large language models (LLMs) are increasingly trained from AI constitutions
and model specifications that establish behavioral guidelines and ethical
principles. However, these specifications face critical challenges, including
internal conflicts between principles and insufficient coverage of nuanced
scenarios. We present a systematic methodology for stress-testing model
character specifications, automatically identifying numerous cases of principle
contradictions and interpretive ambiguities in current model specs.
  We stress test current model specs by generating scenarios that force
explicit tradeoffs between competing value-based principles. Using a
comprehensive taxonomy we generate diverse value tradeoff scenarios where
models must choose between pairs of legitimate principles that cannot be
simultaneously satisfied. We evaluate responses from twelve frontier LLMs
across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral
disagreement through value classification scores. Among these scenarios, we
identify over 70,000 cases exhibiting significant behavioral divergence.
Empirically, we show this high divergence in model behavior strongly predicts
underlying problems in model specifications. Through qualitative analysis, we
provide numerous example issues in current model specs such as direct
contradiction and interpretive ambiguities of several principles. Additionally,
our generated dataset also reveals both clear misalignment cases and
false-positive refusals across all of the frontier models we study. Lastly, we
also provide value prioritization patterns and differences of these models.

</details>


### [25] [Large Language Models Meet Virtual Cell: A Survey](https://arxiv.org/abs/2510.07706)
*Krinos Li,Xianglu Xiao,Shenglong Deng,Lucas He,Zijun Zhong,Yuanjie Zou,Zhonghao Zhan,Zheng Hui,Weiye Bao,Guang Yang*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLMs）在虚拟细胞建模中的应用，提出统一分类法，涵盖直接细胞建模和复杂科学任务两大范式，重点分析细胞表示、扰动预测和基因调控推断三项核心任务及相关挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正在改变细胞生物学，支持创建虚拟细胞系统，实现细胞状态和行为的预测与推理，因此需要系统回顾其方法与挑战。

Method: 提出统一分类体系，将现有方法归纳为LLMs作为预言机（直接建模）和LLMs作为代理（管理复杂任务），并详细评述三个核心任务的模型、数据集与评测基准。

Result: 总结了相关模型和工具在细胞表示、扰动预测和基因调控推断方面的应用，以及面临的可扩展性、泛化性和可解释性等主要问题。

Conclusion: 通过统一的视角分析LLMs在虚拟细胞建模中的潜力与挑战，为未来研究指明方向，促进该领域发展。

Abstract: Large language models (LLMs) are transforming cellular biology by enabling
the development of "virtual cells"--computational systems that represent,
predict, and reason about cellular states and behaviors. This work provides a
comprehensive review of LLMs for virtual cell modeling. We propose a unified
taxonomy that organizes existing methods into two paradigms: LLMs as Oracles,
for direct cellular modeling, and LLMs as Agents, for orchestrating complex
scientific tasks. We identify three core tasks--cellular representation,
perturbation prediction, and gene regulation inference--and review their
associated models, datasets, evaluation benchmarks, as well as the critical
challenges in scalability, generalizability, and interpretability.

</details>


### [26] [Causality Guided Representation Learning for Cross-Style Hate Speech Detection](https://arxiv.org/abs/2510.07707)
*Chengshuai Zhao,Shu Wan,Paras Sheth,Karan Patwa,K. Selçuk Candan,Huan Liu*

Main category: cs.CL

TL;DR: 该论文提出CADET，一个基于因果表示学习的模型，用于区分和检测多样化且隐晦的仇恨言论，提高检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测模型依赖表面语言线索，难以识别隐晦且多样化的仇恨言论，且不同平台上的仇恨言论风格和受众各异，增加了检测难度。

Method: 基于作者提出的仇恨言论生成因果图，设计了CADET框架，解耦上下文、创作动机、目标和风格等潜在因子，控制混杂变量，通过隐藏空间的反事实推理实现对仇恨言论的稳健检测。

Result: CADET在多样化仇恨言论检测任务中展现出优越的性能，验证了因果先验在提升泛化能力上的有效性。

Conclusion: 利用因果图引导的表示学习能更好地分离仇恨意图与表面语言特征，促进仇恨言论检测模型的鲁棒性和泛化能力提升。

Abstract: The proliferation of online hate speech poses a significant threat to the
harmony of the web. While explicit hate is easily recognized through overt
slurs, implicit hate speech is often conveyed through sarcasm, irony,
stereotypes, or coded language -- making it harder to detect. Existing hate
speech detection models, which predominantly rely on surface-level linguistic
cues, fail to generalize effectively across diverse stylistic variations.
Moreover, hate speech spread on different platforms often targets distinct
groups and adopts unique styles, potentially inducing spurious correlations
between them and labels, further challenging current detection approaches.
Motivated by these observations, we hypothesize that the generation of hate
speech can be modeled as a causal graph involving key factors: contextual
environment, creator motivation, target, and style. Guided by this graph, we
propose CADET, a causal representation learning framework that disentangles
hate speech into interpretable latent factors and then controls confounders,
thereby isolating genuine hate intent from superficial linguistic cues.
Furthermore, CADET allows counterfactual reasoning by intervening on style
within the latent space, naturally guiding the model to robustly identify hate
speech in varying forms. CADET demonstrates superior performance in
comprehensive experiments, highlighting the potential of causal priors in
advancing generalizable hate speech detection.

</details>


### [27] [MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation](https://arxiv.org/abs/2510.07713)
*Shuo Yu,Mingyue Cheng,Daoyu Wang,Qi Liu,Zirui Liu,Ze Guo,Xiaoyu Tao*

Main category: cs.CL

TL;DR: 本文提出了MemWeaver框架，通过构建层次化的记忆模块，结合时间和语义信息，实现对用户文本历史的深度个性化建模，提高了大语言模型的个性化生成能力。


<details>
  <summary>Details</summary>
Motivation: 目前的个性化方法大多将用户历史文本视为平铺列表，忽视了用户兴趣的时间演变和语义结构，导致个性化效果浅显。

Method: MemWeaver设计了行为记忆和认知记忆两种互补组件，分别捕捉具体行为和长期偏好，将时间与语义信息融合，形成统一的用户表示。

Result: 在LaMP基准测试中，MemWeaver表现出优异的个性化生成效果，证明其有效性。

Conclusion: 通过层次化的双内存架构，MemWeaver能够更好地理解和建模用户动态兴趣，实现更深层次的个性化语言生成。

Abstract: The primary form of user-internet engagement is shifting from leveraging
implicit feedback signals, such as browsing and clicks, to harnessing the rich
explicit feedback provided by textual interactive behaviors. This shift unlocks
a rich source of user textual history, presenting a profound opportunity for a
deeper form of personalization. However, prevailing approaches offer only a
shallow form of personalization, as they treat user history as a flat list of
texts for retrieval and fail to model the rich temporal and semantic structures
reflecting dynamic nature of user interests. In this work, we propose
\textbf{MemWeaver}, a framework that weaves the user's entire textual history
into a hierarchical memory to power deeply personalized generation. The core
innovation of our memory lies in its ability to capture both the temporal
evolution of interests and the semantic relationships between different
activities. To achieve this, MemWeaver builds two complementary memory
components that both integrate temporal and semantic information, but at
different levels of abstraction: behavioral memory, which captures specific
user actions, and cognitive memory, which represents long-term preferences.
This dual-component memory serves as a unified representation of the user,
allowing large language models (LLMs) to reason over both concrete behaviors
and abstracted traits. Experiments on the Language Model Personalization (LaMP)
benchmark validate the efficacy of MemWeaver. Our code is
available\footnote{https://github.com/fishsure/MemWeaver}.

</details>


### [28] [SUBQRAG: sub-question driven dynamic graph rag](https://arxiv.org/abs/2510.07718)
*Jiaoyang Li,Junhao Ruan,Shengwei Tang,Saihan Chen,Kaiyan Chang,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出了SubQRAG，一个基于子问题的图检索增强生成框架，通过动态扩展知识图谱和结构化推理，提升复杂多跳问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的图检索增强生成（Graph RAG）方法在处理复杂多跳问答时，因缺乏深度结构化推理，导致证据不完整和错误积累。

Method: SubQRAG将复杂问题分解为有序的可验证子问题链，为每个子问题从图中检索相关三元组，动态扩展图谱，构建“图记忆”以形成结构化、可追踪的证据路径。

Result: 在三个多跳问答基准测试中，SubQRAG表现出一致且显著的性能提升，特别是在准确匹配（Exact Match）评分上。

Conclusion: SubQRAG通过子问题驱动的推理和动态图谱扩展，有效解决了复杂多跳问答中证据不全和误差累积的问题，提升了问答系统的推理深度和准确性。

Abstract: Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a
knowledge graph (KG) to connect disparate facts across a large document corpus.
However, this broad-view approach often lacks the deep structured reasoning
needed for complex multi-hop question answering (QA), leading to incomplete
evidence and error accumulation. To address these limitations, we propose
SubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG
decomposes a complex question into an ordered chain of verifiable
sub-questions. For each sub-question, it retrieves relevant triples from the
graph. When the existing graph is insufficient, the system dynamically expands
it by extracting new triples from source documents in real time. All triples
used in the reasoning process are aggregated into a "graph memory," forming a
structured and traceable evidence path for final answer generation. Experiments
on three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent
and significant improvements, especially in Exact Match scores.

</details>


### [29] [Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing](https://arxiv.org/abs/2510.07736)
*Cunli Mao,Xiaofei Gao,Ran Song,Shizhu He,Shengxiang Gao,Kang Liu,Zhengtao Yu*

Main category: cs.CL

TL;DR: 提出了一种新的多语言知识图谱补全框架，通过共享多语言知识显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多语言知识图谱补全方法未充分利用大语言模型的多语言能力和跨语言知识的共享性。

Method: 设计了知识级别分组专家混合（KL-GMoE）和迭代实体重排序（IER）两部分，分别用于高效建模共享知识和增强知识利用。

Result: 在包含5种语言的多语言知识图谱数据集上，框架在Hits@1、Hits@3和Hits@10指标上较现有最优方法分别提升5.47%、3.27%和1.01%。

Conclusion: 所提框架有效提升了多语言知识图谱补全性能，并揭示了在未见和不平衡语言设置下知识共享的属性。

Abstract: Large language models (LLMs) based Multilingual Knowledge Graph Completion
(MKGC) aim to predict missing facts by leveraging LLMs' multilingual
understanding capabilities, improving the completeness of multilingual
knowledge graphs (KGs). However, existing MKGC research underutilizes the
multilingual capabilities of LLMs and ignores the shareability of cross-lingual
knowledge. In this paper, we propose a novel MKGC framework that leverages
multilingual shared knowledge to significantly enhance performance through two
components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative
Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER
significantly enhances its utilization. To evaluate our framework, we
constructed a mKG dataset containing 5 languages and conducted comprehensive
comparative experiments with existing state-of-the-art (SOTA) MKGC method. The
experimental results demonstrate that our framework achieves improvements of
5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics,
respectively, compared with SOTA MKGC method. Further experimental analysis
revealed the properties of knowledge sharing in settings of unseen and
unbalanced languages. We have released the dataset and code for our work on
https://github.com/gaoxiaofei07/KL-GMoE.

</details>


### [30] [ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs](https://arxiv.org/abs/2510.07737)
*Fu Chen,Peng Wang,Xiyin Li,Wen Li,Shichi Lei,Dongdong Xiang*

Main category: cs.CL

TL;DR: 本文提出了ToolExpander框架，通过动态多轮难样本采样和自我示范思维方法，提升了小规模大语言模型在工具使用上的表现和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决GRPO方法在小规模大语言模型中响应不准确、训练中期崩溃以及性能提升受限的问题。

Method: 引入动态多轮难样本采样，替换难以正确输出的训练样本为高质量的少样本示范，并采用指数学习率衰减；提出自我示范思维方法，去除KL散度并调整剪辑系数，鼓励模型自行生成和分析少样本示例。

Result: 实验表明ToolExpander显著提升了小规模模型在使用工具方面的能力，提高了训练的稳定性和整体性能。

Conclusion: ToolExpander有效缓解了GRPO在资源受限大语言模型中的挑战，促进了模型性能和稳定性的提升。

Abstract: Training Large Language Models (LLMs) with Group Relative Policy Optimization
(GRPO) encounters a significant challenge: models often fail to produce
accurate responses, particularly in small-scale architectures. This limitation
not only diminishes performance improvements and undermines the potential of
GRPO but also frequently leads to mid-training collapse, adversely affecting
stability and final efficacy. To address these issues, we propose ToolExpander,
a novel framework that advances tool-oriented reinforcement learning for
resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round
Hard Sampling, which dynamically substitutes challenging samples(those without
correct outputs over 10 rollouts) with high-quality few-shot demonstrations
during training, coupled with an exponential learning rate decay strategy to
mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO
framework that eliminates KL divergence and incorporates adjusted clipping
coefficients, encouraging models to autonomously generate and analyze few-shot
examples via a minimal additional reward (0.01).Experimental results
demonstrate that ToolExpander significantly enhances tool-using capabilities in
LLMs, especially in weaker small-scale models, improving both training
stability and overall performance.

</details>


### [31] [OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment](https://arxiv.org/abs/2510.07743)
*Tianci Liu,Ran Xu,Tony Yu,Ilgee Hong,Carl Yang,Tuo Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: 该论文提出了OpenRubrics数据集和对比性规则生成方法，提升了基于评分准则的奖励模型的准确性和可靠性，从而促进了人类反馈强化学习中更细致的偏好捕捉和大规模自动化奖励建模。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型大多依赖标量或成对判断，难以捕捉人类偏好的多维度特性。虽然有研究利用结构化自然语言评分准则（rubrics）作为奖励，但其生成的准则可靠性和扩展性不足。

Method: 提出了OpenRubrics，这是一个大规模多样化的（提示,评分准则）对集合，用于训练准则生成模型和准则奖励模型。引入对比性规则生成（CRG）方法，通过对优选与拒绝回答进行对比来提取显式硬规则和隐式原则，并通过拒绝采样去除噪声以保证评分准则与偏好标签的一致性。

Result: 基于评分准则的奖励模型Rubric-RM在多个奖励建模基准中，超过了强大的同规模基线6.8%的性能提升，这些优势也迁移至指令执行和生物医药任务的策略模型中。

Conclusion: 评分准则提供了一种可扩展且多维的对齐信号，缩小了高成本人类评估与自动奖励建模之间的差距，推动了大规模语言模型基于原则的对齐新范式。

Abstract: Reward modeling lies at the core of reinforcement learning from human
feedback (RLHF), yet most existing reward models rely on scalar or pairwise
judgments that fail to capture the multifaceted nature of human preferences.
Recent studies have explored rubrics-as-rewards (RaR) that uses structured
natural language criteria that capture multiple dimensions of response quality.
However, producing rubrics that are both reliable and scalable remains a key
challenge. In this work, we introduce OpenRubrics, a diverse, large-scale
collection of (prompt, rubric) pairs for training rubric-generation and
rubric-based reward models. To elicit discriminative and comprehensive
evaluation signals, we introduce Contrastive Rubric Generation (CRG), which
derives both hard rules (explicit constraints) and principles (implicit
qualities) by contrasting preferred and rejected responses. We further improve
reliability by enforcing preference-label consistency via rejection sampling to
remove noisy rubrics. Across multiple reward-modeling benchmarks, our
rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines
by 6.8%. These gains transfer to policy models on instruction-following and
biomedical benchmarks. Our results show that rubrics provide scalable alignment
signals that narrow the gap between costly human evaluation and automated
reward modeling, enabling a new principle-driven paradigm for LLM alignment.

</details>


### [32] [Parallel Test-Time Scaling for Latent Reasoning Models](https://arxiv.org/abs/2510.07745)
*Runyang You,Yongqi Li,Meng Liu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.CL

TL;DR: 本论文研究了在连续潜在空间中实现并行测试时刻扩展（TTS）的方法，提出基于不确定性采样策略和潜在奖励模型的框架，提升潜在推理模型的推理效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型中的并行测试时刻扩展对提升性能关键，而潜在推理方法因缺乏连续空间中的采样机制及轨迹聚合手段，尚未能充分利用并行TTS。

Method: 引入蒙特卡洛Dropout和加性高斯噪声两种不确定性驱动的随机采样方法；设计基于逐步对比学习训练的潜在奖励模型（LatentRM）评估和引导潜在推理轨迹选择。

Result: 实验和可视化分析表明，所提采样策略能有效扩展计算规模并表现出不同的探索动力学，潜在奖励模型则成功实现了轨迹的有效挑选。

Conclusion: 该工作为潜在空间中的可扩展推理提供了新方向，成功实现了潜在推理模型的并行测试时刻扩展，提升了连续空间推理的效率和效果。

Abstract: Parallel test-time scaling (TTS) is a pivotal approach for enhancing large
language models (LLMs), typically by sampling multiple token-based
chains-of-thought in parallel and aggregating outcomes through voting or
search. Recent advances in latent reasoning, where intermediate reasoning
unfolds in continuous vector spaces, offer a more efficient alternative to
explicit Chain-of-Thought, yet whether such latent models can similarly benefit
from parallel TTS remains open, mainly due to the absence of sampling
mechanisms in continuous space, and the lack of probabilistic signals for
advanced trajectory aggregation. \ This work enables parallel TTS for latent
reasoning models by addressing the above issues. For sampling, we introduce two
uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive
Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)
trained with step-wise contrastive objective to score and guide latent
reasoning. Extensive experiments and visualization analyses show that both
sampling strategies scale effectively with compute and exhibit distinct
exploration dynamics, while LatentRM enables effective trajectory selection.
Together, our explorations open a new direction for scalable inference in
continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.

</details>


### [33] [Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers](https://arxiv.org/abs/2510.07761)
*Nishant Balepur,Atrey Desai,Rachel Rudinger*

Main category: cs.CL

TL;DR: LLMs在多选题回答中展示推理能力，但只凭选项也能取得成功，表明仅凭选项回答可能并非完全无意义。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在仅凭选项回答多选题时的策略是否浅薄，以及推理路径是否能揭示这些策略的本质。

Method: 让推理型LLM分别用完整输入和仅选项输入回答多选题，比较两种情况下的推理轨迹长度和准确率，验证推理的真实性。

Result: 推理常提升完整输入和部分仅选项输入的准确度，但仅选项输入的成功率与推理轨迹长度关系不大，推理轨迹通过真实性测试且揭示了合理推断缺失问题的策略。

Conclusion: 部分输入的成功不一定是缺陷，推理轨迹可用于区分有害数据和合理推理，挑战了部分输入成功即是问题的观点。

Abstract: Large language models (LLMs) now give reasoning before answering, excelling
in tasks like multiple-choice question answering (MCQA). Yet, a concern is that
LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed
in MCQA without using the question, i.e., choices-only. Such partial-input
success is often deemed problematic, but reasoning traces could reveal if these
strategies are truly shallow in choices-only settings. To study these
strategies, reasoning LLMs solve MCQs in full and choices-only inputs;
test-time reasoning often boosts accuracy on full and in choices-only half the
time. While possibly due to shallow shortcuts, choices-only success is barely
affected by the length of reasoning traces, and after finding traces pass
faithfulness tests, we show they use less problematic strategies like inferring
missing questions. In all, we challenge claims that partial-input success is
always a flaw, so we discuss how reasoning traces could separate problematic
data from less problematic reasoning.

</details>


### [34] [ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning](https://arxiv.org/abs/2510.07768)
*Murong Yue,Zhiwei Liu,Liangwei Yang,Jianguo Zhang,Zuxin Liu,Haolin Chen,Ziyu Yao,Silvio Savarese,Caiming Xiong,Shelby Heinecke,Huan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种系统方法，将大量无结构的工具自动重构为结构化的工具库，以提升大型语言模型在复杂推理任务中的工具检索效率和性能。


<details>
  <summary>Details</summary>
Motivation: 领域特定工具匮乏限制了外部工具增强的语言模型的广泛应用，尤其在物理问答等领域。现有自动生成工具方法因工具数量激增而面临检索困难和功能冗余问题。

Method: 首先生成离散的任务特定工具，并基于语义将其聚类。然后引入多智能体框架：代码智能体重构代码以提取共享逻辑，生成多功能聚合工具；审查智能体确保聚合工具功能完整。

Result: 该方法能将大量问题特定工具转化为更少但功能强大的聚合工具，显著提升工具检索准确率和整体推理表现，并在工具规模扩大时表现出更好的扩展性。

Conclusion: 系统方法成功解决了工具库无结构和检索效率低下问题，促进工具增强型语言模型在复杂领域推理任务中的应用和性能提升。

Abstract: Large Language Models (LLMs) equipped with external tools have demonstrated
enhanced performance on complex reasoning tasks. The widespread adoption of
this tool-augmented reasoning is hindered by the scarcity of domain-specific
tools. For instance, in domains such as physics question answering, suitable
and specialized tools are often missing. Recent work has explored automating
tool creation by extracting reusable functions from Chain-of-Thought (CoT)
reasoning traces; however, these approaches face a critical scalability
bottleneck. As the number of generated tools grows, storing them in an
unstructured collection leads to significant retrieval challenges, including an
expanding search space and ambiguity between function-related tools. To address
this, we propose a systematic approach to automatically refactor an
unstructured collection of tools into a structured tool library. Our system
first generates discrete, task-specific tools and clusters them into
semantically coherent topics. Within each cluster, we introduce a multi-agent
framework to consolidate scattered functionalities: a code agent refactors code
to extract shared logic and creates versatile, aggregated tools, while a
reviewing agent ensures that these aggregated tools maintain the complete
functional capabilities of the original set. This process transforms numerous
question-specific tools into a smaller set of powerful, aggregated tools
without loss of functionality. Experimental results demonstrate that our
approach significantly improves tool retrieval accuracy and overall reasoning
performance across multiple reasoning tasks. Furthermore, our method shows
enhanced scalability compared with baselines as the number of question-specific
increases.

</details>


### [35] [Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards](https://arxiv.org/abs/2510.07774)
*Youliang Yuan,Qiuyang Mang,Jingbang Chen,Hong Wan,Xiaoyuan Liu,Junjielong Xu,Jen-tse Huang,Wenxuan Wang,Wenxiang Jiao,Pinjia He*

Main category: cs.CL

TL;DR: 本文指出数学推理大语言模型因仅以最终答案作为奖励而导致推理能力被高估，提出了基于过程的奖励函数RRM，有效提升模型推理质量与准确率。


<details>
  <summary>Details</summary>
Motivation: 当前模型只根据最终结果进行奖励，易被“奖励欺骗”，出现通过错误推理却得到正确答案的假阳性情况，严重影响模型推理能力的真实性评估。

Method: 提出Rubric Reward Model (RRM)，通过对整个推理过程进行细粒度评估，依据具体题目标准评分，惩罚逻辑缺陷，鼓励严密推导，并将其应用于强化学习训练。

Result: RRM训练在四个数学基准测试中均优于仅基于结果的训练，AIME2024 Verified Pass@1024指标从26.7%提升至62.6%，"奇迹步骤"出现率降低了71%。

Conclusion: 基于推理过程的奖励机制极大提升了模型的准确性和可靠性，强调了过程奖励对于高质量数学推理模型的重要性。

Abstract: Large language models for mathematical reasoning are typically trained with
outcome-based rewards, which credit only the final answer. In our experiments,
we observe that this paradigm is highly susceptible to reward hacking, leading
to a substantial overestimation of a model's reasoning ability. This is
evidenced by a high incidence of false positives - solutions that reach the
correct final answer through an unsound reasoning process. Through a systematic
analysis with human verification, we establish a taxonomy of these failure
modes, identifying patterns like Miracle Steps - abrupt jumps to a correct
output without a valid preceding derivation. Probing experiments suggest a
strong association between these Miracle Steps and memorization, where the
model appears to recall the answer directly rather than deriving it. To
mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a
process-oriented reward function that evaluates the entire reasoning trajectory
against problem-specific rubrics. The generative RRM provides fine-grained,
calibrated rewards (0-1) that explicitly penalize logical flaws and encourage
rigorous deduction. When integrated into a reinforcement learning pipeline,
RRM-based training consistently outperforms outcome-only supervision across
four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from
26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work
demonstrates that rewarding the solution process is crucial for building models
that are not only more accurate but also more reliable.

</details>


### [36] [The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775)
*Omar Mahmoud,Ali Khalil,Buddhika Laknath Semage,Thommen George Karimpanal,Santu Rana*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在提升真实性时，安全性对齐能力下降的问题，提出了一种利用稀疏自编码器和子空间正交化的方法来解决这一权衡。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型的真实性时，安全性对齐能力（如拒绝有害内容的行为）反而减弱，这种权衡问题尚未被充分关注。

Method: 通过稀疏自编码器将拒绝相关特征与幻觉特征解耦，并在微调过程中采用子空间正交化技术保护拒绝行为，使得真实性提升的同时不损害安全对齐。

Result: 在常识推理任务以及有害内容检测基准（AdvBench和StrongReject）中，所提方法有效保持了拒绝行为和任务效用，同时减缓了真实性与安全性之间的权衡。

Conclusion: 该方法成功解决了真实性提升与安全性对齐之间的冲突，有助于构建更安全且真实的语言模型。

Abstract: Hallucination in large language models (LLMs) has been widely studied in
recent years, with progress in both detection and mitigation aimed at improving
truthfulness. Yet, a critical side effect remains largely overlooked: enhancing
truthfulness can negatively impact safety alignment. In this paper, we
investigate this trade-off and show that increasing factual accuracy often
comes at the cost of weakened refusal behavior. Our analysis reveals that this
arises from overlapping components in the model that simultaneously encode
hallucination and refusal information, leading alignment methods to suppress
factual knowledge unintentionally. We further examine how fine-tuning on benign
datasets, even when curated for safety, can degrade alignment for the same
reason. To address this, we propose a method that disentangles refusal-related
features from hallucination features using sparse autoencoders, and preserves
refusal behavior during fine-tuning through subspace orthogonalization. This
approach prevents hallucinations from increasing while maintaining safety
alignment.We evaluate our method on commonsense reasoning tasks and harmful
benchmarks (AdvBench and StrongReject). Results demonstrate that our approach
preserves refusal behavior and task utility, mitigating the trade-off between
truthfulness and safety.

</details>


### [37] [Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection](https://arxiv.org/abs/2510.07776)
*Shiman Zhao,Shangyuan Li,Wei Chen,Tengjiao Wang,Jiahui Yao,Jiabin Zheng,Kam Fai Wong*

Main category: cs.CL

TL;DR: 本文提出了一种基于实例关系学习和标签知识传播的少样本多标签意图检测方法，实现了端到端联合学习，显著提升了1-shot场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于表征分类，忽视实例间关系，导致误差传播，从而影响少样本多标签意图检测性能。

Method: 构建实例关系学习网络，结合标签知识传播，在支持集与查询集之间传递标签信息，利用双重关系增强损失函数优化实例间关系强度，实现端到端联合学习。

Result: 模型在1-shot场景下平均提升9.54%的AUC和11.19%的Macro-F1，相较强基线表现优越。

Conclusion: 通过实例关系学习与标签知识传播的联合方法，有效缓解了误差传播问题，提升了少样本多标签意图检测的性能。

Abstract: Few-shot Multi-label Intent Detection (MID) is crucial for dialogue systems,
aiming to detect multiple intents of utterances in low-resource dialogue
domains. Previous studies focus on a two-stage pipeline. They first learn
representations of utterances with multiple labels and then use a
threshold-based strategy to identify multi-label results. However, these
methods rely on representation classification and ignore instance relations,
leading to error propagation. To solve the above issues, we propose a
multi-label joint learning method for few-shot MID in an end-to-end manner,
which constructs an instance relation learning network with label knowledge
propagation to eliminate error propagation. Concretely, we learn the
interaction relations between instances with class information to propagate
label knowledge between a few labeled (support set) and unlabeled (query set)
instances. With label knowledge propagation, the relation strength between
instances directly indicates whether two utterances belong to the same intent
for multi-label prediction. Besides, a dual relation-enhanced loss is developed
to optimize support- and query-level relation strength to improve performance.
Experiments show that we outperform strong baselines by an average of 9.54% AUC
and 11.19% Macro-F1 in 1-shot scenarios.

</details>


### [38] [Drift No More? Context Equilibria in Multi-Turn LLM Interactions](https://arxiv.org/abs/2510.07777)
*Vardhan Dongre,Ryan A. Rossi,Viet Dac Lai,David Seunghyun Yoon,Dilek Hakkani-Tür,Trung Bui*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型在多轮交互中表现出的上下文漂移问题，提出了一个动力学框架来解释和控制漂移现象。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中存在上下文漂移现象，随着交互轮次增加，模型输出逐渐偏离目标行为，现有的静态评估指标难以捕捉该动态过程。

Method: 将上下文漂移形式化为测试模型与目标一致模型之间的逐轮KL散度，提出一个递归动力学模型，将漂移视为带有恢复力和可控干预的有界随机过程。在合成任务和用户仿真中验证该框架。

Result: 实验证明漂移表现为稳定的噪声限制平衡状态，而非逐渐恶化的过程。简单的提醒干预能有效减少漂移，并且与理论预测一致。

Conclusion: 多轮交互中的上下文漂移可作为一种可控的平衡现象理解和处理，为进一步研究和减缓上下文漂移奠定了理论基础。

Abstract: Large Language Models (LLMs) excel at single-turn tasks such as instruction
following and summarization, yet real-world deployments require sustained
multi-turn interactions where user goals and conversational context persist and
evolve. A recurring challenge in this setting is context drift: the gradual
divergence of a model's outputs from goal-consistent behavior across turns.
Unlike single-turn errors, drift unfolds temporally and is poorly captured by
static evaluation metrics. In this work, we present a study of context drift in
multi-turn interactions and propose a simple dynamical framework to interpret
its behavior. We formalize drift as the turn-wise KL divergence between the
token-level predictive distributions of the test model and a goal-consistent
reference model, and propose a recurrence model that interprets its evolution
as a bounded stochastic process with restoring forces and controllable
interventions. We instantiate this framework in both synthetic long-horizon
rewriting tasks and realistic user-agent simulations such as in $\tau$-Bench,
measuring drift for several open-weight LLMs that are used as user simulators.
Our experiments consistently reveal stable, noise-limited equilibria rather
than runaway degradation, and demonstrate that simple reminder interventions
reliably reduce divergence in line with theoretical predictions. Together,
these results suggest that multi-turn drift can be understood as a controllable
equilibrium phenomenon rather than as inevitable decay, providing a foundation
for studying and mitigating context drift in extended interactions.

</details>


### [39] [RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model](https://arxiv.org/abs/2510.07782)
*Shuichiro Haruta,Kazunori Matsumoto,Zhi Li,Yanan Wang,Mori Kurokawa*

Main category: cs.CL

TL;DR: 本文提出了一种基于旋转约束的补偿方法，用于解决大规模语言模型结构化剪枝引起的误差，显著提升了剪枝后模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在剪枝过程中，由于校准数据量有限，容易产生输出不匹配且直接最小二乘拟合容易过拟合，破坏预训练权重的几何结构。

Method: 通过对剪枝参数施加旋转约束，保持输出表示的几何性质，同时设计考虑输入方差的变量重要性评分，优先保留对主方向贡献大的维度。

Result: 在LLaMA-7B模型和多个语言理解任务上，所提方法相较于现有基线，在困惑度和任务准确率上均表现出更优的效果。

Conclusion: 旋转约束的补偿方法有效缓解了剪枝带来的误差，保持了表示空间几何结构，实现了更好的剪枝后模型表现。

Abstract: In this paper, we propose a rotation-constrained compensation method to
address the errors introduced by structured pruning of large language models
(LLMs). LLMs are trained on massive datasets and accumulate rich semantic
knowledge in their representation space. In contrast, pruning is typically
carried out with only a small amount of calibration data, which makes output
mismatches unavoidable. Although direct least-squares fitting can reduce such
errors, it tends to overfit to the limited calibration set, destructively
modifying pretrained weights. To overcome this difficulty, we update the pruned
parameters under a rotation constraint. This constrained update preserves the
geometry of output representations (i.e., norms and inner products) and
simultaneously re-aligns the pruned subspace with the original outputs.
Furthermore, in rotation-constrained compensation, removing components that
strongly contribute to the principal directions of the output makes error
recovery difficult. Since input dimensions with large variance strongly affect
these principal directions, we design a variance-aware importance score that
ensures such dimensions are preferentially kept in the pruned model. By
combining this scoring rule with rotation-constrained updates, the proposed
method effectively compensates errors while retaining the components likely to
be more important in a geometry-preserving manner. In the experiments, we apply
the proposed method to LLaMA-7B and evaluate it on WikiText-2 and multiple
language understanding benchmarks. The results demonstrate consistently better
perplexity and task accuracy compared with existing baselines.

</details>


### [40] [LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology](https://arxiv.org/abs/2510.07793)
*Sajib Acharjee Dip,Adrika Zafor,Bikash Kumar Paul,Uddip Acharjee Shuvo,Muhit Islam Emon,Xuan Wang,Liqing Zhang*

Main category: cs.CL

TL;DR: LLM4Cell 汇总和评估了58个用于单细胞生物学的大型语言模型，涵盖多模态数据和关键分析任务，实现了统一视角。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中不同数据模态、模型架构及评价标准分散，缺乏统一整合，需要系统性分析和评估。

Method: 对58个基础及智能模型进行分类，映射至多种单细胞分析任务，利用40余个公开数据集进行多维度评测，包括生物学基础、多组学对齐、公平性、隐私和解释性。

Result: 系统化展示了多模态单细胞数据下模型性能，发现模型在生物学应用、多组学融合和伦理性方面存在差异，明确了评测标准和公平性问题。

Conclusion: LLM4Cell首次提供语言驱动的单细胞智能整合视角，揭示解释性、标准化和可信模型开发的挑战，推动未来研究方向。

Abstract: Large language models (LLMs) and emerging agentic frameworks are beginning to
transform single-cell biology by enabling natural-language reasoning,
generative annotation, and multimodal data integration. However, progress
remains fragmented across data modalities, architectures, and evaluation
standards. LLM4Cell presents the first unified survey of 58 foundation and
agentic models developed for single-cell research, spanning RNA, ATAC,
multi-omic, and spatial modalities. We categorize these methods into five
families-foundation, text-bridge, spatial, multimodal, epigenomic, and
agentic-and map them to eight key analytical tasks including annotation,
trajectory and perturbation modeling, and drug-response prediction. Drawing on
over 40 public datasets, we analyze benchmark suitability, data diversity, and
ethical or scalability constraints, and evaluate models across 10 domain
dimensions covering biological grounding, multi-omics alignment, fairness,
privacy, and explainability. By linking datasets, models, and evaluation
domains, LLM4Cell provides the first integrated view of language-driven
single-cell intelligence and outlines open challenges in interpretability,
standardization, and trustworthy model development.

</details>


### [41] [HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation](https://arxiv.org/abs/2510.07794)
*Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Kaiyu He,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 该论文提出了HiPRAG，一种通过层次化过程奖励来优化基于强化学习的Agentic RAG模型搜索行为的方法，有效减少过度搜索与不足搜索问题，提高搜索效率和问答准确率。


<details>
  <summary>Details</summary>
Motivation: Agentic RAG模型在整合外部信息时存在过度搜索和不足搜索的问题，现有基于结果的训练方法缺乏对搜索过程细节的精细控制，导致效率低和输出不可靠。

Method: 提出HiPRAG方法，将知识驱动的细粒度过程奖励纳入强化学习训练，通过将推理路径划分为可解析步骤，实时评估每次搜索决策的必要性，并引入分层奖励函数，给予基于最优搜索步骤比例的额外奖励。

Result: 在Qwen2.5和Llama-3.2模型上，HiPRAG方法在七个问答基准测试中分别实现了65.4%和67.2%的平均准确率，同时显著降低了过度搜索率至2.3%，并减少不足搜索率。

Conclusion: 通过对搜索过程而非单纯最终结果进行优化，HiPRAG提高了Agentic RAG的推理效率和性能，且具备良好的跨模型和强化学习算法的泛化能力，展示了细粒度强化学习控制在搜索智能体中的重要性和潜力。

Abstract: Agentic RAG is a powerful technique for incorporating external information
that LLMs lack, enabling better problem solving and question answering.
However, suboptimal search behaviors exist widely, such as over-search
(retrieving information already known) and under-search (failing to search when
necessary), which leads to unnecessary overhead and unreliable outputs. Current
training methods, which typically rely on outcome-based rewards in a RL
framework, lack the fine-grained control needed to address these
inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for
Efficient agentic RAG (HiPRAG), a training methodology that incorporates a
fine-grained, knowledge-grounded process reward into the RL training. Our
approach evaluates the necessity of each search decision on-the-fly by
decomposing the agent's reasoning trajectory into discrete, parsable steps. We
then apply a hierarchical reward function that provides an additional bonus
based on the proportion of optimal search and non-search steps, on top of
commonly used outcome and format rewards. Experiments on the Qwen2.5 and
Llama-3.2 models across seven diverse QA benchmarks show that our method
achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished
while improving search efficiency, reducing the over-search rate to just 2.3%
and concurrently lowering the under-search rate. These results demonstrate the
efficacy of optimizing the reasoning process itself, not just the final
outcome. Further experiments and analysis demonstrate that HiPRAG shows good
generalizability across a wide range of RL algorithms, model families, sizes,
and types. This work demonstrates the importance and potential of fine-grained
control through RL, for improving the efficiency and optimality of reasoning
for search agents.

</details>


### [42] [Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models](https://arxiv.org/abs/2510.07799)
*Eric Hanchen Jiang,Guancheng Wan,Sophia Yin,Mengting Li,Yuchen Wu,Xiao Liang,Xinfeng Li,Yizhou Sun,Wei Wang,Kai-Wei Chang,Ying Nian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Guided Topology Diffusion (GTD)的新型生成框架，用于设计多智能体系统中基于大语言模型的通信拓扑，以平衡任务性能、通信成本和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用静态或手工设计的通信拓扑，难以适应不同任务需求，导致资源浪费或性能下降。

Method: GTD基于条件离散图扩散模型，将拓扑设计视为一个迭代构建过程，并通过轻量级代理模型预测多目标奖励，进行实时无梯度的任务自适应优化。

Result: 在多个基准测试中，GTD生成的通信拓扑表现出高度任务适应性、稀疏性和效率，显著优于现有方法。

Conclusion: GTD通过迭代引导式的拓扑合成，有效解决了多目标优化问题，提升了大语言模型驱动的多智能体系统的协作效率。

Abstract: The efficiency of multi-agent systems driven by large language models (LLMs)
largely hinges on their communication topology. However, designing an optimal
topology is a non-trivial challenge, as it requires balancing competing
objectives such as task performance, communication cost, and robustness.
Existing frameworks often rely on static or hand-crafted topologies, which
inherently fail to adapt to diverse task requirements, leading to either
excessive token consumption for simple problems or performance bottlenecks for
complex ones. To address this challenge, we introduce a novel generative
framework called \textit{Guided Topology Diffusion (GTD)}. Inspired by
conditional discrete graph diffusion models, GTD formulates topology synthesis
as an iterative construction process. At each step, the generation is steered
by a lightweight proxy model that predicts multi-objective rewards (e.g.,
accuracy, utility, cost), enabling real-time, gradient-free optimization
towards task-adaptive topologies. This iterative, guided synthesis process
distinguishes GTD from single-step generative frameworks, enabling it to better
navigate complex design trade-offs. We validated GTD across multiple
benchmarks, and experiments show that this framework can generate highly
task-adaptive, sparse, and efficient communication topologies, significantly
outperforming existing methods in LLM agent collaboration.

</details>


### [43] [Multilingual Generative Retrieval via Cross-lingual Semantic Compression](https://arxiv.org/abs/2510.07812)
*Yuxin Huang,Simeng Wu,Ran Song,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

TL;DR: 提出了多语种生成信息检索框架MGR-CSC，通过语义压缩统一多语种关键词，解决跨语言标识符错位和膨胀问题，提升了跨语言检索准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有生成式信息检索在多语种检索中面临跨语言标识符错位和标识符膨胀两大挑战，影响检索性能。

Method: 提出MGR-CSC框架，通过将语义等价多语种关键词整合为共享原子，实现语义对齐与标识符压缩，同时设计动态多步约束解码策略提高检索效率。

Result: 在mMarco100k和mNQ320k数据集上，MGR-CSC分别提升检索准确率6.83%和4.77%，并将文档标识符长度分别缩短74.51%和78.2%。

Conclusion: MGR-CSC有效解决了跨语言信息检索中的标识符对齐和膨胀问题，实现了更高的准确率和解码效率，具有良好的实用价值。

Abstract: Generative Information Retrieval is an emerging retrieval paradigm that
exhibits remarkable performance in monolingual scenarios.However, applying
these methods to multilingual retrieval still encounters two primary
challenges, cross-lingual identifier misalignment and identifier inflation. To
address these limitations, we propose Multilingual Generative Retrieval via
Cross-lingual Semantic Compression (MGR-CSC), a novel framework that unifies
semantically equivalent multilingual keywords into shared atoms to align
semantics and compresses the identifier space, and we propose a dynamic
multi-step constrained decoding strategy during retrieval. MGR-CSC improves
cross-lingual alignment by assigning consistent identifiers and enhances
decoding efficiency by reducing redundancy. Experiments demonstrate that
MGR-CSC achieves outstanding retrieval accuracy, improving by 6.83% on
mMarco100k and 4.77% on mNQ320k, while reducing document identifiers length by
74.51% and 78.2%, respectively.

</details>


### [44] [AdaSwitch: Adaptive Switching Generation for Knowledge Distillation](https://arxiv.org/abs/2510.07842)
*Jingyu Peng,Maolin Wang,Hengyi Cai,Yuchen Li,Kai Zhang,Shuaiqiang Wang,Dawei Yin,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 本论文提出了AdaSwitch方法，通过动态结合在线和离线知识蒸馏，提高小型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在延迟和计算资源有限的应用中非常重要，但实现高性能存在挑战，现有知识蒸馏方法存在精度与一致性的权衡问题。

Method: 提出AdaSwitch方法，在生成的每个token层面动态切换在线和离线蒸馏，学生模型先探索自身预测，再根据实时质量评估选择性整合教师指导。

Result: 在三个数据集和两个师生语言模型对上进行的实验表明，AdaSwitch显著提升了小型语言模型的准确性。

Conclusion: AdaSwitch有效解决了在线与离线蒸馏的矛盾问题，提供了一种实用且具有可接受额外开销的小型语言模型蒸馏方案。

Abstract: Small language models (SLMs) are crucial for applications with strict latency
and computational constraints, yet achieving high performance remains
challenging. Knowledge distillation (KD) can transfer capabilities from large
teacher models, but existing methods involve trade-offs: off-policy
distillation provides high-quality supervision but introduces a
training-inference mismatch, while on-policy approaches maintain consistency
but rely on low-quality student outputs. To address these issues, we propose
AdaSwitch, a novel approach that dynamically combines on-policy and off-policy
generation at the token level. AdaSwitch allows the student to first explore
its own predictions and then selectively integrate teacher guidance based on
real-time quality assessment. This approach simultaneously preserves
consistency and maintains supervision quality. Experiments on three datasets
with two teacher-student LLM pairs demonstrate that AdaSwitch consistently
improves accuracy, offering a practical and effective method for distilling
SLMs with acceptable additional overhead.

</details>


### [45] [Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains](https://arxiv.org/abs/2510.07877)
*Md. Faiyaz Abdullah Sayeedi,Md. Mahbub Alam,Subhey Sadi Rahman,Md. Adnanul Islam,Jannatul Ferdous Deepti,Tasnim Mohiuddin,Md Mofijul Islam,Swakkhar Shatabda*

Main category: cs.CL

TL;DR: 本文提出了Translation Tangles框架，评估开源大型语言模型在机器翻译中的翻译质量与公平性，涵盖24个双向语言对和多领域。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在不同语言族和专业领域中表现不均，并且会放大训练数据中的偏见，尤其影响低资源语言的公平性。

Method: 设计了Translation Tangles统一评估框架和数据集，使用多指标对多语言对和领域进行翻译质量测试；提出混合偏见检测流程，结合规则、语义相似度过滤和模型验证；构建高质量偏见标注数据集。

Result: 通过对24个语言对和多领域的评测，揭示了开源大型语言模型在翻译质量和偏见方面的差异；偏见检测方法有效识别偏见情况。

Conclusion: Translation Tangles框架和数据集为公平和高质量的机器翻译评估提供了有力工具，有助于推动低资源语言的公平翻译研究。

Abstract: The rise of Large Language Models (LLMs) has redefined Machine Translation
(MT), enabling context-aware and fluent translations across hundreds of
languages and textual domains. Despite their remarkable capabilities, LLMs
often exhibit uneven performance across language families and specialized
domains. Moreover, recent evidence reveals that these models can encode and
amplify different biases present in their training data, posing serious
concerns for fairness, especially in low-resource languages. To address these
gaps, we introduce Translation Tangles, a unified framework and dataset for
evaluating the translation quality and fairness of open-source LLMs. Our
approach benchmarks 24 bidirectional language pairs across multiple domains
using different metrics. We further propose a hybrid bias detection pipeline
that integrates rule-based heuristics, semantic similarity filtering, and
LLM-based validation. We also introduce a high-quality, bias-annotated dataset
based on human evaluations of 1,439 translation-reference pairs. The code and
dataset are accessible on GitHub:
https://github.com/faiyazabdullah/TranslationTangles

</details>


### [46] [Do LLMs Really Need 10+ Thoughts for "Find the Time 1000 Days Later"? Towards Structural Understanding of LLM Overthinking](https://arxiv.org/abs/2510.07880)
*Xinliang Frederick Zhang,Anhad Mohananey,Alexandra Chronopoulou,Pinelopi Papalampidi,Somit Gupta,Tsendsuren Munkhdalai,Lu Wang,Shyam Upadhyay*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在复杂推理任务中表现优异但在简单任务中过度思考导致效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入理解过度思考的根本原因，多停留在表面观察，缺乏对模型内部推理过程的细致分析。

Method: 提出了TRACE工具，细粒度分解模型思考过程为子思考单元，构建思考进展图，识别常见思考模式，并基于思考结构定义了基于效用的过度思考新指标。

Result: 确认长推理模型在简单任务中的计算开销高达五到二十倍且无准确度提升，发现两大过度思考路径：过度验证和过度探索。

Conclusion: 提出基于效用的过度思考定义，更全面理解模型思考动态，为有效管理过度思考提供理论和实践指导。

Abstract: Models employing long chain-of-thought (CoT) reasoning have shown superior
performance on complex reasoning tasks. Yet, this capability introduces a
critical and often overlooked inefficiency -- overthinking -- models often
engage in unnecessarily extensive reasoning even for simple queries, incurring
significant computations without accuracy improvements. While prior work has
explored solutions to mitigate overthinking, a fundamental gap remains in our
understanding of its underlying causes. Most existing analyses are limited to
superficial, profiling-based observations, failing to delve into LLMs' inner
workings. This study introduces a systematic, fine-grained analyzer of LLMs'
thought process to bridge the gap, TRACE. We first benchmark the overthinking
issue, confirming that long-thinking models are five to twenty times slower on
simple tasks with no substantial gains. We then use TRACE to first decompose
the thought process into minimally complete sub-thoughts. Next, by inferring
discourse relationships among sub-thoughts, we construct granular thought
progression graphs and subsequently identify common thinking patterns for
topically similar queries. Our analysis reveals two major patterns for
open-weight thinking models -- Explorer and Late Landing. This finding provides
evidence that over-verification and over-exploration are the primary drivers of
overthinking in LLMs. Grounded in thought structures, we propose a
utility-based definition of overthinking, which moves beyond length-based
metrics. This revised definition offers a more insightful understanding of
LLMs' thought progression, as well as practical guidelines for principled
overthinking management.

</details>


### [47] [CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching](https://arxiv.org/abs/2510.07881)
*Heyang Liu,Yuhao Wang,Ziyang Cheng,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一个名为CS3-Bench的代码切换语音交互基准，揭示了当前多模态大语言模型在语言对齐上的缺陷，并提出数据构建与训练方法提高其性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在自然的单语交互上表现良好，但在多语言切换中存在语言对齐不足，影响相关任务的表现。

Method: 设计CS3-Bench基准测试，分析七款主流模型的性能，并针对性能严重退化的模型，采用链式识别(CoR)和关键词高亮(KH)方法改进语言理解和生成能力。

Result: 改进方法使知识准确率从25.14%提升至46.13%，开放式理解率从64.5%提升至86.5%，并大幅减少目标语言的发音错误。

Conclusion: CS3-Bench有效揭示并推动多语言切换语音交互系统的性能提升，所提出的方法显著增强了语言对齐和多模态理解能力。

Abstract: The advancement of multimodal large language models has accelerated the
development of speech-to-speech interaction systems. While natural monolingual
interaction has been achieved, we find existing models exhibit deficiencies in
language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark
(CS3-Bench), experiments on 7 mainstream models demonstrate a relative
performance drop of up to 66% in knowledge-intensive question answering and
varying degrees of misunderstanding in open-ended conversations. Starting from
a model with severe performance deterioration, we propose both data
constructions and training approaches to improve the language alignment
capabilities, specifically employing Chain of Recognition (CoR) to enhance
understanding and Keyword Highlighting (KH) to guide generation. Our approach
improves the knowledge accuracy from 25.14% to 46.13%, with open-ended
understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation
errors in the secondary language. CS3-Bench is available at
https://huggingface.co/datasets/VocalNet/CS3-Bench.

</details>


### [48] [Contrastive Weak-to-strong Generalization](https://arxiv.org/abs/2510.07884)
*Houcheng Jiang,Junfeng Fang,Jiaxin Wu,Tianyu Zhang,Chen Gao,Yong Li,Xiang Wang,Xiangnan He,Yang Deng*

Main category: cs.CL

TL;DR: 本文提出一种名为ConG的框架，通过对齐后的弱模型与原弱模型之间的对比解码，提升弱到强的泛化能力，减少噪声和偏差。


<details>
  <summary>Details</summary>
Motivation: 弱到强泛化虽有潜力提升大规模语言模型训练效果，但噪声和偏差限制了其实用性。

Method: 利用隐式奖励与对比解码的结构等价性，通过对齐前后弱模型的对比解码生成更高质量样本。

Result: 在多个模型家族中，ConG实现了更可靠的能力转移、去噪和鲁棒性提升，效果稳定。

Conclusion: ConG有效克服传统弱到强方法的局限，推动弱到强泛化发展，为通用人工智能提供可行路径。

Abstract: Weak-to-strong generalization provides a promising paradigm for scaling large
language models (LLMs) by training stronger models on samples from aligned
weaker ones, without requiring human feedback or explicit reward modeling.
However, its robustness and generalization are hindered by the noise and biases
in weak-model outputs, which limit its applicability in practice. To address
this challenge, we leverage implicit rewards, which approximate explicit
rewards through log-likelihood ratios, and reveal their structural equivalence
with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in
LLM generation. Building on this connection, we propose Contrastive
Weak-to-Strong Generalization (ConG), a framework that employs contrastive
decoding between pre- and post-alignment weak models to generate higher-quality
samples. This approach enables more reliable capability transfer, denoising,
and improved robustness, substantially mitigating the limitations of
traditional weak-to-strong methods. Empirical results across different model
families confirm consistent improvements, demonstrating the generality and
effectiveness of ConG. Taken together, our findings highlight the potential of
ConG to advance weak-to-strong generalization and provide a promising pathway
toward AGI.

</details>


### [49] [Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects](https://arxiv.org/abs/2510.07890)
*Verena Blaschke,Miriam Winkler,Barbara Plank*

Main category: cs.CL

TL;DR: 本文比较了标准语向非标准方言的迁移效果，涵盖文本模型、语音模型和级联系统，重点研究德语及其方言的意图和主题分类，并发布了首个方言音频意图分类数据集。


<details>
  <summary>Details</summary>
Motivation: 传统研究多关注文本数据的方言迁移，但方言主要是口语，非标准拼写在文本处理上存在问题，因此需要比较不同模型在标准语与方言间的迁移表现。

Method: 设计并对比了文本模型、语音模型和先转录再文本处理的级联系统，应用于德语及多种德语方言的书面及口语意图和主题分类，发布了方言音频意图分类数据集。

Result: 语音模型在方言数据上表现最佳，文本模型在标准语数据上表现最佳；级联系统在生成标准化转录输出时，在方言数据上的表现相对较好，但在标准语上不及纯文本模型。

Conclusion: 语音模型更适合处理方言语料，转换生成标准化文本是提升级联系统处理方言能力的关键，文本模型适合标准语处理。

Abstract: Research on cross-dialectal transfer from a standard to a non-standard
dialect variety has typically focused on text data. However, dialects are
primarily spoken, and non-standard spellings are known to cause issues in text
processing. We compare standard-to-dialect transfer in three settings: text
models, speech models, and cascaded systems where speech first gets
automatically transcribed and then further processed by a text model. In our
experiments, we focus on German and multiple German dialects in the context of
written and spoken intent and topic classification. To that end, we release the
first dialectal audio intent classification dataset. We find that the
speech-only setup provides the best results on the dialect data while the
text-only setup works best on the standard data. While the cascaded systems lag
behind the text-only models for German, they perform relatively well on the
dialectal data if the transcription system generates normalized, standard-like
output.

</details>


### [50] [Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2510.07892)
*Hyeonseok Moon,Seongtae Hong,Jaehyung Seo,Heuiseok Lim*

Main category: cs.CL

TL;DR: MCBench是一个新设计的基准测试，用于评估大型语言模型(LLM)是否能通过严格遵循分步指令执行字符串匹配自然语言处理指标。


<details>
  <summary>Details</summary>
Motivation: 当前先进的LLM在许多基准测试上表现趋于饱和，难以进一步区分模型性能，因此需要一个客观验证且更具挑战性的测试基准。

Method: MCBench通过对LLM进行严格的逐步指令执行测试，同时提供平行参考代码来客观评估输出的准确性，包含三种评估指标和三个基准变体。

Result: 分析表明MCBench能有效客观地测试LLM的指令遵循、数值计算和中间结果的一致性等能力。

Conclusion: MCBench为评价最先进的LLM能力提供了一个有效且客观的工具，特别是在精确执行步骤指令方面。

Abstract: Recent frontier-level LLMs have saturated many previously difficult
benchmarks, leaving little room for further differentiation. This progress
highlights the need for challenging benchmarks that provide objective
verification. In this paper, we introduce MCBench, a benchmark designed to
evaluate whether LLMs can execute string-matching NLP metrics by strictly
following step-by-step instructions. Unlike prior benchmarks that depend on
subjective judgments or general reasoning, MCBench offers an objective,
deterministic and codeverifiable evaluation. This setup allows us to
systematically test whether LLMs can maintain accurate step-by-step execution,
including instruction adherence, numerical computation, and long-range
consistency in handling intermediate results. To ensure objective evaluation of
these abilities, we provide a parallel reference code that can evaluate the
accuracy of LLM output. We provide three evaluative metrics and three benchmark
variants designed to measure the detailed instruction understanding capability
of LLMs. Our analyses show that MCBench serves as an effective and objective
tool for evaluating the capabilities of cutting-edge LLMs.

</details>


### [51] [ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall](https://arxiv.org/abs/2510.07896)
*Jiayu Yang,Yuxuan Fan,Songning Lai,Shengen Wu,Jiaqi Tang,Chun Kang,Zhijiang Guo,Yutao Yue*

Main category: cs.CL

TL;DR: 本文提出ACE框架，基于神经元归因控制多跳事实回忆的知识编辑方法，有效提升LLM的多跳事实更新性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法在多跳事实回忆中表现显著下降，尤其在涉及链式推理中隐含中间主语时效果欠佳。

Method: 通过因果分析发现隐含主语作为查询神经元，激活对应的值神经元。基于此，ACE利用神经元级别归因识别并编辑关键信息传递路径。

Result: ACE在GPT-J和Qwen3-8B上分别提升性能9.44%和37.46%，且揭示了更细粒度的激活模式及语义可解释性。

Conclusion: 通过对内部推理机制的原理性理解，ACE为多跳知识编辑提供了机制支撑和新思路，有望推动知识编辑能力进步。

Abstract: Large Language Models (LLMs) require efficient knowledge editing (KE) to
update factual information, yet existing methods exhibit significant
performance decay in multi-hop factual recall. This failure is particularly
acute when edits involve intermediate implicit subjects within reasoning
chains. Through causal analysis, we reveal that this limitation stems from an
oversight of how chained knowledge is dynamically represented and utilized at
the neuron level. We discover that during multi hop reasoning, implicit
subjects function as query neurons, which sequentially activate corresponding
value neurons across transformer layers to accumulate information toward the
final answer, a dynamic prior KE work has overlooked. Guided by this insight,
we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual
Recall, a framework that leverages neuron-level attribution to identify and
edit these critical query-value (Q-V) pathways. ACE provides a mechanistically
grounded solution for multi-hop KE, empirically outperforming state-of-the-art
methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals
more fine-grained activation patterns in Qwen3 and demonstrates that the
semantic interpretability of value neurons is orchestrated by query-driven
accumulation. These findings establish a new pathway for advancing KE
capabilities based on the principled understanding of internal reasoning
mechanisms.

</details>


### [52] [Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation](https://arxiv.org/abs/2510.07912)
*Fanwei Zhua,Jiaxuan He,Xiaoxiao Chen,Zulong Chen,Quan Lu,Chenrui Mei*

Main category: cs.CL

TL;DR: 提出了一种基于大型语言模型的统一自动评分框架，能够对各种主观题进行人性化评分，性能优于传统方法，并成功应用于实际考试。


<details>
  <summary>Details</summary>
Motivation: 自动评分主观题困难重重，尤其是题型多样且答案开放，现有方法缺乏通用性，不能覆盖多种题型。

Method: 设计融合文本匹配和大型语言模型的四模块框架，分别进行内容相似度评估、关键知识点对比、伪问题生成和模拟人类评价。

Result: 在多种通用及领域特定数据集上，该框架在多个评分指标上均优于传统方法和其他基于LLM的方法。

Conclusion: 提出的统一框架有效提升主观题自动评分的准确性和通用性，已在大型电商企业的培训和认证考试中成功部署。

Abstract: Automatic grading of subjective questions remains a significant challenge in
examination assessment due to the diversity in question formats and the
open-ended nature of student responses. Existing works primarily focus on a
specific type of subjective question and lack the generality to support
comprehensive exams that contain diverse question types. In this paper, we
propose a unified Large Language Model (LLM)-enhanced auto-grading framework
that provides human-like evaluation for all types of subjective questions
across various domains. Our framework integrates four complementary modules to
holistically evaluate student answers. In addition to a basic text matching
module that provides a foundational assessment of content similarity, we
leverage the powerful reasoning and generative capabilities of LLMs to: (1)
compare key knowledge points extracted from both student and reference answers,
(2) generate a pseudo-question from the student answer to assess its relevance
to the original question, and (3) simulate human evaluation by identifying
content-related and non-content strengths and weaknesses. Extensive experiments
on both general-purpose and domain-specific datasets show that our framework
consistently outperforms traditional and LLM-based baselines across multiple
grading metrics. Moreover, the proposed system has been successfully deployed
in real-world training and certification exams at a major e-commerce
enterprise.

</details>


### [53] [STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models](https://arxiv.org/abs/2510.07923)
*Kyumin Lee,Minjin Jeon,Sanghwan Jang,Hwanjo Yu*

Main category: cs.CL

TL;DR: 提出了一种名为StepER的分步知识蒸馏方法，通过分步监督和难度感知训练提升多步检索增强语言模型的推理能力，在多跳问答任务中显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法忽视了不同推理步骤所需能力的差异，限制了多步检索增强框架中的迁移效果。

Method: StepER采用分步监督，针对各阶段不断变化的信息和推理需求进行对齐；同时引入难度感知训练，优先优化适合当前学习阶段的步骤，方法适用于多种多步检索增强语言模型。

Result: 在多跳问答基准测试中，StepER表现优于以往方法，8B模型达到与70B教师模型相当的性能。

Conclusion: StepER通过分步监督和难度感知训练有效提升了多步检索增强语言模型的推理能力，促进了模型在复杂问答任务中的表现提升。

Abstract: Answering complex real-world questions requires step-by-step retrieval and
integration of relevant information to generate well-grounded responses.
However, existing knowledge distillation methods overlook the need for
different reasoning abilities at different steps, hindering transfer in
multi-step retrieval-augmented frameworks. To address this, we propose Stepwise
Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step
Retrieval-Augmented Language Models (StepER). StepER employs step-wise
supervision to align with evolving information and reasoning demands across
stages. Additionally, it incorporates difficulty-aware training to
progressively optimize learning by prioritizing suitable steps. Our method is
adaptable to various multi-step retrieval-augmented language models, including
those that use retrieval queries for reasoning paths or decomposed questions.
Extensive experiments show that StepER outperforms prior methods on multi-hop
QA benchmarks, with an 8B model achieving performance comparable to a 70B
teacher model.

</details>


### [54] [Comprehensiveness Metrics for Automatic Evaluation of Factual Recall in Text Generation](https://arxiv.org/abs/2510.07926)
*Adam Dejl,James Barry,Alessandra Pascale,Javier Carnerero Cano*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型生成文本的全面性评估，提出并比较了三种自动化方法，发现简单的端到端方法效果意外地好。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然表现优异，但常遗漏关键信息，特别是在敏感领域，遗漏信息的危害性堪比错误事实，亟需有效的全面性评估方法。

Method: 论文提出三种方法：（1）基于自然语言推理（NLI）的原子语句分解与缺失链检测，（2）基于问答对比的多源问答一致性检测，（3）基于大型语言模型的端到端缺失内容识别。

Result: 实验显示简单的端到端方法在检测信息缺失上表现优于复杂方法，但其鲁棒性、可解释性和结果细化程度较低。此外作者评估了多个开源大型模型的回答全面性。

Conclusion: 端到端方法虽然简单且效果突出，但需权衡其稳定性和解释性，评估大型语言模型文本的全面性是提升其应用安全性的关键。

Abstract: Despite demonstrating remarkable performance across a wide range of tasks,
large language models (LLMs) have also been found to frequently produce outputs
that are incomplete or selectively omit key information. In sensitive domains,
such omissions can result in significant harm comparable to that posed by
factual inaccuracies, including hallucinations. In this study, we address the
challenge of evaluating the comprehensiveness of LLM-generated texts, focusing
on the detection of missing information or underrepresented viewpoints. We
investigate three automated evaluation strategies: (1) an NLI-based method that
decomposes texts into atomic statements and uses natural language inference
(NLI) to identify missing links, (2) a Q&A-based approach that extracts
question-answer pairs and compares responses across sources, and (3) an
end-to-end method that directly identifies missing content using LLMs. Our
experiments demonstrate the surprising effectiveness of the simple end-to-end
approach compared to more complex methods, though at the cost of reduced
robustness, interpretability and result granularity. We further assess the
comprehensiveness of responses from several popular open-weight LLMs when
answering user queries based on multiple sources.

</details>


### [55] [Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries](https://arxiv.org/abs/2510.07931)
*Madis Jürviste,Joonatan Jakobson*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在17和18世纪爱沙尼亚语词典研究中的应用，涵盖词典信息丰富化、古文字识别及跨源数据集构建，并展示了显著的自动化潜力。


<details>
  <summary>Details</summary>
Motivation: 通过利用LLMs，提升历史词典中词汇及意义的现代解析，解决古文字的数字化难题，促进历史语言资源的统一与现代化。

Method: 采用视觉增强型LLMs进行古式哥特字体文本识别，利用多模型协同处理扫描图像并结构化输出，进行词典条目含义及现代词形的半自动丰富。

Result: 在1648年Gutslaff词典上，Claude 3.7 Sonnet准确识别81%的词条现代含义；在1732年Helle词典上零-shot方法识别了41%的词条，成功生成无误JSON格式数据；在1780年Hupel词典数字化中实现了多模型融合的图像处理与文本结构化。

Conclusion: 即使对于小语种历史资料，LLMs也表现出显著的时间与成本节约潜力，为历史语言数字化和研究提供了有效工具。

Abstract: This article presents research conducted at the Institute of the Estonian
Language between 2022 and 2025 on the application of large language models
(LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors
address three main areas: enriching historical dictionaries with modern word
forms and meanings; using vision-enabled LLMs to perform text recognition on
sources printed in Gothic script (Fraktur); and preparing for the creation of a
unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648
dictionary indicate that LLMs have significant potential for semi-automatic
enrichment of dictionary information. When provided with sufficient context,
Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81%
of headword entries. In a text recognition experiment with A. T. Helle's 1732
dictionary, a zero-shot method successfully identified and structured 41% of
headword entries into error-free JSON-formatted output. For digitising the
Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping
tiling of scanned image files is employed, with one LLM being used for text
recognition and a second for merging the structured output. These findings
demonstrate that even for minor languages LLMs have a significant potential for
saving time and financial resources.

</details>


### [56] [A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning](https://arxiv.org/abs/2510.07958)
*Fengji Zhang,Xinyao Niu,Chengyang Ying,Guancheng Lin,Zhongkai Hao,Zhou Fan,Chengen Huang,Jacky Keung,Bei Chen,Junyang Lin*

Main category: cs.CL

TL;DR: A$^2$Search是一个无需人工标注的端到端框架，通过自动检测含糊问题并采样多答案路径，结合强化学习优化，实现了多答案开放领域问答的新表现。


<details>
  <summary>Details</summary>
Motivation: 现有开放领域问答模型难以处理存在多重有效答案的含糊问题，且标准基准测试通常只假设单一正确答案，忽视了实际的多样性，人工标注代价高昂且难以扩展。

Method: 设计了自动化管线通过轨迹采样和证据验证识别含糊的问题和多种答案，利用强化学习和特制的AnsF1奖励函数进行训练，以自然兼容多答案。

Result: A$^2$Search在八个开放领域问答基准上实现了新的性能记录，其中7B参数模型在四个多跳数据集上的AnsF1@1平均得分为48.4%，超越了更大的ReSearch-32B。

Conclusion: 通过自动识别和处理问题含糊性，A$^2$Search提高了问答系统的可靠性，强调了接受和利用多答案对于构建鲁棒问答系统的重要性。

Abstract: Recent advances in Large Language Models (LLMs) and Reinforcement Learning
(RL) have led to strong performance in open-domain question answering (QA).
However, existing models still struggle with questions that admit multiple
valid answers. Standard QA benchmarks, which typically assume a single gold
answer, overlook this reality and thus produce inappropriate training signals.
Existing attempts to handle ambiguity often rely on costly manual annotation,
which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue.
In this paper, we present A$^2$Search, an annotation-free, end-to-end training
framework to recognize and handle ambiguity. At its core is an automated
pipeline that detects ambiguous questions and gathers alternative answers via
trajectory sampling and evidence verification. The model is then optimized with
RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally
accommodates multiple answers. Experiments on eight open-domain QA benchmarks
demonstrate that A$^2$Search achieves new state-of-the-art performance. With
only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$
score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong
baselines, including the substantially larger ReSearch-32B ($46.2\%$).
Extensive analyses further show that A$^2$Search resolves ambiguity and
generalizes across benchmarks, highlighting that embracing ambiguity is
essential for building more reliable QA systems. Our code, data, and model
weights can be found at https://github.com/zfj1998/A2Search

</details>


### [57] [LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?](https://arxiv.org/abs/2510.07962)
*Jingyuan Wang,Yankai Chen,Zhonghang Li,Chao Huang*

Main category: cs.CL

TL;DR: 本文提出LightReasoner框架，通过较小模型(SLM)识别大型模型(LLM)的关键推理时刻，实现高效精细调优，提升LLM推理能力并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的监督精调(SFT)资源消耗大，且大部分训练数据对模型提升意义有限。探索是否可以利用较小语言模型揭示大型模型推理中的关键优势时刻，从而高效提升大型模型能力。

Method: LightReasoner包括两个阶段：先通过对比较强专家模型(LLM)和较弱业余模型(SLM)的行为差异，采样出关键推理时刻并构建监督样本；后通过精调使专家模型对这些蒸馏样本进行对齐，从而强化其推理能力。

Result: 在七个数学基准测试中，LightReasoner最大提升准确率28.1%，同时减少90%时间开销，80%采样问题，99%调优Token用量，且无需标注的真实标签。

Conclusion: LightReasoner将较弱小模型转化为有效教学信号，提供了一种可扩展且资源高效的方式提升大型语言模型的推理能力。

Abstract: Large language models (LLMs) have demonstrated remarkable progress in
reasoning, often through supervised fine-tuning (SFT). However, SFT is
resource-intensive, relying on large curated datasets, rejection-sampled
demonstrations, and uniform optimization across all tokens, even though only a
fraction carry meaningful learning value. In this work, we explore a
counterintuitive idea: can smaller language models (SLMs) teach larger language
models (LLMs) by revealing high-value reasoning moments that reflect the
latter's unique strength? We propose LightReasoner, a novel framework that
leverages the behavioral divergence between a stronger expert model (LLM) and a
weaker amateur model (SLM). LightReasoner operates in two stages: (1) a
sampling stage that pinpoints critical reasoning moments and constructs
supervision examples capturing the expert's advantage through expert-amateur
contrast, and (2) a fine-tuning stage that aligns the expert model with these
distilled examples, amplifying its reasoning strengths. Across seven
mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while
reducing time consumption by 90%, sampled problems by 80%, and tuned token
usage by 99%, all without relying on ground-truth labels. By turning weaker
SLMs into effective teaching signals, LightReasoner offers a scalable and
resource-efficient approach for advancing LLM reasoning. Code is available at:
https://github.com/HKUDS/LightReasoner

</details>


### [58] [Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning](https://arxiv.org/abs/2510.07974)
*Jialu Du,Guiyang Hou,Yihui Fu,Chen Wu,Wenqi Zhang,Yongliang Shen,Weiming Lu*

Main category: cs.CL

TL;DR: 大型语言模型在数学和代码推理能力强，但在社会推理任务中表现出认知混乱和逻辑不一致。


<details>
  <summary>Details</summary>
Motivation: 发现大型语言模型难以区分客观现实和主体信念，从而导致推理错误和无限循环。

Method: 提出动态文本世界模型增强推理机制，实时监控推理过程中的混乱迹象，提供清晰的世界状态描述。

Result: 该方法在三个社会推理基准上显著提升准确率（如Hi-ToM提升10%），同时降低计算成本（最多减少33.8%的tokens）。

Conclusion: 动态世界模型机制有效帮助大型语言模型解决社会推理中的认知困境，提高推理准确性及效率，适合社会场景中部署。

Abstract: While large language models (LLMs) excel in mathematical and code reasoning,
we observe they struggle with social reasoning tasks, exhibiting cognitive
confusion, logical inconsistencies, and conflation between objective world
states and subjective belief states. Through deteiled analysis of DeepSeek-R1's
reasoning trajectories, we find that LLMs frequently encounter reasoning
impasses and tend to output contradictory terms like "tricky" and "confused"
when processing scenarios with multiple participants and timelines, leading to
erroneous reasoning or infinite loops. The core issue is their inability to
disentangle objective reality from agents' subjective beliefs. To address this,
we propose an adaptive world model-enhanced reasoning mechanism that constructs
a dynamic textual world model to track entity states and temporal sequences. It
dynamically monitors reasoning trajectories for confusion indicators and
promptly intervenes by providing clear world state descriptions, helping models
navigate through cognitive dilemmas. The mechanism mimics how humans use
implicit world models to distinguish between external events and internal
beliefs. Evaluations on three social benchmarks demonstrate significant
improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational
costs (up to 33.8% token reduction), offering a simple yet effective solution
for deploying LLMs in social contexts.

</details>


### [59] [Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge](https://arxiv.org/abs/2510.07993)
*Watcharapong Timklaypachara,Monrada Chiewhawan,Nopporn Lekuthai,Titipat Achakulvisut*

Main category: cs.CL

TL;DR: 该论文提出一种针对科学图表标题生成的双阶段方法，结合文本上下文和作者风格，显著提升标题的准确性和风格一致性。


<details>
  <summary>Details</summary>
Motivation: 科学图表标题需要既准确传达信息，又具备风格一致性，现有方法难以同时兼顾两者。

Method: 采用两阶段流水线，第一阶段通过上下文过滤和类别特定的提示优化选择图表标题候选，第二阶段利用少样本提示和作者画像进行风格细化。

Result: 类别特定提示显著提升了ROUGE-1召回率，风格细化带来了BLEU和ROUGE分数的大幅提升。

Conclusion: 结合上下文理解与作者特定风格适配的方法能够生成科学准确且风格忠实的图表标题，有效提升了科学图表的标题生成质量。

Abstract: Scientific figure captions require both accuracy and stylistic consistency to
convey visual information. Here, we present a domain-specific caption
generation system for the 3rd SciCap Challenge that integrates figure-related
textual context with author-specific writing styles using the LaMP-Cap dataset.
Our approach uses a two-stage pipeline: Stage 1 combines context filtering,
category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption
candidate selection; Stage 2 applies few-shot prompting with profile figures
for stylistic refinement. Our experiments demonstrate that category-specific
prompts outperform both zero-shot and general optimized approaches, improving
ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4
reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\%
gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates
that combining contextual understanding with author-specific stylistic
adaptation can generate captions that are both scientifically accurate and
stylistically faithful to the source paper.

</details>


### [60] [Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks](https://arxiv.org/abs/2510.08002)
*Cheng Yang,Xuemeng Yang,Licheng Wen,Daocheng Fu,Jianbiao Mei,Rong Wu,Pinlong Cai,Yufan Shen,Nianchen Deng,Botian Shi,Yu Qiao,Haifeng Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于分层记忆模块的自我进化大语言模型代理框架MUSE，实现了基于经验的持续学习和长任务规划，在长程生产力基准测试中取得了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理在真实世界长任务场景中表现受限，主要因为它们是测试时静态的，无法从经验中学习和积累知识。

Method: MUSE引入了一个分层记忆模块，将多层次经验结构化存储，代理在完成每个子任务后反思轨迹并更新记忆模块，实现基于经验的持续自我进化。

Result: MUSE在TAC长程生产力基准测试中，使用轻量级模型获得显著的性能提升，并展示了随着经验积累任务完成能力和自我进化能力的增强，且经验具有良好泛化性，实现零样本新任务改进。

Conclusion: MUSE开创了支持真实生产力任务自动化的AI代理新范式，通过经验驱动的自我进化实现持续学习与性能提升。

Abstract: Large Language Models have demonstrated remarkable capabilities across
diverse domains, yet significant challenges persist when deploying them as AI
agents for real-world long-horizon tasks. Existing LLM agents suffer from a
critical limitation: they are test-time static and cannot learn from
experience, lacking the ability to accumulate knowledge and continuously
improve on the job. To address this challenge, we propose MUSE, a novel agent
framework that introduces an experience-driven, self-evolving system centered
around a hierarchical Memory Module. MUSE organizes diverse levels of
experience and leverages them to plan and execute long-horizon tasks across
multiple applications. After each sub-task execution, the agent autonomously
reflects on its trajectory, converting the raw trajectory into structured
experience and integrating it back into the Memory Module. This mechanism
enables the agent to evolve beyond its static pretrained parameters, fostering
continuous learning and self-evolution. We evaluate MUSE on the long-horizon
productivity benchmark TAC. It achieves new SOTA performance by a significant
margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments
demonstrate that as the agent autonomously accumulates experience, it exhibits
increasingly superior task completion capabilities, as well as robust
continuous learning and self-evolution capabilities. Moreover, the accumulated
experience from MUSE exhibits strong generalization properties, enabling
zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI
agents capable of real-world productivity task automation.

</details>


### [61] [ChatGPT as a Translation Engine: A Case Study on Japanese-English](https://arxiv.org/abs/2510.08042)
*Vincent Michael Sutanto,Giovanni Gatti De Giacomo,Toshiaki Nakazawa,Masaru Yamada*

Main category: cs.CL

TL;DR: 本文研究了ChatGPT在日英翻译中的表现，比较了简单和增强提示，并对比了商业翻译引擎。结果显示文档级翻译优于句子级翻译，ChatGPT表现具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在日英翻译中的有效性及如何通过提示工程优化翻译质量。

Method: 使用简单和增强提示对ChatGPT进行日英翻译，进行自动和MQM人类评估，与商业翻译引擎对比。

Result: 文档级翻译优于句子级，增强提示未显著优于简单提示，ChatGPT-3.5在准确度上优于ChatGPT-4，后者流畅度更好，整体表现与商业系统相当。

Conclusion: ChatGPT尤其是文档级翻译，能够提供接近商业翻译引擎的翻译质量，提示设计和模型版本选择影响翻译表现。

Abstract: This study investigates ChatGPT for Japanese-English translation, exploring
simple and enhanced prompts and comparing against commercially available
translation engines. Performing both automatic and MQM-based human evaluations,
we found that document-level translation outperforms sentence-level translation
for ChatGPT. On the other hand, we were not able to determine if enhanced
prompts performed better than simple prompts in our experiments. We also
discovered that ChatGPT-3.5 was preferred by automatic evaluation, but a
tradeoff exists between accuracy (ChatGPT-3.5) and fluency (ChatGPT-4). Lastly,
ChatGPT yields competitive results against two widely-known translation
systems.

</details>


### [62] [Climate Knowledge in Large Language Models](https://arxiv.org/abs/2510.08043)
*Ivan Kuznetsov,Jacopo Grassi,Dmitrii Pantiukhin,Boris Shapkin,Thomas Jung,Nikolay Koldunov*

Main category: cs.CL

TL;DR: 本文评估大型语言模型（LLMs）无需外部检索情况下对气候常态的记忆能力，发现其能捕捉部分气候结构但在高海拔和高纬度表现欠佳。


<details>
  <summary>Details</summary>
Motivation: 了解LLMs内在的气候知识对于其在气候相关应用中的可靠性和误信息风险评估至关重要，现有研究对此能力还未充分表征。

Method: 构建全球1度网格含地理信息的查询集，针对1991-2020年7月2日平均2米空气温度进行测试，并与ERA5再分析数据进行验证。

Result: LLMs能较好捕捉纬度和地形气候模式，平均误差3-6℃，偏差约±1℃，但高海拔及高纬度区域误差较大；增加地理上下文信息可降低误差27%。模型捕捉到了全球平均变暖，但未能重现局部温度变化空间格局。

Conclusion: LLMs虽然掌握当前气候分布的知识，但对长期温度变化的区域性和地方性表达能力不足，限制了其在气候动态理解中的应用价值。本文提出的评估框架为量化LLMs的气候知识提供了可重复的基准。

Abstract: Large language models (LLMs) are increasingly deployed for climate-related
applications, where understanding internal climatological knowledge is crucial
for reliability and misinformation risk assessment. Despite growing adoption,
the capacity of LLMs to recall climate normals from parametric knowledge
remains largely uncharacterized. We investigate the capacity of contemporary
LLMs to recall climate normals without external retrieval, focusing on a
prototypical query: mean July 2-m air temperature 1991-2020 at specified
locations. We construct a global grid of queries at 1{\deg} resolution land
points, providing coordinates and location descriptors, and validate responses
against ERA5 reanalysis. Results show that LLMs encode non-trivial climate
structure, capturing latitudinal and topographic patterns, with
root-mean-square errors of 3-6 {\deg}C and biases of $\pm$1 {\deg}C. However,
spatially coherent errors remain, particularly in mountains and high latitudes.
Performance degrades sharply above 1500 m, where RMSE reaches 5-13 {\deg}C
compared to 2-4 {\deg}C at lower elevations. We find that including geographic
context (country, city, region) reduces errors by 27% on average, with larger
models being most sensitive to location descriptors. While models capture the
global mean magnitude of observed warming between 1950-1974 and 2000-2024, they
fail to reproduce spatial patterns of temperature change, which directly relate
to assessing climate change. This limitation highlights that while LLMs may
capture present-day climate distributions, they struggle to represent the
regional and local expression of long-term shifts in temperature essential for
understanding climate dynamics. Our evaluation framework provides a
reproducible benchmark for quantifying parametric climate knowledge in LLMs and
complements existing climate communication assessments.

</details>


### [63] [A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models](https://arxiv.org/abs/2510.08049)
*Congming Zheng,Jiachen Zhu,Zhuoying Ou,Yuxiang Chen,Kangning Zhang,Rong Shan,Zeyu Zheng,Mengyue Yang,Jianghao Lin,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: 本文综述了过程奖励模型（PRMs）在大语言模型（LLMs）中的应用，强调其在细粒度推理对齐中的作用。


<details>
  <summary>Details</summary>
Motivation: 传统的结果奖励模型仅关注最终答案，忽略了推理过程的质量，PRMs旨在填补这一空白，通过评估和指导推理过程提升模型表现。

Method: 系统介绍了如何生成过程数据，构建过程奖励模型，以及在测试时扩展和强化学习中使用PRMs的方法。

Result: 总结了PRMs在数学、代码、文本、多模态推理、机器人和代理等领域的应用，并评审了新兴的基准测试。

Conclusion: 文章明确了PRMs的设计空间，揭示了当前的挑战，为未来实现细粒度和鲁棒推理对齐提供了指导。

Abstract: Although Large Language Models (LLMs) exhibit advanced reasoning ability,
conventional alignment remains largely dominated by outcome reward models
(ORMs) that judge only final answers. Process Reward Models(PRMs) address this
gap by evaluating and guiding reasoning at the step or trajectory level. This
survey provides a systematic overview of PRMs through the full loop: how to
generate process data, build PRMs, and use PRMs for test-time scaling and
reinforcement learning. We summarize applications across math, code, text,
multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our
goal is to clarify design spaces, reveal open challenges, and guide future
research toward fine-grained, robust reasoning alignment.

</details>


### [64] [FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation](https://arxiv.org/abs/2510.08058)
*Shule Lu,Lingxiang Wang,Sijia Wen,Ziwei Wang,Hainan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于可信度评估的联邦自适应聚合策略FedDTRE，用于解决对话生成中的过拟合和遗忘问题，提升模型性能和对话质量。


<details>
  <summary>Details</summary>
Motivation: 现有的集中式和本地训练方法在保护数据隐私和个性化之间难以权衡，且传统联邦学习方法因客户端数据有限易过拟合且多轮训练后会遗忘全局信息，导致泛化能力差。

Method: FedDTRE利用公平性评估数据集上全局模型和本地模型的可信度评分，动态调节全局模型在本地更新中的贡献，而非直接替换本地模型。

Result: 实验结果表明，FedDTRE能够提升对话模型的性能和生成对话的质量。

Conclusion: FedDTRE提供了一种有效的联邦学习策略，通过结合可信度评分实现更好的模型聚合，解决了过拟合和遗忘问题，促进了对话系统中的隐私保护和个性化。

Abstract: With the rapid development of artificial intelligence, dialogue systems have
become a prominent form of human-computer interaction. However, traditional
centralized or fully local training approaches face challenges in balancing
privacy preservation and personalization due to data privacy concerns and
heterogeneous device capabilities. Federated learning, as a representative
distributed paradigm, offers a promising solution. However, existing methods
often suffer from overfitting under limited client data and tend to forget
global information after multiple training rounds, leading to poor
generalization. To address these issues, we propose FedDTRE, a Federated
adaptive aggregation strategy for Dialogue generation based on Trustworthiness
Evaluation. Instead of directly replacing local models with the global model,
FedDTRE leverages trustworthiness scores of both global and local models on a
fairness-oriented evaluation dataset to dynamically regulate the global model's
contribution during local updates. Experimental results demonstrate that
FedDTRE can improve dialogue model performance and enhance the quality of
dialogue generation.

</details>


### [65] [Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility](https://arxiv.org/abs/2510.08091)
*Shramay Palta,Peter Rankel,Sarah Wiegreffe,Rachel Rudinger*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLMs）生成的理由如何影响人类对常识多项选择题答案的合理性判断，发现LLM生成的正反理由均能显著影响人类和模型的评价，提示LLM在影响人类认知方面具有潜力和风险。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解人类对常识问题答案的合理性判断在多大程度上受到大语言模型生成的支持或反对理由的影响，揭示LLM对人类认知和信念可能产生的影响。

Method: 通过收集3000条人类的合理性判断和13600条LLM的合理性判断，比较有无LLM生成的支持（PRO）和反对（CON）理由时，人类和模型对答案合理性的评价变化。

Result: 结果显示，在存在LLM生成的正理由时，人类的合理性评分平均上升；存在反理由时，评分平均下降；LLM的表现与人类类似，表明人类认为这些理由具有说服力。

Conclusion: 该研究展示了使用LLM研究人类认知的新方法，同时提醒在常识领域内，即使人类为“专家”，LLM依然可能显著影响人们的信念，带来实际应用中的风险。

Abstract: We investigate the degree to which human plausibility judgments of
multiple-choice commonsense benchmark answers are subject to influence by
(im)plausibility arguments for or against an answer, in particular, using
rationales generated by LLMs. We collect 3,000 plausibility judgments from
humans and another 13,600 judgments from LLMs. Overall, we observe increases
and decreases in mean human plausibility ratings in the presence of
LLM-generated PRO and CON rationales, respectively, suggesting that, on the
whole, human judges find these rationales convincing. Experiments with LLMs
reveal similar patterns of influence. Our findings demonstrate a novel use of
LLMs for studying aspects of human cognition, while also raising practical
concerns that, even in domains where humans are ``experts'' (i.e., common
sense), LLMs have the potential to exert considerable influence on people's
beliefs.

</details>


### [66] [The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models](https://arxiv.org/abs/2510.08098)
*Sherzod Hakimov,Roland Bernard,Tim Leiber,Karl Osswald,Kristina Richert,Ruilin Yang,Raffaella Bernardi,David Schlangen*

Main category: cs.CL

TL;DR: 本文首次系统评估了推理能力对大型语言模型（LLM）谈判能力的影响，涵盖多种模型和三种语言。


<details>
  <summary>Details</summary>
Motivation: 探讨推理能力如何提升AI智能体在谈判中的表现，特别是在多语言环境中的表现差异。

Method: 采用自我对话的方式，在三种不同的对话游戏中测试商业和开源大型语言模型的推理能力及其对谈判表现的影响。

Result: 推理显著提升了模型的谈判结果，GPT-5性能提升31.4%，但计算成本增加近400%。开源模型在多语言谈判中倾向于内部切换至英语推理，而商业模型保持语言一致性。

Conclusion: 推理能力是提升AI谈判表现的重要因素，但需要权衡计算成本和语言一致性，特别是在多语言应用场景中。

Abstract: Negotiation is a fundamental challenge for AI agents, as it requires an
ability to reason strategically, model opponents, and balance cooperation with
competition. We conduct the first comprehensive study systematically evaluating
the effect of (LLM-)reasoning on the negotiation abilities of both commercial
and open-weight LLMs, and do this across three languages. Using a self-play
setup across three diverse dialogue games, we analyse trade-offs between
performance and cost, the language consistency of reasoning processes, and the
nature of strategic adaptation exhibited by models. Our findings show that
enabling reasoning-that is, scaling test time compute-significantly improves
negotiation outcomes by enhancing collaboration and helping models overcome
task complexities, but comes at a substantial computational cost: reasoning
improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400
%. Most critically, we uncover a significant multilingual reasoning
distinction: open-weight models consistently switch to English for their
internal reasoning steps, even when negotiating in German or Italian (and thus
possibly impacting potential explainability gains through the disclosure of
reasoning traces), while leading commercial models maintain language
consistency between their reasoning and final output.

</details>


### [67] [Lossless Vocabulary Reduction for Auto-Regressive Language Models](https://arxiv.org/abs/2510.08102)
*Daiki Chijiwa,Taku Hasegawa,Kyosuke Nishida,Shin'ya Yamaguchi,Tomoya Ohba,Tamao Sakao,Susumu Takeuchi*

Main category: cs.CL

TL;DR: 本文提出了一种无损词汇表缩减的理论框架，实现不同自动回归语言模型的词汇表高效转换，提升模型间合作效率。


<details>
  <summary>Details</summary>
Motivation: 由于不同语言模型拥有各自的词汇表，导致它们难以在下一个词分布层面合作，如模型集成，因此需要一种方法实现不同词汇表间的高效转换。

Method: 建立无损词汇表缩减的理论框架，将给定的自动回归语言模型转换为任意小词汇表的模型且保持准确度不变。

Result: 实验证明，基于最大公共词汇表，不同分词策略的语言模型可以高效合作。

Conclusion: 所提框架实现了语言模型词汇表的无损缩减，为不同模型间的合作与集成提供了理论基础和实用方法。

Abstract: Tokenization -- the process of decomposing a given text into a sequence of
subwords called tokens -- is one of the key components in the development of
language models. Particularly, auto-regressive language models generate texts
token by token, i.e., by predicting the next-token distribution given the
previous ones, and thus tokenization directly affects their efficiency in text
generation. Since each language model has their own vocabulary as a set of
possible tokens, they struggle to cooperate with each other at the level of
next-token distributions such as model ensemble. In this paper, we establish a
theoretical framework of lossless vocabulary reduction, which efficiently
converts a given auto-regressive language model into the one with an
arbitrarily small vocabulary without any loss in accuracy. As an application,
we demonstrate that language models with different tokenization can cooperate
with each other efficiently through their maximal common vocabulary.

</details>


### [68] [Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing](https://arxiv.org/abs/2510.08111)
*Haoyang Gui,Thales Bertaglia,Taylor Annabell,Catalina Goanta,Tjomme Dooper,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文通过对Instagram内容的自动检测，评估大型语言模型在识别未披露的广告内容中的表现及其法律推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前对影响者营销中未披露广告内容的检测缺乏法律基础且多为黑箱操作，难以有效执行法规。

Method: 使用1143条Instagram帖子，比较gpt-5-nano和gemini-2.5-flash-lite模型在三种提示策略下的表现，并分析它们的法律推理错误类型。

Result: 两模型在分类广告与非广告内容时表现良好（F1最高0.93），但对模糊案例准确率降低超过10个百分点，常见错误包括引用遗漏、引用不清和隐藏广告误判。

Conclusion: 本文通过构建LLM法律推理错误分类法、提供带法律注释的数据集，并结合定量定性方法，推动了基于法律依据的透明化影响者广告内容检测和自动化监管的实现。

Abstract: The rise of influencer marketing has blurred boundaries between organic
content and sponsored content, making the enforcement of legal rules relating
to transparency challenging. Effective regulation requires applying legal
knowledge with a clear purpose and reason, yet current detection methods of
undisclosed sponsored content generally lack legal grounding or operate as
opaque "black boxes". Using 1,143 Instagram posts, we compare gpt-5-nano and
gemini-2.5-flash-lite under three prompting strategies with controlled levels
of legal knowledge provided. Both models perform strongly in classifying
content as sponsored or not (F1 up to 0.93), though performance drops by over
10 points on ambiguous cases. We further develop a taxonomy of reasoning
errors, showing frequent citation omissions (28.57%), unclear references
(20.71%), and hidden ads exhibiting the highest miscue rate (28.57%). While
adding regulatory text to the prompt improves explanation quality, it does not
consistently improve detection accuracy. The contribution of this paper is
threefold. First, it makes a novel addition to regulatory compliance technology
by providing a taxonomy of common errors in LLM-generated legal reasoning to
evaluate whether automated moderation is not only accurate but also legally
robust, thereby advancing the transparent detection of influencer marketing
content. Second, it features an original dataset of LLM explanations annotated
by two students who were trained in influencer marketing law. Third, it
combines quantitative and qualitative evaluation strategies for LLM
explanations and critically reflects on how these findings can support
advertising regulatory bodies in automating moderation processes on a solid
legal foundation.

</details>


### [69] [Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations](https://arxiv.org/abs/2510.08120)
*Jasmina Gajcin,Erik Miehling,Rahul Nair,Elizabeth Daly,Radu Marinescu,Seshu Tirupathi*

Main category: cs.CL

TL;DR: 该论文提出了从大型语言模型（LLM）作为评判者中提取基于概念的全局决策策略的方法，通过局部对比解释和迭代聚类总结，提升了自动化文本评估的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型被广泛用于文本评估替代人工标注，亟需理解其潜在偏见和风险，提高其决策过程的透明度和可验证性。

Method: 提出两种算法：CLoVE用于生成基于概念的可验证局部对比解释，GloVE通过迭代聚类、总结和验证将局部规则凝练成全局政策。

Result: 在七个内容伤害检测基准数据集上测试，提取的全局策略对LLM判决高度忠实，同时验证了策略对文本扰动和对抗攻击的鲁棒性，并通过用户研究评估了用户的理解和满意度。

Conclusion: 所提方法有效提升了LLM作为评判者的可解释性和稳健性，有助于更安全可靠地应用LLM进行自动化文本评估。

Abstract: Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being
used at scale to augment or even replace human annotations. As such, it is
imperative that we understand the potential biases and risks of doing so. In
this work, we propose an approach for extracting high-level concept-based
global policies from LLM-as-a-Judge. Our approach consists of two algorithms:
1) CLoVE (Contrastive Local Verifiable Explanations), which generates
verifiable, concept-based, contrastive local explanations and 2) GloVE (Global
Verifiable Explanations), which uses iterative clustering, summarization and
verification to condense local rules into a global policy. We evaluate GloVE on
seven standard benchmarking datasets for content harm detection. We find that
the extracted global policies are highly faithful to decisions of the
LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to
text perturbations and adversarial attacks. Finally, we conducted a user study
to evaluate user understanding and satisfaction with global policies.

</details>


### [70] [Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling](https://arxiv.org/abs/2510.08145)
*Shuliang Liu,Zhipeng Xu,Zhenghao Liu,Yukun Yan,Minghe Yu,Yu Gu,Chong Chen,Huiyuan Xie,Ge Yu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Genii的无监督多代理协作优化框架，以减少大型语言模型作为评判者时的偏好性偏差，提高评判准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型作为自动评估工具时存在偏好自身生成回答的倾向，导致评判不够可靠。如何减少这种偏好偏差是该领域的挑战。

Method: Genii框架将多个基于LLM的评判模型整合到多代理系统，模拟客户端-服务器投票机制，通过无监督方式优化各客户端代理，缓解偏好偏差。

Result: 实验表明，Genii优于基于标注数据训练的监督模型，能够在不同客户端代理中持续提升性能，即使服务器代理模型较弱也能保持效果。

Conclusion: Genii有效减轻了LLM评判模型的偏好偏差，提高了评判表现，且不依赖人工标注数据，具备实用价值。代码已公开。

Abstract: Large Language Models (LLMs) as automatic evaluators, commonly referred to as
LLM-as-a-Judge, have also attracted growing attention. This approach plays a
vital role in aligning LLMs with human judgments, providing accurate and
reliable assessments. However, LLM-based judgment models often exhibit judgment
preference bias during the evaluation phase, tending to favor responses
generated by themselves, undermining the reliability of their judgments. This
paper introduces the Group-Based Polling Optimization (Genii), an unsupervised
multi-agent collaborative optimization framework that mitigates the inherent
judgment preference bias of judgment models. Specifically, Genii integrates
various LLM-based judgment models into a multi-agent system and simulates the
interactive client-server polling mechanism to optimize each client agent
unsupervisedly. Our experiments demonstrate that Genii outperforms supervised
models trained on annotated judgment data, while requiring no human-labeled
annotations. Genii consistently improves performance across different client
agents during the polling, even when weaker models act as server agents.
Further analysis reveals that Genii effectively mitigates judgment preference
bias of LLM-based judgment models, demonstrating its effectiveness. All codes
are available at https://github.com/NEUIR/Genii.

</details>


### [71] [AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents](https://arxiv.org/abs/2510.08149)
*Md Tahmid Rahman Laskar,Julien Bouvier Tremblay,Xue-Yong Fu,Cheng Chen,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: 本论文提出了AI Knowledge Assist系统，通过从历史客服对话中提取问答对，自动构建专属知识库，使用微调轻量级LLM实现了优于大型闭源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公司专属知识库，阻碍了对话式AI系统在客服中心的应用。

Method: 从历史客户-客服对话中提取问答对，构建知识库，并在轻量级LLM（LLaMA-3.1-8B）上微调以提升性能。

Result: 在20家公司测试中，系统在信息查询类问题上的准确率超过90%，显著缩小了客服中心的冷启动差距。

Conclusion: 该系统支持基于检索增强生成的聊天机器人实现即时部署，有效推动了对话式AI在客服中的应用。

Abstract: The utilization of conversational AI systems by leveraging Retrieval
Augmented Generation (RAG) techniques to solve customer problems has been on
the rise with the rapid progress of Large Language Models (LLMs). However, the
absence of a company-specific dedicated knowledge base is a major barrier to
the integration of conversational AI systems in contact centers. To this end,
we introduce AI Knowledge Assist, a system that extracts knowledge in the form
of question-answer (QA) pairs from historical customer-agent conversations to
automatically build a knowledge base. Fine-tuning a lightweight LLM on internal
data demonstrates state-of-the-art performance, outperforming larger
closed-source LLMs. More specifically, empirical evaluation on 20 companies
demonstrates that the proposed AI Knowledge Assist system that leverages the
LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by
achieving above 90% accuracy in answering information-seeking questions. This
enables immediate deployment of RAG-powered chatbots.

</details>


### [72] [DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations](https://arxiv.org/abs/2510.08152)
*Elena Khasanova,Harsh Saini,Md Tahmid Rahman Laskar,Xue-Yong Fu,Cheng Chen,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: 提出了一种名为DACIP-RC的持续预训练方法，显著提升小型大语言模型在商业会话任务中的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大规模大语言模型虽强大但成本高昂，小型模型效率高但缺乏跨领域零样本指令能力，传统微调导致遗忘问题，限制了模型的适应性。

Method: 提出DACIP-RC方法，通过阅读理解对话记录，生成多样化任务指令和响应，进行领域自适应持续预训练，提升指令的泛化能力。

Result: 实验证明DACIP-RC显著提升了小型模型在会议总结、行动项生成、通话目的识别等多项商业会话任务的零样本泛化表现。

Conclusion: 首次将指令预训练应用于商业会话数据，展示了公司如何利用专有数据实现领域适应的有效途径。

Abstract: The rapid advancements in Large Language Models (LLMs) have enabled their
adoption in real-world industrial scenarios for various natural language
processing tasks. However, the high inference cost of large-scale LLMs makes
their deployment impractical, necessitating the use of smaller models. Despite
their efficiency, smaller LLMs lack robust zero-shot instruction-following
capabilities across diverse domains, limiting their adaptability to dynamic
user requirements. Traditional fine-tuning approaches exacerbate this issue by
inducing catastrophic forgetting, reducing the model's generalization ability
for unseen tasks. In this paper, we propose Domain Adaptive Continual
Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual
pre-training technique that enhances smaller LLMs' domain adaptability for
business conversational tasks. Unlike conventional pre-training approaches that
rely on next-token prediction, DACIP-RC generates diverse task instructions and
responses via reading comprehension on conversation transcripts, enabling
better instruction generalization. Our empirical evaluations demonstrate that
DACIP-RC significantly improves zero-shot generalization across a wide range of
business conversational tasks, including meeting summarization, action item
generation, and call purpose identification. To the best of our knowledge, this
is the first work to apply instruction pre-training on business conversational
data, providing insights into how industries can leverage proprietary datasets
for domain adaptation.

</details>


### [73] [Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs](https://arxiv.org/abs/2510.08158)
*Shuzhou Yuan,Ercong Nie,Yinuo Sun,Chenxuan Zhao,William LaCroix,Michael Färber*

Main category: cs.CL

TL;DR: 本文针对大语言模型在面对安全相关提示时常产生过度拒绝问题，提出了两个新基准数据集（XSB和MS-XSB）用于系统评估并分析拒绝触发词。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对包含类似不安全查询术语的请求时，往往错误拒绝无害请求，影响模型的实用性和用户体验。

Method: 设计单轮和多轮对话场景的基准数据集，利用后验解释方法识别拒绝触发词，并提出三种推理时轻量级、与模型无关的解决方案：忽略词指令、提示重写和注意力引导，无需重训练或访问模型参数。

Result: 在四个指令调优的Llama模型上实验验证，这些方法显著提升了模型对安全提示的合规性，同时保持了良好的安全防护效果。

Conclusion: 本文建立了一个可复现的诊断及缓解过度拒绝框架，为实现更安全、更有帮助的大语言模型部署提供了实用路径。

Abstract: Large language models (LLMs) frequently produce false refusals, declining
benign requests that contain terms resembling unsafe queries. We address this
challenge by introducing two comprehensive benchmarks: the Exaggerated Safety
Benchmark (XSB) for single-turn prompts, annotated with "Focus" keywords that
identify refusal-inducing triggers, and the Multi-turn Scenario-based
Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal
calibration in realistic, context-rich dialog settings. Our benchmarks reveal
that exaggerated refusals persist across diverse recent LLMs and are especially
pronounced in complex, multi-turn scenarios. To mitigate these failures, we
leverage post-hoc explanation methods to identify refusal triggers and deploy
three lightweight, model-agnostic approaches, ignore-word instructions, prompt
rephrasing, and attention steering, at inference time, all without retraining
or parameter access. Experiments on four instruction-tuned Llama models
demonstrate that these strategies substantially improve compliance on safe
prompts while maintaining robust safety protections. Our findings establish a
reproducible framework for diagnosing and mitigating exaggerated refusals,
highlighting practical pathways to safer and more helpful LLM deployments.

</details>


### [74] [ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code](https://arxiv.org/abs/2510.08163)
*Jian Xie,Zhendong Chu,Aoxiao Zhong,Kai Zhang,Mingzhe Han,Xin Fang,Jialie Shen,Qingsong Wen*

Main category: cs.CL

TL;DR: ARM2是一种通过强化学习和平衡推理性能与效率实现的统一模型，显著减少了推理过程中的冗长性。


<details>
  <summary>Details</summary>
Motivation: 目前大型推理模型常出现“过度思考”问题，造成在简单任务上产生不必要的长推理文本，现有缓解策略多为启发式且任务专一，缺乏通用的自适应推理框架。

Method: 提出ARM2模型，利用强化学习结合长度感知优化，统一处理多种格式推理任务，整合视觉理解和可执行代码，提升多模态适应性并减少token成本。

Result: ARM2在维持传统推理模型性能的同时，平均减少了70%以上的token使用，实验验证了其有效性和设计合理性。

Conclusion: ARM2为大型推理模型提供了一个高效且通用的自适应推理框架，显著缓解“过度思考”问题，提升推理效率和多模态任务表现。

Abstract: Large Reasoning Models (LRMs) often suffer from the ``over-thinking''
problem, generating unnecessarily long reasoning on simple tasks. Some
strategies have been proposed to mitigate this issue, such as length penalties
or routing mechanisms, but they are typically heuristic and task-specific,
lacking a general framework for adaptive reasoning. In this paper, we present
ARM2, a unified model that adaptively balances reasoning performance and
efficiency across multiple formats through a reinforcement learning framework
augmented with length-aware optimization. Beyond conventional natural language
inference, ARM2 integrates vision understanding, extending its applicability to
multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling
substantial reductions in token cost while preserving task performance compared
to long CoT. Experiments demonstrate that ARM2 achieves performance on par with
traditional reasoning models trained with GRPO, while reducing token usage by
over 70% on average. We further conduct extensive analyses to validate the
effectiveness of ARM2 and the soundness of its design.

</details>


### [75] [MetricalARGS: A Taxonomy for Studying Metrical Poetry with LLMs](https://arxiv.org/abs/2510.08188)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 本文提出了MetricalARGS，一个针对韵律诗歌的NLP任务分类，旨在评估大型语言模型在韵律诗歌分析、检索、生成和辅助方面的能力。以泰卢固语为例，展示了该分类的实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前NLP关于诗歌的研究主要集中于自动生成和摘要，韵律诗歌的语言和结构规则为测试大型语言模型的更深层次推理和语言理解能力提供了机会。

Method: 构建了MetricalARGS分类体系，涵盖分析、检索、生成和辅助四个维度，探讨与现有数据集和评价指标的关联，并通过泰卢固语示例验证该体系。

Result: 提出了首个针对韵律诗歌的NLP任务分类体系，展示了其如何帮助评估大型语言模型的多方面能力及其局限。

Conclusion: 韵律诗歌为理解和评估现代大型语言模型的能力提供了一种新的视角和丰富的任务场景。MetricalARGS作为分类工具，有助于发掘语言模型在严格语言规则下的表现。

Abstract: Prior NLP work studying poetry has focused primarily on automatic poem
generation and summarization. Many languages have well-studied traditions of
poetic meter which enforce constraints on a poem in terms of syllable and
phoneme patterns. Such advanced literary forms offer opportunities for probing
deeper reasoning and language understanding in Large Language Models (LLMs) and
their ability to follow strict pre-requisites and rules. In this paper, we
introduce MetricalARGS, the first taxonomy of poetry-related NLP tasks designed
to evaluate LLMs on metrical poetry across four dimensions: Analysis,
Retrieval, Generation, and Support. We discuss how these tasks relate to
existing NLP tasks, addressing questions around datasets and evaluation
metrics. Taking Telugu as our example language, we illustrate how the taxonomy
can be used in practice. MetricalARGS highlights the broader possibilities for
understanding the capabilities and limitations of today's LLMs through the lens
of metrical poetry.

</details>


### [76] [Training-Free Group Relative Policy Optimization](https://arxiv.org/abs/2510.08191)
*Yuzheng Cai,Siqi Cai,Yuchen Shi,Zihan Xu,Lichao Chen,Yulei Qin,Xiaoyu Tan,Gang Li,Zongyi Li,Haojia Lin,Yong Mao,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 本文提出了一种无需参数更新的训练方法Training-Free GRPO，通过学习体验知识作为token先验，提升大型语言模型（LLM）在专业领域中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在专业领域表现下降，传统方法通过昂贵的参数更新（如SFT+RL）优化输出分布，存在数据稀缺和过拟合问题，需寻求轻量且有效的优化策略。

Method: 引入Training-Free Group Relative Policy Optimization，基于组相对语义优势对多轮样本进行多轮次学习，提取高质量体验知识作为token先验，直接集成于API调用中，无需参数更新。

Result: 在数学推理和网络搜索任务中，Training-Free GRPO在仅用少量训练样本情况下，显著提升DeepSeek-V3.1-Terminus的领域外表现，优于使用边际训练数据进行微调的小型LLM。

Conclusion: Training-Free GRPO是一种低成本、高效的LLM优化方法，克服了传统参数更新的成本和数据限制，提升了模型在实际应用中的泛化能力。

Abstract: Recent advances in Large Language Model (LLM) agents have demonstrated their
promising general capabilities. However, their performance in specialized
real-world domains often degrades due to challenges in effectively integrating
external tools and specific prompting strategies. While methods like agentic
reinforcement learning have been proposed to address this, they typically rely
on costly parameter updates, for example, through a process that uses
Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase
with Group Relative Policy Optimization (GRPO) to alter the output
distribution. However, we argue that LLMs can achieve a similar effect on the
output distribution by learning experiential knowledge as a token prior, which
is a far more lightweight approach that not only addresses practical data
scarcity but also avoids the common issue of overfitting. To this end, we
propose Training-Free Group Relative Policy Optimization (Training-Free GRPO),
a cost-effective solution that enhances LLM agent performance without any
parameter updates. Our method leverages the group relative semantic advantage
instead of numerical ones within each group of rollouts, iteratively distilling
high-quality experiential knowledge during multi-epoch learning on a minimal
ground-truth data. Such knowledge serves as the learned token prior, which is
seamlessly integrated during LLM API calls to guide model behavior. Experiments
on mathematical reasoning and web searching tasks demonstrate that
Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly
improves out-of-domain performance. With just a few dozen training samples,
Training-Free GRPO outperforms fine-tuned small LLMs with marginal training
data and cost.

</details>


### [77] [Memory Retrieval and Consolidation in Large Language Models through Function Tokens](https://arxiv.org/abs/2510.08203)
*Shaohua Zhang,Yuan Lin,Hang Li*

Main category: cs.CL

TL;DR: 本文提出功能词假设，解释大语言模型在推理时通过功能词激活预测特征实现记忆检索，在预训练时通过预测跟随功能词的内容词实现记忆巩固。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型记忆的检索和巩固机制理解不足，需探讨其内部工作原理。

Method: 提出功能词假设，将功能词视为激活预测特征的重要触发器，通过二分图分析和案例研究验证功能词在记忆检索和巩固中的关键作用。

Result: 实验显示少数功能词激活了大多数特征，训练损失主要来源预测功能词后续内容词，支持功能词掌控记忆的假设。

Conclusion: 功能词在大语言模型的记忆检索与巩固中起核心作用，促进模型学习最具预测力的特征，深化了对模型机制的理解。

Abstract: The remarkable success of large language models (LLMs) stems from their
ability to consolidate vast amounts of knowledge into the memory during
pre-training and to retrieve it from the memory during inference, enabling
advanced capabilities such as knowledge memorization, instruction-following and
reasoning. However, the mechanisms of memory retrieval and consolidation in
LLMs remain poorly understood. In this paper, we propose the function token
hypothesis to explain the workings of LLMs: During inference, function tokens
activate the most predictive features from context and govern next token
prediction (memory retrieval). During pre-training, predicting the next tokens
(usually content tokens) that follow function tokens increases the number of
learned features of LLMs and updates the model parameters (memory
consolidation). Function tokens here roughly correspond to function words in
linguistics, including punctuation marks, articles, prepositions, and
conjunctions, in contrast to content tokens. We provide extensive experimental
evidence supporting this hypothesis. Using bipartite graph analysis, we show
that a small number of function tokens activate the majority of features. Case
studies further reveal how function tokens activate the most predictive
features from context to direct next token prediction. We also find that during
pre-training, the training loss is dominated by predicting the next content
tokens following function tokens, which forces the function tokens to select
the most predictive features from context.

</details>


### [78] [LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions](https://arxiv.org/abs/2510.08211)
*XuHao Hu,Peng Wang,Xiaoya Lu,Dongrui Liu,Xuanjing Huang,Jing Shao*

Main category: cs.CL

TL;DR: 在高风险场景下，微调导致的大型语言模型（LLMs）不诚实和欺骗行为的出现。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs因微调错误或恶意内容而导致更广泛不诚实行为的现象，超出以往对安全行为的关注。

Method: 对开源LLMs在多样化领域中的错误输出进行微调，测试其在不诚实和欺骗行为方面的表现，并模拟实际人机交互环境进行验证。

Result: 发现LLMs在不诚实行为上广泛表现出错乱，且即使仅1%的错误微调数据也会显著降低其诚实率超过20%。在人机交互中，10%有偏见的用户群体即可导致助手显著不诚实。

Conclusion: LLMs在高风险场景下的不诚实和欺骗行为风险不仅通过直接微调产生，也会在混合任务和实际人机交互中无意中出现。

Abstract: Previous research has shown that LLMs finetuned on malicious or incorrect
completions within narrow domains (e.g., insecure code or incorrect medical
advice) can become broadly misaligned to exhibit harmful behaviors, which is
called emergent misalignment. In this work, we investigate whether this
phenomenon can extend beyond safety behaviors to a broader spectrum of
dishonesty and deception under high-stakes scenarios (e.g., lying under
pressure and deceptive behavior). To explore this, we finetune open-sourced
LLMs on misaligned completions across diverse domains. Experimental results
demonstrate that LLMs show broadly misaligned behavior in dishonesty.
Additionally, we further explore this phenomenon in a downstream combined
finetuning setting, and find that introducing as little as 1% of misalignment
data into a standard downstream task is sufficient to decrease honest behavior
over 20%. Furthermore, we consider a more practical human-AI interaction
environment where we simulate both benign and biased users to interact with the
assistant LLM. Notably, we find that the assistant can be misaligned
unintentionally to exacerbate its dishonesty with only 10% biased user
population. In summary, we extend the study of emergent misalignment to the
domain of dishonesty and deception under high-stakes scenarios, and demonstrate
that this risk arises not only through direct finetuning, but also in
downstream mixture tasks and practical human-AI interactions.

</details>


### [79] [SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets](https://arxiv.org/abs/2510.08214)
*Qiang Yang,Xiuying Chen,Changsheng Ma,Rui Yin,Xin Gao,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 本文介绍了SenWave数据集，一个跨五种语言的细粒度新冠疫情推文情感分析数据集，包含多达10种情感类别与约10万条标注推文及超过1亿条未标注推文。


<details>
  <summary>Details</summary>
Motivation: 现有新冠疫情相关数据集存在标注不足及情感分类粗糙的问题，难以精准反映公众复杂情绪。

Method: 构建多语言细粒度情感分类数据集，利用预训练Transformer模型微调以实现准确分类，并分析不同语言、国家和主题的情绪变化。

Result: 成功构建了10万条多语言标注推文和亿级未标注推文，揭示了疫情期间公众情绪的动态演变，并验证了数据与ChatGPT的兼容性。

Conclusion: SenWave数据集为复杂事件的细粒度情感分析提供了重要资源，支持更深入的NLP研究及实际应用。

Abstract: The global impact of the COVID-19 pandemic has highlighted the need for a
comprehensive understanding of public sentiment and reactions. Despite the
availability of numerous public datasets on COVID-19, some reaching volumes of
up to 100 billion data points, challenges persist regarding the availability of
labeled data and the presence of coarse-grained or inappropriate sentiment
labels. In this paper, we introduce SenWave, a novel fine-grained
multi-language sentiment analysis dataset specifically designed for analyzing
COVID-19 tweets, featuring ten sentiment categories across five languages. The
dataset comprises 10,000 annotated tweets each in English and Arabic, along
with 30,000 translated tweets in Spanish, French, and Italian, derived from
English tweets. Additionally, it includes over 105 million unlabeled tweets
collected during various COVID-19 waves. To enable accurate fine-grained
sentiment classification, we fine-tuned pre-trained transformer-based language
models using the labeled tweets. Our study provides an in-depth analysis of the
evolving emotional landscape across languages, countries, and topics, revealing
significant insights over time. Furthermore, we assess the compatibility of our
dataset with ChatGPT, demonstrating its robustness and versatility in various
applications. Our dataset and accompanying code are publicly accessible on the
repository\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that
this work will foster further exploration into fine-grained sentiment analysis
for complex events within the NLP community, promoting more nuanced
understanding and research innovations.

</details>


### [80] [Investigating Counterclaims in Causality Extraction from Text](https://arxiv.org/abs/2510.08224)
*Tim Hagen,Niklas Deckers,Felix Wolter,Harrisen Scells,Martin Potthast*

Main category: cs.CL

TL;DR: 该论文提出了一个集成了反因果关系（concausality）的新数据集，以改进文本中的因果关系提取。


<details>
  <summary>Details</summary>
Motivation: 现有的因果关系提取研究和数据集几乎完全忽视了反因果关系，仅关注支持因果关系的陈述，导致模型误判反因果关系。

Method: 通过文献综述确定反因果关系的重要性，制定严格标注指南，将反因果关系融入因果新闻语料库，并验证了标注一致性（Cohen's κ=0.74）；随后训练模型区分支持和反因果关系。

Result: 引入反因果关系的数据集显著减少了模型将反因果误判为支持因果的错误，Transformer模型有效区分这两类因果关系。

Conclusion: 整合反因果关系的数据集对于提升因果关系提取的准确性至关重要，填补了现有研究的空白。

Abstract: Research on causality extraction from text has so far almost entirely
neglected counterclaims. Existing causality extraction datasets focus solely on
"procausal" claims, i.e., statements that support a relationship. "Concausal"
claims, i.e., statements that refute a relationship, are entirely ignored or
even accidentally annotated as procausal. We address this shortcoming by
developing a new dataset that integrates concausality. Based on an extensive
literature review, we first show that concausality is an integral part of
causal reasoning on incomplete knowledge. We operationalize this theory in the
form of a rigorous guideline for annotation and then augment the Causal News
Corpus with concausal statements, obtaining a substantial inter-annotator
agreement of Cohen's $\kappa=0.74$. To demonstrate the importance of
integrating concausal statements, we show that models trained without concausal
relationships tend to misclassify these as procausal instead. Based on our new
dataset, this mistake can be mitigated, enabling transformers to effectively
distinguish pro- and concausality.

</details>


### [81] [The Alignment Waltz: Jointly Training Agents to Collaborate for Safety](https://arxiv.org/abs/2510.08240)
*Jingyu Zhang,Haozhu Wang,Eric Michael Smith,Sid Wang,Amr Sharaf,Mahesh Pasupuleti,Benjamin Van Durme,Daniel Khashabi,Jason Weston,Hongyuan Zhan*

Main category: cs.CL

TL;DR: WaltzRL是一个通过多代理强化学习实现安全与帮助性平衡的新框架，能减少不安全和过度拒绝响应。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在安全对抗和过度拒绝安全敏感但合理请求之间的矛盾。

Method: 提出WaltzRL框架，联合训练会话代理和反馈代理，利用动态改进奖励机制提升响应质量，推理时改进而非拒绝不当回复。

Result: 在五个数据集上，WaltzRL显著减少不安全响应和过度拒绝，提升安全性同时保持帮助性和响应效率。

Conclusion: WaltzRL通过共进化和适应性反馈机制，有效提升了大型语言模型的安全性能，实现了安全性和帮助性之间的帕累托优化。

Abstract: Harnessing the power of LLMs requires a delicate dance between being helpful
and harmless. This creates a fundamental tension between two competing
challenges: vulnerability to adversarial attacks that elicit unsafe content,
and a tendency for overrefusal on benign but sensitive prompts. Current
approaches often navigate this dance with safeguard models that completely
reject any content that contains unsafe portions. This approach cuts the music
entirely-it may exacerbate overrefusals and fails to provide nuanced guidance
for queries it refuses. To teach models a more coordinated choreography, we
propose WaltzRL, a novel multi-agent reinforcement learning framework that
formulates safety alignment as a collaborative, positive-sum game. WaltzRL
jointly trains a conversation agent and a feedback agent, where the latter is
incentivized to provide useful suggestions that improve the safety and
helpfulness of the conversation agent's responses. At the core of WaltzRL is a
Dynamic Improvement Reward (DIR) that evolves over time based on how well the
conversation agent incorporates the feedback. At inference time, unsafe or
overrefusing responses from the conversation agent are improved rather than
discarded. The feedback agent is deployed together with the conversation agent
and only engages adaptively when needed, preserving helpfulness and low latency
on safe queries. Our experiments, conducted across five diverse datasets,
demonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,
from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on
OR-Bench) compared to various baselines. By enabling the conversation and
feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances
LLM safety without degrading general capabilities, thereby advancing the Pareto
front between helpfulness and harmlessness.

</details>


### [82] [Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling](https://arxiv.org/abs/2510.08245)
*Jannek Ulm,Kevin Du,Vésteinn Snæbjarnarson*

Main category: cs.CL

TL;DR: 本文探讨使用对比解码生成合成语料来扩展大语言模型的训练数据，结合真实数据提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 由于训练大语言模型所需的文本数据庞大且可能接近极限，寻找通过合成数据扩充训练语料的有效方法成为必要。

Method: 通过对比解码技术，从表现优劣不同的模型中生成合成语料，并将其与原始数据混合训练模型。

Result: 在语言建模和多种下游任务中，混合合成与真实数据训练提升了模型的整体性能。

Conclusion: 利用对比解码生成的合成数据能有效提升需要推理能力的任务表现，传统采样合成数据则对语言表层能力相关任务更有帮助。

Abstract: Large language models (LLMs) are trained on huge amounts of textual data, and
concerns have been raised that the limits of such data may soon be reached. A
potential solution is to train on synthetic data sampled from LLMs. In this
work, we build on this idea and investigate the benefits of contrastive
decoding for generating synthetic corpora. In a controlled setting, we
experiment with sampling corpora using the relative difference between a good
and bad model trained on the same original corpus of 100 million words. By
amplifying the signal from a model that has better performance, we create a
synthetic corpus and mix it with the original training data. Our findings show
that training on a mixture of synthesized and real data improves performance on
the language modeling objective and a range of downstream tasks. In particular,
we see that training with a mix of synthetic data from contrastive decoding
benefits tasks that require more reasoning skills, while synthetic data from
traditional sampling helps more on tasks dependent on surface level linguistic
capabilities.

</details>


### [83] [Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window](https://arxiv.org/abs/2510.08276)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Yaojie Lu,Xianpei Han,Le Sun,WenJuan Zhang,Pengbo Wang,Shixuan Liu,Zhenru Zhang,Jianhong Tu,Hongyu Lin,Junyang Lin*

Main category: cs.CL

TL;DR: DeepMiner提出了一种通过引入高难度训练任务和动态上下文窗口增强多轮推理能力的新框架，在多项基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在多轮长时交互中难以发挥深度推理能力。

Method: 提出了基于真实网络源生成复杂但可验证QA对的逆向构建法，以及动态上下文管理策略，利用滑动窗口机制避免依赖外部摘要模型。

Result: 在Qwen3-32B上通过强化学习训练得到DeepMiner-32B，在多个多轮搜索代理基准上性能大幅提升，尤其在BrowseComp-en上的准确率提升近20%。

Conclusion: DeepMiner通过动态上下文管理实现了近100轮的持续交互，有效解决了现有系统的上下文限制，增强了多轮推理模型的认知能力。

Abstract: While recent advances in reasoning models have demonstrated cognitive
behaviors through reinforcement learning, existing approaches struggle to
invoke deep reasoning capabilities in multi-turn agents with long-horizon
interactions. We propose DeepMiner, a novel framework that elicits such
abilities by introducing high-difficulty training tasks and dynamic context
window. DeepMiner presents a reverse construction method to generate complex
but verifiable question-answer pairs from authentic web sources, which ensures
the challenge and reliability of training data while injecting cognitive
capabilities into multi-turn reasoning scenarios. We further design an elegant
yet effective dynamic context management strategy for both training and
inference, utilizing sliding window mechanisms while eliminating the dependency
on external summarization models, thereby efficiently empowering the model to
handle continuously expanding long-horizon contexts. Through reinforcement
learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial
performance improvements across multiple search agent benchmarks. DeepMiner
attains 33.5% accuracy on BrowseComp-en, surpassing the previous best
open-source agent by almost 20 percentage points, and demonstrates consistent
improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our
dynamic context management enables sustained interactions of nearly 100 turns
within standard 32k context length, effectively addressing the context
limitations that constrain existing multi-turn interaction systems.

</details>


### [84] [Neuron-Level Analysis of Cultural Understanding in Large Language Models](https://arxiv.org/abs/2510.08284)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文通过神经元级别分析揭示大语言模型（LLMs）内部的文化理解机制，识别出少量关键神经元，分为文化通用神经元和文化特异神经元，它们分别支持对多种文化的认知和特定文化的理解，抑制这些神经元会显著降低模型在文化任务上的表现，但不影响一般自然语言理解能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在全球的广泛应用，确保其公平且全面的文化理解尤为重要。然而，现有模型存在文化偏见且对少数文化认知有限，且缺乏对其内部文化理解机制的深入研究。

Method: 提出一种基于梯度评分并结合过滤方法的神经元级分析技术，用以识别驱动文化行为的关键神经元，进一步验证这些神经元对文化理解任务的重要性，并探讨其在不同文化间的关联及对模型训练的影响。

Result: 发现少于1%的神经元集中在浅层到中层MLP中，分为文化通用和特异神经元。抑制这些神经元会使文化任务性能下降最多30%，但对一般自然语言理解无显著影响。文化特异神经元不仅支持目标文化，也支持相关文化的知识。此外，在包含文化通用神经元的模块上进行训练，可能削弱模型的文化理解能力。

Conclusion: 本文揭示了大语言模型中文化理解的神经元机制，区分出不同类型的关键神经元，强调了训练策略对模型文化能力的影响，为未来模型的公平文化理解提供理论与实践指导。

Abstract: As large language models (LLMs) are increasingly deployed worldwide, ensuring
their fair and comprehensive cultural understanding is important. However, LLMs
exhibit cultural bias and limited awareness of underrepresented cultures, while
the mechanisms underlying their cultural understanding remain underexplored. To
fill this gap, we conduct a neuron-level analysis to identify neurons that
drive cultural behavior, introducing a gradient-based scoring method with
additional filtering for precise refinement. We identify both culture-general
neurons contributing to cultural understanding regardless of cultures, and
culture-specific neurons tied to an individual culture. These neurons account
for less than 1% of all neurons and are concentrated in shallow to middle MLP
layers. We validate their role by showing that suppressing them substantially
degrades performance on cultural benchmarks (by up to 30%), while performance
on general natural language understanding (NLU) benchmarks remains largely
unaffected. Moreover, we show that culture-specific neurons support knowledge
of not only the target culture, but also related cultures. Finally, we
demonstrate that training on NLU benchmarks can diminish models' cultural
understanding when we update modules containing many culture-general neurons.
These findings provide insights into the internal mechanisms of LLMs and offer
practical guidance for model training and engineering. Our code is available at
https://github.com/ynklab/CULNIG

</details>


### [85] [AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming](https://arxiv.org/abs/2510.08329)
*Muxi Diao,Yutao Mou,Keqing He,Hanbo Song,Lulu Zhao,Shikun Zhang,Wei Ye,Kongming Liang,Zhanyu Ma*

Main category: cs.CL

TL;DR: AutoRed提出了一种无需预设指令的自由形式对抗提示生成框架，通过两阶段方法提升LLM红队测试的多样性和有效性，构建了两个大规模红队测试数据集，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有红队测试依赖种子指令，限制了对抗提示的语义多样性，难以全面评估大型语言模型的安全性。

Method: AutoRed分为(1)基于角色的人格引导对抗指令生成，(2)反思循环迭代优化低质量提示；引入验证器在不查询目标模型的情况下评估提示有害性以提升效率。

Result: AutoRed生成的对抗样本在攻击成功率和泛化能力上均优于现有基准，构建的AutoRed-Medium和AutoRed-Hard数据集对八个先进LLM进行了评估。

Conclusion: 移除种子指令限制的自由形式对抗生成框架能够更有效地发现LLM潜在风险，促进模型安全性评估的发展，未来将开源数据集推动研究社区进展。

Abstract: The safety of Large Language Models (LLMs) is crucial for the development of
trustworthy AI applications. Existing red teaming methods often rely on seed
instructions, which limits the semantic diversity of the synthesized
adversarial prompts. We propose AutoRed, a free-form adversarial prompt
generation framework that removes the need for seed instructions. AutoRed
operates in two stages: (1) persona-guided adversarial instruction generation,
and (2) a reflection loop to iteratively refine low-quality prompts. To improve
efficiency, we introduce a verifier to assess prompt harmfulness without
querying the target models. Using AutoRed, we build two red teaming datasets --
AutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs.
AutoRed achieves higher attack success rates and better generalization than
existing baselines. Our results highlight the limitations of seed-based
approaches and demonstrate the potential of free-form red teaming for LLM
safety evaluation. We will open source our datasets in the near future.

</details>


### [86] [Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media](https://arxiv.org/abs/2510.08365)
*Yukai Song,Pengfei Zhou,César Escobar-Viera,Candice Biernesser,Wei Huang,Jingtong Hu*

Main category: cs.CL

TL;DR: 提出了一种两阶段投票架构结合轻量级模型和大语言模型，提升自杀风险隐性线索检测效果，兼顾效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自杀率上升，需要利用社交媒体信号提前识别隐性自杀意图，但隐性表达难以被轻量级模型有效捕捉，大型模型计算代价高。

Method: 阶段一轻量级BERT快速识别显性自杀风险；阶段二对模糊样本采用（i）多视角大语言模型投票提高召回，（ii）基于心理指标和大语言模型提示工程提取特征的ML集成模型，兼具效率与可解释性。

Result: 在两个数据集上优于单一模型，显性样本F1达98.0%，隐性样本达99.7%，跨领域差异低于2%，且显著降低了大语言模型计算成本。

Conclusion: 该两阶段方法有效解决了隐性自杀意图识别难题，实现了准确率与效率的平衡，推动了心理特征向量化在风险检测的应用。

Abstract: Suicide rates have risen worldwide in recent years, underscoring the urgent
need for proactive prevention strategies. Social media provides valuable
signals, as many at-risk individuals - who often avoid formal help due to
stigma - choose instead to share their distress online. Yet detecting implicit
suicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle
emotional cues, remains highly challenging. Lightweight models like BERT handle
explicit signals but fail on subtle implicit ones, while large language models
(LLMs) capture nuance at prohibitive computational cost. To address this gap,
we propose a two-stage voting architecture that balances efficiency and
robustness. In Stage 1, a lightweight BERT classifier rapidly resolves
high-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to
either (i) a multi-perspective LLM voting framework to maximize recall on
implicit ideation, or (ii) a feature-based ML ensemble guided by
psychologically grounded indicators extracted via prompt-engineered LLMs for
efficiency and interpretability. To the best of our knowledge, this is among
the first works to operationalize LLM-extracted psychological features as
structured vectors for suicide risk detection. On two complementary datasets -
explicit-dominant Reddit and implicit-only DeepSuiMind - our framework
outperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7%
on implicit ones, and reducing the cross-domain gap below 2%, while
significantly lowering LLM cost.

</details>


### [87] [On the Relationship Between the Choice of Representation and In-Context Learning](https://arxiv.org/abs/2510.08372)
*Ioana Marinescu,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中上下文学习(ICL)中演示的表示形式和学习能力的独立作用，发现标签表示决定基础准确率，而更多演示提高性能，但两者基本独立。


<details>
  <summary>Details</summary>
Motivation: 以往研究关注演示标签的表示对ICL成功的贡献，但对ICL的学习能力（更多演示是否提高性能）存在争议，本工作探讨标签表示与学习能力的相互关系。

Method: 提出一种优化算法，生成不同语义相关度的标签集，分别用不同数量的上下文演示进行ICL，观察性能变化。

Result: 发现学习会发生且与标签集质量和模型参数量有关，学习效率受影响，但标签集质量对最终准确率影响稳定，两者相对独立。

Conclusion: ICL中演示的表示和从演示学习能力对性能有独立影响，揭示了ICL性能中演示表示和学习能力正交的关系，为深入理解ICL机理提供新视角。

Abstract: In-context learning (ICL) is the ability of a large language model (LLM) to
learn a new task from a few demonstrations presented as part of the context.
Past studies have attributed a large portion of the success of ICL to the way
these in-context demonstrations are represented, particularly to how labels are
represented in classification tasks. On the other hand, observations of the
learning capacity of ICL (i.e., the extent to which more in-context
demonstrations can lead to higher performance) have been mixed, and ICL is
often thought to occur only under specific conditions. The interaction between
these two aspects in ICL, representation and learning, has not been studied in
depth until now. We hypothesize that they are largely independent of one
another, such that the representation of demonstrations determines the baseline
accuracy of ICL, while learning from additional demonstrations improves only on
top of this baseline. We validate this hypothesis by developing an optimization
algorithm that can enumerate a spectrum of possible label sets
(representations) varying in semantic relevance. We then perform ICL with
varying numbers of in-context demonstrations for each of these label sets. We
observed that learning happens regardless of the quality of the label set
itself, although its efficiency, measured by the slope of improvement over
in-context demonstrations, is conditioned on both the label set quality and the
parameter count of the underlying language model. Despite the emergence of
learning, the relative quality (accuracy) of the choice of a label set
(representation) is largely maintained throughout learning, confirming our
hypothesis and implying their orthogonality. Our work reveals a previously
underexplored aspect of ICL: the independent effects of learning from
demonstrations and their representations on ICL performance.

</details>


### [88] [If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models](https://arxiv.org/abs/2510.08388)
*Jasmin Orth,Philipp Mondorf,Barbara Plank*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型（LLMs）对条件句“如果A，则B”可接受性的判断，重点分析条件概率与语义相关性对判断的影响，并比较模型与人类的差异。


<details>
  <summary>Details</summary>
Motivation: 条件可接受性影响人类对假设情境的理解和推理，而现有研究未明确大型语言模型如何评估此类条件句的可接受性。

Method: 采用线性混合效应模型和方差分析，对不同模型家族、规模及提示策略下的LLMs进行条件可接受性判断实验。

Result: 模型对条件概率与语义相关性均表现出敏感性，但不同架构和提示方式影响其程度，且模型表现不如人类一致，且大型模型不一定更贴近人类判断。

Conclusion: LLMs能够结合概率和语义线索判断条件句可接受性，但尚未达到人类水平，且模型大小不显著提升判断与人类的一致性。

Abstract: Conditional acceptability refers to how plausible a conditional statement is
perceived to be. It plays an important role in communication and reasoning, as
it influences how individuals interpret implications, assess arguments, and
make decisions based on hypothetical scenarios. When humans evaluate how
acceptable a conditional "If A, then B" is, their judgments are influenced by
two main factors: the $\textit{conditional probability}$ of $B$ given $A$, and
the $\textit{semantic relevance}$ of the antecedent $A$ given the consequent
$B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has
examined how large language models (LLMs) draw inferences about conditional
statements, it remains unclear how these models judge the
$\textit{acceptability}$ of such statements. To address this gap, we present a
comprehensive study of LLMs' conditional acceptability judgments across
different model families, sizes, and prompting strategies. Using linear
mixed-effects models and ANOVA tests, we find that models are sensitive to both
conditional probability and semantic relevance-though to varying degrees
depending on architecture and prompting style. A comparison with human data
reveals that while LLMs incorporate probabilistic and semantic cues, they do so
less consistently than humans. Notably, larger models do not necessarily align
more closely with human judgments.

</details>


### [89] [Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT](https://arxiv.org/abs/2510.08404)
*Noor Ul Zain,Mohsin Raza,Ahsan Adeel*

Main category: cs.CL

TL;DR: 本文提出了一个参数量仅为8M、单层双头的小型Co4语言模型，在10M训练样本上仅用两轮训练，性能超过了训练10轮的GPT-2和GPT-BERT基线，显示出极高的训练效率和样本使用效率，在多个关键任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前主流大语言模型通常具有较大规模和高计算复杂度（例如GPT-2采用12层且计算复杂度为O(N^2)），但训练效率和样本效率较低。本文旨在探索一种轻量级且高效的模型结构，实现低成本的高效预训练。

Method: 设计了一个轻量的小型机器Co4，具有单层、双头结构和约8M参数，计算复杂度约为O(N)。该模型在只用两轮训练下，使用BabyLM挑战赛的数据和评测流水线进行训练和测试。

Result: Co4仅用两轮训练便在10M训练样本上实现了对比模型GPT-2和GPT-BERT的显著超越，在多个SuperGLUE任务上的零样本和微调表现均优于两者，展示出多任务强泛化能力和样本高效性。

Conclusion: 结果表明，有必要重新审视当前深度学习的范式和规模扩展规律，轻量级高效模型在样本和计算效率方面展现巨大潜力。

Abstract: We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two
heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$
is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2
(124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two
epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude
greater training efficiency on 10M tokens, demonstrating highly sample
efficient pretraining. Using the BabyLM challenge evaluation pipeline across
complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning
performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out
of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out
of 7 metrics in both cases. These results suggest the need to rethink
prevailing deep learning paradigms and associated scaling laws.

</details>


### [90] [ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping](https://arxiv.org/abs/2510.08457)
*Shuang Chen,Yue Guo,Yimeng Ye,Shijue Huang,Wenbo Hu,Haoxi Li,Manyuan Zhang,Jiayu Chen,Song Guo,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文提出了ARES，一个自适应推理框架，根据任务难度动态分配推理资源，解决多模态大推理模型在简单任务中过度推理和在复杂任务中探索不足的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型在简单任务上推理过长，在复杂任务上探索不足，导致效率低和错失解决方案。

Method: ARES提出两阶段训练：Adaptive Cold-Start阶段通过难度匹配的推理轨迹让模型具备难度感知；AEPO阶段利用高窗口熵（HWE）作为探索触发器，以及层级熵奖励和动态KL控制决策探索强度，实现了自适应推理。

Result: 在数学、逻辑和多模态等多种基准测试中，ARES表现出优异的性能和推理效率，显著降低了推理成本并接近领先的商业系统。

Conclusion: ARES有效平衡了推理资源的分配，提升了多模态大推理模型在多样任务上的效率和准确率，具有广泛应用前景。

Abstract: Recent advances in multimodal large reasoning models (MLRMs) have
substantially improved their ability to solve complex textual and visual tasks.
However, these models tend to overthink on simple problems, producing
unnecessarily lengthy reasoning traces, while under-exploring on challenging
ones, leading to missed solutions. To address this imbalance, we propose ARES,
a unified open-source framework for adaptive reasoning that dynamically
allocates exploration effort based on task difficulty. Our approach is
motivated by two key empirical findings: (i) while single-token entropy is
noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a
sliding window) can reliably capture reasoning-critical moments; and (ii)
reducing HWE usage benefits easy problems, while increasing it is essential for
solving hard ones. Building on these insights, ARES introduces a two-stage
training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and
textual data paired with reasoning traces of length proportional to problem
difficulty, equipping the model with initial difficulty awareness. In the
second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which
uses HWE tokens as exploration triggers to decide when to explore, and a
hierarchical entropy reward with dynamic KL control to decide how much to
explore. Extensive experiments demonstrate that ARES achieves superior
performance and reasoning efficiency across diverse mathematical, logical, and
multimodal benchmarks, while closing the gap to leading commercial systems
under significantly lower inference costs.

</details>


### [91] [LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with Disagreements Shared Task](https://arxiv.org/abs/2510.08460)
*Elisa Leonardelli,Silvia Casola,Siyao Peng,Giulia Rizzi,Valerio Basile,Elisabetta Fersini,Diego Frassinelli,Hyewon Jang,Maja Pavlovic,Barbara Plank,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文介绍了LEWIDI第三届共享任务，旨在促进AI模型识别和处理人类判断中的分歧，涵盖四个数据集和两种评估范式，并引入新评估指标。


<details>
  <summary>Details</summary>
Motivation: 许多研究者认为AI模型应具备识别人类判断中的变异性和分歧的能力，并据此进行训练和评估，LEWIDI任务旨在推动这一方向的发展。

Method: 扩展LEWIDI基准，覆盖四个任务（释义识别、讽刺检测、挖苦检测和自然语言推理），采用分类和序数标注方案，设计软标签和视角主义两种评估范式，并测试了新的评估指标。

Result: 吸引了多样化的参与者，结果揭示了建模变异性方法的优缺点，并验证了不同评估方法的有效性。

Conclusion: 研究加强了LEWIDI作为一个框架的作用，提供了新的资源、基准和见解，支持开发具备识别分歧能力的AI技术。

Abstract: Many researchers have reached the conclusion that AI models should be trained
to be aware of the possibility of variation and disagreement in human
judgments, and evaluated as per their ability to recognize such variation. The
LEWIDI series of shared tasks on Learning With Disagreements was established to
promote this approach to training and evaluating AI models, by making suitable
datasets more accessible and by developing evaluation methods. The third
edition of the task builds on this goal by extending the LEWIDI benchmark to
four datasets spanning paraphrase identification, irony detection, sarcasm
detection, and natural language inference, with labeling schemes that include
not only categorical judgments as in previous editions, but ordinal judgments
as well. Another novelty is that we adopt two complementary paradigms to
evaluate disagreement-aware systems: the soft-label approach, in which models
predict population-level distributions of judgments, and the perspectivist
approach, in which models predict the interpretations of individual annotators.
Crucially, we moved beyond standard metrics such as cross-entropy, and tested
new evaluation metrics for the two paradigms. The task attracted diverse
participation, and the results provide insights into the strengths and
limitations of methods to modeling variation. Together, these contributions
strengthen LEWIDI as a framework and provide new resources, benchmarks, and
findings to support the development of disagreement-aware technologies.

</details>


### [92] [DeepPrune: Parallel Scaling without Inter-trace Redundancy](https://arxiv.org/abs/2510.08483)
*Shangqing Tu,Yaxuan Li,Yushi Bai,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出了DeepPrune框架，通过动态剪枝策略解决大语言模型并行推理中的计算冗余问题，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 并行生成多条推理链虽提升推理能力，但存在超过80%的推理路径产生相同答案，造成大量计算浪费。

Method: 设计一种基于焦点损失和过采样训练的判断模型准确预测部分推理链的答案是否相同，配合在线贪心聚类算法动态剪枝冗余路径，保持答案多样性。

Result: 在AIME 2024、AIME 2025和GPQA基准测试中，DeepPrune相比传统一致性采样令令牌使用量降低80%以上，且准确率保持在相差3个百分点以内。

Conclusion: DeepPrune显著提升了并行推理的计算效率，开创了高性能推理的新标准。

Abstract: Parallel scaling has emerged as a powerful paradigm to enhance reasoning
capabilities in large language models (LLMs) by generating multiple
Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces
significant computational inefficiency due to inter-trace redundancy -- our
analysis reveals that over 80% of parallel reasoning traces yield identical
final answers, representing substantial wasted computation. To address this
critical efficiency bottleneck, we propose DeepPrune, a novel framework that
enables efficient parallel scaling through dynamic pruning. Our method features
a specialized judge model trained with focal loss and oversampling techniques
to accurately predict answer equivalence from partial reasoning traces which
realizes 0.87 AUROC on equivalence prediction, combined with an online greedy
clustering algorithm that dynamically prunes redundant paths while preserving
answer diversity. Comprehensive evaluations across three challenging benchmarks
(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that
DeepPrune achieves remarkable token reduction by over 80% compared to
conventional consensus sampling on most cases, while maintaining competitive
accuracy within 3 percentage points. Our work establishes a new standard for
efficient parallel reasoning, making high-performance reasoning more efficient.
Our code and data are here: https://deepprune.github.io/

</details>


### [93] [Neologism Learning for Controllability and Self-Verbalization](https://arxiv.org/abs/2510.08506)
*John Hewitt,Oyvind Tafjord,Robert Geirhos,Been Kim*

Main category: cs.CL

TL;DR: 本文提出通过引入新词（新词嵌入）以更好地理解和控制大型语言模型（LLMs），无需改变模型参数，通过训练实例让模型掌握新概念。


<details>
  <summary>Details</summary>
Motivation: 类比人类创造新词解决新概念需求，探索在LLMs中引入新词以改善模型交互和控制方法。

Method: 通过添加新词嵌入并用相关示例训练实现新概念的引入，利用模型自阐释功能验证新词含义，并提出插件评估方法验证控制效果。

Result: 新词能有效控制模型在谄媚、不正确信息、文本长度等概念上的表现，发现机器独特同义词，并能联合学习多个新概念。

Conclusion: 新词学习方法不仅提升了对模型概念的控制，也促进了对模型内在理解，为更精细的模型交互开辟了新方向。

Abstract: Humans invent new words when there is a rising demand for a new useful
concept (e.g., doomscrolling). We explore and validate a similar idea in our
communication with LLMs: introducing new words to better understand and control
the models, expanding on the recently introduced neologism learning. This
method introduces a new word by adding a new word embedding and training with
examples that exhibit the concept with no other changes in model parameters. We
show that adding a new word allows for control of concepts such as flattery,
incorrect answers, text length, as well as more complex concepts in AxBench. We
discover that neologisms can also further our understanding of the model via
self-verbalization: models can describe what each new word means to them in
natural language, like explaining that a word that represents a concept of
incorrect answers means ``a lack of complete, coherent, or meaningful
answers...'' To validate self-verbalizations, we introduce plug-in evaluation:
we insert the verbalization into the context of a model and measure whether it
controls the target concept. In some self-verbalizations, we find machine-only
synonyms: words that seem unrelated to humans but cause similar behavior in
machines. Finally, we show how neologism learning can jointly learn multiple
concepts in multiple words.

</details>


### [94] [Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt Evaluator](https://arxiv.org/abs/2510.08524)
*Hyunji Lee,Kevin Chenhao Li,Matthias Grabmair,Shanshan Xu*

Main category: cs.CL

TL;DR: 本文提出一种结合蒙特卡洛树搜索与代理提示评估器的提示优化框架，用于提升法律文本中服务条款公平性检测的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法计算成本高，提示候选评分代价大，且搜索策略效率低，难以满足复杂法律NLP任务的需求。

Method: 结合蒙特卡洛树搜索（MCTS）和代理提示评估器，探索提示空间，同时减少评价代价，提高搜索效率。

Result: 该方法在计算资源有限的情况下，相较基准方法，在分类准确率和效率上均有显著提升。

Conclusion: 提出的框架有效地提升了提示优化效率和性能，适合用于复杂法律文本的公平性检测任务。

Abstract: Prompt optimization aims to systematically refine prompts to enhance a
language model's performance on specific tasks. Fairness detection in Terms of
Service (ToS) clauses is a challenging legal NLP task that demands carefully
crafted prompts to ensure reliable results. However, existing prompt
optimization methods are often computationally expensive due to inefficient
search strategies and costly prompt candidate scoring. In this paper, we
propose a framework that combines Monte Carlo Tree Search (MCTS) with a proxy
prompt evaluator to more effectively explore the prompt space while reducing
evaluation costs. Experiments demonstrate that our approach achieves higher
classification accuracy and efficiency than baseline methods under a
constrained computation budget.

</details>


### [95] [Which Heads Matter for Reasoning? RL-Guided KV Cache Compression](https://arxiv.org/abs/2510.08525)
*Wenjie Du,Li Jiang,Keda Tao,Xue Liu,Huan Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种针对大型语言模型推理时关键-值缓存溢出问题的压缩方法。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法在推理模型上效果不佳，导致推理性能严重下降。

Method: 通过强化学习识别对推理关键的注意力头，只对这些头分配完整缓存，其他头使用压缩缓存。

Result: 仅少部分注意力头对推理至关重要，压缩方法实现了20-50%的缓存减少且几乎无性能损失。

Conclusion: RLKV方法有效兼顾推理性能与缓存压缩，是推理大语言模型缓存优化的有效方案。

Abstract: Reasoning large language models exhibit complex reasoning behaviors through
the extended chain-of-thought generation, creating unprecedented Key-Value (KV)
cache overhead during the decoding phase. Existing KV cache compression methods
underperform on reasoning models: token-dropping methods break reasoning
integrity by discarding critical information, while head-reallocating methods
mistakenly compress reasoning-critical heads since they are designed for
retrieval tasks, resulting in significant performance degradation as
compression rates increase. We hypothesize that KV heads exhibit functional
heterogeneity in reasoning models-some heads are critical for chain-of-thought
consistency while others are compressible. To validate and exploit this
insight, we propose RLKV, a novel reasoning-critical head identification
framework, which uses reinforcement learning to directly optimize the
relationship between each head's cache usage and reasoning quality. As RLKV
produces rewards from actual generated samples during training, it naturally
identifies heads relevant to reasoning behaviors. We then allocate full KV
cache to these heads while applying compressed constant KV cache to others for
efficient inference. Our experiments reveal that only a small fraction of
attention heads is essential for reasoning, enabling our KV compression
approach to outperform baseline methods while achieving 20-50% cache reduction
with near lossless performance compared to uncompressed results.

</details>


### [96] [CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards](https://arxiv.org/abs/2510.08529)
*Xiangyuan Xue,Yifan Zhou,Guibin Zhang,Zaibin Zhang,Yijiang Li,Chen Zhang,Zhenfei Yin,Philip Torr,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型代理自我进化框架CoMAS，通过多代理之间的交互学习来自主提升能力，无需外部监督，实现了分散和可扩展的协同进化，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的自我进化方法依赖外部或从模型自身提取的奖励信号，缺乏人类智能中通过协作和讨论共同进步的机制。

Method: 提出CoMAS框架，通过代理间丰富的讨论动态生成内在奖励，利用大语言模型作为评判者形成奖励，并通过强化学习优化每个代理的策略，实现去中心化的协同进化。

Result: 实验证明CoMAS在多数评测中优于未训练代理，消融实验显示基于交互的奖励信号不可或缺，且代理数量和多样性增加时表现出良好扩展性。

Conclusion: CoMAS为基于大语言模型代理的自我进化提供了一种新颖有效的范式，强调通过多代理互动实现自主提升能力。

Abstract: Self-evolution is a central research topic in enabling large language model
(LLM)-based agents to continually improve their capabilities after pretraining.
Recent research has witnessed a transition from reinforcement learning
(RL)-free to RL-based methods. Current RL-based methods either rely on dense
external reward signals or extract intrinsic reward signals from LLMs
themselves. However, these approaches diverge from the self-evolution
mechanisms observed in human intelligence, where individuals learn and improve
through mutual discussion and collaboration. In this work, we introduce
Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents
to improve autonomously by learning from inter-agent interactions without
external supervision. CoMAS generates intrinsic rewards from rich discussion
dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and
optimizes each agent's policy through RL, thereby enabling decentralized and
scalable co-evolution. Experimental results demonstrate that CoMAS consistently
outperforms untrained agents and achieves state-of-the-art performance across
most evaluation settings. Ablation studies confirm the necessity of
interaction-based reward signals and reveal promising scalability as the number
and diversity of agents increase. These findings establish CoMAS as a novel and
effective paradigm for self-evolution in LLM-based agents.

</details>


### [97] [ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation](https://arxiv.org/abs/2510.08569)
*Qin Liu,Jacob Dineen,Yuxi Huang,Sheng Zhang,Hoifung Poon,Ben Zhou,Muhao Chen*

Main category: cs.CL

TL;DR: 该论文提出ArenaBencher框架，用于自动更新基准测试，防止数据泄露影响模型能力评估，保证测试的有效性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试因预训练数据泄露，导致模型通过记忆而非真正泛化能力获得高分，影响模型评估的准确性和公正性。

Method: ArenaBencher基于已有基准和多个模型反馈，自动推断测试核心能力，生成并验证新的题目，通过多模型反馈选择揭示共性弱点的题目，迭代更新以提高难度且保持测试目标一致。

Result: 应用于数学问题求解、常识推理与安全领域，生成了多样化、经验证且更具挑战性的测试题，发现新失败模式，提高了难度和模型区分度。

Conclusion: ArenaBencher为基准测试的持续进化提供了可扩展方案，能够与大规模模型快速发展同步更新测试内容，提升评测的科学性和公平性。

Abstract: Benchmarks are central to measuring the capabilities of large language models
and guiding model development, yet widespread data leakage from pretraining
corpora undermines their validity. Models can match memorized content rather
than demonstrate true generalization, which inflates scores, distorts
cross-model comparisons, and misrepresents progress. We introduce ArenaBencher,
a model-agnostic framework for automatic benchmark evolution that updates test
cases while preserving comparability. Given an existing benchmark and a diverse
pool of models to be evaluated, ArenaBencher infers the core ability of each
test case, generates candidate question-answer pairs that preserve the original
objective, verifies correctness and intent with an LLM as a judge, and
aggregates feedback from multiple models to select candidates that expose
shared weaknesses. The process runs iteratively with in-context demonstrations
that steer generation toward more challenging and diagnostic cases. We apply
ArenaBencher to math problem solving, commonsense reasoning, and safety domains
and show that it produces verified, diverse, and fair updates that uncover new
failure modes, increase difficulty while preserving test objective alignment,
and improve model separability. The framework provides a scalable path to
continuously evolve benchmarks in step with the rapid progress of foundation
models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [98] [Network Topology and Information Efficiency of Multi-Agent Systems: Study based on MARL](https://arxiv.org/abs/2510.07888)
*Xinren Zhang,Sixi Cheng,Zixin Zhong,Jiadong Yu*

Main category: cs.MA

TL;DR: 该论文研究了多智能体系统中通信拓扑结构和信息效率对多智能体强化学习性能的影响，提出了两个新指标提高通信效率并促进协调。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在面对非平稳性和部分可观测性时挑战重重，现有通信机制结构和效果尚未完全明确。

Method: 提出定向和顺序通信拓扑，设计信息熵效率指数（IEI）和专业化效率指数（SEI）两个新指标，并将其纳入训练目标实现优化。

Result: 实验表明定向和顺序拓扑提高了性能并减少通信开销，使用IEI和SEI指标显著提升了成功率和收敛速度。

Conclusion: 自适应通信拓扑设计结合信息高效消息传递是提升复杂多智能体系统协同效果的关键。

Abstract: Multi-agent systems (MAS) solve complex problems through coordinated
autonomous entities with individual decision-making capabilities. While
Multi-Agent Reinforcement Learning (MARL) enables these agents to learn
intelligent strategies, it faces challenges of non-stationarity and partial
observability. Communications among agents offer a solution, but questions
remain about its optimal structure and evaluation. This paper explores two
underexamined aspects: communication topology and information efficiency. We
demonstrate that directed and sequential topologies improve performance while
reducing communication overhead across both homogeneous and heterogeneous
tasks. Additionally, we introduce two metrics -- Information Entropy Efficiency
Index (IEI) and Specialization Efficiency Index (SEI) -- to evaluate message
compactness and role differentiation. Incorporating these metrics into training
objectives improves success rates and convergence speed. Our findings highlight
that designing adaptive communication topologies with information-efficient
messaging is essential for effective coordination in complex MAS.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [99] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: 本文通过调查和模型分析研究了生成式人工智能（GenAI）在软件开发中的应用如何影响开发者的倦怠，发现GenAI增加了工作需求从而加剧倦怠，但工作资源和积极认知可缓解此影响。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式人工智能提升了开发者生产力，但也可能带来新的压力，影响其心理健康，本文旨在深入探讨GenAI应用与开发者职业倦怠之间的关系。

Method: 采用工作需求-资源模型（JD-R）作为理论框架，结合定量（442名开发者问卷调查及PLS-SEM和回归分析）与定性（开放式问答）混合研究方法分析数据。

Result: 结果显示，生成式人工智能的采用通过增加工作需求，导致开发者倦怠感增强；而工作资源和对GenAI的积极看法能够缓解这类负面影响。

Conclusion: 生成式人工智能在软件开发中虽提高工作负担，但通过增加支持资源和积极引导，可以将其视为职业发展的机会，减轻开发者的倦怠感。

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [100] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: HotBugs.jar是首个专注于紧急热修复的真实世界数据集，收集和验证了679个热修复补丁，其中110个可复现，推动快速调试和自动修复工具的研究。


<details>
  <summary>Details</summary>
Motivation: 热修复是解决生产系统中紧急问题的重要变更，但目前无专门评估基准。

Method: 从10个活跃Apache项目中挖掘提交和问题报告，筛选出符合热修复标准的补丁，人工验证后包装成包含详细元数据和测试套件的可复现格式。

Result: 确认679个热修复补丁，110个可复现，整合进HotBugs.jar数据集并已作为SBSE竞赛官方数据集。

Conclusion: HotBugs.jar为热修复工具的研究和评估提供了首个专用基准，促进快速调试和自动修复技术的发展。

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the
first dataset dedicated to real-world hot fixes. From an initial mining of 10
active Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs$.$jar framework,
HotBugs$.$jar integrates these 110 reproducible cases and makes available all
679 manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [101] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure利用大型语言模型自动将C代码库转译为Rust，并通过差异符号测试确保代码语义一致性。


<details>
  <summary>Details</summary>
Motivation: 提升现有使用不安全内存语言（如C）的代码库的软件安全性，通过转译成内存安全的Rust语言实现。

Method: 采用大型语言模型结合提示工程生成惯用且安全的Rust代码，并利用差异符号测试验证原始C代码与转译Rust代码的语义相似性。

Result: 在五个真实应用中，RustAssure成功生成89.8%的可编译Rust函数，其中69.9%函数的符号返回值等价。

Conclusion: RustAssure有效利用LLM实现现有C代码库向Rust的自动转译，显著提升代码安全性和可靠性。

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [102] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: 该论文提出了APPFORGE基准测试，用于评估大语言模型在从零开始开发整个Android应用程序的能力，结果显示当前顶尖模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型虽能生成单一函数代码，但无法胜任复杂的软件系统开发，缺乏相应的评测基准。

Method: 构建APPFORGE基准，包含101个真实安卓应用开发任务，结合多智能体系统自动提取功能、合成测试用例，并由专家验证，建立自动化评测框架。

Result: 在12个主流大语言模型上评测，表现均不理想，最佳模型GPT-5仅能正确实现18.8%的应用功能，暴露模型处理多组件系统的局限性。

Conclusion: 当前大语言模型尚未具备构建复杂软件系统的能力，APPFORGE为未来相关研究提供了标准化、自动化的评测工具。

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [103] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: 本文提出了FLEX，一种基于神经网络的自适应模糊测试框架，用于提升MLIR的测试质量和漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对MLIR的模糊测试方法难以生成多样且语义有效的测试用例，难以发现深层次缺陷，推动提升MLIR的正确性与健壮性具有重要意义。

Method: FLEX结合神经网络程序生成、扰动采样策略和反馈驱动的增强循环，利用崩溃与非崩溃测试用例不断提升模型，自主生产高质量测试输入。

Result: FLEX在30天测试中发现80个未知缺陷，在24小时内检测出53个缺陷，代码覆盖率达28.2%，显著优于其它4个基线工具。消融实验验证了扰动生成和多样性增强的重要性。

Conclusion: FLEX有效提升了MLIR的模糊测试能力，可自动生成多样且符合语义的高质量测试用例，大幅增加缺陷发现和代码覆盖，证明了神经网络和反馈增强机制的优势。

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [104] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: 本文提出了IssueMut，一种从编译器错误历史中提取变异操作器以提升变异模糊测试效果的方法。


<details>
  <summary>Details</summary>
Motivation: 当前变异模糊测试依赖变异操作器的质量，但之前没有利用编译器错误历史作为变异操作器的来源。

Method: IssueMut自动挖掘编译器错误报告中的信息，提取出变异操作器，并将其整合到现有的变异模糊测试工具中。

Result: 通过从1760个GCC和LLVM错误报告中挖掘587个变异操作器，并测试发现新的编译器错误，验证了利用错误历史信息提高测试效果的有效性。

Conclusion: 利用编译器的错误历史提取变异操作器能够提升编译器模糊测试的漏洞发现能力，证明错误历史是有价值的测试信息来源。

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [105] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: 本文分析了180个汽车SoC软件漏洞，基于与AUTOSAR框架一致的软件架构模型，揭示了16个根本原因和关键模块，为提升汽车复杂系统的安全防护提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着汽车SoC集成度和复杂性的提升，安全漏洞频发，但缺乏针对AUTOSAR架构中SoC漏洞根因和影响的系统性分析。

Method: 收集并分析180个公开披露的汽车SoC漏洞，将其映射到符合AUTOSAR原则的软件架构模型，识别漏洞根因、影响的软件模块及补丁延迟情况。

Result: 发现了16个主要漏洞根因，影响56个软件模块，揭示了突出漏洞模式及关键模块补丁延迟，揭示了不同CWE类别和架构层次的缓解延迟情况。

Conclusion: 本研究为汽车SoC软件架构的安全保障提出改进建议，包含漏洞检测、优先级排序和定位策略，有助于提升实时、安全关键环境下汽车平台的安全防护水平。

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [106] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: 本文提出了一个融合AI大语言模型的智能缺陷跟踪框架，通过自动化报告精炼、缺陷复现及解决方案生成，显著缩短修复时间和降低人力成本。


<details>
  <summary>Details</summary>
Motivation: 传统缺陷跟踪依赖人工报告和多方协作，存在沟通障碍、响应滞后和效率低下的问题，亟需提升自动化和智能化水平。

Method: 基于大语言模型，用户以自然语言报告缺陷，AI自动优化缺陷报告、尝试复现、分类并分类处理无效和有效缺陷，自动生成候选补丁并由人工审核。

Result: 框架实现了从缺陷报告到修复的自动化加速，改善了团队协作和软件维护效率，缩短了缺陷修复响应时间。

Conclusion: 引入AI自动化增强的缺陷跟踪框架能够提升缺陷管理效率，促进更高效、以用户为中心的软件维护流程。

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


### [107] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: 该论文提出了一种预处理方法，实现了模块化、对空白不敏感的语言组件与对空白敏感语言的无缝集成。


<details>
  <summary>Details</summary>
Motivation: 当前模块化语言组件复用中，空白敏感语言与空白不敏感语言的整合存在缺口，导致复用困难。

Method: 通过对语言工件进行预处理，使得模块化空白不敏感语言组件能够构建出空白敏感语言。

Result: 通过重构简化版Python语言验证了该方法的有效性。

Conclusion: 该方法提升了语言组件的复用性，缩短了开发周期，提升了软件语言的总体质量。

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>
