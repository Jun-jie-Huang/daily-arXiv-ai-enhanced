<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 105]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: 本文提出了OpenStaxQA，一种基于43本公开许可的大学教材构建的多语言大学水平教育应用评测基准，使用7亿参数的量化低秩适配器模型进行微调和评估，并在AI2推理挑战集上进行零样本评测，探讨其对其他任务的提升效果及数据集的广泛影响。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对大学水平教育应用的多语言大型语言模型评测基准，且希望通过新的数据集提升模型在多任务上的表现。

Method: 基于43本英文、西班牙文和波兰文的公开许可大学教材构建OpenStaxQA数据集，采用7亿参数的量化低秩适配器（QLoRa）对大型语言模型进行微调，并在OpenStaxQA和AI2推理挑战开发集上进行评测。

Result: 模型在OpenStaxQA数据集上表现良好，且零样本评测结果显示OpenStaxQA训练可提升模型在其他推理任务上的表现。

Conclusion: OpenStaxQA作为一个开放许可、多语言的大学水平教育评测基准，可有效促进大型语言模型在教育领域的应用和多任务能力提升，同时数据集构建方案及其影响值得深入探讨。

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [2] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: 提出FURINA-Builder，一种自动构建角色扮演（RP）基准的新型多智能体协作管线，支持大规模、多场景、多格式自定义评测，打造了FURINA-Bench综合RP基准，用以评估大型语言模型的RP能力与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有RP基准范围狭窄，交互方式过时，适应多样化场景能力有限，难以满足大型语言模型快速发展的需求。

Method: 设计多智能体协作管线FURINA-Builder，模拟测试角色与多角色场景对话，结合LLM评判者选择细粒度评价维度并调整回答，自动生成多样化RP评测基准FURINA-Bench。

Result: 通过人类评估和分辨性分析验证管线和基准有效性；评测多个先进LLM发现不同模型和角色表现差异，规模与幻觉无单调关系，推理能力提升表现但加剧幻觉，表现与可靠性存在权衡。

Conclusion: FURINA-Builder有效推动多场景RP评测建设，FURINA-Bench为RP任务带来新的挑战，揭示性能与可靠性间的内在权衡，对LLM RP能力评估具有重要意义。

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [3] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 提出了基于知识图谱引导的多智能体系统蒸馏方法，提高工业问答系统的准确率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业问答系统需要比通用对话模型更高的安全性和可靠性，现有多智能体大语言模型存在迭代失控和输出不可验证的问题，传统蒸馏方法难以将协作推理能力转移给轻量化模型。

Method: 将蒸馏过程视为马尔可夫决策过程，结合知识图谱作为可验证的结构化先验，丰富状态表示确保收敛性，通过协作推理与知识落地生成高置信度的指令微调数据，同时联合蒸馏推理深度和可验证性至紧凑的学生模型。

Result: 在工业问答数据集上，所提方法相比基线提升了2.4%至20.1%的准确率，显著增强了模型的可靠性。

Conclusion: KG-MASD方法有效提升了工业问答系统的准确性和可靠性，促进安全关键工业场景中可信赖的AI部署。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [4] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 本文提出了一种针对人类开放式问卷调查回答的两阶段质量评估框架，能够有效过滤无意义回答，并基于努力度、相关性和完整性三维度利用大模型进行评价，结果优于现有自动评价方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动评价方法主要针对大模型生成文本，难以有效评估具有独特特征的人类调查回答，且低质量回答会增加人工筛选负担并影响结论准确性。

Method: 设计两阶段框架：第一阶段为无意义文本过滤，第二阶段基于实际调研数据，运用大模型分别评估回答的努力度、相关性和完整性三个维度。

Result: 在英韩数据集上的验证显示该框架优于现有指标，且与专家评估高度相关，具有较强的实际应用价值，如质量预测和回答剔除。

Conclusion: 所提框架有效提升了人类开放式问卷回答的自动质量评估水平，适合实际调研中的自动筛选和质量保证需求。

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [5] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: 本论文提出了一种名为CoT Referring的多模态大语言模型训练策略，通过链式思考结构提升跨模态推理能力，改善复杂指代表达的理解和分割任务效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在复杂指代表达理解和分割任务中表现不足，需要更好地整合语言理解与图像理解能力，提升模型对复杂查询的推理准确性。

Method: 提出CoT Referring策略，将文本结构解析为序列化的指称步骤，确保关系识别和指称一致性；重构训练数据，设计新输出格式并提供新注释；构建专门针对复杂指称情况的评测基准；整合检测与分割功能于统一框架，并采用自适应加权损失函数优化训练。

Result: 在新构建的复杂指称测试基准及公开数据集RefCOCO/+/g上，方法显著提升性能，准确度较基线模型提高2.5%以上。

Conclusion: CoT Referring有效增强了多模态大语言模型的跨模态推理能力和指称理解性能，特别在复杂查询场景中表现优异，具有广泛应用前景。

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [6] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 本研究旨在科学领域寻找最佳词表示和分词方法，并构建一个评估套件对其进行测试。


<details>
  <summary>Details</summary>
Motivation: 同一词在不同领域和语境中的意义不同，需要针对科学领域的数据找到最优的词表示算法。

Method: 构建包含多个下游任务和数据集的评估套件，用以测试不同的词表示和分词算法。

Result: 通过评估套件对多种词表示和分词算法进行测试，验证其在科学领域下游任务中的表现。

Conclusion: 提出了适用于科学领域的词表示和分词优化方案，并提供了可持续更新和评测的评估平台。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [7] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 本文提出了一种基于跨语言相似性对解码器单向多语言大语言模型中间层进行约束的方法TRepLiNa，显著提升了印地语和英语作为枢轴语的低资源语言（Mundari、Santali、Bhili）到高资源语言的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 印度多样的低资源语言缺乏充足的语言资源，导致翻译质量不高，亟需有效方法提升低资源语言到高资源语言的翻译效果。

Method: 结合Centered Kernel Alignment（CKA）度量语言表示的相似性与REPINA正则化方法限制参数更新，形成联合方法TRepLiNa，在解码器的中间层强制跨语言相似性，同时使用Aya-23 8B模型及QLoRA技术，在零样本、少样本和微调环境下进行实验。

Result: 在MMLoSo共享任务中多组语言对（Mundari、Santali、Bhili对印地语/英语）测试，TRepLiNa成功提升了中层对齐，显著改善了翻译效果，尤其在数据稀缺条件下效果更明显。

Conclusion: 通过在解码器中层强制跨语言表示对齐，结合参数更新约束，TRepLiNa是一种成本低且实用的方法，有效提升了多低资源语言到高资源语言的翻译性能。

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [8] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: 该论文提出了一种可扩展的多语种个人身份信息（PII）数据标注框架，覆盖13个地区和约336种PII类型，通过分阶段、人机协同的标注方法显著提升标注质量，适用于大规模语言模型的微调。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在各地被广泛采用，确保它们在不同法规环境下准确处理个人身份信息变得非常重要。现有多语种PII标注资源匮乏，难以支持模型可靠性。

Method: 设计了一个分阶段、结合语言专家和严格质量控制的人机循环标注流程，通过标注者一致性度量和根因分析，系统地发现和解决标注不一致问题。

Result: 标注流程显著提高了召回率和降低了误报率，产出高保真度的数据集，支持监督微调，并通过分析标注者常见困难，提升了整体标注质量和模型稳定性。

Conclusion: 利用迭代、数据驱动的标注管线，可有效提升多语种PII标注的质量及下游语言模型的可靠性，为法规合规性提供坚实数据支持。

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [9] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 本数据集提供了基于经典阿育吠陀原则设计的英语-印地语双语体质评估问卷的响应数据，涵盖身体、食欲、睡眠、能量和气质等方面。


<details>
  <summary>Details</summary>
Motivation: 为了系统、准确地收集个体的体质数据，支持计算智能和个性化健康分析研究，并避免参与者偏见。

Method: 设计24项多项选择题问卷，题目中立且强制回答，隐去体质标签，通过谷歌表单采集数据并自动评分映射到三种体质类型得分。

Result: 生成结构化的数据集，适用于体质特征分布、相关性分析及预测建模等研究。

Conclusion: 该数据集为未来基于体质的研究和智能健康应用开发提供了重要参考和支持。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [10] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: 本文提出了Vibe Checker测试平台，通过引入30条可验证的代码指令及其验证器，结合功能正确性，评估大语言模型在代码生成中的指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码评估指标仅关注功能正确性，忽视了用户更注重的代码“感觉正确”、可读性和意图保留等非功能性要求。

Method: 构建了VeriCode指令分类体系及对应的确定性验证器，结合传统评测方法形成Vibe Checker，用于综合评估模型的指令遵循和功能正确性。

Result: 对31个主流大语言模型的评测表明，强模型仍难同时满足多条指令要求，且功能正确性水平出现倒退。综合考虑功能和指令遵循的评分与人类偏好相关性最高。

Conclusion: 代码生成中的人类偏好不仅依赖功能正确性，更关键在于指令遵循能力，本文方法为后续模型的优化与评测提供了方向。

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [11] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 提出了一种运行于嵌入式设备的两阶段电子健康记录( EHR )离线摘要系统，实现隐私保护和高效临床信息提取。


<details>
  <summary>Details</summary>
Motivation: EHR中包含大量无结构临床数据，给急诊医生查找关键信息带来困难，同时隐私保护要求系统能够离线运行。

Method: 设计了双设备架构，Jetson Nano-R负责本地检索相关病历片段，Jetson Nano-S运行小语言模型生成结构化摘要，摘要包含固定格式的关键信息列表和上下文相关的叙述，系统通过轻量级套接字通信。

Result: 基于MIMIC-IV和真实匿名EHR数据的初步实验表明，该系统在30秒内能有效生成准确、完整且清晰的临床摘要。

Conclusion: 该离线双设备架构结合小型语言模型，实现了隐私保护前提下的高效临床文本摘要，有助于急诊医生快速获取关键信息。

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [12] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）中的幻觉现象，分析其成因、检测方法及缓解策略。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然表现出色，但常生成事实不准确的信息（幻觉），影响其可靠性和可信度。

Method: 梳理了幻觉类型及其成因，分析了幻觉在生成任务中的表现，提出了检测和缓解方法的分类体系，并评估了现有方法的优缺点。

Result: 详尽总结了幻觉的检测和缓解技术及评估指标，指出现有方法的局限性。

Conclusion: 呼吁未来研究关注幻觉问题，推动开发更真实可信的LLMs。

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [13] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: 本文利用深度学习方法分析了过去七十年美国Billboard音乐排行榜上的歌词内容，发现自1990年以来色情与辱骂性内容显著增加。


<details>
  <summary>Details</summary>
Motivation: 音乐中辱骂和色情内容增加引起儿童和青少年行为变化，缺乏系统研究验证这一趋势，影响政策制定。

Method: 使用深度学习和语言模型对Billboard歌词进行情感分析和辱骂检测，进行纵向趋势分析。

Result: 从1990年起，歌词中含有辱骂、色情及不适当语言的歌曲显著增多。语言模型有效捕捉了歌词内容的细微变化和社会规范的转变。

Conclusion: 音乐内容中辱骂和色情语言的增长趋势显著，深度学习模型是剖析这类内容演变及其反映社会文化变迁的有效工具，对相关政策制定具有参考价值。

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [14] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: 本文复现了Ma等人提出的XRec模型，使用Llama 3替代GPT-3.5-turbo进行评测，并修改了Mixture of Experts模块的嵌入方式。


<details>
  <summary>Details</summary>
Motivation: 验证和扩展XRec模型在推荐系统中的效果，尤其是其解释能力和多专家嵌入的作用。

Method: 基于Ma等人提供的代码，使用Llama 3模型进行再现，改动XRec的输入或输出嵌入，进行评估。

Result: XRec能够生成个性化解释，融入协同信息提升稳定性，但未在所有指标上一致优于基线模型。修改嵌入方式影响了解释结构，突出协同信号与语言模型的交互作用。

Conclusion: 通过复现和扩展，验证了XRec的解释能力和模型稳定性，公开了评测实现，促进研究和应用的可访问性。

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [15] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: 研究多语言变压器模型对问题的形态句法属性表示，提出跨七种语言的QTC数据集，评估模型捕捉问题类型和复杂性的能力。


<details>
  <summary>Details</summary>
Motivation: 探究多语言变压器模型如何表示问题的形态句法特征，提升对问题复杂性和类型的理解。

Method: 构建包含类型和复杂度指标（依存长度、树深度、词汇密度）注释的QTC数据集；采用扩展的探针方法进行回归标签的评估，并与统计基线和微调模型对比分析。

Result: 统计特征在显式标记语言中有效分类问题，神经探针更好地捕捉细粒度结构复杂性。

Conclusion: 上下文表示优于统计基线的时机依赖问题类型，参数更新可能减少预训练语言信息的可用性。

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [16] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: 本文提出了一种加权自适应损失微调方法，以实现大型语言模型（LLM）输出的性别-职业分布与期望分布（无论是平等分布还是现实世界分布）的一致性，从而减少偏见。


<details>
  <summary>Details</summary>
Motivation: 现有偏见缓解工作大多集中在社会平等和人口统计平衡，较少关注让LLM输出符合期望分布以支持事实基础。

Method: 提出基于加权自适应损失的微调方法，使用2024年美国劳动力统计中的三组职业（男性主导、女性主导和性别平衡）对模型进行微调，同时保持语言模型能力。

Result: 在三种掩码语言模型中，均观察到不同分布下的偏见。该方法在平等分布下实现近乎完全消除偏见，在现实世界分布下减少30-75%的偏见。自回归LLM在平等分布下无明显偏见但现实分布下有显著偏见，Llama Instruct模型减少偏见50-62%。

Conclusion: 通过加权自适应损失微调，本文方法有效减少了LLM输出的性别职业偏见，支持根据不同需求调整输出分布，兼顾事实基础和社会公平。

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [17] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: 本文提出EVALUESTEER基准，用于评估大语言模型（LLMs）和奖励模型（RMs）根据用户多样化价值观和风格偏好进行引导的能力。通过生成165,888对偏好对，涵盖四个价值维度和四个风格维度，测试模型在不同情景下的表现。结果显示，当提供完整用户配置文件时，最佳模型准确率不到75%，凸显当前模型识别及适应用户偏好信息的局限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的全球部署，构建能够适应全球用户多样偏好和价值观的系统变得必要，现有数据集缺乏对奖励模型引导能力的受控评估。

Method: 设计EVALUESTEER基准，合成生成165,888对偏好对，覆盖四个价值维度（传统、世俗理性、生存、自我表达）和四个风格维度（冗长性、可读性、自信、温暖），并在多种提示条件和偏好比较情景下测试六种开放和专有模型。

Result: 在包含用户完整价值和风格偏好的情况下，最佳模型准确率低于75%，而仅提供相关偏好时准确率超过99%，表明当前奖励模型难以准确识别和适应用户完整偏好。

Conclusion: EVALUESTEER揭示了当前奖励模型在捕捉用户多样价值和风格偏好方面的不足，提供了一个挑战性的测试平台，推动更好地引导模型适应不同的人类价值观与偏好。

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [18] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: 本论文提出了EverydayMMQA框架及OASIS数据集，旨在提升多模态模型处理具有文化背景的生活化知识问答的能力，尤其关注低资源语言和多样文化场景。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多模态模型在视觉问答任务中表现优异，但难以应对需要文化背景和日常常识的查询，特别是在低资源和少数语言环境中。

Method: 设计EverydayMMQA框架，创建OASIS数据集，集成语音、图像和文本，包含约920万图像及1480万问答对，支持多输入形式（仅语音，仅文本，语音+图像，文本+图像），涵盖18个国家的英语及阿拉伯语变体，内容体现现实世界多样文化情境。

Result: 该数据集测试模型在超越简单物体识别的语用、常识及文化感知推理任务中的表现，评测了多种闭源与开源模型及微调模型，验证了框架与数据集的有效性。

Conclusion: EverydayMMQA与OASIS为打造文化相关的多模态大语言模型提供了标杆及训练资源，框架与数据集将公开，助力社区推动该领域发展。

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [19] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: 本文提出了语义正则表达式（semantic regexes）作为自动化可解释性的一种方法，将大型语言模型特征转化为结构化且精确的描述。


<details>
  <summary>Details</summary>
Motivation: 传统的自然语言特征描述往往模糊、不一致且需要人工重新标注，影响可解释性的准确性和效率。

Method: 本文设计了语义正则表达式，通过结合捕捉语言和语义特征模式的基本元素与上下文化、组合和量化的修饰符，实现了精确且富有表现力的特征描述。

Result: 在定量基准测试和定性分析中，语义正则表达式与自然语言描述的准确性相当，但能产生更简洁且一致的描述；其结构特性支持量化特征复杂度和从单一特征扩展到模型全局的分析。

Conclusion: 语义正则表达式提升了特征描述的精确性与一致性，支持更深入的自动化可解释性分析，且用户研究表明此方法有助于用户构建更准确的模型特征理解。

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [20] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: 本文提出了一种防止基于搜索的文本链接攻击的方法，既保护文本身份隐私又保持语义完整。


<details>
  <summary>Details</summary>
Motivation: 去标识化模型无法防止文本被关联回原始数据的风险，即链接攻击。

Method: 通过构建文档集中N-gram的倒排索引，筛选出出现频次不足k的短语，然后利用大语言模型迭代重写这些短语，直到无法实现链接。

Result: 在法院案例数据集上的实验表明，该方法有效防止了搜索式链接，同时保持了文本内容的真实性。

Conclusion: 结合倒排索引和LLM重写策略可以有效抵御基于搜索的链接攻击，保障文本隐私且不损失语义。

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [21] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: 本文提出RegDiff，一种正则化扩散模型，通过训练时注入属性信息，实现高效的可控文本生成，减少计算成本，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的可控文本生成方法中，分类器自由引导（CFG）保留语义但控制属性效果差，分类器引导（CG）控制效果好但计算成本高且依赖预训练分类器。

Method: 提出RegDiff，采用基于VAE的编码器-解码器结构和带属性监督的潜变量扩散模型，在训练时注入属性信息，生成阶段无需预训练分类器，从而降低计算成本。

Result: 在五个数据集上测试，涵盖多种风格属性，RegDiff在生成风格文本方面优于强基线方法。

Conclusion: RegDiff有效结合了属性控制与生成质量，提供了一种高效且效果优异的可控文本扩散生成方案。

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [22] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: 本文分析了语言模型中的奖励模型（RM），揭示了其在对不同社会群体偏见和观点对齐方面的不足，强调仅靠提示引导无法解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 奖励模型用以模拟人类偏好指导语言模型行为，但其内部偏见及观点对齐情况尚不清楚，需深入理解以避免传播社会偏见。

Method: 作者提出了一个框架来衡量奖励模型观点对齐度，研究了RM在不同社会人口学群体中的偏见，以及通过提示引导来调整奖励偏好的效果。

Result: 发现奖励模型对多个群体的观点对齐较差，存在系统性的有害刻板印象奖励，提示引导不足以克服这些缺陷。

Conclusion: 在偏好学习的模型对齐过程中，应更加谨慎审视奖励模型行为，防止社会偏见在语言技术中传播。

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [23] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型生成符合教学目标的虚拟实验问题的框架，通过教师与模型的对话实现教学目标理解，结合问题分类和提示控制，显著提升了问题质量和格式符合度。


<details>
  <summary>Details</summary>
Motivation: 虚拟实验虽提供了宝贵的实践学习机会，但教师难以将其问题与教学目标对齐，现有第三方材料不匹配且自制资源难以规模化。

Method: 引入一个包含教师-模型对话教学目标理解、实验知识单元分析、问题认知与教学意图分类体系及提示细节控制的多组件框架，利用大型语言模型生成问题。

Result: 通过对19个开源大型语言模型生成的1100余个问题的评估，发现目标和实验理解提升了问题质量，问题分类提升认知需求，优化提示提升问题格式符合度。大模型表现最佳，格式可解析性提升37.1%，符合度提升25.7%，质量提升0.8分。

Conclusion: 该框架有效支持教师通过自然语言交互利用大型语言模型生成符合教学目标且质量高的问题，展示了大型语言模型在虚拟实验教学中的应用潜力。

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [24] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: FinLFQA基准评估大语言模型在金融领域生成长篇复杂问题答案时的归因能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要侧重于简单的文本证据引用，无法满足金融等实际应用中复杂归因需求。

Method: 引入FinLFQA，针对金融报告证据、中间数值推理步骤和领域知识进行人类注释，构建自动化评估框架。

Result: 通过对八个大语言模型的实验，发现细粒度指标有效区分模型能力，端到端生成与后处理方法性能可比，迭代优化需结合外部反馈。

Conclusion: 细粒度归因评估对实际应用至关重要，未来需继续探索结合外部反馈的模型优化方式。

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [25] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: UniRST是首个能处理11种语言中18个树库的统一RST风格语篇分析器，采用两种策略解决标签不兼容问题，表现优于大多数单一树库模型。


<details>
  <summary>Details</summary>
Motivation: 现有语篇分析器通常针对单一语言或树库，导致跨语言和跨资源的语义理解受限，需构建一个多语言、多树库统一模型以提升泛化能力和资源利用效率。

Method: 提出两种训练策略：Multi-Head为每个树库设立独立的关系分类层，Masked-Union通过标签掩码实现参数共享；先进行单一树库增强训练，再训练统一模型。

Result: Masked-Union方法参数高效且性能最佳，UniRST模型在18个单语言树库中击败了16个基线模型，证明统一模型的优势。

Conclusion: 通过统一多语言树库的训练，UniRST实现了高效且准确的语篇结构解析，显示出多资源跨语言语篇分析器的可行性和优越性。

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [26] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: 本文介绍了MathRobust-LV，这是一个用于评估大语言模型在高中数学问题中对语言变化的鲁棒性的测试集和方法，发现多数模型在语言变体下表现下降。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学推理中表现出色，但它们对语言表述变化的鲁棒性在真实教育环境中的高效评估尚未充分研究。

Method: 构建MathRobust-LV测试集，通过保持数值结构和答案不变，仅更改问题的表面细节（如名字、情境和变量），模拟教师在评估中对题目的重述方式，以考察模型对语言变体的适应能力。

Result: 在34个模型的实验中，模型在语言变体问题上的准确率普遍下降，较小模型下降达9-11%，较强模型也有下降，只有GPT-5和Gemini-2.5pro表现较为稳定。

Conclusion: 语言表述的变异性对大语言模型的数学推理能力构成挑战，语言鲁棒性是模型推理能力的一个根本薄弱环节，需要引起重视。

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [27] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 本文系统综述了自主大型语言模型代理在网络安全领域的应用、威胁及防御，构建了相关的分类体系，并指出了当前研究的空白点。


<details>
  <summary>Details</summary>
Motivation: 随着从被动大型语言模型向自主代理的转变，新的安全风险随之出现，迫切需要全面理解这一新兴领域的安全态势。

Method: 通过对150多篇相关文献进行分类和分析，构建了围绕应用、威胁和防御三大支柱的框架，深入探讨了代理的使用方式、潜在漏洞及防护措施。

Result: 提出了详细的分类体系和跨领域分析，揭示了代理架构中的新趋势，同时发现了模型和模态覆盖方面的关键研究空白。

Conclusion: 该综述为自主LLM代理的安全研究提供了系统性指导，促进未来安全防护策略的设计和实施。

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [28] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: 本论文针对低资源语言，尤其是濒危的澳大利亚土著语言Yan-nhangu，探讨了ASR系统中基于音素和文字的分词策略对识别性能的影响，并验证了ASR在语言文献记录中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现代ASR多依赖大量数据的transformer模型，难以适用于资源匮乏的语言，亟需探索适合低资源语言的ASR方法及其在语言记录中的应用。

Method: 使用wav2vec2模型对Yan-nhangu语料进行微调，同时比对基于音素和正字法的分词策略，评估不同策略对WER（词错误率）和CER（字符错误率）的影响，并验证手动纠正ASR输出的效率。

Result: 基于音素的分词策略显著提升了WER和CER指标，且经过ASR模型输出手工校正的工作效率远高于从零开始转录。

Conclusion: 结合语言学知识的音素分词策略可有效提升ASR性能，ASR可作为记录低资源语言的重要辅助工具，提高文献记录效率。

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [29] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本文研究了推理模型在机器翻译任务中推理时长扩展（TTS）的影响，发现其对通用模型的直接翻译提升有限，但在领域专用微调和后期编辑中效果显著。


<details>
  <summary>Details</summary>
Motivation: 探究增加推理模型推理时间是否能提升机器翻译质量，特别是在不同应用场景中的表现。

Method: 评估了12个推理模型在多领域机器翻译基准上的表现，测试了直接翻译、强制推理外推和后期编辑三种场景。

Result: 对于通用模型，TTS对直接翻译效果提升有限且不稳定；领域专用微调后TTS带来一致且最佳的推理深度提升；强制超出自然停点推理会降低质量；TTS在后期编辑中表现尤为有效。

Conclusion: 推理时间扩展在机器翻译中的价值不在于提升单步翻译表现，而在于结合任务专用模型和多步自我纠正流程的定向应用。

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [30] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出了Webscale-RL数据引擎和包含1.2百万样本的Webscale-RL数据集，通过强化学习显著提升大模型的推理能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 仿真学习存在训练与生成间的差距，且强化学习受限于数据集规模小和多样性不足。

Method: 开发Webscale-RL流水线，大规模转换预训练文本为多域、多样且可验证的问答对，用于强化学习训练。

Result: 基于Webscale-RL数据集训练的模型在多个基准测试中效果明显优于持续预训练和数据精炼方法，且训练效率提升约100倍。

Conclusion: Webscale-RL方法为强化学习扩展到预训练规模提供了可行路径，提高了语言模型的能力和训练效率。

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [31] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: 本文研究了基于已有预训练模型进行二次预训练（bootstrapped pretraining）的效果，发现其效率随着基础模型使用的预训练数据量增加而呈对数递减趋势，揭示了多阶段预训练中的一个权衡问题。


<details>
  <summary>Details</summary>
Motivation: 降低从零训练语言模型的成本，探究在已有基础模型基础上的二次预训练效果及其扩展性，尤其针对过度训练的基础模型的有效性尚不明确。

Method: 通过实验研究基于已有预训练模型进行二次预训练时，其规模效应的表现，提出并验证一个简单的扩展定律来描述第一阶段与第二阶段训练数据量对模型性能的联合影响。

Result: 发现二次预训练的规模指数随基础模型预训练的数据量以对数方式减小，说明基础模型预训练越充分，二次预训练带来的额外收益越有限，这种饱和效应揭示多阶段预训练存在基本的权衡问题。

Conclusion: 在多阶段预训练策略中，预训练越充分，bootstrapped预训练带来的增益越小，这为高效训练语言模型提供了实用指导，同时对过度训练模型的再利用提出了重要考虑。

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [32] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: 该论文提出了专门针对用户角色设计的用户语言模型（User LMs），以模拟现实中的多轮对话用户行为，发现传统助手模型作为用户模拟效果较差，且性能越好的助手模型模拟效果越差。


<details>
  <summary>Details</summary>
Motivation: 现有研究多用经过优化以成为助手的语言模型来模拟用户，但助手模型并不适合模拟用户，导致模仿效果不真实，影响多轮对话中的性能评估准确性。

Method: 论文提出专门进行后训练的用户语言模型(User LMs)，使其能更好地模拟人在多轮对话中的发言特点，增强模拟的真实性和鲁棒性。

Result: 通过多种评估，User LMs更符合真实用户行为，且在编码和数学对话模拟中，助手模型性能显著下降（从74.6%降至57.4%），验证了模拟环境的真实性提高后对助手挑战加大。

Conclusion: 引入用户语言模型提升了多轮对话中用户模拟的真实度，为助手模型性能的真实评估提供了更合理的测试环境。

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [33] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: 本文提出了一种基于类型理论语义的新方法，通过蒙太古风格的逻辑形式解析自然语言，实现合规性指导，以解决现有语言模型生成的幻觉和合规性问题。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型虽然流畅，但常犯语义类型错误，导致幻觉、脆弱的内容审核和不透明的合规结果。作者认为这是缺乏类型语义而非数据或规模问题。

Method: 提出Savassan神经符号架构，将自然语言输入解析为蒙太古风格的逻辑形式，并映射到扩展的类型本体，结合神经网络与符号推理进行类型检查和跨司法区映射，实现合规性分析。

Result: 系统能够在跨司法区场景下“一次解析”，将语义映射到不同法律本体中，提供解释性强的合规决策，减少幻觉和类型错误。

Conclusion: 可信自主系统需要基于组合类型语义进行推理，明确描述内容、规范和责任，Savassan为实现此目标提供了理论与工程框架。

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [34] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出了TinyScientist框架，用于简化和改进自动化科研中多智能体系统的复杂工作流程。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）自动化科研的兴起，多智能体系统、规划、工具使用等流程变得复杂，扩展和维护变得困难。

Method: TinyScientist识别自动化科研工作流的核心组成部分，设计了一个互动性强、可扩展且可控的框架，便于适配新工具并支持迭代发展，提供了开源代码、交互式网页演示及Python包。

Result: 该框架成功实现了对自动化科研流程的有效管理和简化，提高了科研工具的可访问性和可扩展性。

Conclusion: TinyScientist框架为自动化科研提供了一个易用且灵活的平台，有助于研究人员和开发者快速构建和扩展复杂的自动化研究流程。

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [35] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的“越狱”攻击，分析了模型内部隐藏层对攻击与正常提示的响应差异，揭示了潜在的检测与防御方向。


<details>
  <summary>Details</summary>
Motivation: 随着对话式大型语言模型的普及，恶意用户利用精心设计的提示进行越狱攻击，诱导模型输出限制或敏感内容，防御难度大且现有方法无法完全抵抗。

Method: 通过分析开源模型GPT-J和状态空间模型Mamba2的内部表示，特别是隐藏层对越狱提示和正常提示的区别响应，挖掘不同层的表现以理解越狱现象。

Result: 发现模型不同层的响应模式在越狱提示和正常提示间存在明显差异，初步揭示了内部模型动态特征，这为越狱检测提供了有潜力的线索。

Conclusion: 内部隐藏层动态的差异为鲁棒的越狱检测和防御提供了新的研究方向，有望推动更有效的安全机制设计。

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [36] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 本文对状态空间模型（SSMs）和基于Transformer模型（TBMs）在长序列处理中表示传播的机制进行了首次统一的层级分析，揭示两者在信息流动和表示同质化上的差异。


<details>
  <summary>Details</summary>
Motivation: 尽管SSMs在长序列处理效率和资源消耗上优于TBMs，但其内部信息如何在层和token间传播尚不明晰。

Method: 采用中心核对齐、稳定性指标及探针技术，结合理论分析和参数随机化，研究SSMs和TBMs中表示在层内外的演化特征。

Result: 发现TBMs迅速使token表示同质化，之后多样性在深层恢复；SSMs则初期保持token独特性，后期逐渐趋于同质化。TBMs的过度平滑源于架构设计，而SSMs主要源于训练过程。

Conclusion: 研究揭示了两种模型的归纳偏置，为未来长上下文推理模型和训练设计提供理论指导。

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [37] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: 本文提出了一种无需外部人工标注或评判模型的自我合成训练框架SAO，利用大模型自身生成的数据进行自我对齐优化，提升了模型的对话能力和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于人工反馈的强化学习方法成本高且依赖外部模型标注，急需一种更经济且自给自足的对齐方法。

Method: 通过让大语言模型扮演角色，自行生成多样化的提示和回答，进而进行自我评估与优化，形成完整的自合成训练流程。

Result: 在AlpacaEval 2.0等标准评测中，SAO显著提升了模型的对话能力，同时在问答、数学推理等下游任务上保持较强表现。

Conclusion: SAO为大语言模型的自我改进和对齐提供了可行且经济的解决方案，推动了无需人工介入的模型调优发展。

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [38] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为ToolMem的记忆机制，使基于大语言模型或视觉语言模型的智能体能够通过历史交互总结工具性能，提升任务下工具选择的准确性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常使用固定工具，缺乏灵活选择最适合特定任务工具的能力，而人类能通过交互积累工具能力经验来优化选择。

Method: 提出ToolMem，通过总结历史交互中工具的优缺点并存储于记忆，在推理时检索相关信息，辅助智能体选择最优工具。

Result: 在文本生成和文本到图像生成任务中，ToolMem增强的智能体比无记忆智能体预测工具性能准确度分别提升14.8%和28.7%；最佳工具选择准确率分别提升21%和24%。

Conclusion: ToolMem有效提升了多工具环境下的工具性能预测和选择能力，增强了智能体处理多样化任务的灵活性和准确性。

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [39] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 本文提出了PiKa数据集，通过仅30k高质量示例实现了大语言模型的高效指令对齐，性能超过了使用百万级数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 现有指令对齐数据集昂贵且难以复现，且需要大量数据才能高效微调大语言模型，限制了资源有限社区的发展。

Method: 设计PiKa-SFT数据集，使用高质量、专家级示例进行少量监督微调，并对多个模型和基准进行对比评估。

Result: PiKa-SFT在AlpacaEval 2.0和Arena-Hard基准测试中优于使用更多数据训练的模型，甚至超越了官方Llama-3-8B-Instruct模型。

Conclusion: 高质量的指令对齐能够通过显著减少训练数据量实现，为开源大语言模型的指令对齐提供了可扩展方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [40] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 本文介绍了一种增量式客户支持摘要系统，通过智能生成简洁要点，减少客服切换和重复检查，提高效率。


<details>
  <summary>Details</summary>
Motivation: 提升客服在对话中的工作效率，减少切换上下文和重复复核的时间。

Method: 结合微调的Mixtral-8x7B模型进行连续笔记生成，利用DeBERTa分类器过滤无关内容，并通过客服编辑反馈不断优化模型。

Result: 系统上线后，相比批量摘要，处理时间减少3%，复杂案例减少高达9%，并获得较高的客服满意度。

Conclusion: 基于连续反馈的增量式摘要有效提升了摘要质量和客服生产力，适合大规模应用。

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [41] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: 本文针对机器翻译任务中提示工程优化，提出了一种基于小模型和反向翻译策略的新颖方法，实现了高效性能和训练开销降低。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法主要优化指令部分且依赖大参数模型，难以适用于以输入部分为关键的机器翻译任务。

Method: 本文提出利用小参数模型，结合反向翻译策略进行提示优化，降低训练成本同时保持效果。

Result: 所提方法不仅训练开销小，还能达到较高的机器翻译性能，且具备扩展到其他下游任务的潜力。

Conclusion: 专门针对机器翻译设计的提示优化方法有效弥补了现有方法的不足，促进了提示工程在特定任务中的应用。

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [42] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）中逻辑有效性与语义合理性的联系，发现两者在模型内部表现为线性且高度相关的表示，导致模型混淆这两个概念。通过操控向量，作者实现了对模型判断的因果影响，并设计去偏向向量提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型如何编码逻辑有效性和语义合理性，并解释为何模型表现出类似人类的内容效应偏差。

Method: 分析模型内部表示的几何结构，使用线性向量分离与操控相关概念，构建去偏向向量以降低内容效应。

Result: 发现逻辑有效性与语义合理性在表示空间中高度对齐，且通过操作相应向量能有效改变模型判断，成功减少偏差并提高推理准确率。

Conclusion: 大型语言模型中抽象逻辑概念的线性表示导致内容效应产生，针对这一机制的表示干预是提高模型逻辑推理能力的有效途径。

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [43] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 本论文提出了一种基于摘要管理上下文的强化学习方法，解决大语言模型在长期多轮工具使用中的上下文长度限制问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有强化学习框架中因上下文长度限制导致指令执行下降、成本增加等问题。

Method: 引入基于LLM生成摘要的上下文管理方法，通过策略梯度优化工具使用和摘要策略，提出了SUPO算法，实现训练时突破固定上下文限制。

Result: 在交互式函数调用和搜索任务中，SUPO显著提升成功率，同时保持或减少上下文长度。在复杂搜索任务中，测试时增加摘要轮次进一步提升评估性能。

Conclusion: 基于摘要的上下文管理为突破固定上下文限制训练强化学习代理提供了有效且可扩展的解决方案。

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [44] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: 本文提出了一个新的、动态的句子嵌入评价协议PTEB，通过生成语义保持的释义进行多次评估，揭示了句子编码器性能对词汇空间变化的敏感性，并验证了小模型与大模型的影响无显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前的句子嵌入模型评估多依赖固定测试集，易导致性能膨胀和忽视实际鲁棒性。

Method: 引入基于大型语言模型（LLM）的动态释义生成方法，结合语义文本相似度金标准，在评价时随机生成多样化释义，进行多次测试并聚合结果。

Result: 实验证明，LLM生成的释义在保持语义的同时具有词汇多样性，句子编码器性能对词汇空间变化敏感，且小模型和大模型受影响程度相当，多语言实验结果一致。

Conclusion: 提出了一种基于动态释义生成的句子嵌入评估新范式，更能反映模型实际性能与鲁棒性，减少对静态、预定义基准的依赖，推动NLP评估向动态计算转变。

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [45] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Rank Anything First（RAF）的两阶段令牌优化方法，通过生成自然语言的微小扰动，成功实现对大语言模型（LLM）信息检索结果的目标项排名提升，同时难以被检测。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为检索结果重排序器，其排名行为易被小规模、自然语言的提示语操控，存在安全隐患。

Method: RAF采用两阶段优化方法：第一阶段通过贪心坐标梯度法结合排名梯度和可读性分数筛选候选词；第二阶段基于熵动态权重和温度控制采样精细选择词汇，兼顾排名提升和语言自然度。

Result: 实验证明，RAF能显著提升目标项排名，且在推广目标项和保持语言自然度方面优于现有方法，显示出更强的鲁棒性。

Conclusion: LLM重排序器易受到对抗式操控，提出了现代检索系统在可信性和鲁棒性上的新挑战。

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [46] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种基于权重矩阵、无需训练的指纹识别方法，有效识别大规模语言模型的来源，即使在多种复杂后训练操作下也能保持高度鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 保护大规模语言模型的知识产权，尤其是在模型经过多个复杂后训练过程后，亟需一种可靠手段判断模型是否衍生自已有模型。

Method: 利用线性分配问题（LAP）和无偏中心核对齐（CKA）相似度，消除参数操纵影响，构建鲁棒度高且高保真的相似度度量。

Result: 在60个正样本和90个负样本模型对的测试集上，该方法对六类后训练过程展现出极强鲁棒性，误报率接近零，分类指标均达完美分数，且计算速度快（30秒内完成）。

Conclusion: 该方法为大规模语言模型的可靠溯源验证提供了坚实基础，且公开了代码，便于实际应用和后续研究。

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [47] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文提出了一种无训练、无标签的短文本聚类新方法，适用于任何文本嵌入器，通过迭代向量更新和大语言模型（LLM）引导实现聚类，性能优于或可比现有对比学习方法。


<details>
  <summary>Details</summary>
Motivation: 在客户聊天机器人场景中，存在大量无标签且类簇数未知的用户语句文本，现有聚类方法难以适应这种实际商业需求。

Method: 基于迭代向量更新，构建基于代表文本的稀疏向量，并在大语言模型引导下不断细化向量，且不依赖训练或标签。

Result: 在多个多样化数据集和较小的大语言模型下取得优异聚类效果，且适用于任何嵌入器和多种聚类算法，具备良好的扩展性和低计算成本。

Conclusion: 该方法资源消耗低，适应性强，可扩展到大规模数据，更符合实际商业中短文本聚类的需求，优于现有依赖训练及标签的聚类技术。

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [48] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出了一种基于流利度的多参考评估框架，用于评估语法错误纠正，解决了以往以单一参考为主的问题，实现多语言通用。


<details>
  <summary>Details</summary>
Motivation: 当前语法错误纠正评估方法多基于编辑距离且以英语为中心，缺乏对多样性人类纠正的包容性，也难以应用于多语言和生成式环境。

Method: 构建流利度基础的多参考评估框架，将n-gram相似度视为多合法纠正结果的聚合问题，设计了四种聚合策略（select-best，simple-average，weighted-average，merged-counts）并分析其性质。

Result: 在捷克语、爱沙尼亚语、乌克兰语和中文语料上的实验表明，这些策略分别捕捉了流利性与覆盖率的互补特征，效果优异。

Conclusion: 该框架统一了多参考评估方法，基于流利度，兼顾语言多样性且不惩罚合理变体，适用于多语言环境。

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [49] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级、无训练的超叠部署策略，通过在推理时选择性地从大推理模型（LRM）中“遗忘”部分信息，减少计算量并防止过度思考，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在结构化任务中表现优异，但过度思考会导致性能下降和资源浪费。现有方案通过同时部署多个模型并路由输入来解决，但成本高且实践中困难。

Method: 本文分析奇异值的累计能量，找到最优的低秩投影，对LRM进行选择性“忘记”以调节推理过程，从而关闭部分计算，达到优化推理的目的，无需额外训练。

Result: 提出的方法能在保持推理能力的同时有效降低计算开销，防止LRM的过度推理，提升整体性能和资源利用率。

Conclusion: 该超叠部署策略为大模型推理提供了一种高效且经济的解决方案，通过模型内部调整避免了多模型部署的复杂性，具有广泛应用潜力。

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [50] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了一种自适应多范式神经符号推理框架，通过自动识别自然语言问题中的形式推理策略，动态选择并应用专门的逻辑求解器，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号方法集成逻辑求解器的方式较为静态，限制了利用多样化形式推理策略的能力。

Method: 引入自适应多范式推理框架，利用大语言模型自动识别推理策略，并通过自动形式化接口动态选择专用逻辑求解器。

Result: 大语言模型在预测推理策略上的准确率超过90%，框架性能分别优于GPT-4o和DeepSeek-V3.1基线27%和6%，并在纯LLM方法中带来10%、5%和6%的提升。

Conclusion: 该框架为自适应神经符号推理奠定基础，促进了材料推理与形式推理在多样化推理任务中的统一。

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [51] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文系统性研究了大型语言模型（LLMs）知识提取的终止性、可重复性和鲁棒性，验证了知识结构化提取的可靠性及其局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型拥有丰富的事实知识，但如何衡量和系统化这些知识，尤其是通过递归提取方法进行结构化，仍存在挑战，需要验证该过程是否止步、输出可否重复以及对不同条件的鲁棒性。

Method: 通过构建miniGPTKBs（领域特定且可控的子数据集），从终止率、词汇和语义相似性三个维度，针对不同种子、语言、随机性和模型类型，在历史、娱乐及金融三个领域进行实验分析。

Result: 发现提取过程一般能高概率终止但依赖模型，输出在不同条目间具备一定可重复性，且对种子和温度参数鲁棒性高，但对语言和模型变更的鲁棒性较低。

Conclusion: 大型语言模型的知识结构化提取方法能可靠挖掘核心知识，但存在模型依赖性和跨语言、跨模型表现不稳定的局限。

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [52] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: 本文介绍了PAN 2025生成性抄袭检测任务，构建了基于三大语言模型的大规模自动生成抄袭数据集，并分析了参赛方法的表现和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对科学文章中自动生成文本抄袭的检测需求，需构建相应数据集及有效检测方法。

Method: 利用Llama、DeepSeek-R1和Mistral三种大型语言模型生成抄袭数据，评估多种检测方法并与2015年的检测结果进行对比。

Result: 简单的基于嵌入向量的语义相似性方法在当前数据集上表现较好（召回率0.8，准确率0.5），但在2015年数据集表现较差，说明泛化能力不足。

Conclusion: 现有方法在新数据集上表现良好，但缺乏多样性与泛化能力，未来需要开发更具鲁棒性的抄袭检测技术。

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [53] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: 该论文研究了在大型语言模型中结合多种电路定位方法以提高性能，提出了并行和顺序集成两种方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前电路定位方法存在精度和效率的权衡，单一方法难以达到最佳性能，集成多种方法可能提升电路识别的精度和可靠性。

Method: 论文提出并行集成（结合不同方法的边缘归因分数）和顺序集成（用EAP-IG方法结果作为边缘修剪方法的初始化）的两种集成策略，对比并验证集成方法效果。

Result: 两种集成方法均在基准指标上表现出显著提升，尤其是结合多种方法的并行集成取得了最佳效果。

Conclusion: 通过集成多种电路定位方法，可以提高大型语言模型中电路识别的准确性，为相关任务提供更精确的线路定位解决方案。

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [54] [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
*Chenpeng Wang,Xiaojie Cheng,Chunye Wang,Linfeng Yang,Lei Zhang*

Main category: cs.CL

TL;DR: MTR是一种针对工具增强推理的仿真优先训练框架，通过模拟工具响应和多代理架构，提升语言模型的推理能力，避免了对实时API的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强语言模型依赖实时API访问，导致训练与部署中存在可扩展性和可靠性挑战。

Method: MTR采用多代理架构，包括ToolMaker生成工具接口，AutoAgent生成结构化推理序列，ToolActor模拟响应；训练分为两个阶段：监督微调学习推理序列语法，策略优化提升答案准确性和内部一致性。

Result: 在多个多跳问答基准测试中，MTR取得了与实时API系统相当的准确率，且在推理密集型任务上表现优异。

Conclusion: 通过结构化推理轨迹的学习，MTR有效实现了工具增强推理，无需实时API交互，也能获得竞争力表现。

Abstract: Tool-augmented language models have demonstrated strong capabilities, but
their reliance on live API access creates scalability and reliability
challenges during training and deployment. We propose MTR, a simulation-first
training framework for tool-augmented reasoning. Instead of relying on live
APIs, MTR learns from complete ReAct traces with schema-validated, simulated
observations. Our approach operates through a multi-agent architecture where a
ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an
AutoAgent produces structured think-act-observe sequences, and a ToolActor
simulates realistic responses. Training proceeds in two stages: Stage-1
Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning
sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy
with a composite trace reward that balances answer correctness and internal
consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,
2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to
live-API systems and excels on reasoning-intensive tasks, suggesting that
effective tool reasoning can be learned from structured traces without live
interactions.

</details>


### [55] [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
*Kaixiang Mo,Yuxin Shi,Weiwei Weng,Zhiqiang Zhou,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 本文提出了大型语言模型中期训练作为一个统一范式的首次综述，系统总结了中期训练的分类、策略和效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过大规模预训练和任务微调成功，但中期训练阶段对于提高训练稳定性和模型能力至关重要，现有研究缺乏系统总结。

Method: 本文对中期训练进行了首次系统分类，涵盖数据分布调整、学习率调度和长上下文扩展，归纳了实用经验，汇编了评估基准，并分析了效果提升。

Result: 通过综述和比较，确认中期训练在缓解噪声影响、稳定模型收敛和扩展能力方面有效，提升了泛化和抽象能力。

Conclusion: 中期训练作为大型语言模型训练的重要环节具有显著价值，未来研究需解决现存挑战，推动该范式的发展和应用。

Abstract: Large language models (LLMs) are typically developed through large-scale
pre-training followed by task-specific fine-tuning. Recent advances highlight
the importance of an intermediate mid-training stage, where models undergo
multiple annealing-style phases that refine data quality, adapt optimization
schedules, and extend context length. This stage mitigates diminishing returns
from noisy tokens, stabilizes convergence, and expands model capability in late
training. Its effectiveness can be explained through gradient noise scale, the
information bottleneck, and curriculum learning, which together promote
generalization and abstraction. Despite widespread use in state-of-the-art
systems, there has been no prior survey of mid-training as a unified paradigm.
We introduce the first taxonomy of LLM mid-training spanning data distribution,
learning-rate scheduling, and long-context extension. We distill practical
insights, compile evaluation benchmarks, and report gains to enable structured
comparisons across models. We also identify open challenges and propose avenues
for future research and practice.

</details>


### [56] [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
*Giorgos Filandrianos,Orfeas Menis Mastromichalakis,Wafaa Mohammed,Giuseppe Attanasio,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 本文引入了一个大规模的数据集，用于检测自动翻译质量评估指标中的性别偏见，覆盖多语言、多职业领域，且设计上成对比较含不同性别词汇的译文。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明机器翻译质量评估指标存在性别偏见，但多数分析受限于数据规模小、职业类别狭窄及语言覆盖有限，难以全面反映问题。

Method: 基于GAMBIT语料库，扩展至3个源语言与11个目标语言，形成33对语言组合；每个源文本配对两种目标译文版本（男性 vs 女性职业词），并调整所有相关语法元素，保障平行对比。

Result: 通过该大规模平行数据集，实现了按职业细分和跨语言系统化的细粒度性别偏见分析，揭示质量评估指标对不同性别职业词的评分差异。

Conclusion: 本文提供了一个全面、系统的工具，促进更深入的自动翻译质量评估指标性别偏见研究，推动公平性改进。

Abstract: Gender bias in machine translation (MT) systems has been extensively
documented, but bias in automatic quality estimation (QE) metrics remains
comparatively underexplored. Existing studies suggest that QE metrics can also
exhibit gender bias, yet most analyses are limited by small datasets, narrow
occupational coverage, and restricted language variety. To address this gap, we
introduce a large-scale challenge set specifically designed to probe the
behavior of QE metrics when evaluating translations containing gender-ambiguous
occupational terms. Building on the GAMBIT corpus of English texts with
gender-ambiguous occupations, we extend coverage to three source languages that
are genderless or natural-gendered, and eleven target languages with
grammatical gender, resulting in 33 source-target language pairs. Each source
text is paired with two target versions differing only in the grammatical
gender of the occupational term(s) (masculine vs. feminine), with all dependent
grammatical elements adjusted accordingly. An unbiased QE metric should assign
equal or near-equal scores to both versions. The dataset's scale, breadth, and
fully parallel design, where the same set of texts is aligned across all
languages, enables fine-grained bias analysis by occupation and systematic
comparisons across languages.

</details>


### [57] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于自信号驱动的多大语言模型辩论方法（SID），通过利用模型级置信度和词元级语义关注来优化多模型迭代辩论过程。


<details>
  <summary>Details</summary>
Motivation: 现有多大语言模型辩论方法多依赖外部结构与评判机制，未充分利用生成过程中的自信号，导致计算冗余及性能下降。

Method: 本文提出SID方法，采用模型级置信度实现高置信度模型提前退出辩论，利用注意力机制压缩冗余内容，从而自适应引导辩论过程。

Result: 在多个大语言模型及多模态任务基准测试中，SID方法在准确率和令牌消耗方面均优于现有多模型辩论技术。

Conclusion: 利用生成过程中的自信号显著提升了多大语言模型辩论系统的性能与效率，证明了这一思路的有效性。

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [58] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: OpenJAI-v1.0是基于Qwen3-14B模型开发的泰语和英语开源大语言模型，针对实用任务优化，通过精心挑选数据提升指令执行、长上下文理解及工具使用能力，表现优于其它泰语开源模型。


<details>
  <summary>Details</summary>
Motivation: 提升泰语及英语大语言模型在实际应用场景中的表现，满足泰语AI社区对于高性能开源模型的需求。

Method: 基于Qwen3-14B模型，利用针对指令遵循、长上下文理解和工具使用的精选数据进行训练和微调。

Result: OpenJAI-v1.0在多项基准测试中超过了其基础模型及其他领先的开源泰语模型，并且避免了灾难性遗忘。

Conclusion: OpenJAI-v1.0作为开源资源公开发布，为泰语AI社区提供了一个有效的替代方案，促进相关技术的发展。

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [59] [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
*Wafaa Mohammed,Vlad Niculae,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLMs）在语境感知机器翻译中的话语现象处理能力，提出质量感知解码（QAD）方法，提升翻译质量和语义丰富度。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在机器翻译中表现优秀，但在处理话语现象如代词消解和词汇衔接时仍存在不足，亟需改进。

Method: 通过深入分析LLMs在话语现象上的表现，发现其内含话语知识，并提出质量感知解码（QAD）方法以有效提取这些知识，进行详细对比分析。

Result: QAD方法优于其他解码方式，提升了翻译的语义丰富性，使翻译结果更符合人类偏好。

Conclusion: 利用LLMs内在的话语知识并结合质量感知解码可以显著改善机器翻译中的话语现象处理能力，促进翻译质量提升。

Abstract: Large language models (LLMs) have emerged as strong contenders in machine
translation.Yet, they still struggle to adequately handle discourse phenomena,
such as pronoun resolution and lexical cohesion at the document level. In this
study, we thoroughly investigate the discourse phenomena performance of LLMs in
context-aware translation. We demonstrate that discourse knowledge is encoded
within LLMs and propose the use of quality-aware decoding (QAD) to effectively
extract this knowledge, showcasing its superiority over other decoding
approaches through comprehensive analysis. Furthermore, we illustrate that QAD
enhances the semantic richness of translations and aligns them more closely
with human preferences.

</details>


### [60] [$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
*Yining Wang,Jinman Zhao,Chuangxin Zhao,Shuhao Guan,Gerald Penn,Shinan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的自适应代币权重控制方法$lambdau00$-GRPO，用于改善大规模语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法如GRPO在处理响应时存在长度偏差问题，因为奖励均匀分配给所有代币，导致较长输出的奖励被稀释，影响训练效果。

Method: 本文统一现有强化学习框架并引入可学习参数$lambda$，使模型能在优化过程中自动调整代币权重，实现自适应加权，从而缓解长度偏差。

Result: 在多个数学推理基准测试中，$lambda$-GRPO相比GRPO和DAPO展现出一致的性能提升，在不同规模的Qwen2.5模型上，准确率平均提升1.0%至1.9%。

Conclusion: 学习代币偏好能够有效提升强化学习训练效果，无需修改训练数据或增加计算资源，具有较好的实用价值。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant
approach for improving the reasoning capabilities of Large Language Models
(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has
simplified this paradigm by replacing the reward and value models with
rule-based verifiers. A prominent example is Group Relative Policy Optimization
(GRPO). However, GRPO inherently suffers from a length bias, since the same
advantage is uniformly assigned to all tokens of a response. As a result,
longer responses distribute the reward over more tokens and thus contribute
disproportionately to gradient updates. Several variants, such as DAPO and Dr.
GRPO, modify the token-level aggregation of the loss, yet these methods remain
heuristic and offer limited interpretability regarding their implicit token
preferences. In this work, we explore the possibility of allowing the model to
learn its own token preference during optimization. We unify existing
frameworks under a single formulation and introduce a learnable parameter
$\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO
to denote our method, and we find that $\lambda$-GRPO achieves consistent
improvements over vanilla GRPO and DAPO on multiple mathematical reasoning
benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO
improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO,
respectively. Importantly, these gains come without any modifications to the
training data or additional computational cost, highlighting the effectiveness
and practicality of learning token preferences.

</details>


### [61] [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: MeXtract是一系列轻量级语言模型，通过微调Qwen 2.5模型，实现科学论文元数据的高效准确提取，在MOLE基准测试中表现优异，具备良好的鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前元数据提取任务依赖规则或特定模型，难以跨领域和多模式泛化，迫切需要轻量且通用的解决方案。

Method: 基于Qwen 2.5模型微调生成0.5B至3B参数规模的MeXtract模型，并扩展MOLE基准以支持更具挑战性的跨领域测试。

Result: MeXtract在MOLE基准中达到了同规模模型的最先进性能，微调特定模式有效提高准确率且能迁移到未见过的模式。

Conclusion: MeXtract展示了在元数据提取任务中的强大性能和适应能力，模型及数据开源促进科研社区应用和发展。

Abstract: Metadata plays a critical role in indexing, documenting, and analyzing
scientific literature, yet extracting it accurately and efficiently remains a
challenging task. Traditional approaches often rely on rule-based or
task-specific models, which struggle to generalize across domains and schema
variations. In this paper, we present MeXtract, a family of lightweight
language models designed for metadata extraction from scientific papers. The
models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5
counterparts. In their size family, MeXtract achieves state-of-the-art
performance on metadata extraction on the MOLE benchmark. To further support
evaluation, we extend the MOLE benchmark to incorporate model-specific
metadata, providing an out-of-domain challenging subset. Our experiments show
that fine-tuning on a given schema not only yields high accuracy but also
transfers effectively to unseen schemas, demonstrating the robustness and
adaptability of our approach. We release all the code, datasets, and models
openly for the research community.

</details>


### [62] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了Long-RewardBench，用于长上下文奖励模型评估，并提出多阶段训练策略提升模型在长上下文中的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型主要针对短上下文，忽视了长上下文与回应一致性的重要性，无法满足实际应用中长历史轨迹场景的需求。

Method: 提出Long-RewardBench基准测试，包括配对比较和最佳选择任务，并设计多阶段训练策略以提升奖励模型在长上下文下的表现。

Result: 通过多阶段训练策略，模型在长上下文评估中性能大幅提升，且保持短上下文能力。8B参数模型超越了70B大模型基线，达到Gemini 2.5 Pro模型的性能水平。

Conclusion: 多阶段训练策略有效提升了奖励模型应对长上下文任务的能力，实现了小模型匹敌大模型的效果，推动了奖励模型在复杂应用场景中的适用性。

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [63] [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: 本文提出了SHANKS框架，使得语音语言模型在用户说话时同步进行无声的链式思维推理，实时响应用户，降低延迟，提高交互效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型和语音语言模型只能在用户说话结束后才开始思考及响应，导致交互延迟，不适合需要实时低延迟的语音对话。

Method: SHANKS框架将用户语音分块输入，边接收边生成无声的思维链，基于历史语音和推理判断是否打断用户或调用工具，实现实时交互。

Result: 在数学步骤解题场景中，SHANKS打断错误的准确率提升37.1%；在工具辅助对话中，提前完成工具调用比例达56.9%。

Conclusion: SHANKS推动了模型实现对话中持续思考，而非仅在用户说话后进行思考，显著提升了实时语音交互的自然度与效率。

Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin
thinking and taking actions only after the user has finished their turn. This
prevents the model from interacting during the user's turn and can lead to high
response latency while it waits to think. Consequently, thinking after
receiving the full input is not suitable for speech-to-speech interaction,
where real-time, low-latency exchange is important. We address this by noting
that humans naturally "think while listening." In this paper, we propose
SHANKS, a general inference framework that enables SLMs to generate unspoken
chain-of-thought reasoning while listening to the user input. SHANKS streams
the input speech in fixed-duration chunks and, as soon as a chunk is received,
generates unspoken reasoning based on all previous speech and reasoning, while
the user continues speaking. SHANKS uses this unspoken reasoning to decide
whether to interrupt the user and to make tool calls to complete the task. We
demonstrate that SHANKS enhances real-time user-SLM interaction in two
scenarios: (1) when the user is presenting a step-by-step solution to a math
problem, SHANKS can listen, reason, and interrupt when the user makes a
mistake, achieving 37.1% higher interruption accuracy than a baseline that
interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can
complete 56.9% of the tool calls before the user finishes their turn. Overall,
SHANKS moves toward models that keep thinking throughout the conversation, not
only after a turn ends. Animated illustrations of Shanks can be found at
https://d223302.github.io/SHANKS/

</details>


### [64] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: 本文提出了开放自动语音识别（ASR）排行榜，涵盖11个数据集和60多个系统，标准化文本归一化，报告准确率和运行效率，支持多语种和长文本评测。


<details>
  <summary>Details</summary>
Motivation: 现有ASR评测主要集中于短英文，缺乏效率指标，且系统无法充分比较和复现。

Method: 构建一个可复现的排行榜，统一文本归一化标准，同时报告词错误率（WER）和逆实时因子（RTFx），并覆盖多语种及长文本评测。

Result: Conformer编码器配合大语言模型解码器在英文转录中WER最高但速度较慢，CTC和TDT解码器速度快，适合长文本和离线场景；Whisper编码器微调提升英文准确率但多语种覆盖减少。

Conclusion: 该排行榜促进了ASR系统的准确性与效率的公平比较，代码和数据均开源，支持透明且可扩展的评测流程。

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [65] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 本文利用大型语言模型(LLMs)生成符合数学教育标准且个性化的数学文字题，建立了首个教师标注的数据集，并验证了所训练模型在题目质量和学生偏好上的优势。


<details>
  <summary>Details</summary>
Motivation: 教师因班级规模大和负担重难以针对每个学生定制数学文字题，LLMs有潜力帮助生成个性化且符合教育标准的题目，提升学习效果。

Method: 采用专家与LLM联合评判方法评价超过11000道LLM生成的数学文字题，创建教师标注数据集，训练了12B开源模型及文本分类器，并进行实地学生测试以验证模型表现。

Result: 训练的12B模型在性能上可匹配更大模型，训练的文本分类器使30B模型超越闭源基线，模型生成的题目更贴合人工题目且学生更倾向于定制化题目，且学习效果相似。

Conclusion: LLMs能有效生成符合教育标准并个性化的数学文字题，既减轻教师负担，又获得学生认可，展现出应用于数学教育的巨大潜力。

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [66] [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
*Geng Liu,Feng Li,Junjie Mu,Mengxiao Zhu,Francesco Pierri*

Main category: cs.CL

TL;DR: 本论文研究了中文大型语言模型（LLMs）在社会身份框架下的偏见，发现模型对内群体表现出积极偏见，对外群体表现出消极偏见，这种偏见在合成提示和真实用户对话中均存在。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在面向用户的应用中广泛部署，存在其可能反映和放大社会偏见的担忧，因此需要针对中文环境进行社会身份偏见的系统性研究。

Method: 采用针对中文的特定提示，测试了十个典型中文大型语言模型对内群体（"我们"）和外群体（"他们"）的反应，并扩展到240个在中国语境中显著的社会群体，结合真实用户与聊天机器人的对话语料进行分析。

Result: 各模型普遍表现出内群体积极和外群体消极的倾向，这些偏见不仅存在于受控的合成提示中，也体现在真实的对话中，显示用户交互可能加剧偏见。

Conclusion: 本研究建立了适用于中文大型语言模型的语言感知评估框架，证明了在英语中观察到的社会身份偏见具有跨语言通用性，且在面向用户的实际使用场景中偏见可能被强化。

Abstract: Large language models (LLMs) are increasingly deployed in user-facing
applications, raising concerns about their potential to reflect and amplify
social biases. We investigate social identity framing in Chinese LLMs using
Mandarin-specific prompts across ten representative Chinese LLMs, evaluating
responses to ingroup ("We") and outgroup ("They") framings, and extending the
setting to 240 social groups salient in the Chinese context. To complement
controlled experiments, we further analyze Chinese-language conversations from
a corpus of real interactions between users and chatbots. Across models, we
observe systematic ingroup-positive and outgroup-negative tendencies, which are
not confined to synthetic prompts but also appear in naturalistic dialogue,
indicating that bias dynamics might strengthen in real interactions. Our study
provides a language-aware evaluation framework for Chinese LLMs, demonstrating
that social identity biases documented in English generalize
cross-linguistically and intensify in user-facing contexts.

</details>


### [67] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: 本文提出一种通过摘要增强块划分（SAC）方法，解决法律领域中检索增强生成（RAG）系统中的文档级检索不匹配（DRM）问题，提高检索精度和召回率。


<details>
  <summary>Details</summary>
Motivation: RAG方法在法律应用中易受检索环节失败的影响，尤其是面对大量结构相似的法律文档时，检索系统容易选择错误的文档，导致生成内容出现幻觉。

Method: 提出Summary-Augmented Chunking（SAC）方法，通过为每个文本块添加文档级综合摘要，注入全局上下文信息，减少错误文档的选择。

Result: 实验表明，SAC显著降低了DRM，提升了文本级检索的准确率和召回率。同时，通用摘要策略优于使用法律专家知识的定向摘要方法。

Conclusion: SAC是一种实用、可扩展且易于集成的技术，显著提升了RAG系统在大规模法律文档数据集上的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [68] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 本论文提出了一种结合翻译和合成扩展的人机循环流程，创建了覆盖10种印度语言的高质量多样化后训练数据集，以提升大型语言模型对印度语言的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集在多语言覆盖、文化根基和任务多样性方面存在不足，特别是针对印度语言，这限制了大型语言模型的效果。

Method: 通过人机循环流程结合翻译与合成扩展，构建了Pragyaan-IT和Pragyaan-Align两个数据集，涵盖广泛语言和任务类别，强调多轮对话、指令一致性、安全对齐及文化细节保留。

Result: 成功构建了包含22.5K和100K条样本的高质量多样化印度语言数据集，涵盖10种语言和多任务类别，填补了多语言数据集的空白。

Conclusion: 该数据集为打造更具包容性和有效性的多语言大型语言模型奠定了基础，促进了印度语言的模型性能提升。

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [69] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的混合注意力机制Native Hybrid Attention (NHA)，结合线性和全注意力，提升长序列建模的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在序列建模中效果出色但计算复杂度高，而线性注意力虽然效率更优但长序列上准确率降低，存在权衡问题。

Method: NHA通过融合线性RNN维护的长期上下文和滑动窗口中的短期令牌，统一采用softmax注意力机制实现上下文相关加权；通过调整滑动窗口大小实现线性与全注意力的平滑切换，结构统一简单。

Result: 实验证明NHA在召回率密集和常识推理任务上优于传统Transformer及其他混合注意力模型；预训练的大型语言模型可结构性地集成NHA，保持竞争准确率的同时大幅提升效率。

Conclusion: NHA有效结合线性与全注意力优势，在保证结构统一的同时提升长序列建模的性能和效率，是一种具备广泛应用潜力的混合注意力机制。

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [70] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: 本论文深入分析了先进大语言模型GPT-4.1的事实知识，发现其与已有知识库存在显著差异且准确率低于预期，同时存在不一致性、模糊性和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型中事实知识的理解不足，且通常基于偏见样本进行分析，影响对模型真实能力的评价。

Method: 基于GPTKB v1.5，递归提取了GPT-4.1中一亿条信念，系统性地分析其事实知识表现。

Result: 发现GPT-4.1的事实知识与现有知识库有很大差别，准确率低于之前基准测试报告，并且存在显著的不一致性、模糊和幻觉问题。

Conclusion: 当前先进LLM的事实知识仍有较大改进空间，这些发现为未来提升模型事实知识的研究指明了方向。

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [71] [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
*Rajvee Sheth,Samridhi Raj Sinha,Mahavir Patil,Himanshu Beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLM）在代码切换（CSW）处理中的最新研究，分析了多项研究成果、任务和数据集，指出了现有挑战并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型快速发展，但在处理代码切换（语言和文字混合）输入时仍存在困难，且相关数据集和评估存在偏差，影响其在多语言环境中的应用。

Method: 本文系统回顾了多项涉及CSW的大语言模型研究，涵盖5个研究领域、12个NLP任务、30多个数据集及80多种语言，从模型架构、训练策略和评估方法进行分类分析。

Result: 总结了大语言模型在CSW建模方面取得的进展，并指出数据集不够包容、评估不公平和模型缺乏语言学基础等挑战依然存在。

Conclusion: 呼吁构建包容性数据集、公平的评估体系以及更具语言学依据的模型，以实现真正的多语言智能。

Abstract: Code-switching (CSW), the alternation of languages and scripts within a
single utterance, remains a fundamental challenge for multiling ual NLP, even
amidst the rapid advances of large language models (LLMs). Most LLMs still
struggle with mixed-language inputs, limited CSW datasets, and evaluation
biases, hindering deployment in multilingual societies. This survey provides
the first comprehensive analysis of CSW-aware LLM research, reviewing
\total{unique_references} studies spanning five research areas, 12 NLP tasks,
30+ datasets, and 80+ languages. We classify recent advances by architecture,
training strategy, and evaluation methodology, outlining how LLMs have reshaped
CSW modeling and what challenges persist. The paper concludes with a roadmap
emphasizing the need for inclusive datasets, fair evaluation, and
linguistically grounded models to achieve truly multilingual intelligence. A
curated collection of all resources is maintained at
https://github.com/lingo-iitgn/awesome-code-mixing/.

</details>


### [72] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: 提出了Search-R3框架，通过让大语言模型（LLMs）在推理过程中直接生成搜索嵌入向量，显著提升信息检索效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs具备强大的自然语言理解能力，但在信息检索任务中的应用仍不充分，需探索如何利用其推理能力生成更有效的嵌入。

Method: 采用三阶段机制：1）监督学习用于提升嵌入质量；2）使用强化学习优化嵌入和推理过程；3）设计特殊的强化学习环境，避免每次训练迭代时对全量语料库进行重新编码。

Result: 在多个基准测试中，Search-R3显著优于现有方法，有效统一了推理和嵌入生成的流程，增强了处理复杂知识密集型任务的能力。

Conclusion: 通过将推理与嵌入生成深度整合，Search-R3为利用LLMs进行复杂信息检索任务提供了重要的新方向和技术突破。

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [73] [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
*Miriam Wanner,Sophia Hager,Anjalie Field*

Main category: cs.CL

TL;DR: 当地新闻台被辛克莱收购后，更侧重报道国家新闻，减少本地议题的报道，同时增加了对极化国家议题的关注。


<details>
  <summary>Details</summary>
Motivation: 探讨辛克莱广播集团收购众多当地新闻台后，这些新闻台报道内容如何变化，尤其在本地与国家新闻报道的平衡上。

Method: 利用计算方法分析当地新闻台在被收购前后，以及与国家新闻媒体的互联网内容变化。

Result: 发现被收购的当地新闻台更多报道国家新闻，减少本地新闻内容，且报道极化的国家议题有所增加。

Conclusion: 辛克莱收购导致当地新闻台报道重点从本地问题转向国家及极化议题，影响了新闻的非政治化特性。

Abstract: Local news stations are often considered to be reliable sources of
non-politicized information, particularly local concerns that residents care
about. Because these stations are trusted news sources, viewers are
particularly susceptible to the information they report. The Sinclair Broadcast
group is a broadcasting company that has acquired many local news stations in
the last decade. We investigate the effects of local news stations being
acquired by Sinclair: how does coverage change? We use computational methods to
investigate changes in internet content put out by local news stations before
and after being acquired by Sinclair and in comparison to national news
outlets. We find that there is clear evidence that local news stations report
more frequently on national news at the expense of local topics, and that their
coverage of polarizing national topics increases.

</details>


### [74] [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
*Amir Hossein Yari,Kalmit Kulkarni,Ahmad Raza Khan,Fajri Koto*

Main category: cs.CL

TL;DR: 提出了ITEM基准测试，评估26种自动评价指标与人工判断在六种印度语言中的一致性，发现大语言模型评价者表现最好，异常值显著影响评价，一些指标更善于捕捉内容，另一些更反映流畅度。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译和文本摘要自动评价指标主要针对英语等高资源语言，忽略了印度语系，使得评价手段的普适性存疑。

Method: 构建ITEM大型基准测试，覆盖六大印度语言，通过细粒度标注系统评估26种指标与人工判断间一致性，考察异常值影响、语言可靠性、指标间相关及对扰动的鲁棒性。

Result: 1) 大语言模型评价指标与人工判断对齐最好；2) 异常值显著影响评价一致性；3) 文本摘要指标更能体现内容忠实度，机器翻译指标更体现流畅度；4) 各指标对扰动反应和稳健性存在差异。

Conclusion: 该研究为印度语言自动评价指标设计和评估提供了重要指导，推动相关技术进步。

Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text
Summarization (TS), existing metrics have been developed and validated almost
exclusively for English and other high-resource languages. This narrow focus
leaves Indian languages, spoken by over 1.5 billion people, largely overlooked,
casting doubt on the universality of current evaluation practices. To address
this gap, we introduce ITEM, a large-scale benchmark that systematically
evaluates the alignment of 26 automatic metrics with human judgments across six
major Indian languages, enriched with fine-grained annotations. Our extensive
evaluation, covering agreement with human judgments, sensitivity to outliers,
language-specific reliability, inter-metric correlations, and resilience to
controlled perturbations, reveals four central findings: (1) LLM-based
evaluators show the strongest alignment with human judgments at both segment
and system levels; (2) outliers exert a significant impact on metric-human
agreement; (3) in TS, metrics are more effective at capturing content fidelity,
whereas in MT, they better reflect fluency; and (4) metrics differ in their
robustness and sensitivity when subjected to diverse perturbations.
Collectively, these findings offer critical guidance for advancing metric
design and evaluation in Indian languages.

</details>


### [75] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 本文针对卢森堡语等低资源语言缺乏高质量指令调优数据集的问题，提出了一种跨语言指令调优方法，通过利用英语、法语和德语的对齐数据，构建高质量数据集，避免了机器翻译带来的语义和文化不准确问题。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺少高质量的指令调优数据集，传统依赖机器翻译导致语义错位和文化不准确，影响模型性能。

Method: 利用英语、法语和德语的对齐数据，构建跨语言的卢森堡语指令调优数据集，避免使用机器翻译生成的卢森堡语数据。

Result: 跨语言指令调优提升了语言间的表示对齐能力和模型在卢森堡语生成任务中的表现。

Conclusion: 跨语言数据策划方法避免了机器翻译的常见问题，促进了低资源语言的发展和模型能力提升。

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [76] [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Zirui Wu,Yu Tian,Victoria W.,Guorui Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为LocalLeap的自适应并行解码策略，显著提高了扩散大语言模型的推理速度，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散大语言模型在质量和速度之间存在权衡，贪心解码虽保证质量但速度慢，导致推理效率低下。

Method: LocalLeap基于局部确定性传播和空间一致性递减两大经验原则，进行局部锚点识别和放宽的局部并行解码，提前确定已确定的标记以减少推理步骤，无需训练。

Result: 在多个基准测试中，LocalLeap实现了6.94倍的吞吐量提升，解码步骤降低至原来的14.2%，且几乎不影响性能。

Conclusion: LocalLeap有效解决了扩散大语言模型中因保守采样导致的延迟解码问题，极大提升了推理效率，具有良好的实用价值。

Abstract: Diffusion large language models (dLLMs) represent a significant advancement
in text generation, offering parallel token decoding capabilities. However,
existing open-source implementations suffer from quality-speed trade-offs that
impede their practical deployment. Conservative sampling strategies typically
decode only the most confident token per step to ensure quality (i.e., greedy
decoding), at the cost of inference efficiency due to repeated redundant
refinement iterations--a phenomenon we term delayed decoding. Through
systematic analysis of dLLM decoding dynamics, we characterize this delayed
decoding behavior and propose a training-free adaptive parallel decoding
strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built
on two fundamental empirical principles: local determinism propagation centered
on high-confidence anchors and progressive spatial consistency decay. By
applying these principles, LocalLeap identifies anchors and performs localized
relaxed parallel decoding within bounded neighborhoods, achieving substantial
inference step reduction through early commitment of already-determined tokens
without compromising output quality. Comprehensive evaluation on various
benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput
improvements and reduces decoding steps to just 14.2\% of the original
requirement, achieving these gains with negligible performance impact. The
source codes are available at: https://github.com/friedrichor/LocalLeap.

</details>


### [77] [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
*Miriam Wanner,Leif Azzopardi,Paul Thomas,Soham Dan,Benjamin Van Durme,Nick Craswell*

Main category: cs.CL

TL;DR: 本文提出了VITALERRORS数据集和VITAL指标，旨在提升大语言模型回答事实性评估对关键信息错误的敏感度。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法对所有信息赋予同等权重，忽视了针对问询关键信息的错误、遗漏的辨识，导致事实性评估不准确。

Method: 构建包含6733条查询与修改后的回答（含关键信息遗漏或错误）的VITALERRORS数据集，并基于相关性和信息重要性设计VITAL指标以提升错误检测敏感度。

Result: 实验证明，VITAL指标相较现有方法更有效地检测出关键信息中的错误。

Conclusion: 通过VITALERRORS数据集和VITAL指标，本文为更准确、鲁棒的LLM事实性评估奠定了基础。

Abstract: Existing methods for evaluating the factuality of large language model (LLM)
responses treat all claims as equally important. This results in misleading
evaluations when vital information is missing or incorrect as it receives the
same weight as peripheral details, raising the question: how can we reliably
detect such differences when there are errors in key information? Current
approaches that measure factuality tend to be insensitive to omitted or false
key information. To investigate this lack of sensitivity, we construct
VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses
designed to omit or falsify key information. Using this dataset, we demonstrate
the insensitivities of existing evaluation metrics to key information errors.
To address this gap, we introduce VITAL, a set of metrics that provide greater
sensitivity in measuring the factuality of responses by incorporating the
relevance and importance of claims with respect to the query. Our analysis
demonstrates that VITAL metrics more reliably detect errors in key information
than previous methods. Our dataset, metrics, and analysis provide a foundation
for more accurate and robust assessment of LLM factuality.

</details>


### [78] [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型和检索增强机制的讽刺感知语音合成框架，实现了更自然、更具讽刺表达力的语音合成。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成主要关注广泛情感类别，对讽刺这种细腻的非字面语言缺乏研究，讽刺的语义、语境和韵律线索复杂，合成难度大。

Method: 利用LoRA微调的LLaMA 3获取讽刺的语义嵌入，结合检索增强生成模块(RAG)检索韵律示例，双重条件输入VITS架构，实现讽刺语音合成。

Result: 实验显示该方法在客观指标和主观评价中均优于基线，在语音自然度、讽刺表达力和下游讽刺检测任务上都有提升。

Conclusion: 提出的结合大语言模型和检索增强的双重条件框架有效提升了讽刺语音合成的质量和表现力。

Abstract: Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.

</details>


### [79] [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
*Guo Yutong,Wanying Wang,Yue Wu,Zichen Miao,Haoyu Wang*

Main category: cs.CL

TL;DR: 本文提出了TALENT框架，通过结合小型视觉语言模型的OCR文本和自然语言描述，再利用大语言模型进行多模态推理，实现了高效且精确的表格视觉问答。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然可以直接从图像中回答表格相关问题，但计算资源消耗大，不适合移动端部署；小模型OCR结合大语言模型的方法存在表示不优化且误差较大问题。

Method: 提出TALENT框架，利用小型视觉语言模型同时生成OCR文本和自然语言叙述，结合提问输入给大语言模型进行推理，转变为以大语言模型为核心的多模态推理任务。

Result: 实验证明，TALENT框架下的小型视觉语言模型与大语言模型组合，在公共数据集和新构建的更具挑战性的ReTabVQA数据集上表现匹配或优于单一大型视觉语言模型，同时计算成本显著降低。

Conclusion: TALENT有效提升了表格视觉问答的效率和准确性，适合资源受限环境，且为多模态推理提供了新的思路和方法。

Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large
vision-language models (VLMs). While such models can answer directly from
images, they often miss fine-grained details unless scaled to very large sizes,
which are computationally prohibitive, especially for mobile deployment. A
lighter alternative is to have a small VLM perform OCR and then use a large
language model (LLM) to reason over structured outputs such as Markdown tables.
However, these representations are not naturally optimized for LLMs and still
introduce substantial errors. We propose TALENT (Table VQA via Augmented
Language-Enhanced Natural-text Transcription), a lightweight framework that
leverages dual representations of tables. TALENT prompts a small VLM to produce
both OCR text and natural language narration, then combines them with the
question for reasoning by an LLM. This reframes Table VQA as an LLM-centric
multimodal reasoning task, where the VLM serves as a perception-narration
module rather than a monolithic solver. Additionally, we construct ReTabVQA, a
more challenging Table VQA dataset requiring multi-step quantitative reasoning
over table images. Experiments show that TALENT enables a small VLM-LLM
combination to match or surpass a single large VLM at significantly lower
computational cost on both public datasets and ReTabVQA.

</details>


### [80] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: 该论文提出了一种利用大语言模型的上下文学习能力和两步元学习训练过程来建模人类标注差异的系统，该系统在LeWiDi竞赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理任务中常涉及主观性、歧义或标注者间的合理分歧，需要有效建模这些人类变异。

Method: 通过大语言模型的上下文学习能力，结合两步元学习训练：1）在多个需要上下文学习的数据集上进行后训练；2）通过上下文元学习针对具体数据分布进行专门化训练。

Result: 系统在LeWiDi竞赛的两个任务中均获总冠军，通过消融实验验证了各组件的重要性，如上下文中包括标注者示例、数据集特定微调、后训练及模型扩展均对性能有积极影响。

Conclusion: 系统成功利用上下文学习与元学习有效捕捉人类标注差异，提升了多任务表现，表明该方法对处理标注分歧问题具有应用价值。

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [81] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: 提出了一种名为TRIM的新方法，通过多层注意力机制的代表性模式匹配，选择高质量子集用于大语言模型的指令调优，效率高且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有指令调优方法依赖大规模多样化数据集或基于梯度的计算，成本高且忽略了细粒度特征，难以高效选取小而高质量的子集。

Method: TRIM方法不使用梯度，而是通过基于注意力机制的“指纹”匹配目标样本的代表性模式，实现一个前向、基于token的高效选择框架。

Result: TRIM选出的核心子集在多个下游任务上超过现有最优基线9%，部分情况下甚至优于全数据微调，同时计算成本仅为传统方法的一小部分。

Conclusion: TRIM提供了一种高效、可扩展且敏感于任务结构特征的子集选择方法，为构建高质量指令调优数据集提供了强有力的替代方案。

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [82] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: 本文比较了人类和大型语言模型(LLMs)在七种复杂句子结构上的理解能力，发现LLMs在花园路径句子上表现较差，表现与人类在某些方面相似但也有差异。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否会经历类似人类的语言处理困难，特别是在复杂句子结构中的表现。

Method: 在统一实验框架中，收集人类和五类不同规模及训练方式的LLMs对七种语言结构的句子理解数据，并与匹配的基线句子进行对比分析。

Result: LLMs总体在复杂结构上表现欠佳，尤其是花园路径句；表现随着模型规模增加与人类表现的相关性提高；同时观察到在人类和LLMs之间共存的收敛与分歧。

Conclusion: LLMs和人类在句子理解上既有相似之处，也存在显著差异，揭示了两者的语言处理机制的复杂关系。

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [83] [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
*Lekang Jiang,Wenjun Sun,Stephan Goetz*

Main category: cs.CL

TL;DR: 本文提出了一种针对层次文本分类的推理框架RHC，通过多阶段训练大语言模型，实现了更高的准确率、可解释性和扩展性，在专利主题分类及其他任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 自动专利主题分类作为层次文本分类中的难点，标签众多且需要领域知识，现有方法只给出平面标签，缺乏解释性。

Method: 提出RHC框架，将层次文本分类视为逐步推理任务，训练大语言模型分两个阶段：冷启动阶段对齐链式思考格式，强化学习阶段增强多步推理能力。

Result: RHC在准确率和宏观F1得分上领先现有基线约3%，生成自然语言解释便于人工审核，且模型规模增大时性能提升显著，且在多种层次文本分类基准上表现出色。

Conclusion: 该研究展示了基于推理的层次分类方法在准确性、解释性、扩展性及应用范围上的显著优势，为复杂层次文本分类任务提供了有效解决方案。

Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels
of a pre-defined taxonomy. Automated patent subject classification represents
one of the hardest HTC scenarios because of domain knowledge difficulty and a
huge number of labels. Prior approaches only output a flat label set, which
offers little insight into the reason behind predictions. Therefore, we propose
Reasoning for Hierarchical Classification (RHC), a novel framework that
reformulates HTC as a step-by-step reasoning task to sequentially deduce
hierarchical labels. RHC trains large language models (LLMs) in two stages: a
cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning
format and a reinforcement learning (RL) stage to enhance multi-step reasoning
ability. RHC demonstrates four advantages in our experiments. (1)
Effectiveness: RHC surpasses previous baselines and outperforms the supervised
fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)
Explainability: RHC produces natural-language justifications before prediction
to facilitate human inspection. (3) Scalability: RHC scales favorably with
model size with larger gains compared to standard fine-tuning. (4)
Applicability: Beyond patents, we further demonstrate that RHC achieves
state-of-the-art performance on other widely used HTC benchmarks, which
highlights its broad applicability.

</details>


### [84] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: 本文系统评估了数学推理领域开源数据集和数据合成技术，提出了实用的数据选择策略，发现优化数据结构和蒸馏高质量模型比盲目扩大数据量更有效，旨在为工业应用中的大语言模型训练提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的推理能力依赖于训练数据质量，但实用中的数据构造方法效果尚未充分验证。

Method: 统一构建训练与部署场景模拟的评估流水线，对开源数据和数据合成方法进行全面分析和比较，并提取有效的数据选择策略。

Result: 结果表明，将数据结构化为更易解释的格式或从更强模型蒸馏数据，比简单增加数据规模更能提升性能。

Conclusion: 本研究提供了成本效益高且可扩展的数据集成方法，为现实推理任务中的“数据量”与“数据质量”权衡提供了实践指导。

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [85] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: 本文提出了首个专门面向护理领域的大型语言模型NurseLLM，并构建大规模护理多项选择题数据集及评测体系，实验证明其优于通用和医疗专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在护理等专业领域的应用尚未充分探索，护理领域需要专门优化的语言模型以提升任务表现。

Method: 设计多阶段数据生成流程构建护理多项选择题数据集，训练NurseLLM，并开发多项护理基准测试以进行系统评估。

Result: NurseLLM在多个护理测评基准上均超过了相似规模的通用和医疗专用大型语言模型，表现更佳。

Conclusion: 专门针对护理领域的大型语言模型具有显著优势，结合推理和多智能体协作具备广阔研究和应用前景。

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [86] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一个框架系统地量化大语言模型（LLMs）在心理测评问卷中的数据污染，发现主流问卷存在较强的数据污染现象。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用心理测验问卷评估LLMs的高层心理特质，但缺乏对数据污染程度的系统量化，影响评估可靠性。

Method: 建立一个系统框架，从题目记忆、评测记忆和目标得分匹配三方面测量LLMs心理测评中的数据污染，并在21个模型与4个问卷上应用。

Result: 发现BFI-44和PVQ-40等热门问卷存在强烈数据污染，模型不仅记忆问卷题目，还能调整回答以达到目标得分。

Conclusion: 心理测验问卷评估LLMs时需要警惕数据污染问题，防止模型通过记忆问卷数据影响评估结果的可靠性。

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [87] [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
*Yong-En Tian,Yu-Chien Tang,An-Zi Yen,Wen-Chih Peng*

Main category: cs.CL

TL;DR: 提出了一个新的任务CARPAS，针对给定的摘要方面在实际应用中的不完整或不相关问题，动态调整摘要方面以生成更精准的摘要。


<details>
  <summary>Details</summary>
Motivation: 传统基于方面的摘要方法依赖预定义的方面，但现实中这些方面可能不完整、不相关或缺失，用户希望系统能根据内容动态调整。

Method: 引入CARPAS任务，构建三个数据集，并利用大型语言模型（LLMs）和四种提示策略进行探索。提出预测相关方面数量的子任务，作为引导减少推理难度。

Result: 实验表明，通过预测方面数量指导LLMs，显著提升了所有数据集上的摘要表现，生成内容更聚焦且与用户期望更一致。

Conclusion: 针对实际应用中方面不完全问题，动态调整方面、合理预测方面数量能有效提升基于方面的摘要质量，本文为基于内容的方面优化提供了重要方法和实证支持。

Abstract: Aspect-based summarization has attracted significant attention for its
ability to generate more fine-grained and user-aligned summaries. While most
existing approaches assume a set of predefined aspects as input, real-world
scenarios often present challenges where these given aspects may be incomplete,
irrelevant, or entirely missing from the document. Users frequently expect
systems to adaptively refine or filter the provided aspects based on the actual
content. In this paper, we initiate this novel task setting, termed
Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with
the aim of dynamically adjusting the provided aspects based on the document
context before summarizing. We construct three new datasets to facilitate our
pilot experiments, and by using LLMs with four representative prompting
strategies in this task, we find that LLMs tend to predict an overly
comprehensive set of aspects, which often results in excessively long and
misaligned summaries. Building on this observation, we propose a preliminary
subtask to predict the number of relevant aspects, and demonstrate that the
predicted number can serve as effective guidance for the LLMs, reducing the
inference difficulty, and enabling them to focus on the most pertinent aspects.
Our extensive experiments show that the proposed approach significantly
improves performance across all datasets. Moreover, our deeper analyses uncover
LLMs' compliance when the requested number of aspects differs from their own
estimations, establishing a crucial insight for the deployment of LLMs in
similar real-world applications.

</details>


### [88] [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
*Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLM）是否能区分人类可能的语言与不可能的语言，结果显示GPT-2无法有效区分二者。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否与人类共享先天学习偏好，即它们是否能区分自然语言和人为生成的不可能语言。

Method: 使用与前人类似的方法，分析GPT-2在多种自然语言及其不可能扰动版本上的学习曲线及困惑度曲线，评估其区分能力。

Result: 大多数情况下，GPT-2对自然语言和不可能语言的学习难度相近，且未能在整体上区分两类语言。

Conclusion: 大型语言模型不具备类似人类的内在语言偏好，无法区分人类可能语言与不可能语言。

Abstract: Are large language models (LLMs) sensitive to the distinction between humanly
possible languages and humanly impossible languages? This question is taken by
many to bear on whether LLMs and humans share the same innate learning biases.
Previous work has attempted to answer it in the positive by comparing LLM
learning curves on existing language datasets and on "impossible" datasets
derived from them via various perturbation functions. Using the same
methodology, we examine this claim on a wider set of languages and impossible
perturbations. We find that in most cases, GPT-2 learns each language and its
impossible counterpart equally easily, in contrast to previous claims. We also
apply a more lenient condition by testing whether GPT-2 provides any kind of
separation between the whole set of natural languages and the whole set of
impossible languages. By considering cross-linguistic variance in various
metrics computed on the perplexity curves, we show that GPT-2 provides no
systematic separation between the possible and the impossible. Taken together,
these perspectives show that LLMs do not share the human innate biases that
shape linguistic typology.

</details>


### [89] [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
*Benjamin Akera,Evelyn Nafula Ouma,Gilbert Yiga,Patrick Walukagga,Phionah Natukunda,Trevor Saaka,Solomon Nsumba,Lilian Teddy Nabukeera,Joel Muhanguzi,Imran Sekalala,Nimpamya Janat Namara,Engineer Bainomugisha,Ernest Mwebaze,John Quinn*

Main category: cs.CL

TL;DR: 本文提出针对乌干达多语言环境开发的Sunflower 14B和32B语言模型，提升乌干达语言的技术支持。


<details>
  <summary>Details</summary>
Motivation: 非洲有大量语言未被语言技术覆盖，现有大模型多优先支持使用者多的语言，导致多样语言支持不均衡。

Method: 采用区域性聚焦方法，基于Qwen 3开发适用于乌干达多数语言的Sunflower 14B和32B模型。

Result: 新模型在乌干达大部分语言上的理解能力达到先进水平。

Conclusion: 区域聚焦语言模型可有效提升多语言支持，促进语言障碍的减少，推动实际应用发展。

Abstract: There are more than 2000 living languages in Africa, most of which have been
bypassed by advances in language technology. Current leading LLMs exhibit
strong performance on a number of the most common languages (e.g. Swahili or
Yoruba), but prioritise support for the languages with the most speakers first,
resulting in piecemeal ability across disparate languages. We contend that a
regionally focussed approach is more efficient, and present a case study for
Uganda, a country with high linguistic diversity. We describe the development
of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the
art comprehension in the majority of all Ugandan languages. These models are
open source and can be used to reduce language barriers in a number of
important practical applications.

</details>


### [90] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 本文发现大型语言模型在中间层使用少量特定维度实现跨语言转换，并提出一种无需训练即可识别和操控这些维度的方法，能够有效控制输出语言。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言环境中表现出强大的能力，其跨语言映射机制尚不明确，理解和操控这一机制有助于增强多语言生成控制。

Method: 提出了一种简单且无需训练的方法识别跨语言转换所涉及的稀疏维度，利用少量平行或单语句子进行分析和操控。

Result: 在多语言生成控制任务中，该方法能够解释相关维度，操控这些维度实现语言切换，同时保持语义内容，性能优于此前基于神经元的方法且成本更低。

Conclusion: 跨语言转换过程由少量稀疏维度统一控制，针对这些维度的干预为多语言模型生成控制提供有效且低成本手段。

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [91] [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
*Benjamin Akera,Evelyn Nafula,Patrick Walukagga,Gilbert Yiga,John Quinn,Ernest Mwebaze*

Main category: cs.CL

TL;DR: 本论文研究了基于OpenAI Whisper模型的低资源非洲班图语音自动识别系统，探讨了实现可用性能所需的最小训练数据量以及系统的主要错误模式。


<details>
  <summary>Details</summary>
Motivation: 低资源非洲语言语音识别系统由于训练数据匮乏而难以开发，需明确实际应用中对数据量和错误类型的需求。

Method: 通过在两种班图语（Kinyarwanda和Kikuyu）上的大规模训练数据扩展实验和详细错误分析，评估Whisper模型的性能变化和失败原因。

Result: 实验发现只需50小时训练数据即可实现WER低于13%的实用性能，200小时数据时性能继续提升（WER低于10%）；38.6%的高错误率由数据质量问题（特别是噪声标签）引起。

Conclusion: 数据质量与数量同等重要，本文提供了低资源班图语言ASR系统开发的实用基准和部署指导，有助于推动类似语言的实际应用。

Abstract: The development of Automatic Speech Recognition (ASR) systems for
low-resource African languages remains challenging due to limited transcribed
speech data. While recent advances in large multilingual models like OpenAI's
Whisper offer promising pathways for low-resource ASR development, critical
questions persist regarding practical deployment requirements. This paper
addresses two fundamental concerns for practitioners: determining the minimum
data volumes needed for viable performance and characterizing the primary
failure modes that emerge in production systems. We evaluate Whisper's
performance through comprehensive experiments on two Bantu languages:
systematic data scaling analysis on Kinyarwanda using training sets from 1 to
1,400 hours, and detailed error characterization on Kikuyu using 270 hours of
training data. Our scaling experiments demonstrate that practical ASR
performance (WER < 13\%) becomes achievable with as little as 50 hours of
training data, with substantial improvements continuing through 200 hours (WER
< 10\%). Complementing these volume-focused findings, our error analysis
reveals that data quality issues, particularly noisy ground truth
transcriptions, account for 38.6\% of high-error cases, indicating that careful
data curation is as critical as data volume for robust system performance.
These results provide actionable benchmarks and deployment guidance for teams
developing ASR systems across similar low-resource language contexts. We
release accompanying and models see
https://github.com/SunbirdAI/kinyarwanda-whisper-eval

</details>


### [92] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: 本文提出了一种高效预训练小型语言模型(SLM)的框架，结合结构稀疏子网络初始化、进化搜索发现优质初始化点、以及从大模型进行知识蒸馏，大幅提升预训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型资源消耗巨大，小型模型需要在保持性能的同时降低计算成本。

Method: 提出利用结构稀疏子网初始化提升性能，结合进化搜索自动发现优质初始化，采用知识蒸馏加速训练和提高泛化能力。

Result: 在相同计算预算下，最佳模型以9.2倍更少的预训练token数达到与Pythia小型模型相似的验证困惑度。

Conclusion: 该方法显著提升了小型语言模型的预训练效率，为成本效益高的小型模型开发提供了可行路径。

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [93] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的个性化用户行为模拟方法Customer-R1，用于在线购物环境，能显著提升对用户下一步行为的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽能模拟步骤式人类行为，但通常为群体级策略，缺乏对个体用户角色的个性化建模，导致模拟不够精准。

Method: 提出基于强化学习的Customer-R1方法，将策略条件化于显式用户角色，利用行为正确性奖励信号优化下一步推理和行为生成。

Result: 在OPeRA数据集上的实验显示，Customer-R1在下一步行为预测上显著优于提示和监督微调基线，并且更好匹配用户实际行为分布。

Conclusion: Customer-R1有效提升了个性化步骤式用户行为模拟的真实度和准确性，对于个性化建模具有重要意义。

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [94] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: 本文提出了一个基于经济和金融顶级期刊中严格因果关系的全新因果推理基准，揭示当前大型语言模型在因果推理任务上的明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理基准存在合成数据依赖和领域覆盖有限的问题，无法真实评价LMM的因果推理能力。

Method: 构建了包含4万余评价项的因果推理基准，覆盖健康、环境、技术、法律和文化等多个领域，基于经济学中工具变量、双重差分和断点回归等严谨因果识别方法。

Result: 对8款先进大型语言模型进行评测，最高准确率仅57.6%，且模型规模与性能不成正比，先进推理模型仍难以正确识别因果关系。

Conclusion: 当前大型语言模型在因果推理方面能力不足，难以满足高风险应用对可靠因果推理的需求。

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [95] [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
*Zhivar Sourati,Zheng Wang,Marianne Menglin Liu,Yazhe Hu,Mengqing Guo,Sujeeth Bharadwaj,Kyu Han,Tao Sheng,Sujith Ravi,Morteza Dehghani,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出了LAD-RAG，一种布局感知的动态检索增强生成框架，用于提升视觉丰富文档中的多页问答表现。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成方法在处理视觉丰富文档时，仅编码孤立内容块，忽视结构和跨页依赖，且固定检索页数导致证据不全，影响回答质量。

Method: LAD-RAG在文档摄取阶段构建符号文档图，捕捉布局及跨页依赖，与神经嵌入共同表示文档；推理阶段由大语言模型代理动态交互两种索引，适应性检索证据。

Result: 在多个视觉丰富文档问答基准上，LAD-RAG实现了超过90%的完美召回率，提升召回率最高达20%，并在保持低延迟的同时提高问答准确率。

Conclusion: LAD-RAG有效整合布局结构与动态检索机制，显著提升了多页视觉丰富文档的问答性能。

Abstract: Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.

</details>


### [96] [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
*Xunyi Jiang,Dingyi Chang,Julian McAuley,Xin Xu*

Main category: cs.CL

TL;DR: 研究发现现有用于评估大型语言模型真实性的基准测试存在过时问题，导致评估结果不可靠。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和现实世界快速发展，而现有评估基准更新滞后，造成评估结果的时效性和可靠性问题。

Method: 系统调查了五个流行的真实性基准和八个不同年份发布的语言模型，设计了事实检索管线和三种指标来量化基准测试的衰老及其对真实性评估的影响。

Result: 实验结果显示广泛使用的真实性基准中有很大部分样本信息过时，导致对语言模型真实性的评估不准确。

Conclusion: 该研究提出了一个测试平台用于评估基准的可靠性，并呼吁更多关注评估基准的时效性问题。

Abstract: The rapid evolution of large language models (LLMs) and the real world has
outpaced the static nature of widely used evaluation benchmarks, raising
concerns about their reliability for evaluating LLM factuality. While
substantial works continue to rely on the popular but old benchmarks, their
temporal misalignment with real-world facts and modern LLMs, and their effects
on LLM factuality evaluation remain underexplored. Therefore, in this work, we
present a systematic investigation of this issue by examining five popular
factuality benchmarks and eight LLMs released across different years. An
up-to-date fact retrieval pipeline and three metrics are tailored to quantify
benchmark aging and its impact on LLM factuality evaluation. Experimental
results and analysis illustrate that a considerable portion of samples in the
widely used factuality benchmarks are outdated, leading to unreliable
assessments of LLM factuality. We hope our work can provide a testbed to assess
the reliability of a benchmark for LLM factuality evaluation and inspire more
research on the benchmark aging issue. Codes are available in
https://github.com/JiangXunyi/BenchAge.

</details>


### [97] [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
*Christos Ziakas,Nicholas Loo,Nishita Jain,Alessandra Russo*

Main category: cs.CL

TL;DR: 本文提出了Red-Bandit，一种能够在线适应不同攻击风格的自动红队框架，通过强化学习和多臂老虎机策略，高效发现和利用大型语言模型的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有自动红队方法难以高效适应特定模型在推理时的脆弱点，亟需一种能动态调整和针对性攻击的机制。

Method: Red-Bandit利用多参数高效的LoRA专家网络，每个专家专攻一种攻击风格，通过基于规则的安全模型进行强化学习训练；推理时采用多臂老虎机策略，根据模型响应安全性动态选择专家，平衡探索和利用。

Result: 在AdvBench基准测试中，Red-Bandit在充分探索条件下取得了最先进的攻击成功率（ASR@10），且生成的攻击提示更加易读（困惑度更低）。

Conclusion: Red-Bandit不仅提升了红队攻击的效果和质量，还通过其动态策略帮助诊断模型的特定安全漏洞，具有广泛应用前景。

Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.

</details>


### [98] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: 提出了HERO融合验证器信号和奖励模型，改进大语言模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 二元验证信号虽可靠但过于简单，难以处理部分正确或多样答案，限制了学习效果。

Method: HERO利用层次归一和方差感知加权整合验证器和奖励模型信号，实现稳定且细致的奖励优化。

Result: 在多数学理推理基准测试中，HERO性能优于仅用奖励模型或仅用验证器的基线，特别在复杂任务表现显著提升。

Conclusion: 混合奖励设计能保持验证器的稳定性，同时利用奖励模型的细腻反馈，提升大语言模型推理表现。

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [99] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的无参照法律领域大语言模型评估方法，将长回答分解为法律数据点，提高了评估的准确性和一致性，并公开了部分数据以推动研究。


<details>
  <summary>Details</summary>
Motivation: 现有法律领域大语言模型评估方法依赖昂贵的参考数据或标准化评估，存在显著局限，尤其在专家信任和评估可靠性方面表现不佳。

Method: 将长回答拆分为自包含的法律数据点，并设计一种无参照的评估方法，模拟律师的评估习惯。

Result: 该方法在专有数据集和开源LegalBench数据集上均优于多种基线方法，且与专家评估的一致性更高，提升了标注者间的一致性。

Conclusion: 该方法有效提升了法律领域大语言模型输出的评估质量和可信度，并通过开源部分数据促进该领域研究发展。

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [100] [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
*Jonggeun Lee,Woojung Song,Jongwook Han,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的PA-Tool方法，通过自动重命名工具组件以适配预训练知识，解决小型语言模型在工具选择和参数识别上的模式不匹配问题，大幅提升性能并减少错误。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在工具使用任务中因模式不匹配而频繁出现幻觉工具名称，影响性能。

Method: PA-Tool利用峰值性信号自动生成多个命名候选，选择输出集中度最高的名称，以实现工具模式对预训练知识的自适应对齐。

Result: 在MetaTool和RoTBench数据集上，PA-Tool提升了最多17个百分点的性能，模式不匹配错误减少了80%。

Conclusion: 通过适配工具模式而非强制模型适应，PA-Tool显著提升了小型模型的工具使用能力，实现了计算效率与性能的平衡。

Abstract: Small language models (SLMs) offer significant computational advantages for
tool-augmented AI systems, yet they struggle with tool-use tasks, particularly
in selecting appropriate tools and identifying correct parameters. A common
failure mode is schema misalignment: models hallucinate plausible but
non-existent tool names that reflect naming conventions internalized during
pretraining but absent from the provided tool schema. Rather than forcing
models to adapt to arbitrary schemas, we propose adapting schemas to align with
models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool
Schema Generation), a training-free method that leverages peakedness-a signal
from contamination detection indicating pretraining familiarity-to
automatically rename tool components. By generating multiple candidates and
selecting those with highest output concentration across samples, PA-Tool
identifies pretrain-aligned naming patterns. Experiments on MetaTool and
RoTBench show improvements of up to 17% points, with schema misalignment errors
reduced by 80%. PA-Tool enables small models to approach state-of-the-art
performance while maintaining computational efficiency for adaptation to new
tools without retraining. Our work demonstrates that schema-level interventions
can unlock the tool-use potential of resource-efficient models by adapting
schemas to models rather than models to schemas.

</details>


### [101] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 本文提出了一种动态调整评估标准的方法，用于提高大语言模型的训练效果，相较于静态评分标准有明显提升。


<details>
  <summary>Details</summary>
Motivation: 静态评分标准容易被模型利用且无法适应训练中出现的新需求，需要动态更新以更好地指导训练。

Method: 通过在线比较当前策略与参考策略生成的回答，对评分标准进行动态调整，持续发现与纠正训练中的错误。

Result: 在多个评测指标上，动态评分标准方法相比静态方法提升了一致性的表现，最多达到8%的提升。

Conclusion: 动态在线生成评分标准能够更有效地引导大语言模型训练，提高模型表现，并体现出如透明度、实用性、组织性和推理能力等关键特征。

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [102] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLMs）在没有具体细节指导下，通过内部知识进行自我纠正，特别是道德自我纠正的机制和效果。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs的内在自我纠正，尤其是道德自我纠正的有效性及其工作原理尚不清楚。

Method: 通过多轮交互实验，观察模型的性能收敛现象，并分析持续注入自我纠正指令如何激活道德概念、减少模型不确定性。

Result: 发现持续的自我纠正指令能够激活并稳固道德概念，从而减少不确定性，促进模型性能在多轮交互中收敛。

Conclusion: 道德自我纠正机制有助于性能稳定收敛，证明了其内在自我纠正的强大潜力。

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [103] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 本文提出的M-Thinker模型通过语言一致性奖励和跨语言思维对齐奖励，提升大规模推理模型在非英语语言上的表现和语言一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推理模型在处理非英语语言时存在语言不一致和推理路径错误导致准确率低的问题，影响用户体验和模型全球推广。

Method: 引入GRPO算法，设计语言一致性（LC）奖励和跨语言思维对齐（CTA）奖励，通过强化学习使模型在非英语推理路径上与英语路径相匹配。

Result: M-Thinker-1.5B/7B模型实现了接近100%的语言一致性，在多语言基准测试MMATH和PolyMath上表现优异，并对领域外语言展现良好泛化能力。

Conclusion: 通过奖励机制引导，M-Thinker有效解决了多语言推理中的语言一致性和推理准确率问题，推动了大规模推理模型的多语言应用和全球部署。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [104] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: 提出了一个针对真实商业场景设计的新型text-to-SQL基准数据集CORGI，用于评估大语言模型在复杂商业查询上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的text-to-SQL基准主要聚焦于事实检索，缺乏对真实商业环境中复杂、多步骤推理的考察，限制了模型在实际业务智能中的应用。

Method: 构建了基于真实企业场景（DoorDash、Airbnb、Lululemon）的合成数据库，并设计四类递增复杂度的业务查询：描述性、解释性、预测性和推荐性，考察模型的因果推理、时间预测和策略推荐能力。

Result: 发现当前大语言模型在高阶复杂业务查询上表现明显下降，特别是在准确预测和提供可行方案方面，整体执行成功率较现有BIRD基准低约21%。

Conclusion: CORGI基准揭示了主流大语言模型与真实商业智能需求之间的差距，推动未来模型针对复杂业务推理能力的提升。并且公开发布了数据集、评价框架及提交平台。

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


### [105] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: 本文提出了一种结合Transformer短期记忆和人工海马网络长期压缩记忆的长序列建模方法，显著提升性能并降低计算与内存消耗。


<details>
  <summary>Details</summary>
Motivation: 长序列建模中，RNN类模型的固定大小压缩记忆效率高但易丢失信息，Transformer全注意力机制虽然信息完整但计算和内存开销巨大，亟需一种兼顾效率与信息保真度的记忆框架。

Method: 受认知科学多存储模型启发，设计了一种记忆框架：用Transformer的KV缓存作为无损短期记忆，另通过可学习的人工海马网络（AHN）将超出窗口范围的信息循环压缩到固定大小的长期记忆中。采用现代RNN架构实现AHN，如Mamba2、DeltaNet等。

Result: 在LV-Eval和InfiniteBench长上下文基准测试中，AHN增强模型性能优于滑动窗口基线，达到甚至超越全注意力模型，同时推理FLOPs减少40.5%，内存缓存减少74.0%，且LV-Eval上的平均分从4.41提升至5.88。

Conclusion: 该方法有效结合了短期无损和长期压缩记忆，提升了长序列建模性能和效率，为实际应用中的长序列处理提供了可行方案。

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [106] [Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems](https://arxiv.org/abs/2510.06343)
*Fikret Mert Gültekin,Oscar Lilja,Ranim Khojah,Rebekka Wohlrab,Marvin Damschen,Mazen Mohamad*

Main category: cs.SE

TL;DR: 利用本地部署的大型语言模型（LLM）结合检索增强生成技术，辅助森林领域的网络安全风险评估，保证数据隐私，提升评估效率。


<details>
  <summary>Details</summary>
Motivation: 网络安全专家资源有限，软件工程师需承担更多安全评估任务，需工具辅助进行漏洞和威胁评估。

Method: 设计科学研究，采用12名专家的访谈、互动和调查，探讨本地部署的LLM在风险评估中的应用。

Result: LLM能生成初步风险评估，识别威胁，提供冗余检查，但需人工监督以保证准确性和合规，专家愿意在辅助角色中使用LLM。

Conclusion: 本地部署的基于LLM的辅助工具能有效支持安全关键领域的网络安全风险评估，促进风险评估流程的改进与应用。

Abstract: In safety-critical software systems, cybersecurity activities become
essential, with risk assessment being one of the most critical. In many
software teams, cybersecurity experts are either entirely absent or represented
by only a small number of specialists. As a result, the workload for these
experts becomes high, and software engineers would need to conduct
cybersecurity activities themselves. This creates a need for a tool to support
cybersecurity experts and engineers in evaluating vulnerabilities and threats
during the risk assessment process. This paper explores the potential of
leveraging locally hosted large language models (LLMs) with retrieval-augmented
generation to support cybersecurity risk assessment in the forestry domain
while complying with data protection and privacy requirements that limit
external data sharing. We performed a design science study involving 12 experts
in interviews, interactive sessions, and a survey within a large-scale project.
The results demonstrate that LLMs can assist cybersecurity experts by
generating initial risk assessments, identifying threats, and providing
redundancy checks. The results also highlight the necessity for human oversight
to ensure accuracy and compliance. Despite trust concerns, experts were willing
to utilize LLMs in specific evaluation and assistance roles, rather than solely
relying on their generative capabilities. This study provides insights that
encourage the use of LLM-based agents to support the risk assessment process of
cyber-physical systems in safety-critical domains.

</details>


### [107] [Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study](https://arxiv.org/abs/2510.06363)
*Ololade Babatunde,Tomisin Ayodabo,Raqibul Raqibul*

Main category: cs.SE

TL;DR: 本文研究了一种基于Git的作业提交系统，显著提升了高等教育中作业追踪和协作效率。


<details>
  <summary>Details</summary>
Motivation: 传统高等教育作业提交方法存在效率低和管理复杂的问题，亟需改进。

Method: 采用迭代软件开发及以用户为中心的设计方法，将系统整合于真实大学环境，并通过可用性测试及学生反馈进行评估。

Result: 结果显示该系统显著改善了作业追踪、协作和提交效率，85%的教师和84%的学生更倾向使用该系统，提交和审核时间减少38%，存储需求减少48%。

Conclusion: 基于Git的提交系统有效提升了教学环境中师生的参与度和管理效率，对软件工程等相关学科的教学具有重要启示。

Abstract: This study addresses challenges in traditional assignment submission methods
used in higher education by introducing and evaluating a customized Git-based
submission system. Employing iterative software development and user-centered
design methodologies, the system was integrated within a real-world university
environment. Empirical evaluation, including usability testing and student
feedback, indicated significant improvements in assignment tracking,
collaboration, and submission efficiency. Students reported positive
experiences using distributed version control workflows, highlighting improved
learning outcomes and reduced administrative burden. Challenges related to
initial adoption and student learning curves were identified and mitigated
through iterative improvements. The proposed system contributes practical
insights for integrating distributed version control into educational settings,
enhancing both instructor oversight and student engagement in software
engineering and related disciplines. Based on our results, the research showed
that 85% of instructors found the git based system easier to use, with 84% of
students preferring it over traditional methods, as it provides a 38% reduction
in time taken for submission and review, while also leading to a 48% reduction
in storage requirements.

</details>


### [108] [Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review](https://arxiv.org/abs/2510.06483)
*Judith Michael,Lukas Netz,Bernhard Rumpe,Ingo Müller,John Grundy,Shavindra Wickramathilaka,Hourieh Khalajzadeh*

Main category: cs.SE

TL;DR: 本综述评估了模型驱动工程(MDE)在视力障碍辅助软件开发中的应用，发现目前研究多依赖WCAG但缺乏具体建模技术和实证验证，导致支持不足。


<details>
  <summary>Details</summary>
Motivation: 软件应用对视力障碍用户存在使用障碍，MDE提供系统化方法有潜力提升无障碍兼容性。

Method: 系统文献综述，从447篇相关文献中筛选出30篇符合条件的研究，分析其对视力障碍无障碍的支持情况。

Result: 多数研究使用WCAG指导但多为项目定制，缺乏具体建模技巧和完整系统验证，实证数据和开发者无障碍专业知识有限。

Conclusion: 当前MDE在视力障碍无障碍支持方面不足，需提出研究议程以更有效地嵌入视力无障碍需求。

Abstract: Software applications often pose barriers for users with accessibility needs,
e.g., visual impairments. Model-driven engineering (MDE), with its systematic
nature of code derivation, offers systematic methods to integrate accessibility
concerns into software development while reducing manual effort. This paper
presents a systematic literature review on how MDE addresses accessibility for
vision impairments. From 447 initially identified papers, 30 primary studies
met the inclusion criteria. About two-thirds reference the Web Content
Accessibility Guidelines (WCAG), yet their project-specific adaptions and
end-user validations hinder wider adoption in MDE. The analyzed studies model
user interface structures, interaction and navigation, user capabilities,
requirements, and context information. However, only few specify concrete
modeling techniques on how to incorporate accessibility needs or demonstrate
fully functional systems. Insufficient details on MDE methods, i.e.,
transformation rules or code templates, hinder the reuse, generalizability, and
reproducibility. Furthermore, limited involvement of affected users and limited
developer expertise in accessibility contribute to weak empirical validation.
Overall, the findings indicate that current MDE research insufficiently
supports vision-related accessibility. Our paper concludes with a research
agenda outlining how support for vision impairments can be more effectively
embedded in MDE processes.

</details>


### [109] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: 本文提出了一种针对代码补全上下文收集的有效检索策略，提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要充分且相关的上下文信息来提升代码补全质量，但大规模代码库中上下文有限且易受噪声影响，构建有效上下文面临挑战。

Method: 设计并评估文件级和代码块级的检索策略，重点考察上下文大小和文件顺序对模型性能的影响，引入基于静态分析的代码块检索方法。

Result: 基于代码块的检索比文件级策略提升6%，相比无上下文基线提升16%，证明了检索粒度、顺序和混合策略的重要性。

Conclusion: 精准的上下文检索和组织策略能显著提升代码补全效果，对实际开发场景中的上下文收集至关重要。

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [110] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika Mäntylä*

Main category: cs.SE

TL;DR: AiSysRev是一款基于大语言模型的文献筛选工具，辅助系统评价中的论文筛选工作。


<details>
  <summary>Details</summary>
Motivation: 系统评价中，筛选大量文献费时费力，需要高效辅助工具。

Method: 基于大语言模型，通过Web应用接收论文标题和摘要，结合用户设定的纳排标准进行零样本或少样本筛选，同时支持人工复核。

Result: 使用AiSysRev对137篇论文进行试验，发现论文可分为易纳入、易排除及边界两类，边界类需人类介入处理。

Conclusion: 尽管大语言模型不能完全替代人工判断，但AiSysRev能显著减轻系统评价中大量文献筛选的工作量。

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [111] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: 本文研究了11家公司的大语言模型聊天机器人使用政策，探讨了影响政策制定的因素，旨在帮助管理者安全地将聊天机器人整合到开发流程中。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型聊天机器人在软件组织中应用的风险增加，迫切需要制定明确的使用政策以确保安全。

Method: 通过调研11家公司，分析它们如何制定相关政策以及影响这些政策的关键因素。

Result: 总结了影响政策制定的多种因素，为管理者提供了制定安全使用政策的参考。

Conclusion: 明确和合理的政策对于安全、高效地将大型语言模型聊天机器人整合进软件开发流程至关重要。

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [112] [Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis](https://arxiv.org/abs/2510.06844)
*Nicole Hoess,Carlos Paradis,Rick Kazman,Wolfgang Mauerer*

Main category: cs.SE

TL;DR: 该论文研究了软件挖掘工具在软件项目演化分析中的有效性威胁，通过复现已有研究，比较不同工具在数据抽取和结论上的差异，发现工具设计细节显著影响结果，强调了工具选择和复用的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前软件挖掘工具广泛应用于研究和实践，但其局限性和结果一致性尚未被充分理解，存在威胁有效性的风险。

Method: 通过文献回顾选取三项软件工程相关研究，使用四个独立挖掘工具进行正式复现，量化并定性比较数据、分析结果和结论的一致性。

Result: 发现工具设计和实现中的技术细节在复杂分析流程中积累，导致基线数据及统计分析结果有较大差异，进而影响研究结论。

Conclusion: 建议用户谨慎选择工具并评估其局限性，提倡工具复用。研究者和工具开发者应提供重现包和开展比较研究以减少不确定性。

Abstract: Context: Mining software repositories is a popular means to gain insights
into a software project's evolution, monitor project health, support decisions
and derive best practices. Tools supporting the mining process are commonly
applied by researchers and practitioners, but their limitations and agreement
are often not well understood.
  Objective: This study investigates some threats to validity in complex tool
pipelines for evolutionary software analyses and evaluates the tools' agreement
in terms of data, study outcomes and conclusions for the same research
questions.
  Method: We conduct a lightweight literature review to select three studies on
collaboration and coordination, software maintenance and software quality from
high-ranked venues, which we formally replicate with four independent,
systematically selected mining tools to quantitatively and qualitatively
compare the extracted data, analysis results and conclusions.
  Results: We find that numerous technical details in tool design and
implementation accumulate along the complex mining pipelines and can cause
substantial differences in the extracted baseline data, its derivatives,
subsequent results of statistical analyses and, under specific circumstances,
conclusions.
  Conclusions: Users must carefully choose tools and evaluate their limitations
to assess the scope of validity in an adequate way. Reusing tools is
recommended. Researchers and tool authors can promote reusability and help
reducing uncertainties by reproduction packages and comparative studies
following our approach.

</details>


### [113] [An empirical study on declined proposals: why are these proposals declined?](https://arxiv.org/abs/2510.06984)
*Masanari Kondo,Mahmoud Alfadel,Shane McIntosh,Yasutaka Kamei,Naoyasu Ubayashi*

Main category: cs.SE

TL;DR: 研究分析了Go语言开源项目中提案被拒的原因及预测方法，发现提案拒绝率高、决策耗时长，且提出了提案被拒的九大原因分类，同时展示了基于GPT的模型可有效早期预测提案结果。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目中的设计决策通过提案实现，但过程资源消耗大且导致贡献者挫败感，尤其是被拒时缺乏明确反馈，限制了流程优化与贡献者指导。

Method: 对Go语言项目的1091个提案进行混合方法实证研究，包括统计提案结果，构建拒绝原因分类，及评估大语言模型对提案结果的预测能力。

Result: 发现提案拒绝比率高、决策时间超过一个月，且只有14.7%的被拒提案会重新提交。定义了九大拒绝原因，如重复、用例有限、违反项目原则。GPT模型以F1=0.71的绩效实现早期预测。

Conclusion: 提案过程存在低效，系统化的拒绝原因分类和早期预测模型可改善审查效率，减少贡献者挫败，提高流程透明度和贡献质量。

Abstract: Design-level decisions in open-source software (OSS) projects are often made
through structured mechanisms such as proposals, which require substantial
community discussion and review. Despite their importance, the proposal process
is resource-intensive and often leads to contributor frustration, especially
when proposals are declined without clear feedback. Yet, the reasons behind
proposal rejection remain poorly understood, limiting opportunities to
streamline the process or guide contributors effectively. This study
investigates the characteristics and outcomes of proposals in the Go
programming language to understand why proposals are declined and how such
outcomes might be anticipated. We conduct a mixed-method empirical study on
1,091 proposals submitted to the Go project. We quantify proposal outcomes,
build a taxonomy of decline reasons, and evaluate large language models (LLMs)
for predicting these outcomes. We find that proposals are more often declined
than accepted, and resolution typically takes over a month. Only 14.7% of
declined proposals are ever resubmitted. Through qualitative coding, we
identify nine key reasons for proposal decline, such as duplication, limited
use cases, or violations of project principles. This taxonomy can help
contributors address issues in advance, e.g., checking for existing
alternatives can reduce redundancy. We also demonstrate that GPT-based models
can predict decline decisions early in the discussion (F1 score = 0.71 with
partial comments), offering a practical tool for prioritizing review effort.
Our findings reveal inefficiencies in the proposal process and highlight
actionable opportunities for improving both contributor experience and reviewer
workload by enabling early triage and guiding contributors to strengthen their
proposals using a structured understanding of past decline reasons.

</details>


### [114] [Human-aligned AI Model Cards with Weighted Hierarchy Architecture](https://arxiv.org/abs/2510.06989)
*Pengyue Yang,Haolin Jin,Qingwen Zeng,Jiawen Wen,Harry Rao,Huaming Chen*

Main category: cs.SE

TL;DR: 本文提出了一个名为CRAI-MCF的负责任AI模型卡框架，用以解决当前大型语言模型生态中模型发现和采用的困难，提供量化评估和跨模型比较机制。


<details>
  <summary>Details</summary>
Motivation: 当前领域特定大型语言模型增长迅速，但文档不一致、不完整且不平衡，导致用户难以发现和采用模型，现有文档框架缺少量化对比手段。

Method: 基于价值敏感设计（VSD）原则，通过对240个开源项目的实证分析，提炼217个参数，构建了含8个模块的价值导向架构，引入量化充分性标准，实现统一的跨模型比较。

Result: CRAI-MCF提供了技术、伦理和运营维度的平衡框架，使用户能够更高效、负责地评估、选择和采用大型语言模型。

Conclusion: CRAI-MCF能够改进模型文档的质量和可比性，促进负责任的模型采用，提升大型语言模型生态的整体健康和创新速度。

Abstract: The proliferation of Large Language Models (LLMs) has led to a burgeoning
ecosystem of specialized, domain-specific models. While this rapid growth
accelerates innovation, it has simultaneously created significant challenges in
model discovery and adoption. Users struggle to navigate this landscape due to
inconsistent, incomplete, and imbalanced documentation across platforms.
Existing documentation frameworks, such as Model Cards and FactSheets, attempt
to standardize reporting but are often static, predominantly qualitative, and
lack the quantitative mechanisms needed for rigorous cross-model comparison.
This gap exacerbates model underutilization and hinders responsible adoption.
To address these shortcomings, we introduce the Comprehensive Responsible AI
Model Card Framework (CRAI-MCF), a novel approach that transitions from static
disclosures to actionable, human-aligned documentation. Grounded in Value
Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240
open-source projects, distilling 217 parameters into an eight-module,
value-aligned architecture. Our framework introduces a quantitative sufficiency
criterion to operationalize evaluation and enables rigorous cross-model
comparison under a unified scheme. By balancing technical, ethical, and
operational dimensions, CRAI-MCF empowers practitioners to efficiently assess,
select, and adopt LLMs with greater confidence and operational integrity.

</details>


### [115] [Building an Open AIBOM Standard in the Wild](https://arxiv.org/abs/2510.07070)
*Gopi Krishnan Rajbahadur,Keheliya Gallaba,Elyas Rashno,Arthit Suriyawongkul,Karen Bennet,Kate Stewart,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文报告了AI材料清单（AIBOM）规范的开发过程，该规范扩展了ISO/IEC软件包数据交换标准，用于捕捉AI系统中的数据集和训练工件。


<details>
  <summary>Details</summary>
Motivation: 虽然开源社区驱动的软件工程标准日益重要，但在快速发展的AI领域，这类标准的制定过程尚未被充分研究。

Method: 采用行动研究方法，组织了90多名全球多方利益相关者，经过多个行动研究周期开发规范，并通过法规对齐、行业案例分析、专家访谈和工业案例研究四种方法验证规范。

Result: 开发了经过验证的AIBOM规范，成功覆盖AI组件需求，并确保了其与主要法规和伦理标准的一致性。

Conclusion: 本文不仅交付了一个有效的AI材料清单规范，还总结了标准制定过程中经验教训，为未来软件工程标准化工作提供参考。

Abstract: Modern software engineering increasingly relies on open, community-driven
standards, yet how such standards are created in fast-evolving domains like
AI-powered systems remains underexplored. This paper presents a detailed
experience report on the development of the AI Bill of Materials AIBOM
specification, an extension of the ISO/IEC 5962:2021 Software Package Data
Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI
components such as datasets and iterative training artifacts. Framed through
the lens of Action Research (AR), we document a global, multi-stakeholder
effort involving over 90 contributors and structured AR cycles. The resulting
specification was validated through four complementary approaches: alignment
with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000
standards), systematic mapping to six industry use cases, semi-structured
practitioner interviews, and an industrial case study. Beyond delivering a
validated artefact, our paper documents the process of building the AIBOM
specification in the wild, and reflects on how it aligns with the AR cycle, and
distills lessons that can inform future standardization efforts in the software
engineering community.

</details>


### [116] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 本文提出Secure-Instruct框架，通过自动合成安全与漏洞代码实例及微调指令，显著提升大语言模型生成安全代码的能力，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成安全代码的方法因数据集有限且不平衡，导致效果和泛化能力不足，亟需新的方法提升大语言模型的安全代码生成能力。

Method: Secure-Instruct通过自动合成高质量漏洞和安全代码样本，生成微调指令，并对大语言模型进行指令微调，使模型在任务描述与安全代码生成间更好对齐。

Result: 在两个基准测试CWEBench和CWEval上，Secure-Instruct对四种代表性大语言模型均显著提升安全代码生成能力和功能正确性，相较于预训练模型安全比例提升约14%，并优于SafeCoder。

Conclusion: Secure-Instruct有效提高了大语言模型自动生成安全代码的性能，兼顾安全性和功能正确性，优于当前最佳方法，展示了自动合成指令微调在代码安全领域的潜力。

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [117] [R3R: Decentralized Multi-Agent Collision Avoidance with Infinite-Horizon Safety](https://arxiv.org/abs/2510.06436)
*Thomas Marshall Vielmetti,Devansh R. Agrawal,Dimitra Panagou*

Main category: cs.MA

TL;DR: 本文提出了R3R，一种首次实现距离通信约束下多智能体运动规划的去中心化、异步且具有无限时间安全保障的框架。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体运动规划方法缺乏对通信受限系统的正式无限时间安全保障。

Method: 结合门控安全框架和几何约束（R-有界性），基于智能体通信半径限制规划轨迹，实现仅用局部信息即可保证轨迹长期安全。算法支持异步操作，适应时间变化网络。

Result: 在多达128辆Dubins车辆的高密度障碍环境仿真中，实现100%安全，性能随智能体密度而非问题规模扩展。

Conclusion: R3R为可扩展、具有理论保障的多智能体系统提供了一种实用的安全运动规划解决方案。

Abstract: Existing decentralized methods for multi-agent motion planning lack formal,
infinite-horizon safety guarantees, especially for communication-constrained
systems. We present R3R, to our knowledge the first decentralized and
asynchronous framework for multi-agent motion planning under distance-based
communication constraints with infinite-horizon safety guarantees for systems
of nonlinear agents. R3R's novelty lies in combining our gatekeeper safety
framework with a geometric constraint called R-Boundedness, which together
establish a formal link between an agent's communication radius and its ability
to plan safely. We constrain trajectories to within a fixed planning radius
that is a function of the agent's communication radius, which enables
trajectories to be shown provably safe for all time, using only local
information. Our algorithm is fully asynchronous, and ensures the forward
invariance of these guarantees even in time-varying networks where agents
asynchronously join, leave, and replan. We validate our approach in simulations
of up to 128 Dubins vehicles, demonstrating 100% safety in dense, obstacle rich
scenarios. Our results demonstrate that R3R's performance scales with agent
density rather than problem size, providing a practical solution for scalable
and provably safe multi-agent systems.

</details>
