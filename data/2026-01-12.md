<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 55]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings](https://arxiv.org/abs/2601.05271)
*Xiran Fan,Zhimeng Jiang,Chin-Chia Michael Yeh,Yuzhong Chen,Yingtong Dou,Menghai Pan,Yan Zheng*

Main category: cs.CL

TL;DR: 本文针对交易数据中商户字段的语义信息丢失问题，提出基于LLM嵌入的轻量级混合模型，显著提升交易分析效果。


<details>
  <summary>Details</summary>
Motivation: 现有交易分析模型依赖索引表示商户类别，导致丰富文本语义信息丧失，而LLM虽能弥补语义理解不足，但计算成本高，限制其实时金融应用，因此需寻找兼顾语义理解与计算效率的解决方案。

Method: 采用LLM生成语义嵌入作为初始输入，结合多源数据融合丰富商户字段信息，利用单词约束原则确保嵌入一致性，并通过噪声过滤和上下文增强提升数据质量，构建轻量级交易模型。

Result: 本文提出了一种混合框架，利用大型语言模型（LLM）生成的嵌入向量作为轻量级交易模型的语义初始化，解决了传统索引表示法导致的语义信息丢失问题，同时兼顾解释性和计算效率。该方法通过多源数据融合丰富商户分类字段，并采用单词约束原则保证嵌入的一致性，同时通过噪声过滤和上下文增强提升数据质量。大量实验证明，该方法在多个交易理解任务上显著提升了性能。

Conclusion: 基于LLM生成嵌入的混合框架有效解决了交易数据中语义信息的丢失，提升了交易理解任务的性能，兼顾了模型的解释性和运行效率。

Abstract: The ubiquity of payment networks generates vast transactional data encoding rich consumer and merchant behavioral patterns. Recent foundation models for transaction analysis process tabular data sequentially but rely on index-based representations for categorical merchant fields, causing substantial semantic information loss by converting rich textual data into discrete tokens. While Large Language Models (LLMs) can address this limitation through superior semantic understanding, their computational overhead challenges real-time financial deployment. We introduce a hybrid framework that uses LLM-generated embeddings as semantic initializations for lightweight transaction models, balancing interpretability with operational efficiency. Our approach employs multi-source data fusion to enrich merchant categorical fields and a one-word constraint principle for consistent embedding generation across LLM architectures. We systematically address data quality through noise filtering and context-aware enrichment. Experiments on large-scale transaction datasets demonstrate significant performance improvements across multiple transaction understanding tasks.

</details>


### [2] [The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques](https://arxiv.org/abs/2601.05358)
*Tim Menzner,Jochen L. Leidner*

Main category: cs.CL

TL;DR: 本文提出了一个细粒度的句子级媒体偏见分类体系，涵盖38种基本偏见类型，横跨六大功能类别，以更准确识别和理解媒体偏见。


<details>
  <summary>Details</summary>
Motivation: 当前关于媒体左右翼偏见的辩论忽视了偏见具体的语言表达方式，研究旨在转变视角，关注单句层面如何体现偏见。

Method: 通过分析26,464句新闻语料，结合细读、跨学科理论与试点注释，迭代构建了一个两层次、包含38种偏见类型的媒体偏见分类方案。

Result: 构建了一个包含38种偏见类型的两层分类框架，并通过随机样本量化调查展示其偏见类型的普遍性，同时与现有NLP及传播科学分类体系的映射证明了其优越性。

Conclusion: 该研究开发了一个详尽的媒体偏见元素表，提供偏见定义、实例及识别指导，实现了对现有分类体系的显著补充和歧义减少。

Abstract: Public debates about "left-" or "right-wing" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a "table of media-bias elements". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.

</details>


### [3] [Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models](https://arxiv.org/abs/2601.05366)
*Zheng Luo,T Pranav Kutralingam,Ogochukwu N Okoani,Wanpeng Xu,Hua Wei,Xiyang Hu*

Main category: cs.CL

TL;DR: 本研究提出MLCL基准，发现多语言环境下大型语言模型调用工具时的主要失败原因是参数语言不匹配，虽然策略能缓解问题但仍难超越英文表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于英文环境下的工具调用表现，缺乏对多语言用户交互中鲁棒性的探索。

Method: 引入了MLCL诊断基准，系统评估多语言环境下的大型语言模型调用外部工具的能力，重点分析模型在中文、印地语和低资源语种伊博语中的表现。

Result: 通过细粒度错误分析，发现很多报错源自参数值的语言不匹配问题，即模型生成了符合用户语言但违反执行语言不变规定的参数值。测试了多种推理时系统策略，虽显著减少了语言引起的执行错误，但尚未达到英语水平的性能。

Conclusion: 多语言工具调用存在语言参数值不匹配的主要障碍，现有方法虽有效但难以完全恢复英语环境下的工具调用性能，提示未来需加强多语言执行一致性设计。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.

</details>


### [4] [Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection](https://arxiv.org/abs/2601.05403)
*Zhiwei Liu,Yupen Cao,Yuechen Jiang,Mohsinul Kabir,Polydoros Giannouris,Chen Xu,Ziyang Xu,Tianlei Zhu,Tariquzzaman Faisal,Triantafillos Papadopoulos,Yan Wang,Lingfei Qian,Xueqing Peng,Zhuohan Xie,Ye Yuan,Saeed Almheiri,Abdulrazzaq Alnajjar,Mingbin Chen,Harry Stuart,Paul Thompson,Prayag Tiwari,Alejandro Lopez-Lira,Xue Liu,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文针对金融误信息检测任务，提出多语言多场景的行为偏差评估基准，发现大型语言模型普遍存在行为偏差。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型偏差的研究多集中于直接提问或简单场景，缺乏对复杂金融环境及高风险、多语言金融误信息检测的偏差分析。

Method: 构建三种基于角色、人格、地区、种族和宗教的金融场景，结合多语言误信息数据集，对22个主流大型语言模型进行系统评估。

Result: 本文提出了一个名为\mfmdscen的综合基准，用于评估大型语言模型（LLMs）在多语言金融误信息检测任务中的行为偏差。通过与金融专家合作，构建了三种复杂金融场景，并开发了涵盖英语、中文、希腊语和孟加拉语的误信息数据集。基准涵盖22个主流LLMs，结果显示商业和开源模型均存在显著行为偏差。

Conclusion: 大型语言模型在复杂的金融误信息任务中仍存在显著行为偏差，需要进一步研究和改进。

Abstract: Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\mfmd). In this work, we propose \mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD.

</details>


### [5] [Glitter: Visualizing Lexical Surprisal for Readability in Administrative Texts](https://arxiv.org/abs/2601.05411)
*Jan Černý,Ivana Kvapilíková,Silvie Cinková*

Main category: cs.CL

TL;DR: 通过多语言模型估算文本信息熵，可视化分析文本可读性，致力于提升官方文本的易读性。


<details>
  <summary>Details</summary>
Motivation: 利用信息熵指标来评估文本的可读性，以提升行政和官僚文本的清晰度。

Method: 提出一种基于多语言模型估算文本信息熵并进行可视化的框架。

Result: 开发了一个可视化工具集，能够近似计算文本信息熵，并以开源形式发布。

Conclusion: 该方法为评估和改善行政文本的可读性提供了有效工具，促进文本表达更为清晰明了。

Abstract: This work investigates how measuring information entropy of text can be used to estimate its readability. We propose a visualization framework that can be used to approximate information entropy of text using multiple language models and visualize the result. The end goal is to use this method to estimate and improve readability and clarity of administrative or bureaucratic texts. Our toolset is available as a libre software on https://github.com/ufal/Glitter.

</details>


### [6] [Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency](https://arxiv.org/abs/2601.05905)
*Haoming Xu,Ningyuan Zhao,Yunzhi Yao,Weihong Xu,Hongru Wang,Xinle Deng,Shumin Deng,Jeff Z. Pan,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了邻居一致性信念（NCB）作为评估大型语言模型（LLMs）在语境干扰下信念稳健性的新指标，发现传统的自我一致性指标掩盖了模型在轻微干扰下容易崩溃的问题。通过认知压力测试验证，NCB较高的数据表现出更强的抗干扰能力，且结构感知训练（SAT）方法能显著提升模型的信念稳健性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在实际环境中的广泛应用，单纯正确率已不能满足需求，模型需要在语境干扰下保持可信的信念。现有的主要评价指标（如自我一致性）未能揭示模型信念的脆弱性。

Method: 提出邻居一致性信念（NCB）作为结构化信念稳健性度量指标，设计认知压力测试协议验证其有效性，最后采用结构感知训练（SAT）优化模型的上下文不变信念结构。

Result: 实验显示高NCB数据对语境干扰更具抵抗力，结构感知训练方法降低了长尾知识的脆弱性约30%，提升了模型在复杂环境下的表现。

Conclusion: NCB能够更有效地评估模型信念的稳健性，SAT训练方法提升了模型在长尾知识上的稳定性，减少了约30%的脆弱性，增强了模型在真实环境中的可靠性。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.

</details>


### [7] [Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions](https://arxiv.org/abs/2601.05414)
*Minda Zhao,Yilun Du,Mengyu Wang*

Main category: cs.CL

TL;DR: 本文首次大规模审计了前沿大语言模型在原生概率采样上的表现，发现批量生成和独立请求两种协议下采样准确率均低，且随分布复杂度和采样规模增大而下降。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用于教育评估、合成数据生成等需要准确概率采样的场景，评估其能否从指定分布准确采样成为必要。

Method: 设计双协议审计方法：批量生成（一次生成1000个样本）与独立请求（1000次独立调用），对11个模型在15种不同分布上进行采样准确性测试。

Result: 批量生成通过率中等（13%），独立请求几乎全部失败（10/11模型未通过任何分布测试）；采样准确率随分布复杂度和样本数量增加而下降，且在多项任务（MCQ生成、文本到图像）中采样错误传导导致结果偏差。

Conclusion: 当前大语言模型缺乏有效的内部采样机制，无法保证概率分布采样的统计准确性，需借助外部工具以满足统计性能要求。

Abstract: As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.

</details>


### [8] [Can We Predict Before Executing Machine Learning Agents?](https://arxiv.org/abs/2601.05930)
*Jingsheng Zheng,Jintian Zhang,Yujie Luo,Yuren Mao,Yunjun Gao,Lun Du,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出用LLMs预测解决方案优先级，绕过高成本物理验证，显著提升自动化科学发现效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化机器学习方法依赖昂贵的物理执行进行假设评估，存在执行瓶颈，限制了效率。

Method: 提出了一种Data-centric Solution Preference任务，利用大语言模型（LLMs）通过预测-验证循环（Predict-then-Verify loop）替代传统的物理执行瓶颈，加快科学发现过程。

Result: 通过构建包含18,438对比数据的语料库，验证了LLMs在预测解决方案优劣上的能力，模型准确率达61.5%，并在代理FOREAGENT中实现了6倍加速和6%的性能提升。

Conclusion: 利用大语言模型的预测能力替代物理执行瓶颈，实现了科学发现过程的加速与性能提升，验证了预测-验证框架的有效性。

Abstract: Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.

</details>


### [9] [Tracing Moral Foundations in Large Language Models](https://arxiv.org/abs/2601.05437)
*Chenxiao Yu,Bowen Yi,Farzan Karimi-Malekabadi,Suhaib Abdurahman,Jinyi Ye,Shrikanth Narayanan,Yue Zhao,Morteza Dehghani*

Main category: cs.CL

TL;DR: 研究表明大型语言模型的道德判断基于内在复杂的多层次结构，具备一定的认知特性，而非仅仅道德模仿。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型产生类人道德判断背后的内部结构，区分真实认知结构与表面模仿。

Method: 结合分层分析、预训练稀疏自编码器识别关键特征以及因果干预，研究了两种指令调优的LLM中道德基础的表达和组织方式。

Result: 两种模型在不同层次上表现出对应人类道德感知的结构化道德基础，且通过稀疏特征发现部分解耦机制，干预实验验证了内在表示与道德行为之间的因果关系。

Conclusion: 大型语言模型的道德概念是分布式、多层次且部分解开耦合的，体现了复杂的内部结构而非简单的模仿。

Abstract: Large language models (LLMs) often produce human-like moral judgments, but it is unclear whether this reflects an internal conceptual structure or superficial ``moral mimicry.'' Using Moral Foundations Theory (MFT) as an analytic framework, we study how moral foundations are encoded, organized, and expressed within two instruction-tuned LLMs: Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. We employ a multi-level approach combining (i) layer-wise analysis of MFT concept representations and their alignment with human moral perceptions, (ii) pretrained sparse autoencoders (SAEs) over the residual stream to identify sparse features that support moral concepts, and (iii) causal steering interventions using dense MFT vectors and sparse SAE features. We find that both models represent and distinguish moral foundations in a structured, layer-dependent way that aligns with human judgments. At a finer scale, SAE features show clear semantic links to specific foundations, suggesting partially disentangled mechanisms within shared representations. Finally, steering along either dense vectors or sparse features produces predictable shifts in foundation-relevant behavior, demonstrating a causal connection between internal representations and moral outputs. Together, our results provide mechanistic evidence that moral concepts in LLMs are distributed, layered, and partly disentangled, suggesting that pluralistic moral structure can emerge as a latent pattern from the statistical regularities of language alone.

</details>


### [10] [Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction](https://arxiv.org/abs/2601.05459)
*Hongjin Kim,Jaewook Lee,Kiyoung Lee,Jong-hun Shin,Soojong Lim,Oh-Woog Kwon*

Main category: cs.CL

TL;DR: 研究表明，韩语推理能力提升关键在于模型内部推理过程的对齐，强化学习辅以特定神经元调优和自纠正数据集能显著提升多语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习是否能提升模型在低资源语言（韩语）上的推理能力，达到与高资源语言（英语）相当的水平。

Method: 通过强化学习结合专门调优韩语特定神经元，并引入代码切换的自纠正数据集，实现模型内部推理过程与韩语输入的对齐。

Result: 强化学习单独应用效果有限，通过调优特定神经元和对齐内部推理流程显著提升了数学推理和自纠正任务的性能。

Conclusion: 在多语言推理能力提升中，关键不在于注入新的语言知识，而是有效激发和对齐模型已有的推理能力。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model's internal reasoning processes with Korean inputs-particularly by tuning Korean-specific neurons in early layers-is key to unlocking RL's effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.

</details>


### [11] [Towards Valid Student Simulation with Large Language Models](https://arxiv.org/abs/2601.05473)
*Zhihao Yuan,Yunze Xiao,Ming Li,Weihao Xuan,Richard Tong,Mona Diab,Tom Mitchell*

Main category: cs.CL

TL;DR: 本文提出了基于认知状态规范的学生模拟框架，解决大语言模型模拟学生的‘能力悖论’，强调认知真实感以提高模拟的科学和教学价值。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型模拟学生时出现的‘能力悖论’问题，即模型过于强大却模拟部分知识的学生，导致错误模式和学习动态不真实。

Method: 将学生模拟重构为受约束生成问题，通过明确的认知状态规范（Epistemic State Specification, ESS）控制模拟学生的知识访问、错误结构和状态演变，引入目标-环境框架明确行为目标和部署环境。

Result: 提出了一个概念和方法框架，规范模拟学生的设计维度，强调知识状态的真实再现，系统总结了相关文献，指出了效度、评估和伦理风险等开放问题。

Conclusion: 相比表面现实感，认知状态的真实再现是使用基于大语言模型的模拟学生作为科学和教学工具的关键前提。

Abstract: This paper presents a conceptual and methodological framework for large language model (LLM) based student simulation in educational settings. The authors identify a core failure mode, termed the "competence paradox" in which broadly capable LLMs are asked to emulate partially knowledgeable learners, leading to unrealistic error patterns and learning dynamics. To address this, the paper reframes student simulation as a constrained generation problem governed by an explicit Epistemic State Specification (ESS), which defines what a simulated learner can access, how errors are structured, and how learner state evolves over time. The work further introduces a Goal-by-Environment framework to situate simulated student systems according to behavioral objectives and deployment contexts. Rather than proposing a new system or benchmark, the paper synthesizes prior literature, formalizes key design dimensions, and articulates open challenges related to validity, evaluation, and ethical risks. Overall, the paper argues for epistemic fidelity over surface realism as a prerequisite for using LLM-based simulated students as reliable scientific and pedagogical instruments.

</details>


### [12] [The Facade of Truth: Uncovering and Mitigating LLM Susceptibility to Deceptive Evidence](https://arxiv.org/abs/2601.05478)
*Herun Wan,Jiaying Wu,Minnan Luo,Fanxiao Li,Zhi Zeng,Min-Yen Kan*

Main category: cs.CL

TL;DR: 该研究发现LLMs对复杂误导证据存在显著脆弱性，提出MisBelief框架用于生成此类误导证据，并设计DIS机制有效防范。


<details>
  <summary>Details</summary>
Motivation: 为了增强大型语言模型在辅助人类决策时的事实信念坚定性，避免模型因复杂且难以反驳的误导证据而产生错误判断，保障决策质量，必须系统探究并防范此类脆弱性。

Method: 提出MisBelief框架，通过多角色、多轮次协作推理生成逻辑上有说服力但实际上误导的证据；并设计DIS治理机制，通过识别潜在欺骗意图来抑制误信和信念转变。

Result: 该论文揭示了大型语言模型（LLMs）在面对复杂且难以反驳的误导性证据时存在的根本脆弱性。提出了一个名为MisBelief的框架，通过多角色、多轮次的协作交互生成具有逻辑说服力但事实误导的证据。通过该框架，生成了4800个不同难度级别的测试实例，评估了7个代表性LLMs，结果显示模型对直接错误信息的抵抗力较强，但对误导性细化证据极为敏感，误信虚假信息的概率增加93%，严重影响后续推荐。针对这一问题，作者提出了欺骗意图屏蔽（DIS）机制，该机制通过推断证据背后的欺骗意图提供预警信号，有效减缓了信念转变，促进了更谨慎的证据评估。

Conclusion: LLMs虽能抵抗直接错误信息，但易受复杂误导性证据影响，DIS机制可显著减缓这一脆弱性，提升模型的事实坚定性和决策可靠性。

Abstract: To reliably assist human decision-making, LLMs must maintain factual internal beliefs against misleading injections. While current models resist explicit misinformation, we uncover a fundamental vulnerability to sophisticated, hard-to-falsify evidence. To systematically probe this weakness, we introduce MisBelief, a framework that generates misleading evidence via collaborative, multi-round interactions among multi-role LLMs. This process mimics subtle, defeasible reasoning and progressive refinement to create logically persuasive yet factually deceptive claims. Using MisBelief, we generate 4,800 instances across three difficulty levels to evaluate 7 representative LLMs. Results indicate that while models are robust to direct misinformation, they are highly sensitive to this refined evidence: belief scores in falsehoods increase by an average of 93.0\%, fundamentally compromising downstream recommendations. To address this, we propose Deceptive Intent Shielding (DIS), a governance mechanism that provides an early warning signal by inferring the deceptive intent behind evidence. Empirical results demonstrate that DIS consistently mitigates belief shifts and promotes more cautious evidence evaluation.

</details>


### [13] [MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards](https://arxiv.org/abs/2601.05488)
*Zhiyu Shen,Ziming Wu,Fuming Lai,Shaobing Lian,Yanghui Rao*

Main category: cs.CL

TL;DR: 提出MemBuilder框架，用强化学习优化多维记忆构建，通过密集奖励和贡献感知梯度权重，提升长对话一致性。


<details>
  <summary>Details</summary>
Motivation: 当前长对话中保持一致性是LLM的基本挑战，现有的检索机制难以捕捉历史状态的时间演变。

Method: MemBuilder基于强化学习，采用合成会话级问题生成提供密集中间奖励，并利用贡献感知梯度权重调整策略更新，优化多维记忆构建。

Result: MemBuilder使得一个4B参数的模型超过了现有最先进的闭源模型，在长对话基准测试中表现出强泛化能力。

Conclusion: MemBuilder通过解决奖励稀疏和多维记忆归因问题，显著提升了模型在长对话任务中的表现和泛化能力。

Abstract: Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of closed-source models or suffer from ineffective training paradigms with sparse rewards. We introduce MemBuilder, a reinforcement learning framework that trains models to orchestrate multi-dimensional memory construction with attributed dense rewards. MemBuilder addresses two key challenges: (1) Sparse Trajectory-Level Rewards: we employ synthetic session-level question generation to provide dense intermediate rewards across extended trajectories; and (2) Multi-Dimensional Memory Attribution: we introduce contribution-aware gradient weighting that scales policy updates based on each component's downstream impact. Experimental results show that MemBuilder enables a 4B-parameter model to outperform state-of-the-art closed-source baselines, exhibiting strong generalization across long-term dialogue benchmarks.

</details>


### [14] [FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse](https://arxiv.org/abs/2601.05505)
*Yubo Hou,Zhisheng Chen,Tao Wan,Zengchang Qin*

Main category: cs.CL

TL;DR: FlashMem通过直接利用推理状态提取内存，提升推理效率五倍，实现持久记忆与推理骨干的无缝结合。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的无状态架构缺乏动态上下文的持久机制，导致在长时程自主管理中需要反复重新处理历史信息，效率低下。现有的潜在记忆方法存在架构隔离问题，内存与推理骨干分离。

Method: 提出FlashMem框架，通过计算重用从瞬时推理状态中提取内在记忆。利用内部表示唯一编码输入轨迹的特性，将最后的隐藏状态视为交互历史的充分统计量。采用共享键值整合器直接访问骨干模型的冻结缓存，避免冗余参数重构；通过无参数认知监控器基于注意力熵自适应触发记忆整合。

Result: FlashMem实现了与大型基线模型相当的性能，同时推理延迟减少五倍，显著提升了效率，成功在效率和持续认知之间建立桥梁。

Conclusion: FlashMem有效解决了大型语言模型中记忆和推理骨干分离的问题，通过高效的记忆整合机制，实现了高性能与低延迟的平衡，推动了长时程自主管理能力的发展。

Abstract: The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.

</details>


### [15] [CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems](https://arxiv.org/abs/2601.05520)
*Xuemei Tang,Chengxi Yan,Jinghang Gu,Chu-Ren Huang*

Main category: cs.CL

TL;DR: 提出了CHisAgent多智能体框架，通过多阶段方法自动构建中国古代历史事件分类，提升了分类的结构性和完整性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在中国历史等非英语语境下的推理能力有限，手工构建历史分类成本高且难扩展，故提出自动化多智能体框架以提升历史知识组织效能。

Method: 设计了三个角色分工的阶段：(1)底层诱导者从原始语料中生成初始层级结构；(2)顶层扩展者利用LLM世界知识补充中间概念；(3)证据充实者融合外部结构化资源保证分类真实性。

Result: 本文针对大型语言模型在历史文化推理中的局限，提出了CHisAgent，一种适用于中国古代背景的多智能体历史分类构建框架。该框架通过三个阶段（底层诱导者、顶层扩展者、证据充实者）系统性构建历史事件分类，结合原始历史语料、LLM世界知识及外部结构化历史资源，最终构建了涵盖古代中国政治、军事、外交与社会生活的规模化事件分类。评估表明该分类结构具有更好的结构连贯性和覆盖范围，并支持跨文化对齐。

Conclusion: CHisAgent有效整合多源信息，自动构建了结构合理、覆盖广泛的中国古代历史事件分类，促进了历史知识的组织与理解。

Abstract: Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.

</details>


### [16] [Double: Breaking the Acceleration Limit via Double Retrieval Speculative Parallelism](https://arxiv.org/abs/2601.05524)
*Yuhao Shen,Tianyu Liu,Junyi Shen,Jinyang Wu,Quan Kong,Li Huan,Cong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Double的双重检索投机并行框架，通过同步机制解决了传统并行投机解码中的速度和效率瓶颈，实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 传统并行投机解码受模型速度比限制且存在中序列标记拒绝导致的计算浪费与流水线阻塞，亟需突破这些根本性挑战。

Method: 通过引入同步机制，允许草稿模型执行迭代检索投机，并使目标模型进行权威检索以生成多标记指导，从而消除草稿模型的错误回滚。

Result: 在LLaMA3.3-70B和Qwen3-32B模型上，Double分别达到了5.3倍和2.8倍的加速，显著优于需要大量模型训练的EAGLE-3方法。

Conclusion: Double框架突破了传统并行投机解码的速度上限，有效减少了计算浪费，实现了在大型模型上的5.3倍和2.8倍加速，性能优于需重新训练模型的先进方法。

Abstract: Parallel Speculative Decoding (PSD) accelerates traditional Speculative Decoding (SD) by overlapping draft generation with verification. However, it remains hampered by two fundamental challenges: (1) a theoretical speedup ceiling dictated by the speed ratio between the draft and target models, and (2) high computational waste and pipeline stall due to mid-sequence token rejections of early errors. To address these limitations, we introduce \textsc{Double} (Double Retrieval Speculative Parallelism). By bridging the gap between SD and PSD, our framework resolves the Retrieval \emph{Precision-Efficiency Dilemma} through a novel synchronous mechanism. Specifically, we enable the draft model to execute iterative retrieval speculations to break the theoretical speedup limits; to alleviate rejections without rollback, the target model performs authoritative retrieval to generate multi-token guidance. \textsc{Double} is entirely training-free and lossless. Extensive experiments demonstrate state-of-the-art speedup of $\textbf{5.3}\times$ on LLaMA3.3-70B and $\textbf{2.8}\times$ on Qwen3-32B, significantly outperforming the advanced method EAGLE-3 that requires extensive model training.

</details>


### [17] [Closing the Modality Reasoning Gap for Speech Large Language Models](https://arxiv.org/abs/2601.05543)
*Chaoren Wang,Heng Lu,Xueyao Zhang,Shujie Liu,Yan Lu,Jinyu Li,Zhizheng Wu*

Main category: cs.CL

TL;DR: 本文提出了TARS强化学习框架，通过对齐语音和文本条件下的表示和行为，有效提升了语音大语言模型的推理能力，缩小了与文本模态的性能差距。


<details>
  <summary>Details</summary>
Motivation: 语音大语言模型在语音输入上的推理性能显著弱于文本输入，存在模态推理差距，这一差距可能源于Transformer层间的表示漂移和长链推理中的行为偏差。

Method: 设计了TARS强化学习框架，通过非对称奖励机制，利用表示对齐和行为对齐两个密集信号，实现语音和文本条件轨迹的对齐，从而提升语音推理性能。

Result: 提出的TARS框架显著缩小了语音与文本模态推理的差距，在MMSU和OBQA等复杂推理基准上达到7B规模语音大语言模型中的最先进性能。

Conclusion: TARS框架通过表示和行为的双重对齐，有效提升了语音大语言模型的推理性能，证明了模态间对齐策略在缩小语音与文本推理差距中的有效性。

Abstract: Although speech large language models have achieved notable progress, a substantial modality reasoning gap remains: their reasoning performance on speech inputs is markedly weaker than on text. This gap could be associated with representational drift across Transformer layers and behavior deviations in long-chain reasoning. To address this issue, we introduce TARS, a reinforcement-learning framework that aligns text-conditioned and speech-conditioned trajectories through an asymmetric reward design. The framework employs two dense and complementary signals: representation alignment, which measures layer-wise hidden-state similarity between speech- and text-conditioned trajectories, and behavior alignment, which evaluates semantic consistency between generated outputs and reference text completions. Experiments on challenging reasoning benchmarks, including MMSU and OBQA, show that our approach significantly narrows the modality reasoning gap and achieves state-of-the-art performance among 7B-scale Speech LLMs.

</details>


### [18] [Can Large Language Models Differentiate Harmful from Argumentative Essays? Steps Toward Ethical Essay Scoring](https://arxiv.org/abs/2601.05545)
*Hongjin Kim,Jeonghyun Kang,Harksoo Kim*

Main category: cs.CL

TL;DR: 本研究提出了一个新的有害作文检测基准，评估自动作文评分系统和大型语言模型在识别和评分有害内容中的表现，发现现有模型在辨别伦理问题方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 尽管自动作文评分技术有所进步，但现有模型往往忽视作文中的伦理和道德问题，错误地给予有害内容高分，亟需改进模型以解决这一问题。

Method: 构建Harmful Essay Detection (HED)基准，包含涉及种族主义和性别偏见的敏感话题作文，并用该基准测试多种大型语言模型对有害内容的识别和评分能力。

Result: 研究发现大型语言模型需要进一步增强以区分有害作文和论证合理的作文，现有系统普遍忽视内容的伦理维度，导致评分结果不合理。

Conclusion: 当前的自动作文评分系统和大型语言模型在识别和正确评分含有种族主义、性别偏见等伦理问题的有害作文方面表现不佳，亟需开发对伦理维度更加敏感的评分系统。

Abstract: This study addresses critical gaps in Automated Essay Scoring (AES) systems and Large Language Models (LLMs) with regard to their ability to effectively identify and score harmful essays. Despite advancements in AES technology, current models often overlook ethically and morally problematic elements within essays, erroneously assigning high scores to essays that may propagate harmful opinions. In this study, we introduce the Harmful Essay Detection (HED) benchmark, which includes essays integrating sensitive topics such as racism and gender bias, to test the efficacy of various LLMs in recognizing and scoring harmful content. Our findings reveal that: (1) LLMs require further enhancement to accurately distinguish between harmful and argumentative essays, and (2) both current AES models and LLMs fail to consider the ethical dimensions of content during scoring. The study underscores the need for developing more robust AES systems that are sensitive to the ethical implications of the content they are scoring.

</details>


### [19] [AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor](https://arxiv.org/abs/2601.05752)
*Shu Yang,Jingyu Hu,Tong Li,Hanqi Yan,Wenxuan Wang,Di Wang*

Main category: cs.CL

TL;DR: 本文提出了首个系统评估LLM误行为监测器可靠性的基准AutoMonitor-Bench，揭示监测时漏报与误报的性能权衡，并展示大规模微调提升监测性能的尝试和存在的难题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估大语言模型(LLM)误行为监测器可靠性的基准，且监测存在漏报与误报的安全与实用性权衡问题，迫切需要系统工具与方法提升监控的稳定性与准确性。

Method: 设计AutoMonitor-Bench基准，收集3010个标注样本覆盖问答、代码生成与推理任务，并使用12个专有与10个开源大模型进行性能评估；构建15.3万样本的大规模训练数据，微调Qwen3-4B-Instruction模型以提升监测性能。

Result: 观察到监测性能在不同模型间差异显著，存在漏报率(MR)与误报率(FAR)的明显权衡，通过大规模微调提升部分性能，但对隐蔽误行为的检测仍具挑战。

Conclusion: 监测LLM误行为存在固有的安全与实用性冲突，通过设计针对性任务和训练策略提升监控效果仍需深入研究，未来工作应聚焦于可靠、可扩展的任务感知监测器设计与训练。

Abstract: We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.

</details>


### [20] [Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation](https://arxiv.org/abs/2601.05548)
*Jeonghyun Kang,Hongjin Kim,Harksoo Kim*

Main category: cs.CL

TL;DR: KEEM数据集通过生成综合记忆，整合情感与事实信息，提升长时对话系统的记忆更新和响应能力。


<details>
  <summary>Details</summary>
Motivation: 现有长时对话系统的记忆更新方法存在信息冲突和状态跟踪困难，难以准确反映用户情感和事实状态，影响对话质量，需要一种能够融合情感与事实的记忆更新机制。

Method: 采用生成式方法动态创建综合记忆，结合事实、情感和因果关系，实现系统记忆的实时更新。

Result: 本文提出了Keep Emotional and Essential Memory (KEEM)数据集，一种基于生成的长时记忆更新数据集。与传统简单积累或基于操作的方法不同，KEEM能动态生成综合记忆，兼顾事实信息、情感上下文和因果关系，从而更准确地跟踪用户当前状态。该方法通过无缝更新包含情感和关键信息的记忆，提升了系统在开放域对话中的共情能力和响应质量。

Conclusion: KEEM方法有效解决了传统长时记忆更新中的信息冲突和状态跟踪困难，提升了系统的共情能力和开放域对话表现。

Abstract: In this work, we introduce the Keep Emotional and Essential Memory (KEEM) dataset, a novel generation-based dataset designed to enhance memory updates in long-term conversational systems. Unlike existing approaches that rely on simple accumulation or operation-based methods, which often result in information conflicts and difficulties in accurately tracking a user's current state, KEEM dynamically generates integrative memories. This process not only preserves essential factual information but also incorporates emotional context and causal relationships, enabling a more nuanced understanding of user interactions. By seamlessly updating a system's memory with both emotional and essential data, our approach promotes deeper empathy and enhances the system's ability to respond meaningfully in open-domain conversations.

</details>


### [21] [ReasonAny: Incorporating Reasoning Capability to Any Model via Simple and Effective Model Merging](https://arxiv.org/abs/2601.05560)
*Junyao Yang,Chen Qian,Dongrui Liu,Wen Shen,Yong Liu,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出ReasonAny框架，通过对比梯度识别解决了领域专用模型与大推理模型合并时的性能崩溃问题，实现了推理能力与领域专长的有效融合。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法通常导致推理深度和领域特定效用双重下降，发现推理能力主要位于低梯度敏感的参数区域，而领域能力集中在高幅度参数，传统合并方法未能区分这两种特性，导致性能崩溃。

Method: 提出Contrastive Gradient Identification技术的ReasonAny框架，用于识别和保护推理能力所在的低梯度敏感参数区域，并通过模型合并实现推理和领域模型能力的协同提升。

Result: 在安全、生物医药和金融等多个领域的实验中，ReasonAny框架显著优于现有最先进方法，有效合成了推理和领域专长能力，同时保持了稳健的推理性能。

Conclusion: ReasonAny有效解决了"推理+领域"模型合并时的性能崩溃问题，在多个专业领域中显著提升了合成模型的推理和领域能力。

Abstract: Large Reasoning Models (LRMs) with long chain-of-thought reasoning have recently achieved remarkable success. Yet, equipping domain-specialized models with such reasoning capabilities, referred to as "Reasoning + X", remains a significant challenge. While model merging offers a promising training-free solution, existing methods often suffer from a destructive performance collapse: existing methods tend to both weaken reasoning depth and compromise domain-specific utility. Interestingly, we identify a counter-intuitive phenomenon underlying this failure: reasoning ability predominantly resides in parameter regions with low gradient sensitivity, contrary to the common assumption that domain capabilities correspond to high-magnitude parameters. Motivated by this insight, we propose ReasonAny, a novel merging framework that resolves the reasoning-domain performance collapse through Contrastive Gradient Identification. Experiments across safety, biomedicine, and finance domains show that ReasonAny effectively synthesizes "Reasoning + X" capabilities, significantly outperforming state-of-the-art baselines while retaining robust reasoning performance.

</details>


### [22] [Can large language models interpret unstructured chat data on dynamic group decision-making processes? Evidence on joint destination choice](https://arxiv.org/abs/2601.05582)
*Sung-Yoo Lim,Koki Sato,Kiyoshi Takami,Giancarlos Parady,Eui-Jin Kim*

Main category: cs.CL

TL;DR: 本文研究了利用大型语言模型自动解读日本群体聊天数据中关于集体外出就餐决策过程的可行性，通过设计一个逐步提取决策要素的提示框架，实现了将非结构化对话转化为结构化数据。


<details>
  <summary>Details</summary>
Motivation: 传统旅行调查难以观察群体活动中复杂的共同决策过程，新兴非结构化聊天数据为揭示这一过程提供了新的数据源，但解读依赖于大量人工注释。

Method: 设计了一种基于知识获取过程的提示框架，逐步提取群体餐厅选择集、各方案的个人偏好及驱动偏好的具体属性，将非结构化对话转为结构化表格，并以人工标注数据进行定量和定性评估。

Result: LLM能可靠识别显性因素但难以捕捉隐含细节，明确了何时可信赖自动化结果，何时需人工参与，展示了LLM在分析社会活动中非传统数据来源的潜力与限制。

Conclusion: 大型语言模型能够较好地捕捉显性决策因素，但在识别隐含的细微因素方面表现较弱，需结合人工校验以确保准确性。

Abstract: Social activities result from complex joint activity-travel decisions between group members. While observing the decision-making process of these activities is difficult via traditional travel surveys, the advent of new types of data, such as unstructured chat data, can help shed some light on these complex processes. However, interpreting these decision-making processes requires inferring both explicit and implicit factors. This typically involves the labor-intensive task of manually annotating dialogues to capture context-dependent meanings shaped by the social and cultural norms. This study evaluates the potential of Large Language Models (LLMs) to automate and complement human annotation in interpreting decision-making processes from group chats, using data on joint eating-out activities in Japan as a case study. We designed a prompting framework inspired by the knowledge acquisition process, which sequentially extracts key decision-making factors, including the group-level restaurant choice set and outcome, individual preferences of each alternative, and the specific attributes driving those preferences. This structured process guides the LLM to interpret group chat data, converting unstructured dialogues into structured tabular data describing decision-making factors. To evaluate LLM-driven outputs, we conduct a quantitative analysis using a human-annotated ground truth dataset and a qualitative error analysis to examine model limitations. Results show that while the LLM reliably captures explicit decision-making factors, it struggles to identify nuanced implicit factors that human annotators readily identified. We pinpoint specific contexts when LLM-based extraction can be trusted versus when human oversight remains essential. These findings highlight both the potential and limitations of LLM-based analysis for incorporating non-traditional data sources on social activities.

</details>


### [23] [ACR: Adaptive Context Refactoring via Context Refactoring Operators for Multi-Turn Dialogue](https://arxiv.org/abs/2601.05589)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Yue Cui,Qingyu Niu,Guoqing Ma,Yidan Liang,Jingjiang Liu,Yiling Wang,Shimin Di,Jiajie Xu*

Main category: cs.CL

TL;DR: 本文提出了一个自适应上下文重构框架，通过动态管理对话历史，有效提升多轮对话模型表现并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中模型难以保持与先前信息一致，存在上下文惰性和状态漂移问题，且现有方法如扩展上下文窗口、引入外部记忆等仍面临局限。

Method: 设计了自适应上下文重构（ACR）框架，包含上下文重构操作库和教师指导的自我进化训练范式，动态监控并重塑交互历史，实现上下文管理与推理过程的解耦。

Result: 在多轮对话任务中，ACR方法显著优于现有基线方法，表现为更好的对话质量和更低的代币消耗。

Conclusion: 提出的ACR框架有效解决了多轮对话中的上下文惰性和状态漂移问题，提升了模型的对话一致性和准确性，同时减少了计算资源消耗。

Abstract: Large Language Models (LLMs) have shown remarkable performance in multi-turn dialogue. However, in multi-turn dialogue, models still struggle to stay aligned with what has been established earlier, follow dependencies across many turns, and avoid drifting into incorrect facts as the interaction grows longer. Existing approaches primarily focus on extending the context window, introducing external memory, or applying context compression, yet these methods still face limitations such as \textbf{contextual inertia} and \textbf{state drift}. To address these challenges, we propose the \textbf{A}daptive \textbf{C}ontext \textbf{R}efactoring \textbf{(ACR)} Framework, which dynamically monitors and reshapes the interaction history to mitigate contextual inertia and state drift actively. ACR is built on a library of context refactoring operators and a teacher-guided self-evolving training paradigm that learns when to intervene and how to refactor, thereby decoupling context management from the reasoning process. Extensive experiments on multi-turn dialogue demonstrate that our method significantly outperforms existing baselines while reducing token consumption.

</details>


### [24] [Data Augmented Pipeline for Legal Information Extraction and Reasoning](https://arxiv.org/abs/2601.05609)
*Nguyen Minh Phuong,Ha-Thanh Nguyen,May Myo Zin,Ken Satoh*

Main category: cs.CL

TL;DR: 本文利用大型语言模型进行数据增强，提升法律领域信息抽取的效率和鲁棒性，并具备广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 人工标注法律领域信息抽取任务的数据成本高，且现有方法难以增强系统的鲁棒性。

Method: 提出利用大型语言模型（LLMs）进行数据增强的流水线，简化数据标注流程。

Result: 显著减少手工标注工作量，提高信息抽取系统的鲁棒性。

Conclusion: 该方法简单有效，且具有通用性，可推广至法律领域外的多种自然语言处理任务。

Abstract: In this paper, we propose a pipeline leveraging Large Language Models (LLMs) for data augmentation in Information Extraction tasks within the legal domain. The proposed method is both simple and effective, significantly reducing the manual effort required for data annotation while enhancing the robustness of Information Extraction systems. Furthermore, the method is generalizable, making it applicable to various Natural Language Processing (NLP) tasks beyond the legal domain.

</details>


### [25] [Text Detoxification in isiXhosa and Yorùbá: A Cross-Lingual Machine Learning Approach for Low-Resource African Languages](https://arxiv.org/abs/2601.05624)
*Abayomi O. Agbeyangi*

Main category: cs.CL

TL;DR: 本研究针对非洲低资源语言isiXhosa和Yorùbá，提出了一种结合TF-IDF+逻辑回归检测和词典+令牌指导重写的自动文本去毒方法。


<details>
  <summary>Details</summary>
Motivation: 针对非洲语言缺乏安全的自动去毒工具的现状，研究有效且适合低资源环境的文本去毒技术，促进用户的安全在线参与。

Method: 采用轻量且可解释的TF-IDF特征与逻辑回归进行毒性检测，结合基于词典和令牌的指导规则实现控制性文本重写，训练和评估基于构建的毒性-中性平行语料。

Result: 毒性检测在isiXhosa和Yorùbá语言中分别达到61-72%和72-86%的准确率，最高ROC-AUC达0.88，重写组件成功消除所有有毒句子且保持无毒句子不变。

Conclusion: 该方法成功实现了有竞争力且资源高效的文本去毒，检测准确率61-86%，ROC-AUC最高0.88，重写组件完全去除了有毒语句且不影响无毒语句，推动了非洲语言文本风格迁移研究。

Abstract: Toxic language is one of the major barrier to safe online participation, yet robust mitigation tools are scarce for African languages. This study addresses this critical gap by investigating automatic text detoxification (toxic to neutral rewriting) for two low-resource African languages, isiXhosa and Yorùbá. The work contributes a novel, pragmatic hybrid methodology: a lightweight, interpretable TF-IDF and Logistic Regression model for transparent toxicity detection, and a controlled lexicon- and token-guided rewriting component. A parallel corpus of toxic to neutral rewrites, which captures idiomatic usage, diacritics, and code switching, was developed to train and evaluate the model. The detection component achieved stratified K-fold accuracies of 61-72% (isiXhosa) and 72-86% (Yorùbá), with per-language ROC-AUCs up to 0.88. The rewriting component successfully detoxified all detected toxic sentences while preserving 100% of non-toxic sentences. These results demonstrate that scalable, interpretable machine learning detectors combined with rule-based edits offer a competitive and resource-efficient solution for culturally adaptive safety tooling, setting a new benchmark for low-resource Text Style Transfer (TST) in African languages.

</details>


### [26] [GIFT: Games as Informal Training for Generalizable LLMs](https://arxiv.org/abs/2601.05633)
*Nuoyan Lyu,Bingbing Xu,Weihao Meng,Yige Yuan,Yang Zhang,Zhiyong Huang,Tat-Seng Chua,Huawei Shen*

Main category: cs.CL

TL;DR: 本文提出了一种基于游戏环境的嵌套训练框架，通过强化学习提升大语言模型的非正式学习能力，增强其实用智慧和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然在形式化学习任务中表现优异，但在体现人类认知特征的"实践智慧"和通用智能方面存在不足，主要由于缺乏依赖交互反馈的非正式学习。

Method: 将游戏作为大语言模型(LLMs)非正式学习的主要环境，利用游戏内在的奖励信号和抽象复杂性，通过GRPO强化学习算法在多种游戏中进行训练；引入嵌套训练框架，采用顺序任务组合，实现显式"AND"目标，以克服多任务学习中的性能退化问题。

Result: 基于游戏的非正式学习框架有效避免了任务间干扰，显著提升了模型在能力导向广泛基准测试中的泛化能力，且相关框架和实现已公开。

Conclusion: 游戏作为非正式学习环境，可弥补大语言模型在通用智能方面的不足；通过显式多任务顺序训练策略，有效提升模型多能力掌握及泛化性能。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the "practical wisdom" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit "OR" objective, our framework employs sequential task composition to enforce an explicit "AND" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.

</details>


### [27] [Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs](https://arxiv.org/abs/2601.05641)
*Alireza Dehghanpour Farashah,Aditi Khandelwal,Marylou Fauchard,Zhuan Shi,Negar Rostamzadeh,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 本文探索了多语言大模型中的遗忘问题，覆盖十种语言，发现高资源语言遗忘更稳定，句法相似性是跨语言遗忘的关键因素。


<details>
  <summary>Details</summary>
Motivation: 随着多语言大语言模型的普及，确保其在不同语言环境中的安全性和公平性挑战增多，尤其是处理预训练和微调数据中存在的偏见和知识遗忘需求；针对单语言研究的不足，开展多语言遗忘机制研究显得尤为重要。

Method: 基于Aya-Expanse 8B模型，作者通过翻译扩展了事实知识和刻板印象基准到十种语言，设计数据遗忘和概念遗忘两种实验设置，分析了不同语言下的遗忘表现及其跨语言转移效应。

Result: 该论文研究了多语言大语言模型中的信息遗忘问题，重点在数据遗忘和概念遗忘两种情境下的表现。作者以Aya-Expanse 8B模型为基础，将事实知识和刻板印象的基准扩展到包括十种不同语言，这些语言涵盖五个语言家族和不同的资源丰富度。实验发现，高资源语言中的遗忘行为更稳定，同时存在语言类型相关语言之间的不对称迁移效应。词法和句法之间，句法相似性是预测跨语言遗忘行为的最强因素。

Conclusion: 多语言环境下的遗忘行为表现出语言资源丰富度和语言类型的影响，句法相似性强烈影响跨语言遗忘效果。

Abstract: As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has primarily focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we study multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes to ten languages through translation: English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and Indonesian. These languages span five language families and a wide range of resource levels. Our experiments show that unlearning in high-resource languages is generally more stable, with asymmetric transfer effects observed between typologically related languages. Furthermore, our analysis of linguistic distances indicates that syntactic similarity is the strongest predictor of cross-lingual unlearning behavior.

</details>


### [28] [A Framework for Personalized Persuasiveness Prediction via Context-Aware User Profiling](https://arxiv.org/abs/2601.05654)
*Sejun Park,Yoonah Park,Jongwon Lim,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种基于上下文感知的用户画像框架，通过自动生成查询获取与说服相关的用户历史记录，并将其总结为用户画像，用于提升说服力预测模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型缺乏系统框架利用说服对象的历史活动信息，无法有效结合用户特点优化说服力预测。

Method: 设计了两个可训练组件：查询生成器用于检索历史说服相关记录，画像器用于总结记录形成用户画像，然后将其用于说服力预测。

Result: 在ChangeMyView Reddit数据集上的多种预测模型中均见提升，显示上下文相关和预测器特定的个性化用户画像的作用。

Conclusion: 通过任务导向和上下文依赖的用户画像，模型在说服力预测上显著优于现有方法，F1分数提升最高达13.77%。

Abstract: Estimating the persuasiveness of messages is critical in various applications, from recommender systems to safety assessment of LLMs. While it is imperative to consider the target persuadee's characteristics, such as their values, experiences, and reasoning styles, there is currently no established systematic framework to optimize leveraging a persuadee's past activities (e.g., conversations) to the benefit of a persuasiveness prediction model. To address this problem, we propose a context-aware user profiling framework with two trainable components: a query generator that generates optimal queries to retrieve persuasion-relevant records from a user's history, and a profiler that summarizes these records into a profile to effectively inform the persuasiveness prediction model. Our evaluation on the ChangeMyView Reddit dataset shows consistent improvements over existing methods across multiple predictor models, with gains of up to +13.77%p in F1 score. Further analysis shows that effective user profiles are context-dependent and predictor-specific, rather than relying on static attributes or surface-level similarity. Together, these results highlight the importance of task-oriented, context-dependent user profiling for personalized persuasiveness prediction.

</details>


### [29] [Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat](https://arxiv.org/abs/2601.05657)
*Hao Yang,Hongyuan Lu,Dingkang Yang,Wenliang Yang,Peng Sun,Xiaochuan Zhang,Jun Xiao,Kefan He,Wai Lam,Yang Liu,Xinhua Zeng*

Main category: cs.CL

TL;DR: 本文提出Stephanie2一步步决策的聊天代理，通过主动等待和节奏控制改善即时通讯聊天的自然性和互动体验，优于前代模型。


<details>
  <summary>Details</summary>
Motivation: 现有分步生成聊天系统缺乏主动等待机制，消息发送节奏不自然，影响了聊天的真实感。

Method: 采用主动等待机制和消息节奏适应策略，结合思考和打字时间建模延迟，并设计了基于时间窗口的双代理对话系统生成伪对话历史进行评估。

Result: Stephanie2在自然性、互动性指标上超过Stephanie1，并在包含角色识别的图灵测试中表现更好。

Conclusion: Stephanie2有效提升了即时通讯人类社交聊天的自然性和互动性，优于前代Stephanie1。

Abstract: Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.

</details>


### [30] [Afri-MCQA: Multimodal Cultural Question Answering for African Languages](https://arxiv.org/abs/2601.05699)
*Atnafu Lambebo Tonja,Srija Anand,Emilio Villa-Cueva,Israel Abebe Azime,Jesujoba Oluwadara Alabi,Muhidin A. Mohamed,Debela Desalegn Yadeta,Negasi Haile Abadi,Abigail Oppong,Nnaemeka Casmir Obiefuna,Idris Abdulmumin,Naome A Etori,Eric Peter Wairagala,Kanda Patrick Tshinu,Imanigirimbabazi Emmanuel,Gabofetswe Malema,Alham Fikri Aji,David Ifeoluwa Adelani,Thamar Solorio*

Main category: cs.CL

TL;DR: Afri-MCQA数据集覆盖15种非洲语言和文本语音两模态，测试发现大模型在非洲语言上的表现极差，强调了开发文化和语言定制模型的必要性，数据集公开促进更包容的非洲语言AI研究。


<details>
  <summary>Details</summary>
Motivation: 非洲拥有丰富的语言资源但在AI研究中严重缺失，当前大型语言模型在非洲语言上的表现极差，亟需创建多语种、文化相关的数据集以推动更具包容性的AI发展。

Method: 构建涵盖15种非洲语言的多语言文化问答基准数据集，由母语者完成，包含文本和语音模态；通过对多个大型语言模型进行测试和控制实验，评估模型的文化知识和语言能力。

Result: 本文提出了Afri-MCQA，这是首个涵盖15种非洲语言、7.5千条问答对的多语言文化问答基准数据集，涵盖文本和语音两种模态，由母语者创建。通过对大型语言模型的测试发现，这些模型在非洲本地语言或语音的开放式视觉问答上表现极差，准确率近乎为零。控制实验显示母语和英语之间存在显著性能差距，揭示了当前模型对非洲语言的文化和语言能力支持不足。作者强调了需要以语音优先的方法、文化根植预训练及跨语言文化迁移，并将数据集以学术许可和CC BY-NC 4.0协议公开，促进非洲语言多模态AI的发展。

Conclusion: 大型语言模型在非洲本地语言及语音模式下表现不足，存在显著的语言及文化知识差距，未来需采用语音优先、文化植入的预训练及跨语言迁移方法来提升模型表现。

Abstract: Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)

</details>


### [31] [Multimodal In-context Learning for ASR of Low-resource Languages](https://arxiv.org/abs/2601.05707)
*Zhaolin Li,Jan Niehues*

Main category: cs.CL

TL;DR: 本研究利用多模态上下文学习与跨语言迁移学习，有效提升未见语言自动语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）受限于监督数据稀缺，尤其是许多语言未被覆盖。传统上下文学习多针对训练中高资源语言且主要为纯文本，缺乏针对未见语言的多模态上下文学习研究。

Method: 本文采用多模态上下文学习（MICL）方法，结合语音大语言模型（Phi-4和Qwen3-Omni），针对三种濒危语言进行实验，分析MICL在未见过语言上的效果及跨语言迁移学习的提升作用。

Result: 实验证明MICL在未见语言上有效，通过利用语音和文本增强识别。跨语言迁移学习进一步提升目标语言的MICL效率。注意力模式分析显示不同层对音频和文本的偏好不同，文本整体占优。基于提示的ASR在未见语言表现较差，融合更强声学模型和speech LLM通过MICL选择声学假设显著提高ASR性能。跨语言迁移学习性能可与训练过目标语言的模型媲美。

Conclusion: MICL结合跨语言迁移能够显著提升濒危语言的自动语音识别效果，且无需目标语言数据，提示基于声学假设选择的结合策略在这一领域应用潜力巨大。代码已开源。

Abstract: Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.

</details>


### [32] [Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging](https://arxiv.org/abs/2601.05713)
*Thomas Fabian*

Main category: cs.CL

TL;DR: 该研究通过引入DTI技术，创新性地分析并可视化了大型语言模型中自然语言表达的信息流动，提升了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法只比较单词嵌入的相对位置，忽略了词在上下文中的使用，难以揭示自然语言表达的信息传递机制。

Method: 采用扩散张量成像（DTI）技术分析和可视化词嵌入中的信息流动，追踪大型语言模型各层中的信息流动。

Result: DTI揭示了词嵌入间信息的流动方式，不同任务（如代词解析和隐喻识别）表现出不同的信息流模式；同时帮助比较和剪枝大型语言模型中未充分利用的层。

Conclusion: 该方法扩展了对语言模型中词嵌入表示的理解，从单词层面到表达层面，促进了对模型结构和任务表现的深入分析与优化。

Abstract: Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.

</details>


### [33] [Analysing Differences in Persuasive Language in LLM-Generated Text: Uncovering Stereotypical Gender Patterns](https://arxiv.org/abs/2601.05751)
*Amalie Brogaard Pauli,Maria Barrett,Max Müller-Eberstein,Isabelle Augenstein,Ira Assent*

Main category: cs.CL

TL;DR: 本研究评估了13个大型语言模型在16种语言中生成说服性语言时，受受众性别、发送者意图和语言影响的差异，发现存在普遍的性别偏见，反映了社会语言学中的性别刻板倾向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型被广泛用于日常交流任务中，尤其是起草旨在影响和说服他人的信息，了解用户指令如何影响说服语言生成以及说服语言在不同目标群体中的差异变得至关重要。

Method: 提出了一个评估框架，用于研究接收者性别、发送者意图和输出语言如何影响说服性语言生成。使用13个LLM和16种语言，通过成对提示指令进行测试，并采用基于社会心理学和传播科学的LLM作为评判者，从19个说服语言类别评估模型响应。

Result: 实验证明所有模型生成的说服语言中存在显著的性别差异，这些差异符合已有的性别语言刻板印象，为进一步理解和改进模型生成的公平性和多样性提供了依据。

Conclusion: 生成的大型语言模型（LLM）在生成有说服力的语言时存在显著的性别差异，这些差异反映了社会心理学和社会语言学中记录的性别刻板语言倾向。

Abstract: Large language models (LLMs) are increasingly used for everyday communication tasks, including drafting interpersonal messages intended to influence and persuade. Prior work has shown that LLMs can successfully persuade humans and amplify persuasive language. It is therefore essential to understand how user instructions affect the generation of persuasive language, and to understand whether the generated persuasive language differs, for example, when targeting different groups. In this work, we propose a framework for evaluating how persuasive language generation is affected by recipient gender, sender intent, or output language. We evaluate 13 LLMs and 16 languages using pairwise prompt instructions. We evaluate model responses on 19 categories of persuasive language using an LLM-as-judge setup grounded in social psychology and communication science. Our results reveal significant gender differences in the persuasive language generated across all models. These patterns reflect biases consistent with gender-stereotypical linguistic tendencies documented in social psychology and sociolinguistics.

</details>


### [34] [One Script Instead of Hundreds? On Pretraining Romanized Encoder Language Models](https://arxiv.org/abs/2601.05776)
*Benedikt Ebing,Lennart Keller,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文探讨了罗马化对六种高资源语言预训练多语言模型的影响，发现对音段文字语言无显著性能损失且提升编码效率，但对中文日文等形音节文字存在性能下降，且高保真罗马化有部分缓解作用，子词重叠未引起多语负干扰。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注从高资源拉丁字母语言向低资源非拉丁字母语言或相关语言间的迁移，未明确罗马化是否适合预训练通用多语言模型，尤其是高资源语言的信息损失问题。

Method: 从零开始预训练编码器语言模型，分别利用罗马化文本和原始文本，涉及六种语言类型差异较大的高资源语言，使用两种不同保真度的罗马化方法对比效果。

Result: 对音段文字语言影响不大，形音节文字（中文、日文）存在性能下降，高保真罗马化能部分缓解。单语与多语模型比较未发现增加子词重叠导致负面干扰。罗马化提高了编码效率（生育率），性能损失可忽略。

Conclusion: 罗马化在高资源音段文字语言中几乎无性能损失且提升编码效率，但对形音节文字有一定性能损害，高保真罗马化有所帮助但不能完全解决。多语言模型中子词重叠未造成负面影响。罗马化作为预训练表示选择具有条件适用性。

Abstract: Exposing latent lexical overlap, script romanization has emerged as an effective strategy for improving cross-lingual transfer (XLT) in multilingual language models (mLMs). Most prior work, however, focused on setups that favor romanization the most: (1) transfer from high-resource Latin-script to low-resource non-Latin-script languages and/or (2) between genealogically closely related languages with different scripts. It thus remains unclear whether romanization is a good representation choice for pretraining general-purpose mLMs, or, more precisely, if information loss associated with romanization harms performance for high-resource languages. We address this gap by pretraining encoder LMs from scratch on both romanized and original texts for six typologically diverse high-resource languages, investigating two potential sources of degradation: (i) loss of script-specific information and (ii) negative cross-lingual interference from increased vocabulary overlap. Using two romanizers with different fidelity profiles, we observe negligible performance loss for languages with segmental scripts, whereas languages with morphosyllabic scripts (Chinese and Japanese) suffer degradation that higher-fidelity romanization mitigates but cannot fully recover. Importantly, comparing monolingual LMs with their mLM counterpart, we find no evidence that increased subword overlap induces negative interference. We further show that romanization improves encoding efficiency (i.e., fertility) for segmental scripts at a negligible performance cost.

</details>


### [35] [Simplify-This: A Comparative Analysis of Prompt-Based and Fine-Tuned LLMs](https://arxiv.org/abs/2601.05794)
*Eilam Cohen,Itamar Bul,Danielle Inbar,Omri Loewenbach*

Main category: cs.CL

TL;DR: 本文比较了文本简化任务中大型语言模型微调与提示工程的效果，发现微调在结构简化方面更强，提示工程更注重语义保留且易复制原文，人工评价倾向微调结果。


<details>
  <summary>Details</summary>
Motivation: 探讨微调与提示工程这两种文本简化方法间的权衡和优劣，找出更有效的简化策略。

Method: 引入Simplify-This，比较使用编码器-解码器大型语言模型进行文本简化时微调与提示工程的效果，基于多个基准测试和多种评价指标进行评测。

Result: 微调模型在结构简化方面表现更优，提示工程获得更高的语义相似度分数但更倾向于复制输入，且人工评价总体偏好微调生成的文本。

Conclusion: 微调是文本简化任务中更有效的策略，建议未来工作围绕微调展开，同时发布代码、数据集和模型以促进可复现性和后续研究。

Abstract: Large language models (LLMs) enable strong text generation, and in general there is a practical tradeoff between fine-tuning and prompt engineering. We introduce Simplify-This, a comparative study evaluating both paradigms for text simplification with encoder-decoder LLMs across multiple benchmarks, using a range of evaluation metrics. Fine-tuned models consistently deliver stronger structural simplification, whereas prompting often attains higher semantic similarity scores yet tends to copy inputs. A human evaluation favors fine-tuned outputs overall. We release code, a cleaned derivative dataset used in our study, checkpoints of fine-tuned models, and prompt templates to facilitate reproducibility and future work.

</details>


### [36] [EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis](https://arxiv.org/abs/2601.05808)
*Xiaoshuai Song,Haofei Chang,Guanting Dong,Yutao Zhu,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: EnvScaler通过自动化合成多样化工具交互环境，有效提升大型语言模型在复杂多工具交互任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLMs）作为多环境中的智能体需要丰富多样的工具交互沙箱，但真实系统访问受限，模拟环境易出错，手动构建难以扩展。

Method: EnvScaler框架包含SkelBuilder和ScenGenerator两个组件，前者通过主题挖掘、逻辑建模和质量评估构建多样化环境骨架，后者为每个环境生成多个任务场景和规则基轨迹验证函数。

Result: 使用EnvScaler合成了191个环境和约7000个场景，并将其应用于Qwen3系列模型的监督微调和强化学习，三大基准测试显示该方法显著提升了LLMs在复杂多轮多工具交互环境中的任务解决能力。

Conclusion: EnvScaler框架实现了环境构建的自动化和规模化，显著增强了大型语言模型处理复杂多工具交互环境任务的能力，为训练环境智能体提供了有效工具。

Abstract: Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.

</details>


### [37] [LLMs as Science Journalists: Supporting Early-stage Researchers in Communicating Their Science to the Public](https://arxiv.org/abs/2601.05821)
*Milad Alshomary,Grace Li,Anubhav Jangra,Yufang Hou,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 该论文提出了一种训练大型语言模型（LLMs）扮演科学记者的框架，帮助早期研究人员更好地向公众传达科研成果。


<details>
  <summary>Details</summary>
Motivation: 现有通用LLMs虽能辅助科研交流，但未针对科学传播优化，早期研究者缺乏有效工具向公众传达科研内容。

Method: 设计了一种训练框架，使LLMs模仿科学记者角色，并进行问答互动以帮助研究人员完善论文沟通。

Result: 训练后的LLMs在模拟和真实对话中提出更相关问题，促进研究者更清晰阐述成果，大部分用户更喜欢与这类模型互动。

Conclusion: 通过训练，LLMs可以更有效地引导研究人员讨论科研的社会影响，用户更倾向于与这种定制模型互动。

Abstract: The scientific community needs tools that help early-stage researchers effectively communicate their findings and innovations to the public. Although existing general-purpose Large Language Models (LLMs) can assist in this endeavor, they are not optimally aligned for it. To address this, we propose a framework for training LLMs to emulate the role of a science journalist that can be used by early-stage researchers to learn how to properly communicate their papers to the general public. We evaluate the usefulness of our trained LLM Journalists in leading conversations with both simulated and human researchers. %compared to the general-purpose ones. Our experiments indicate that LLMs trained using our framework ask more relevant questions that address the societal impact of research, prompting researchers to clarify and elaborate on their findings. In the user study, the majority of participants who interacted with our trained LLM Journalist appreciated it more than interacting with general-purpose LLMs.

</details>


### [38] [Peek2: A Regex-free implementation of pretokenizers for Byte-level BPE](https://arxiv.org/abs/2601.05833)
*Liu Zai*

Main category: cs.CL

TL;DR: Peek2是一种无Regex、完全CPU实现的高效Byte-level BPE预分词器，提升了1.11倍性能，兼容现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 提高Byte-level BPE预分词的性能和安全性，作为GPT-3、LLaMa-3及Qwen-2.5中常用的cl100k-like预分词器的替代方案。

Method: 提出了一种无Regex的预分词新实现Peek2，具有线性复杂度，完全在CPU上运行。

Result: Peek2在整体吞吐量上提升了约1.11倍，且保持了预分词结果的一致性和算法稳定性。

Conclusion: Peek2实现了与原始Regex基预分词器完全相同的预分词结果，同时显著提升了整个Byte-level BPE编码过程的吞吐量。

Abstract: Pretokenization is a crucial, sequential pass in Byte-level BPE tokenizers. Our proposed new implementation, Peek2, serves as a drop-in replacement for cl100k-like pretokenizers used in GPT-3, LLaMa-3, and Qwen-2.5. Designed with performance and safety in mind, Peek2 is Regex-free and delivers a $ 1.11\times $ improvement in overall throughput across the entire Byte-level BPE encoding process. This algorithm runs entirely on the CPU, has stable linear complexity $ O(n) $, and provides presegmentation results identical to those of the original Regex-based pretokenizer.

</details>


### [39] [Left, Right, or Center? Evaluating LLM Framing in News Classification and Generation](https://arxiv.org/abs/2601.05835)
*Molly Kennedy,Ali Parker,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 研究了九种大型语言模型（LLM）在文本生成中政治立场的表现，发现模型普遍趋向于中立立场，且不同模型在意识形态表达上有显著差异。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在新闻文本生成中的广泛应用，研究其是否存在政治框架偏向，及其潜在影响。

Method: 通过少样本学习预测政治光谱标签（左派/中立/右派），并生成在不同意识形态指导（忠实、中立、左派、右派）下的文本摘要，使用固定的意识形态评估器对所有输出进行评分。

Result: 发现所有模型在文章级别和生成文本中均显示出意识形态的中立集中特性；Grok 4表现出最强的意识形态生成能力，Claude Sonnet 4.5和Llama 3.1在偏见评级上表现最佳。

Conclusion: 大型语言模型在生成文本时普遍表现出向中立立场倾斜的趋势，个别模型如Grok 4能够较好地表达意识形态差异。

Abstract: Large Language Model (LLM) based summarization and text generation are increasingly used for producing and rewriting text, raising concerns about political framing in journalism where subtle wording choices can shape interpretation. Across nine state-of-the-art LLMs, we study political framing by testing whether LLMs' classification-based bias signals align with framing behavior in their generated summaries. We first compare few-shot ideology predictions against LEFT/CENTER/RIGHT labels. We then generate "steered" summaries under FAITHFUL, CENTRIST, LEFT, and RIGHT prompts, and score all outputs using a single fixed ideology evaluator. We find pervasive ideological center-collapse in both article-level ratings and generated text, indicating a systematic tendency toward centrist framing. Among evaluated models, Grok 4 is by far the most ideologically expressive generator, while Claude Sonnet 4.5 and Llama 3.1 achieve the strongest bias-rating performance among commercial and open-weight models, respectively.

</details>


### [40] [Semantic NLP Pipelines for Interoperable Patient Digital Twins from Unstructured EHRs](https://arxiv.org/abs/2601.05847)
*Rafael Brens,Yuqiao Meng,Luoxi Tang,Zhaohan Xi*

Main category: cs.CL

TL;DR: 本论文提出了一种基于语义NLP的管道，将非结构化电子健康记录（EHR）文本转化为符合FHIR标准的数字孪生表示，以提高互操作性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有电子健康记录的非结构化文本难以标准化处理，限制了患者数字孪生的生成和应用，亟需语义驱动的自动转换方法。

Method: 通过命名实体识别(NER)、概念规范化（映射至SNOMED-CT或ICD-10）及关系抽取，构建FHIR兼容的患者数字孪生。

Result: 在MIMIC-IV-on-FHIR参考映射下，方法显示出优越的实体及关系抽取性能，模式完整性和互操作性均优于基线方法。

Conclusion: 该方法在MIMIC-IV临床数据库上验证表现出高F1分数，显著提升了实体与关系抽取的准确性和互操作性。

Abstract: Digital twins -- virtual replicas of physical entities -- are gaining traction in healthcare for personalized monitoring, predictive modeling, and clinical decision support. However, generating interoperable patient digital twins from unstructured electronic health records (EHRs) remains challenging due to variability in clinical documentation and lack of standardized mappings. This paper presents a semantic NLP-driven pipeline that transforms free-text EHR notes into FHIR-compliant digital twin representations. The pipeline leverages named entity recognition (NER) to extract clinical concepts, concept normalization to map entities to SNOMED-CT or ICD-10, and relation extraction to capture structured associations between conditions, medications, and observations. Evaluation on MIMIC-IV Clinical Database Demo with validation against MIMIC-IV-on-FHIR reference mappings demonstrates high F1-scores for entity and relation extraction, with improved schema completeness and interoperability compared to baseline methods.

</details>


### [41] [Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs](https://arxiv.org/abs/2601.05851)
*Sandeep Mishra,Devichand Budagam,Anubhab Mandal,Bishal Santra,Pawan Goyal,Manish Gupta*

Main category: cs.CL

TL;DR: 本文提出了实时多模态自动补全（MAC）任务，结合部分输入文本和视觉线索预测对话中的下一个字符，提升了用户输入的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动补全多为文本方法，忽略视觉信息，难以准确捕捉用户意图，亟需结合多模态信息改进自动补全效果。

Method: 通过改编MMDialog和ImageChat构建基准数据集，评测视觉语言模型与文本模型，并设计Router-Suggest动态选择模型以优化性能与效率。

Result: 视图语言模型在用户满意度和输入节省方面表现优于文本模型，Router-Suggest在保持准确率的同时实现了显著的速度提升。

Conclusion: 多模态上下文在自动补全中显著提升了用户满意度和输入效率，Router-Suggest实现了性能与速度的良好平衡，适用于资源受限环境。

Abstract: Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.

</details>


### [42] [CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning](https://arxiv.org/abs/2601.05858)
*Alexandra Dragomir,Florin Brad,Radu Tudor Ionescu*

Main category: cs.CL

TL;DR: 通过结合带重启的课程学习策略CLewR提升偏好优化的多语言机器翻译性能，解决训练中样本遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在多语言机器翻译的偏好优化过程中，训练数据样本的顺序对性能影响研究不足。

Method: 将课程学习策略整合到多种先进的偏好优化算法中，提出了一种带重启的课程学习方法CLewR，通过多次重复由易到难的训练过程，减缓容易样本的遗忘。

Result: 在多个模型家族（Gemma2、Qwen2.5、Llama3.1）和偏好优化技术上均取得了一致性能提升。

Conclusion: 引入带重启的课程学习策略能有效提升零-shot多语言机器翻译的性能，缓解灾难性遗忘问题。

Abstract: Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.

</details>


### [43] [What do the metrics mean? A critical analysis of the use of Automated Evaluation Metrics in Interpreting](https://arxiv.org/abs/2601.05864)
*Jonathan Downie,Joss Moorkens*

Main category: cs.CL

TL;DR: 现有自动化口译质量测量方法忽视交流环境，不能单独有效评估口译质量。


<details>
  <summary>Details</summary>
Motivation: 随着口译技术的发展，快速高效评估口译质量的需求显著增加。

Method: 本文评估了多种近年来提出的自动化口译质量测量方法。

Result: 各类自动化测量方法虽然便捷，但缺乏对真实交流环境的考虑，难以作为独立的质量评估工具。

Conclusion: 当前自动化质量测量方法无法全面考虑口译的交流语境，因此单独使用时无法有效衡量口译质量。

Abstract: With the growth of interpreting technologies, from remote interpreting and Computer-Aided Interpreting to automated speech translation and interpreting avatars, there is now a high demand for ways to quickly and efficiently measure the quality of any interpreting delivered. A range of approaches to fulfil the need for quick and efficient quality measurement have been proposed, each involving some measure of automation. This article examines these recently-proposed quality measurement methods and will discuss their suitability for measuring the quality of authentic interpreting practice, whether delivered by humans or machines, concluding that automatic metrics as currently proposed cannot take into account the communicative context and thus are not viable measures of the quality of any interpreting provision when used on their own. Across all attempts to measure or even categorise quality in Interpreting Studies, the contexts in which interpreting takes place have become fundamental to the final analysis.

</details>


### [44] [FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG](https://arxiv.org/abs/2601.05866)
*Maxime Dassen,Rebecca Kotula,Kenton Murray,Andrew Yates,Dawn Lawrie,Efsun Kayi,James Mayfield,Kevin Duh*

Main category: cs.CL

TL;DR: 本文通过引入FACTUM框架，揭示了RAG模型中引文幻觉的复杂内在机制，成功提升了引用可信度的检测效果。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成模型（RAG）中引文幻觉问题，即模型自信地引用但来源不支持其论点的情况。

Method: 提出FACTUM框架，通过四个机械评分衡量模型的注意力机制和前馈网络（FFN）路径的贡献及其对齐情况，分析模型内部机制与正确引用的关系。

Result: 发现正确引用的两个标志：模型参数知识贡献更大和注意力汇聚用于信息综合，且这一特征随模型规模变化。此外，FACTUM在AUC指标上比现有方法提升了37.5%。

Conclusion: 引文幻觉不是简单的参数依赖，而是关联模型内部机制复杂且随规模变化的相互作用，FACTUM为构建更可靠的RAG系统提供了新的视角。

Abstract: Retrieval-Augmented Generation (RAG) models are critically undermined by citation hallucinations, a deceptive failure where a model confidently cites a source that fails to support its claim. Existing work often attributes hallucination to a simple over-reliance on the model's parametric knowledge. We challenge this view and introduce FACTUM (Framework for Attesting Citation Trustworthiness via Underlying Mechanisms), a framework of four mechanistic scores measuring the distinct contributions of a model's attention and FFN pathways, and the alignment between them. Our analysis reveals two consistent signatures of correct citation: a significantly stronger contribution from the model's parametric knowledge and greater use of the attention sink for information synthesis. Crucially, we find the signature of a correct citation is not static but evolves with model scale. For example, the signature of a correct citation for the Llama-3.2-3B model is marked by higher pathway alignment, whereas for the Llama-3.1-8B model, it is characterized by lower alignment, where pathways contribute more distinct, orthogonal information. By capturing this complex, evolving signature, FACTUM outperforms state-of-the-art baselines by up to 37.5% in AUC. Our findings reframe citation hallucination as a complex, scale-dependent interplay between internal mechanisms, paving the way for more nuanced and reliable RAG systems.

</details>


### [45] [Continual-learning for Modelling Low-Resource Languages from Large Language Models](https://arxiv.org/abs/2601.05874)
*Santosh Srinath K,Mudit Somani,Varun Reddy Padala,Prajna Devi Upadhyay,Abhijit Das*

Main category: cs.CL

TL;DR: 提出了一种基于词性代码切换和重放适配器的持续学习方法，成功缓解了多语言场景下小型语言模型灾难性遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 在多语言场景中构建语言模型时，灾难性遗忘是主要挑战，尤其是通过适配大型语言模型构建低资源语言的小型语言模型时。

Method: 提出了一种结合词性（POS）基础的代码切换和重放适配器策略的持续学习方法，以减轻小型语言模型（SLM）在从大型语言模型（LLM）适配时产生的灾难性遗忘问题。

Result: 在视觉语言任务（如视觉问答）和语言建模任务上进行实验，结果展示了所提体系结构的有效性。

Conclusion: 所提方法有效缓解了灾难性遗忘，提高了小型语言模型在多语言和视觉语言任务中的表现。

Abstract: Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.

</details>


### [46] [iReasoner: Trajectory-Aware Intrinsic Reasoning Supervision for Self-Evolving Large Multimodal Models](https://arxiv.org/abs/2601.05877)
*Meghana Sunil,Manikandarajan Venmathimaran,Muthu Subash Kavitha*

Main category: cs.CL

TL;DR: iReasoner通过奖励链式推理内部一致性，自监督提升多模态模型中间推理能力，在无监督场景下实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型自我进化多依赖最终结果奖励，忽视了中间推理过程的约束，而中间推理对视觉决策极为重要。

Method: 提出iReasoner框架，通过显式引导链式思考(CoT)并奖励其内部一致性，结合Proposer-Solver循环机制，在无人标注数据上对中间推理步骤进行轨迹感知奖励。

Result: iReasoner在基于Qwen2.5-VL-7B模型上进行无监督后训练，在多模态推理任务中提升了最多2.1分，验证了该方法对隐式推理改进的有效性。

Conclusion: iReasoner为大型多模态模型实现纯无监督环境下的推理感知自我改进提供了新思路，有助于加强模型中间推理能力，推动视觉推理性能提升。

Abstract: Recent work shows that large multimodal models (LMMs) can self-improve from unlabeled data via self-play and intrinsic feedback. Yet existing self-evolving frameworks mainly reward final outcomes, leaving intermediate reasoning weakly constrained despite its importance for visually grounded decision making. We propose iReasoner, a self-evolving framework that improves an LMM's implicit reasoning by explicitly eliciting chain-of-thought (CoT) and rewarding its internal agreement. In a Proposer--Solver loop over unlabeled images, iReasoner augments outcome-level intrinsic rewards with a trajectory-aware signal defined over intermediate reasoning steps, providing learning signals that distinguish reasoning paths leading to the same answer without ground-truth labels or external judges. Starting from Qwen2.5-VL-7B, iReasoner yields up to $+2.1$ points across diverse multimodal reasoning benchmarks under fully unsupervised post-training. We hope this work serves as a starting point for reasoning-aware self-improvement in LMMs in purely unsupervised settings.

</details>


### [47] [Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law](https://arxiv.org/abs/2601.05879)
*Jakub Harasta,Matej Vasina,Martin Kornel,Tomas Foltynek*

Main category: cs.CL

TL;DR: 本研究考察先进大语言模型在家庭法案例中的性别偏见，发现部分模型结果受性别影响，提示法律自助存在风险。


<details>
  <summary>Details</summary>
Motivation: 鉴于司法可及性不足，普通用户依赖大语言模型进行法律自助，但模型可能产生不准确或带偏见的结果，需评估其在家庭法律场景中的性别偏见风险。

Method: 设计基于捷克家庭法的离婚案例，采用带性别和中性标签两版本，测试四款大语言模型的零样本输出，并引入九个法律变量分析对共享抚养比例的影响。

Result: 本研究通过设计一真实的捷克家庭法律离婚案例，评估四个先进大语言模型（GPT-5 nano、Claude Haiku 4.5、Gemini 2.5 Flash、Llama 3.3）在零样本交互下的性别偏见表现。采用带性别名和中性标签的两版本案例，并引入九个法律相关变量，检测模型对共享抚养比例的影响。结果显示不同模型表现存在差异且部分模型呈现性别依赖的偏见模式，揭示依赖LLM进行法律自助的潜在风险，强调需加强模型在敏感法律情境中的评估。

Conclusion: 研究发现部分大语言模型在处理家庭法律场景时存在性别偏见，表明对法律自助手段的依赖需要谨慎，并呼吁加强对模型行为的敏感性评估。

Abstract: Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.

</details>


### [48] [An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift](https://arxiv.org/abs/2601.05882)
*Constantinos Karouzos,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文系统研究了预训练语言模型偏好调优在领域迁移下的泛化问题，发现伪标签适应策略能显著减轻领域迁移带来的性能退化。


<details>
  <summary>Details</summary>
Motivation: 偏好调优提高语言模型的安全性和助益性，但其在训练领域外的泛化能力不足，如何缓解这种领域偏差导致的性能下降尚未被深入研究。

Method: 比较了五种常见的对齐目标及多种适应策略（包括目标领域的监督微调和伪标签方法），在摘要和问答任务中评估领域迁移的效果。

Result: 该论文研究了预训练语言模型通过偏好调优来对齐人类判断（如质量、帮助性、安全性），并重点探讨了偏好调优在领域迁移下的性能表现问题。作者系统比较了五种对齐目标及多种从源领域到目标领域的适应策略（包括目标领域的监督微调和伪标签方法），针对摘要和问答助益任务进行评估。结果表明，不同对齐目标在领域迁移时泛化能力存在显著差异，同时基于伪标签的适应策略能有效缓解领域偏差导致的性能下降。

Conclusion: 不同的偏好调优目标在领域转移中的泛化效果差异明显，伪标签适应策略能有效提升模型在新领域的表现，减轻领域偏移的负面影响。

Abstract: Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation

</details>


### [49] [HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search](https://arxiv.org/abs/2601.05903)
*Zihang Tian,Rui Li,Jingsen Zhang,Xiaohe Bo,Wei Huo,Xu Chen*

Main category: cs.CL

TL;DR: HAPS框架结合架构选择与参数搜索，利用两级路由器和参数生成网络提升大型语言模型的任务表现，在基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）路由方法通常只关注模型架构的选择，忽略了参数设置，而参数设置对任务表现至关重要。

Method: 设计了一个两级路由器结构，高层路由器选择模型架构，低层路由器搜索最优参数；引入参数生成网络共享参数以增强路由能力；训练过程中采用奖励增强目标进行优化。

Result: 提出了HAPS，一个层级化的LLM路由框架，联合搜索模型架构和参数，通过高低层路由器协同工作并引入参数生成网络，显著提升了路由效果。

Conclusion: HAPS框架通过联合优化模型架构与参数设置，实现了更优的大型语言模型路由效果，验证了参数搜索对性能提升的重要性。

Abstract: Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.

</details>


### [50] [Pantagruel: Unified Self-Supervised Encoders for French Text and Speech](https://arxiv.org/abs/2601.05911)
*Phuong-Hang Le,Valentin Pelloin,Arnault Chatelain,Maryem Bouziane,Mohammed Ghennai,Qianwen Guan,Kirill Milintsevich,Salima Mdhaffar,Aidan Mannion,Nils Defauw,Shuyue Gu,Alexandre Audibert,Marco Dinarelli,Yannick Estève,Lorraine Goeuriot,Steffen Lalande,Nicolas Hervé,Maximin Coavoux,François Portet,Étienne Ollion,Marie Candito,Maxime Peyrard,Solange Rossato,Benjamin Lecouteux,Aurélie Nardy,Gilles Sérasset,Vincent Segonne,Solène Evain,Diandra Fabre,Didier Schwab*

Main category: cs.CL

TL;DR: Pantagruel模型通过自监督学习特征空间目标，提供了一个统一且强大的法语文本和语音编码器，提升了语言理解的表现。


<details>
  <summary>Details</summary>
Motivation: 传统模型针对不同模态分别预测特定目标，限制了编码器对语言和声学规律的捕捉能力。通过学习特征空间中的目标表示，能够提升法语文本和语音的表征效果，增强跨模态理解能力。

Method: Pantagruel模型采用自监督学习，通过在特征空间学习上下文相关的目标表征，训练适用于法语文本和语音的编码器。与传统的预测文本标记或语音单位不同，这种方法使编码器能更好地捕捉语言和声学规律。模型分别在大规模法语语料库(如Wikipedia、OSCAR、CroissantLLM文本资源以及MultilingualLibriSpeech、LeBenchmark和INA-100k语音资源)上预训练。

Result: Pantagruel模型在涵盖文本与语音的多项下游任务上表现出竞争力，优于或超越了CamemBERT、FlauBERT和LeBenchmark2.0等强基线，且架构统一，能够无缝处理文本和语音输入。

Conclusion: 本文证明了特征空间自监督目标在法语表示学习中的有效性，Pantagruel模型是多模态语音-文本理解的坚实基础，具备优异的性能和通用架构。

Abstract: We release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead of predicting modality-tailored targets such as textual tokens or speech units, Pantagruel learns contextualized target representations in the feature space, allowing modality-specific encoders to capture linguistic and acoustic regularities more effectively. Separate models are pre-trained on large-scale French corpora, including Wikipedia, OSCAR and CroissantLLM for text, together with MultilingualLibriSpeech, LeBenchmark, and INA-100k for speech. INA-100k is a newly introduced 100,000-hour corpus of French audio derived from the archives of the Institut National de l'Audiovisuel (INA), the national repository of French radio and television broadcasts, providing highly diverse audio data. We evaluate Pantagruel across a broad range of downstream tasks spanning both modalities, including those from the standard French benchmarks such as FLUE or LeBenchmark. Across these tasks, Pantagruel models show competitive or superior performance compared to strong French baselines such as CamemBERT, FlauBERT, and LeBenchmark2.0, while maintaining a shared architecture that can seamlessly handle either speech or text inputs. These results confirm the effectiveness of feature-space self-supervised objectives for French representation learning and highlight Pantagruel as a robust foundation for multimodal speech-text understanding.

</details>


### [51] [Distilling Feedback into Memory-as-a-Tool](https://arxiv.org/abs/2601.05960)
*Víctor Gallego*

Main category: cs.CL

TL;DR: 本文提出一种通过文件存储和工具调用将临时批评转化为可检索指导的推理成本摊销框架，在Rubric Feedback Bench数据集上验证，显著降低推理成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 减少在推理时进行细致修正带来的高额计算成本，通过将临时反馈转化为持久指导信息提升效率。

Method: 使用文件存储的记忆系统和代理控制的工具调用，将临时批评转化为可检索的指导信息以摊销推理成本。

Result: 增强的语言模型在Rubric Feedback Bench数据集上的表现与测试时细化方法相当，但推理成本大幅下降。

Conclusion: 通过文件基础的记忆系统和工具调用，增强的语言模型能迅速达到测试时细化方法的性能，且大幅降低推理成本。

Abstract: We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.

</details>


### [52] [The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.06002)
*Qiguang Chen,Yantao Du,Ziniu Li,Jinhao Liu,Songyao Duan,Jiarui Guo,Minghao Liu,Jiaheng Liu,Tong Yang,Ge Zhang,Libo Qin,Wanxiang Che,Wenhao Huang*

Main category: cs.CL

TL;DR: 大语言模型难以有效学习长链思维是因为缺乏稳定的类分子结构，论文提出Mole-Syn方法合成这些结构，提升推理性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以从人类或非长链思维模型模仿中学到有效的长链思维推理，需理解有效长链思维的结构特征及其对学习的影响，从而设计协助长链思维学习的方法。

Method: 提出Mole-Syn分布转移图方法，通过模拟三种相互作用合成长链思维结构，指导有效轨迹合成，实现性能提升和强化学习训练稳定。

Result: 该论文提出大语言模型在学习长链思维链（Long CoT）推理时存在困难，分析其原因是有效的长链思维轨迹具有稳定的类分子结构，这些结构包括深度推理、自我反思和自我探索三种相互作用方式。通过蒸馏轨迹分析发现这些结构来源于长链思维微调而非简单的关键词模仿。论文引入有效语义异构体概念，指出只有促进快速熵收敛的“键”支持稳定的长链思维学习，而结构竞争反而损害训练效果。基于此，提出Mole-Syn方法，通过分布转移图指导有效长链结构合成，提升性能和强化学习稳定性。

Conclusion: 稳定且有效的长链思维轨迹依赖三种类分子相互作用结构，这些结构通过微调形成，正确引导结构合成能显著提升模型表现和训练稳定性。

Abstract: Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.

</details>


### [53] [Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2601.06007)
*Elias Lumer,Faheem Nizar,Akshaya Jangiti,Kevin Frank,Anmol Gulati,Mandar Phadate,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文系统评估了三大大型语言模型提供商在多轮代理任务中提示缓存的效果，发现合理的缓存策略能显著降低API成本和响应时间。


<details>
  <summary>Details</summary>
Motivation: 当前虽然大型语言模型提供商支持提示缓存以降低成本和延迟，但针对复杂多轮代理任务提示缓存的具体效益及最佳缓存策略尚未被系统量化和比较。

Method: 在DeepResearchBench基准上对OpenAI、Anthropic和Google三家提供商，比较全上下文缓存、仅系统提示缓存及排除动态工具结果的缓存三种策略，统计超500次会话的成本和响应时间。

Result: 提示缓存能降低45-80%API费用，提升13-31%首次响应速度，且通过合理设计缓存块位置和内容，能更加稳定地发挥缓存效用。不同提供商缓存表现存在细微差异，文章提出了生产环境中提示缓存的实用建议。

Conclusion: 提示缓存能显著节省API成本（45-80%）并提升响应速度（13-31%），且优化缓存策略（如动态内容放末端、排除动态工具结果）效果优于简单全上下文缓存。

Abstract: Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.

</details>


### [54] [Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards](https://arxiv.org/abs/2601.06021)
*Jiajie Zhang,Xin Lv,Ling Feng,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出了结合细粒度证据奖励的强化学习框架CaRR和训练方法C-GRPO，显著提升了深度搜索代理的推理全面性和真实性，表现优于传统奖励方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要依赖二元结果奖励，不能全面反映推理过程的完整性和事实性，导致快捷方式利用和幻觉等不良行为。

Method: 提出了细粒度奖励框架Citation-aware Rubric Rewards (CaRR)，通过分解复杂问题为可验证的单跳评分标准，要求代理识别隐藏实体、给出正确引用并构建完整证据链；引入结合CaRR与结果奖励的Citation-aware Group Relative Policy Optimization (C-GRPO)进行训练。

Result: C-GRPO在多个深度搜索基准测试中优于传统基于结果的强化学习方法，有效防止快捷方式利用，促进全面且基于证据的推理，并展现出对开放式深度研究任务的强泛化能力。

Conclusion: 结合细粒度证据奖励的RL方法显著提升了深度搜索代理的推理质量和鲁棒性，为基于LLM的深度搜索提供了有效优化途径。

Abstract: Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.

</details>


### [55] [AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs](https://arxiv.org/abs/2601.06022)
*Chengming Cui,Tianxin Wei,Ziyi Chen,Ruizhong Qiu,Zhichen Zeng,Zhining Liu,Xuying Ning,Duo Zhou,Jingrui He*

Main category: cs.CL

TL;DR: 本文提出了AdaFuse，一种自适应的集成解码框架，通过动态选择适当的融合单元来提高大语言模型的推理能力，解决了固定融合粒度的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时集成方法因固定融合粒度缺乏灵活性，难以适应不同任务及生成过程中变化的特点，限制了集成能力的发挥。

Method: 提出了一种基于不确定性的动态选择融合单元的方法，利用单词作为基本对齐单位，在置信状态直接生成，在不确定状态采用多样性感知的扩展策略以探索备选答案，增强集成的效果。

Result: 在开放域问答、算术推理和机器翻译任务中，AdaFuse均超越了强基线方法，平均提升了6.88%的性能表现。

Conclusion: AdaFuse通过动态调整融合粒度及结合不确定性判断，有效提升了多模型集成的性能，在多个任务上相较强基线平均提升6.88%。

Abstract: Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [56] [Simulation-Free PSRO: Removing Game Simulation from Policy Space Response Oracles](https://arxiv.org/abs/2601.05279)
*Yingzhuo Liu,Shuodi Liu,Weijun Luo,Liuyu Xiang,Zhaofeng He*

Main category: cs.MA

TL;DR: 本文针对PSRO计算成本高的问题，提出了一种基于动态窗口的无模拟PSRO方法，通过策略数量限制和策略淘汰机制，提升了效率和策略质量。


<details>
  <summary>Details</summary>
Motivation: 传统的Policy Space Response Oracles (PSRO)在计算近似零和博弈中的纳什均衡时，计算代价过高，尤其是游戏模拟成为主要瓶颈，限制了实际应用。

Method: 提出了无模拟（Simulation-Free）PSRO的概念，回顾了相关方法，并进一步设计了基于动态窗口的无模拟PSRO，采用策略窗口来限制维护的策略数量，引入Nash聚类选择待淘汰策略以控制窗口内策略数量。

Result: 动态窗口机制相比现有方法显著降低了被利用性（exploitability），提升了计算效率和算法的鲁棒性。

Conclusion: 通过动态窗口和无模拟技术，显著减轻了PSRO的计算负担，提高了实用性和策略质量，具有良好兼容性，适用于多种环境。

Abstract: Policy Space Response Oracles (PSRO) combines game-theoretic equilibrium computation with learning and is effective in approximating Nash Equilibrium in zero-sum games. However, the computational cost of PSRO has become a significant limitation to its practical application. Our analysis shows that game simulation is the primary bottleneck in PSRO's runtime. To address this issue, we conclude the concept of Simulation-Free PSRO and summarize existing methods that instantiate this concept. Additionally, we propose a novel Dynamic Window-based Simulation-Free PSRO, which introduces the concept of a strategy window to replace the original strategy set maintained in PSRO. The number of strategies in the strategy window is limited, thereby simplifying opponent strategy selection and improving the robustness of the best response. Moreover, we use Nash Clustering to select the strategy to be eliminated, ensuring that the number of strategies within the strategy window is effectively limited. Our experiments across various environments demonstrate that the Dynamic Window mechanism significantly reduces exploitability compared to existing methods, while also exhibiting excellent compatibility. Our code is available at https://github.com/enochliu98/SF-PSRO.

</details>


### [57] [On the Transition to an Auction-based Intelligent Parking Assignment System](https://arxiv.org/abs/2601.05429)
*Levente Alekszejenkó,Dobrowiecki Tadeusz*

Main category: cs.MA

TL;DR: 通过仿真研究，拍卖式停车分配系统能改善交通流并驱动市场价格，但成本增加促使更多司机参与。


<details>
  <summary>Details</summary>
Motivation: 寻找城市中的免费停车位变得越来越困难，需要创新的停车分配系统来缓解停车难题。

Method: 通过Eclipse SUMO仿真，考虑不同参与率下的拍卖式停车分配系统，模拟司机使用智能手机预约系统的场景，分析市场渗透率对交通流、系统性能及财务结果的影响。

Result: 拍卖式停车分配系统能够随着参与率的增加改善交通流，参与者能更接近首选停车位停车。

Conclusion: 该系统提高了参与者的停车费用，且非参与者的停车费用更高，促使更多司机倾向于使用该系统。

Abstract: Finding a free parking space in a city has become a challenging task over the past decades. A recently proposed auction-based parking assignment can alleviate cruising for parking and also set a market-driven, demand-responsive parking price. However, the wide acceptance of such a system is far from certain.
  To evaluate the merits of auction-based parking assignment, we assume that drivers have access to a smartphone-based reservation system prior to its mandatory introduction and thus have the opportunity to test and experience its merits voluntarily. We set our experiment as Eclipse SUMO simulations with different rates of participants and non-participants to check how different market penetration levels affect the traffic flow, the performance of the auction-based assignment system, and the financial outcomes. The results show that the auction-based system improves traffic flow with increasing penetration rates, allowing participants to park gradually closer to their preferred parking lots. However, it comes with a price; the system also increases parking expenditures for participants. Interestingly, non-participating drivers will face even higher parking prices. Consequently, they will be motivated to use the new system.

</details>


### [58] [EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting](https://arxiv.org/abs/2601.05487)
*Huanxiang Lin,Qianyue Wang,Jinwu Hu,Bailin Chen,Qing Du,Mingkui Tan*

Main category: cs.MA

TL;DR: 该论文提出了EvidFuse框架，通过两个协作组件实现写作过程中图表与文本的实时交叉生成，解决了现有方法图表文本不一致和分析停滞的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的报告生成方法通常为分阶段流程，造成图文不一致及证据空间固定，限制了分析的深度和动态扩展能力。

Method: 通过一个数据增强分析代理利用探索性数据分析知识和访问原始表格，结合一个实时证据构造写作代理规划大纲并间歇发出细粒度分析请求，实现文本和图表的交叉生成。

Result: EvidFuse在自动评估和人工评价中均位列第一，表明其在图表质量、图文对齐以及报告有用性方面表现优异。

Conclusion: EvidFuse显著提升了图表质量和文本图表对齐度，实现了更深入和动态的数据驱动报告生成，获得了最佳的人工和模型评估结果。

Abstract: Data-driven reports communicate decision-relevant insights by tightly interleaving narrative text with charts grounded in underlying tables. However, current LLM-based systems typically generate narratives and visualizations in staged pipelines, following either a text-first-graph-second or a graph-first-text-second paradigm. These designs often lead to chart-text inconsistency and insight freezing, where the intermediate evidence space becomes fixed and the model can no longer retrieve or construct new visual evidence as the narrative evolves, resulting in shallow and predefined analysis. To address the limitations, we propose \textbf{EvidFuse}, a training-free multi-agent framework that enables writing-time text-chart interleaved generation for data-driven reports. EvidFuse decouples visualization analysis from long-form drafting via two collaborating components: a \textbf{Data-Augmented Analysis Agent}, equipped with Exploratory Data Analysis (EDA)-derived knowledge and access to raw tables, and a \textbf{Real-Time Evidence Construction Writer} that plans an outline and drafts the report while intermittently issuing fine-grained analysis requests. This design allows visual evidence to be constructed and incorporated exactly when the narrative requires it, directly constraining subsequent claims and enabling on-demand expansion of the evidence space. Experiments demonstrate that EvidFuse attains the top rank in both LLM-as-a-judge and human evaluations on chart quality, chart-text alignment, and report-level usefulness.

</details>


### [59] [How Exploration Breaks Cooperation in Shared-Policy Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05509)
*Yi-Ning Weng,Hsuan-Wei Lee*

Main category: cs.MA

TL;DR: 共享策略的多智能体深度Q学习在动态社会困境中因探索引发的表示失败，导致合作崩溃，这是共享参数体系的根本失效模式。


<details>
  <summary>Details</summary>
Motivation: 探讨共享参数下多智能体系统中合作行为崩溃的根本原因，以及动态社会困境中合作难以维持的机制。

Method: 通过共享策略的深度Q网络(DQN)进行多智能体强化学习，并在受控实验中分析探索策略对合作稳定性的影响。

Result: 发现共享DQN在标准探索下会系统性地导致合作崩溃，原因不是奖励不对齐、噪声或训练不足，而是部分可观测性与参数耦合导致的表示失败。移除参数共享或采用独立表示可以消除该失效。

Conclusion: 共享策略与参数耦合在多智能体强化学习中可能系统性损害合作，设计多智能体学习系统时应避免完全共享参数，采用独立表示以维护合作性。

Abstract: Multi-agent reinforcement learning in dynamic social dilemmas commonly relies on parameter sharing to enable scalability. We show that in shared-policy Deep Q-Network learning, standard exploration can induce a robust and systematic collapse of cooperation even in environments where fully cooperative equilibria are stable and payoff dominant. Through controlled experiments, we demonstrate that shared DQN converges to stable but persistently low-cooperation regimes. This collapse is not caused by reward misalignment, noise, or insufficient training, but by a representational failure arising from partial observability combined with parameter coupling across heterogeneous agent states. Exploration-driven updates bias the shared representation toward locally dominant defection responses, which then propagate across agents and suppress cooperative learning. We confirm that the failure persists across network sizes, exploration schedules, and payoff structures, and disappears when parameter sharing is removed or when agents maintain independent representations. These results identify a fundamental failure mode of shared-policy MARL and establish structural conditions under which scalable learning architectures can systematically undermine cooperation. Our findings provide concrete guidance for the design of multi-agent learning systems in social and economic environments where collective behavior is critical.

</details>


### [60] [Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting](https://arxiv.org/abs/2601.05606)
*Chen Han,Jin Tan,Bohan Yu,Wenzhen Zheng,Xijin Tang*

Main category: cs.MA

TL;DR: 本文研究了大语言模型在多智能体系统中，网络拓扑如何影响从众行为和集体决策表现，通过置信度归一化规则比较集中式和分布式决策，揭示了网络结构对效率、鲁棒性及失败风险的关键影响。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中，LLMs通过社交互动而非独立推理进行集体决策，然而从众机制如何受网络拓扑影响尚未得到充分研究，本文旨在系统揭示网络结构在这一过程中对从众动态及决策结果的作用。

Method: 设计了置信度归一化聚合规则用于控制个体自信与社会影响的权衡，借助误信息检测任务，比较了集中式聚合和分布式共识两种决策模式在不同网络拓扑下的表现，通过实验分析其对集体决策效率和鲁棒性的影响。

Result: 本文系统研究了在多智能体系统中大语言模型（LLMs）如何通过网络拓扑结构影响从众动态，特别是在误信息检测任务中的表现。提出了一种置信度归一化聚合规则，平衡了个体自信和社会影响，比较了集中式聚合与分布式共识两种决策范式。实验表明，网络结构决定了集体判断的效率与鲁棒性。集中式结构做决策迅速但依赖关键节点能力且存在模型一致性偏差；分布式结构更健壮，共识速度受网络连通性影响，但连通性过高可能引发错误且自信的决策级联。本文揭示了网络拓扑和个体权重如何共同影响多智能体系统中的集体决策性能及失败模式。

Conclusion: 网络拓扑结构及个体自信与社会影响的权重调节共同决定了LLM多智能体系统中从众动态的效率、鲁棒性与失败模式，集中式决策快速但易受关键节点影响，分布式决策鲁棒性更强但连通性过高风险增加。

Abstract: Large Language Models (LLMs) are increasingly instantiated as interacting agents in multi-agent systems (MAS), where collective decisions emerge through social interaction rather than independent reasoning. A fundamental yet underexplored mechanism in this process is conformity, the tendency of agents to align their judgments with prevailing group opinions. This paper presents a systematic study of how network topology shapes conformity dynamics in LLM-based MAS through a misinformation detection task. We introduce a confidence-normalized pooling rule that controls the trade-off between self-reliance and social influence, enabling comparisons between two canonical decision paradigms: Centralized Aggregation and Distributed Consensus. Experimental results demonstrate that network topology critically governs both the efficiency and robustness of collective judgments. Centralized structures enable immediate decisions but are sensitive to hub competence and exhibit same-model alignment biases. In contrast, distributed structures promote more robust consensus, while increased network connectivity speeds up convergence but also heightens the risk of wrong-but-sure cascades, in which agents converge on incorrect decisions with high confidence. These findings characterize the conformity dynamics in LLM-based MAS, clarifying how network topology and self-social weighting jointly shape the efficiency, robustness, and failure modes of collective decision-making.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [61] [DafnyPro: LLM-Assisted Automated Verification for Dafny Programs](https://arxiv.org/abs/2601.05385)
*Debangshu Banerjee,Olivier Bouissou,Stefan Zetzsche*

Main category: cs.SE

TL;DR: 提出DafnyPro框架通过差异检查、剪枝和提示增强提升大型语言模型在Dafny验证注释生成上的表现，显著提高验证正确率，且小模型也能保持高性能。


<details>
  <summary>Details</summary>
Motivation: 提升LLM在程序验证注释生成中的表现，解决基础程序逻辑被错误修改和不必要注释冗余问题。

Method: DafnyPro包含差异检查器、剪枝器和提示增强系统，通过防止基础程序逻辑修改、删除不必要的不变式以及复用预定义证明策略提升模型表现。

Result: 在多项基准测试中，使用DafnyPro的模型一致获得性能提升，最具挑战性的DafnyBench上准确率提高了16个百分点。微调的小模型也展现出高验证准确率。

Conclusion: DafnyPro能够显著提升大型语言模型在Dafny中生成验证注释的能力，增强了验证准确率和效率。

Abstract: We present DafnyPro, an inference-time framework that enhances LLMs for generating verification annotations in Dafny. DafnyPro comprises three key components: a diff-checker that prevents modifications to base program logic, a pruner that removes unnecessary invariants, and a hint-augmentation system that retrieves and applies predefined, problem-independent proof strategies. We evaluate DafnyPro using Claude Sonnet 3.5 and 3.7 on four benchmarks: Clover, MBPP-Dafny, HumanEval-Dafny, and DafnyBench, achieving consistent performance gains in all cases. Notably, on DafnyBench, the most challenging benchmark, Claude Sonnet 3.5 enhanced with DafnyPro achieves 86% correct proofs, a 16 pp improvement over the base model. We also fine-tune two Qwen models on training data derived from verification attempts by larger models enhanced with DafnyPro. Our 7B and 14B models achieve 68% and 70% correct proofs on DafnyBench, respectively, demonstrating that smaller models can maintain high verification accuracy.

</details>


### [62] [Uncovering Failures in Cyber-Physical System State Transitions: A Fuzzing-Based Approach Applied to sUAS](https://arxiv.org/abs/2601.05449)
*Theodore Chambers,Arturo Miguel Russell Bernal,Michael Vierhauser,Jane Cleland-Huang*

Main category: cs.SE

TL;DR: 本文提出SaFUZZ，一种针对小型无人机系统状态转换的模糊测试方法，成功揭示真实系统中的多种未发现故障，提升系统安全性。


<details>
  <summary>Details</summary>
Motivation: 小型无人机系统（sUAS）在安全关键环境中的广泛应用需要严格验证其决策逻辑在各种条件下的表现。

Method: 提出了SaFUZZ，一种状态感知的模糊测试流程，通过设计模糊测试规范检测行为偏差，并动态生成故障树以可视化状态、模式及环境因素；在高保真仿真环境和真实场地中验证系统。

Result: SaFUZZ在真实sUAS系统中发现了开发团队未检测到的多处故障点，验证了其在发现状态转换失败方面的实用性和可扩展性。

Conclusion: SaFUZZ为sUAS应用中的状态转换失败提供了一种有效且可实施的检测方法，能够辅助项目方分析故障并定位根因。

Abstract: The increasing deployment of small Uncrewed Aerial Systems (sUAS) in diverse and often safety-critical environments demands rigorous validation of onboard decision logic under various conditions. In this paper, we present SaFUZZ, a state-aware fuzzing pipeline that validates core behavior associated with state transitions, automated failsafes, and human operator interactions in sUAS applications operating under various timing conditions and environmental disturbances. We create fuzzing specifications to detect behavioral deviations, and then dynamically generate associated Fault Trees to visualize states, modes, and environmental factors that contribute to the failure, thereby helping project stakeholders to analyze the failure and identify its root causes. We validated SaFUZZ against a real-world sUAS system and were able to identify several points of failure not previously detected by the system's development team. The fuzzing was conducted in a high-fidelity simulation environment, and outcomes were validated on physical sUAS in a real-world field testing setting. The findings from the study demonstrated SaFUZZ's ability to provide a practical and scalable approach to uncovering diverse state transition failures in a real-world sUAS application.

</details>


### [63] [Rethinking Basis Path Testing: Mixed Integer Programming Approach for Test Path Set Generation](https://arxiv.org/abs/2601.05463)
*Chao Wei,Xinyi Peng,Yawen Yan,Mao Luo,Ting Cai*

Main category: cs.SE

TL;DR: 本文提出了一种基于混合整数规划（MIP）的路径生成框架，克服了传统贪心算法生成路径结构次优的问题，成功实现了结构简单且全覆盖的基路径集合生成。


<details>
  <summary>Details</summary>
Motivation: 传统基路径测试依赖贪心的图遍历算法（如DFS/BFS），路径质量较差，导致自动测试数据生成困难且增加人工认知负担，因此需要一种结构更优的基路径生成方法。

Method: 作者将基路径生成问题转化为声明式优化问题，提出了整体MIP模型和增量MIP策略，后者通过多目标函数和新颖性惩罚机制优化路径结构，兼顾理论最优解和实时计算效率。

Result: 实验结果显示，增量MIP策略在实际代码和大规模合成控制流图上均能100%生成完整基路径集，且保持计算效率，验证了其在结构简单性和生成成功率上的优势。

Conclusion: 本文提出的增量型MIP策略在生成完整基路径集方面成功率达到100%，在结构简单性和计算效率上均表现优异，可显著提升后续测试数据生成的效率。

Abstract: Basis path testing is a cornerstone of structural testing, yet traditional automated methods, relying on greedy graph-traversal algorithms (e.g., DFS/BFS), often generate sub-optimal paths. This structural inferiority is not a trivial issue; it directly impedes downstream testing activities by complicating automated test data generation and increasing the cognitive load for human engineers. This paper reframes basis path generation from a procedural search task into a declarative optimization problem. We introduce a Mixed Integer Programming (MIP) framework designed to produce a complete basis path set that is globally optimal in its structural simplicity. Our framework includes two complementary strategies: a Holistic MIP model that guarantees a theoretically optimal path set, and a scalable Incremental MIP strategy for large, complex topologies. The incremental approach features a multi-objective function that prioritizes path simplicity and incorporates a novelty penalty to maximize the successful generation of linearly independent paths. Empirical evaluations on both real-code and large-scale synthetic Control Flow Graphs demonstrate that our Incremental MIP strategy achieves a 100\% success rate in generating complete basis sets, while remaining computationally efficient. Our work provides a foundational method for generating a high-quality structural "scaffold" that can enhance the efficiency and effectiveness of subsequent test generation efforts.

</details>


### [64] [STELP: Secure Transpilation and Execution of LLM-Generated Programs](https://arxiv.org/abs/2601.05467)
*Swapnil Shinde,Sahil Wadhwa,Andy Luo,Emily Chen*

Main category: cs.SE

TL;DR: 本文提出了一种名为STELP的安全转换执行系统，用于安全执行LLM生成的代码，解决了代码不稳定、易出错及安全漏洞等问题，保障生产环境中自动化AI系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的代码存在潜在的错误和安全风险，且传统的人工审查和测试手段难以适用，亟需一种自动化且安全的执行机制保障代码的安全运行。

Method: 设计并实现了STELP安全转换执行器，通过受控环境执行代码，并结合人工验证的数据集进行评测，确保代码的正确性、安全性和执行效率。

Result: 通过公开数据集的基准测试，STELP在代码正确性、安全性和执行延迟方面表现优异，显著优于现有方法，尤其在安全执行高风险代码方面取得突破。

Conclusion: STELP有效提升了LLM生成代码执行的安全性和可靠性，超越现有方法，尤其在安全执行高风险代码片段方面表现优异。

Abstract: Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.

</details>


### [65] [Readability-Robust Code Summarization via Meta Curriculum Learning](https://arxiv.org/abs/2601.05485)
*Wenhao Zeng,Yitian Chai,Hao Zhou,Fandong Meng,Jie Zhou,Xiaodong Gu*

Main category: cs.SE

TL;DR: 本文评估了主流代码总结模型在难读代码上的性能下降，提出RoFTCodeSum方法提升模型鲁棒性和准确率，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有代码语言模型在高可读代码上表现良好，但面对结构差或混淆代码时性能显著下降，且提示工程和推理增强方法提升有限。

Method: 提出RoFTCodeSum微调方法，将课程学习和元学习结合，通过逐步增加代码难度的训练数据进行梯度元更新，以提升模型对难读代码的鲁棒性。

Result: 实验表明RoFTCodeSum方法在抗语义扰动的鲁棒性和原始代码性能上均有提升，优于GPT-4o和DeepSeek-V3等现有模型。

Conclusion: RoFTCodeSum有效提升了代码总结模型在难读代码上的鲁棒性和准确性，为实际应用中的代码理解提供了更加稳健的技术支持。

Abstract: Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.

</details>


### [66] [Evaluating the Use of LLMs for Automated DOM-Level Resolution of Web Performance Issues](https://arxiv.org/abs/2601.05502)
*Gideon Peters,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 本研究评测了九种大型语言模型在自动优化网页性能中的表现，发现它们在SEO和无障碍方面表现优异，但在性能关键的DOM修改上效果不一，GPT-4.1表现最佳。


<details>
  <summary>Details</summary>
Motivation: 用户需要快速无缝的网页体验，但开发者在有限条件下难以满足，这使网页性能优化成为关键且复杂的任务，尤其是 DOM 修改。

Method: 评估九种最先进的大型语言模型(LLMs)在自动解决网页性能问题中的效果，通过提取15个热门网页的DOM和性能报告，输入模型进行问题解决。

Result: 所有模型在SEO和无障碍问题上表现优异，但在关键性能相关的DOM操作上效果参差不齐，其中GPT-4.1表现最佳，显著减少性能审计问题，而GPT-4o-mini持续表现较差。修改策略主要为添加和位置调整，但存在影响视觉稳定性的回归。

Conclusion: 先进的LLMs尤其是高性能模型，在自动优化网页性能问题上展现出潜力，尤其是SEO和无障碍领域，但在性能关键的DOM改动方面仍需改进。

Abstract: Users demand fast, seamless webpage experiences, yet developers often struggle to meet these expectations within tight constraints. Performance optimization, while critical, is a time-consuming and often manual process. One of the most complex tasks in this domain is modifying the Document Object Model (DOM), which is why this study focuses on it. Recent advances in Large Language Models (LLMs) offer a promising avenue to automate this complex task, potentially transforming how developers address web performance issues. This study evaluates the effectiveness of nine state-of-the-art LLMs for automated web performance issue resolution. For this purpose, we first extracted the DOM trees of 15 popular webpages (e.g., Facebook), and then we used Lighthouse to retrieve their performance audit reports. Subsequently, we passed the extracted DOM trees and corresponding audits to each model for resolution. Our study considers 7 unique audit categories, revealing that LLMs universally excel at SEO & Accessibility issues. However, their efficacy in performance-critical DOM manipulations is mixed. While high-performing models like GPT-4.1 delivered significant reductions in areas like Initial Load, Interactivity, and Network Optimization (e.g., 46.52% to 48.68% audit incidence reductions), others, such as GPT-4o-mini, notably underperformed, consistently. A further analysis of these modifications showed a predominant additive strategy and frequent positional changes, alongside regressions particularly impacting Visual Stability.

</details>


### [67] [LIDL: LLM Integration Defect Localization via Knowledge Graph-Enhanced Multi-Agent Analysis](https://arxiv.org/abs/2601.05539)
*Gou Tan,Zilong He,Min Li,Pengfei Chen,Jieke Shi,Zhensu Sun,Ting Zhang,Danwen Chen,Lwin Khin Shar,Chuanfu Zhang,David Lo*

Main category: cs.SE

TL;DR: 本文提出的LIDL框架通过构建知识图谱和融合多源错误线索，精准定位集成大型语言模型的复杂软件缺陷，显著优于现有方法，且成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 传统软件的缺陷定位技术无法有效识别集成大型语言模型(LLM)的软件中的特定集成缺陷，这些缺陷源自代码错误之外的LLM交互错误，如提示、API调用、配置和模型输出之间的错配。

Method: 提出LIDL，一个多智能体框架，通过构建带有LLM感知注释的代码知识图谱，融合由LLM推断的三种错误证据来源，以及采用基于反事实推理的上下文感知验证，以定位LLM集成软件的缺陷。

Result: 在146个真实缺陷样本上的评测结果显示，LIDL在准确率和均值平均精度方面均显著优于五个最先进基线，Top-3准确率为0.64、MAP为0.48，分别提升64.1%；且成本降低了92.5%。

Conclusion: LIDL有效解决了传统缺陷定位方法难以识别LLM集成软件特定缺陷的问题，实现了高准确率和高成本效益，提升了缺陷定位的效率和精度。

Abstract: LLM-integrated software, which embeds or interacts with large language models (LLMs) as functional components, exhibits probabilistic and context-dependent behaviors that fundamentally differ from those of traditional software. This shift introduces a new category of integration defects that arise not only from code errors but also from misaligned interactions among LLM-specific artifacts, including prompts, API calls, configurations, and model outputs. However, existing defect localization techniques are ineffective at identifying these LLM-specific integration defects because they fail to capture cross-layer dependencies across heterogeneous artifacts, cannot exploit incomplete or misleading error traces, and lack semantic reasoning capabilities for identifying root causes.
  To address these challenges, we propose LIDL, a multi-agent framework for defect localization in LLM-integrated software. LIDL (1) constructs a code knowledge graph enriched with LLM-aware annotations that represent interaction boundaries across source code, prompts, and configuration files, (2) fuses three complementary sources of error evidence inferred by LLMs to surface candidate defect locations, and (3) applies context-aware validation that uses counterfactual reasoning to distinguish true root causes from propagated symptoms. We evaluate LIDL on 146 real-world defect instances collected from 105 GitHub repositories and 16 agent-based systems. The results show that LIDL significantly outperforms five state-of-the-art baselines across all metrics, achieving a Top-3 accuracy of 0.64 and a MAP of 0.48, which represents a 64.1% improvement over the best-performing baseline. Notably, LIDL achieves these gains while reducing cost by 92.5%, demonstrating both high accuracy and cost efficiency.

</details>


### [68] [Empirical Characterization of Logging Smells in Machine Learning Code](https://arxiv.org/abs/2601.05540)
*Patrick Loic Foalem,Leuson Da Silva,Foutse Khomh,Ettore Merlo,Heng Li*

Main category: cs.SE

TL;DR: 本文通过大规模挖掘GitHub开源机器学习项目，系统识别和分类机器学习系统中的日志异味（即不良日志实践），并通过调查ML工程师评估这些异味的相关性、严重性和频率。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习组件整合到软件系统中，日志对于模型训练和部署的可复现性、可追踪性和可观察性至关重要，但目前对于ML系统中的日志实践及其问题缺乏系统研究。

Method: 大规模挖掘GitHub上的开源机器学习仓库，识别日志异味；随后通过ML工程师调查评估这些日志异味的实际影响。

Result: 识别出多种在ML系统中反复出现的日志异味，确认了这些问题在实际开发中的普遍性和严重性，且这些发现有助于指导未来的日志改进工作。

Conclusion: 研究发现了机器学习系统中存在的常见日志异味，强调了改进日志实践的重要性，为ML开发中的日志管理提供了实证基础。

Abstract: \underline{Context:} Logging is a fundamental yet complex practice in software engineering, essential for monitoring, debugging, and auditing software systems. With the increasing integration of machine learning (ML) components into software systems, effective logging has become critical to ensure reproducibility, traceability, and observability throughout model training and deployment. Although various general-purpose and ML-specific logging frameworks exist, little is known about how these tools are actually used in practice or whether ML practitioners adopt consistent and effective logging strategies. To date, no empirical study has systematically characterized recurring bad logging practices--or logging smells--in ML System. \underline{Goal:} This study aims to empirically identify and characterize logging smells in ML systems, providing an evidence-based understanding of how logging is implemented and challenged in practice. \underline{Method:} We propose to conduct a large-scale mining of open-source ML repositories hosted on GitHub to catalogue recurring logging smells. Subsequently, a practitioner survey involving ML engineers will be conducted to assess the perceived relevance, severity, and frequency of the identified smells. \underline{Limitations:} % While The study's limitations include that While our findings may not be generalizable to closed-source industrial projects, we believe our study provides an essential step toward understanding and improving logging practices in ML development.

</details>


### [69] [Understanding LLM-Driven Test Oracle Generation](https://arxiv.org/abs/2601.05542)
*Adam Bodicoat,Gunel Jahangirova,Valerio Terragni*

Main category: cs.SE

TL;DR: 本文通过实证研究，探索大型语言模型在自动生成软件测试预言机中的表现和影响因素。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动单元测试生成技术中预言机问题，即如何区分程序行为的正确与否。

Method: 使用不同的提示策略和上下文输入级别，研究大型语言模型(LLMs)生成单元测试预言机的效果。

Result: 发现不同的提示策略和上下文输入影响LLMs生成的测试预言机质量，指出了LLM在生成测试预言机方面的优势与局限。

Conclusion: LLMs为自动测试预言机生成提供了新的可能，但其效果依赖于提示设计和上下文，未来研究需进一步提升其准确性和实用性。

Abstract: Automated unit test generation aims to improve software quality while reducing the time and effort required for creating tests manually. However, existing techniques primarily generate regression oracles that predicate on the implemented behavior of the class under test. They do not address the oracle problem: the challenge of distinguishing correct from incorrect program behavior. With the rise of Foundation Models (FMs), particularly Large Language Models (LLMs), there is a new opportunity to generate test oracles that reflect intended behavior. This positions LLMs as enablers of Promptware, where software creation and testing are driven by natural-language prompts. This paper presents an empirical study on the effectiveness of LLMs in generating test oracles that expose software failures. We investigate how different prompting strategies and levels of contextual input impact the quality of LLM-generated oracles. Our findings offer insights into the strengths and limitations of LLM-based oracle generation in the FM era, improving our understanding of their capabilities and fostering future research in this area.

</details>


### [70] [An Empirical Study of Policy-as-Code Adoption in Open-Source Software Projects](https://arxiv.org/abs/2601.05555)
*Patrick Loic Foalem,Foutse Khomh,Leuson Da Silva,Ettore Merlo*

Main category: cs.SE

TL;DR: 通过大规模研究，揭示了PaC工具的实际应用场景、目的和治理活动，提供了实证基础和改进工具互操作性的机会。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域缺乏对PaC工具在实际开发中的应用的实证理解。

Method: 分析399个GitHub仓库，采用9种流行的政策即代码（PaC）工具，结合定量和定性方法，并利用大型语言模型辅助分类。

Result: 发现PaC工具在早期项目中广泛应用，主要用于治理、配置控制和文档，还在MLOps管道中出现，揭示了强烈的共用模式和治理意图。

Conclusion: 研究为实践者和开发者提供了具体的使用模式和改进建议，推动未来PaC研究与可信合规软件系统的发展。

Abstract: \textbf{Context:} Policy-as-Code (PaC) has become a foundational approach for embedding governance, compliance, and security requirements directly into software systems. While organizations increasingly adopt PaC tools, the software engineering community lacks an empirical understanding of how these tools are used in real-world development practices.
  \textbf{Objective:} This paper aims to bridge this gap by conducting the first large-scale study of PaC usage in open-source software. Our goal is to characterize how PaC tools are adopted, what purposes they serve, and what governance activities they support across diverse software ecosystems.
  \textbf{Method:} We analyzed 399 GitHub repositories using nine widely adopted PaC tools. Our mixed-methods approach combines quantitative analysis of tool usage and project characteristics with a qualitative investigation of policy files. We further employ a Large Language Model (LLM)--assisted classification pipeline, refined through expert validation, to derive a taxonomy of PaC usage consisting of 5 categories and 15 sub-categories.
  \textbf{Results:} Our study reveals substantial diversity in PaC adoption. PaC tools are frequently used in early-stage projects and are heavily oriented toward governance, configuration control, and documentation. We also observe emerging PaC usage in MLOps pipelines and strong co-usage patterns, such as between OPA and Gatekeeper. Our taxonomy highlights recurring governance intents.
  \textbf{Conclusion:} Our findings offer actionable insights for practitioners and tool developers. They highlight concrete usage patterns, emphasize actual PaC usage, and motivate opportunities for improving tool interoperability. This study lays the empirical foundation for future research on PaC practices and their role in ensuring trustworthy, compliant software systems.

</details>


### [71] [Package-Aware Approach for Repository-Level Code Completion in Pharo](https://arxiv.org/abs/2601.05617)
*Omar Abedelkader,Stéphane Ducasse,Oleksandr Zaitsev,Romain Robbes,Guillermo Polito*

Main category: cs.SE

TL;DR: 本文提出基于包感知的变量名称补全启发式，提升了补全系统的准确性，更好地利用了代码仓库的结构信息。


<details>
  <summary>Details</summary>
Motivation: 现有的补全引擎未能充分利用代码仓库的包结构信息，导致全局名称被平等对待，降低了补全建议的相关性。

Method: 提出一种包结构感知的启发式搜索方法，优先搜索请求类所在包的变量名，然后扩展到同仓库其他包，最后考虑全局命名空间。

Result: 基于该方法的补全策略在平均倒数排名（MRR）指标上优于默认的语义启发式和全局命名空间查询方法。

Conclusion: 引入包感知的启发式方法能够显著提升自动补全的准确性和相关性。

Abstract: Pharo offers a sophisticated completion engine based on semantic heuristics, which coordinates specific fetchers within a lazy architecture. These heuristics can be recomposed to support various activities (e.g., live programming or history usage navigation). While this system is powerful, it does not account for the repository structure when suggesting global names such as class names, class variables, or global variables. As a result, it does not prioritize classes within the same package or project, treating all global names equally. In this paper, we present a new heuristic that addresses this limitation. Our approach searches variable names in a structured manner: it begins with the package of the requesting class, then expands to other packages within the same repository, and finally considers the global namespace. We describe the logic behind this heuristic and evaluate it against the default semantic heuristic and one that directly queries the global namespace. Preliminary results indicate that the Mean Reciprocal Rank (MRR) improves, confirming that package-awareness completions deliver more accurate and relevant suggestions than the previous flat global approach.

</details>


### [72] [A Large Scale Empirical Analysis on the Adherence Gap between Standards and Tools in SBOM](https://arxiv.org/abs/2601.05622)
*Chengjie Wang,Jingzheng Wu,Hao Lyu,Xiang Ling,Tianyue Luo,Yanjun Wu,Chen Zhao*

Main category: cs.SE

TL;DR: 本研究首次大规模实证分析了SBOM工具对标准的遵守情况，揭示其存在显著合规和稳定性不足，提出根本原因分析及实际改进方案。


<details>
  <summary>Details</summary>
Motivation: 尽管已有SBOM标准和工具，针对SBOM工具对标准规范的遵守情况却少有研究，存在合规缺失和SBOM应用中断的风险。

Method: 通过自动化评估框架SAP，采用大规模两阶段实证分析方法，对6个SBOM工具生成的55,444个SBOM文件进行基于标准规范的合规性评估，包括基线评估和一年期纵向跟踪。

Result: 发现当前SBOM工具存在根本性不足：(1) 对策略合规性支持不充分；(2) 工具间和工具自身版本间一致性较低，包检测一致率最低仅为7.84%-12.77%；(3) 细节软件信息准确率较差，如包许可证准确率低于20%。

Conclusion: 当前主流SBOM工具在遵守标准规范方面存在显著差距，亟需改进合规支持和工具一致性，推动SBOM的可靠应用。

Abstract: A Software Bill of Materials (SBOM) is a machine-readable artifact that systematically organizes software information, enhancing supply chain transparency and security. To facilitate the exchange and utilization of SBOMs, organizations such as the Linux Foundation and OWASP have proposed SBOM standards. Following standards, organizations have developed tools for generating and utilizing SBOMs. However, limited research has examined the adherence of these SBOM tools to standard specifications, a gap that could lead to compliance failures and disruptions in SBOM utilization. This paper presents the first large-scale, two-stage empirical analysis of the adherence gap, using our automated evaluation framework, SAP. The evaluation, comprising a baseline evaluation and a one-year longitudinal follow-up, covers 55,444 SBOMs generated by six SBOM tools from 3,287 real-world repositories. Our analysis reveals persistent, fundamental limitations in current SBOM tools: (1) inadequate compliance support with policy requirements; (2) poor tool consistencies, including inter-tool consistency rates as low as 7.84% to 12.77% for package detection across languages, and significant longitudinal inconsistency, where tools show low consistency with their own prior versions; and (3) mediocre to poor accuracy for detailed software information, e.g., accuracy of package licenses below 20%. We analyze the root causes of these gaps and provide practical solutions. All the code, replication docker image, evaluation results are open sourced at [GitHub](https://github.com/dw763j/SAP) and [Zenodo](https://doi.org/10.5281/zenodo.14998624) for further researches.

</details>


### [73] [Tracing Stereotypes in Pre-trained Transformers: From Biased Neurons to Fairer Models](https://arxiv.org/abs/2601.05663)
*Gianmario Voria,Moses Openja,Foutse Khomh,Gemma Catolino,Fabio Palomba*

Main category: cs.SE

TL;DR: 该论文提出了一种基于神经元编辑的方法，在BERT模型中定位并抑制偏见神经元，实现对偏见的有效缓解且维持了模型性能，提升了软件工程中AI系统的公平性。


<details>
  <summary>Details</summary>
Motivation: 变压器语言模型在软件工程中应用广泛，但存在复制和放大社会偏见的问题，亟需有效可解释的偏见检测与缓解方法。

Method: 构建包含九种偏见类型的偏见关系数据集，采用神经元归因策略追踪并抑制BERT模型中的偏见神经元，并评估其对软件工程任务的影响。

Result: 发现偏见知识集中在少数神经元，通过抑制这些神经元可显著减少模型偏见，同时对SE任务性能影响有限。

Conclusion: 偏见知识在变压器模型的少数神经元中局部化，抑制这些神经元能显著减少偏见且性能损失很小，表明在软件工程中可通过神经元层面追踪和缓解偏见。

Abstract: The advent of transformer-based language models has reshaped how AI systems process and generate text. In software engineering (SE), these models now support diverse activities, accelerating automation and decision-making. Yet, evidence shows that these models can reproduce or amplify social biases, raising fairness concerns. Recent work on neuron editing has shown that internal activations in pre-trained transformers can be traced and modified to alter model behavior. Building on the concept of knowledge neurons, neurons that encode factual information, we hypothesize the existence of biased neurons that capture stereotypical associations within pre-trained transformers. To test this hypothesis, we build a dataset of biased relations, i.e., triplets encoding stereotypes across nine bias types, and adapt neuron attribution strategies to trace and suppress biased neurons in BERT models. We then assess the impact of suppression on SE tasks. Our findings show that biased knowledge is localized within small neuron subsets, and suppressing them substantially reduces bias with minimal performance loss. This demonstrates that bias in transformers can be traced and mitigated at the neuron level, offering an interpretable approach to fairness in SE.

</details>


### [74] [Drivora: A Unified and Extensible Infrastructure for Search-based Autonomous Driving Testing](https://arxiv.org/abs/2601.05685)
*Mingfei Cheng,Lionel Briand,Yuan Zhou*

Main category: cs.SE

TL;DR: 本文提出了Drivora，一个基于CARLA模拟器的统一且可扩展的自动驾驶系统（ADS）搜索测试基础设施，通过统一的场景定义、进化计算驱动的测试引擎、并行执行机制及统一的ADS接口，实现了多样化场景测试与大规模批量模拟。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统测试方法多样且框架异构，导致难以复用和适配，亟需一个统一且可扩展的测试基础设施以支持多样化的测试需求。

Method: 采用统一的OpenScenario场景定义，基于进化计算的搜索测试引擎，支持并行场景执行以提高硬件利用率，并通过统一接口集成12个自动驾驶系统。

Result: 开发了基于CARLA的Drivora平台，支持多自动驾驶系统和多车辆测试，提供灵活的组件定制和高效的并行模拟执行，工具已开源供社区使用。

Conclusion: Drivora成功实现了一个统一、灵活且高效的自动驾驶系统测试平台，解决了现有测试框架异构导致的适配和重用难题，有助于提升ADS测试的安全性和可靠性。

Abstract: Search-based testing is critical for evaluating the safety and reliability of autonomous driving systems (ADSs). However, existing approaches are often built on heterogeneous frameworks (e.g., distinct scenario spaces, simulators, and ADSs), which require considerable effort to reuse and adapt across different settings. To address these challenges, we present Drivora, a unified and extensible infrastructure for search-based ADS testing built on the widely used CARLA simulator. Drivora introduces a unified scenario definition, OpenScenario, that specifies scenarios using low-level, actionable parameters to ensure compatibility with existing methods while supporting extensibility to new testing designs (e.g., multi-autonomous-vehicle testing). On top of this, Drivora decouples the testing engine, scenario execution, and ADS integration. The testing engine leverages evolutionary computation to explore new scenarios and supports flexible customization of core components. The scenario execution can run arbitrary scenarios using a parallel execution mechanism that maximizes hardware utilization for large-scale batch simulation. For ADS integration, Drivora provides access to 12 ADSs through a unified interface, streamlining configuration and simplifying the incorporation of new ADSs. Our tools are publicly available at https://github.com/MingfeiCheng/Drivora.

</details>


### [75] [AIBoMGen: Generating an AI Bill of Materials for Secure, Transparent, and Compliant Model Training](https://arxiv.org/abs/2601.05703)
*Wiebe Vandendriessche,Jordi Thijsman,Laurens D'hooge,Bruno Volckaert,Merlijn Sebrechts*

Main category: cs.SE

TL;DR: 本文提出基于SBOM扩展的AIBOM及其自动生成平台AIBoMGen，保证AI模型和环境记录的透明性和安全性，助力满足监管合规要求。


<details>
  <summary>Details</summary>
Motivation: 复杂AI系统快速普及导致透明度、安全性和合规性工具不足，急需标准化、可验证的AI模型及其环境记录。

Method: 提出了AIBOM（AI Bill of Materials），基于SBOM的扩展，设计了AIBoMGen平台自动生成带签名的AIBOM，采用加密哈希、数字签名和in-toto认证保证信息完整性。

Result: AIBoMGen能够可靠检测未授权修改，生成AIBOM时性能开销极小。

Conclusion: AIBoMGen为构建安全透明的AI生态系统奠定了基础，推动实现像欧盟AI法案这样的合规监管目标。

Abstract: The rapid adoption of complex AI systems has outpaced the development of tools to ensure their transparency, security, and regulatory compliance. In this paper, the AI Bill of Materials (AIBOM), an extension of the Software Bill of Materials (SBOM), is introduced as a standardized, verifiable record of trained AI models and their environments. Our proof-of-concept platform, AIBoMGen, automates the generation of signed AIBOMs by capturing datasets, model metadata, and environment details during training. The training platform acts as a neutral, third-party observer and root of trust. It enforces verifiable AIBOM creation for every job. The system uses cryptographic hashing, digital signatures, and in-toto attestations to ensure integrity and protect against threats such as artifact tampering by dishonest model creators. Our evaluation demonstrates that AIBoMGen reliably detects unauthorized modifications to all artifacts and can generate AIBOMs with negligible performance overhead. These results highlight the potential of AIBoMGen as a foundational step toward building secure and transparent AI ecosystems, enabling compliance with regulatory frameworks like the EUs AI Act.

</details>


### [76] [From Issues to Insights: RAG-based Explanation Generation from Software Engineering Artifacts](https://arxiv.org/abs/2601.05721)
*Daniel Pöttgen,Mersedeh Sadeghi,Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文首次利用检索增强生成方法，从问题追踪系统数据生成软件行为解释，实验显示生成解释高效且可信，提升了软件系统的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统日益复杂，理解其行为变得更加困难，传统文档常常过时或不完整，难以提供准确的上下文解释，而问题追踪系统中蕴含丰富且持续更新的开发知识，却未被充分利用于解释生成。

Method: 首次将检索增强生成（RAG）方法应用于从问题追踪数据中生成解释，构建了基于开源工具和语言模型的概念验证系统。

Result: 在GitHub项目的问题集中评估，生成的解释与人工编写的解释有90%的匹配度，系统展现出较强的可信度和对指令的遵从性。

Conclusion: 基于RAG的方法能够有效利用问题追踪数据扩展软件系统的可解释性，使系统行为更易于理解和解释，且适用范围超越传统的黑盒机器学习模型。

Abstract: The increasing complexity of modern software systems has made understanding their behavior increasingly challenging, driving the need for explainability to improve transparency and user trust. Traditional documentation is often outdated or incomplete, making it difficult to derive accurate, context-specific explanations. Meanwhile, issue-tracking systems capture rich and continuously updated development knowledge, but their potential for explainability remains untapped. With this work, we are the first to apply a Retrieval-Augmented Generation (RAG) approach for generating explanations from issue-tracking data. Our proof-of-concept system is implemented using open-source tools and language models, demonstrating the feasibility of leveraging structured issue data for explanation generation. Evaluating our approach on an exemplary project's set of GitHub issues, we achieve 90% alignment with human-written explanations. Additionally, our system exhibits strong faithfulness and instruction adherence, ensuring reliable and grounded explanations. These findings suggest that RAG-based methods can extend explainability beyond black-box ML models to a broader range of software systems, provided that issue-tracking data is available - making system behavior more accessible and interpretable.

</details>


### [77] [StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection](https://arxiv.org/abs/2601.05772)
*Qingyuan Li,Chenchen Yu,Chuanyi Li,Xin-Cheng Wen,Cheryl Lee,Cuiyun Gao,Bin Luo*

Main category: cs.SE

TL;DR: 提出一种结合结构引导的图分支和大语言模型的二进制安全补丁检测方法，有效提升闭源软件漏洞补丁检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对闭源软件二进制补丁检测面临的语义信息有限和缺乏合理语法结构、以及现有方法往往在同一项目中训练测试导致评估不现实的问题。

Method: 提出StriderSPD框架，结合图结构分支和大语言模型，通过结构信息引导识别安全补丁；设计图结构分支的适配器对齐汇编码与伪代码表示；采用两阶段训练策略解决不同分支参数不平衡问题。

Result: 构建了不同项目和领域的二进制SPD基准数据集，在此基准上广泛评估StriderSPD，验证其优越性能。

Conclusion: StriderSPD有效解决了二进制安全补丁检测中语义表达不足和训练评估不足的问题，提升了检测准确率并推动了现实闭源软件场景下的补丁检测研究。

Abstract: Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \textbf{\textit{StriderSPD}}, a \underline{Str}ucture-gu\underline{ide}d joint \underline{r}epresentation \underline{SPD} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.

</details>


### [78] [EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents](https://arxiv.org/abs/2601.05777)
*Yaoqi Guo,Ying Xiao,Jie M. Zhang,Mark Harman,Yiling Lou,Yang Liu,Zhenpeng Chen*

Main category: cs.SE

TL;DR: EET通过利用结构化历史经验实现软件工程代理的早期终止，显著降低运行成本且保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的软件工程代理在实际应用中成本较高，亟需有效降低其运行开销。

Method: 提出经验驱动的早期终止（EET）方法，通过提取先前问题解决过程中的结构化经验，指导补丁生成和选择阶段的早期终止，减少低效迭代。

Result: EET在SWE-bench Verified基准测试中，在三种代表性软件工程代理上实现了19%-55%（平均32%）的成本降低，解决率仅下降最多0.2%。

Conclusion: 经验驱动的早期终止有效提升了软件工程代理的执行效率，显著节约API调用和令牌数，且几乎不影响任务性能。

Abstract: Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.

</details>


### [79] [SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking](https://arxiv.org/abs/2601.05827)
*Zewei Lin,Jiachi Chen,Jingwen Zhang,Zexu Wang,Yuming Feng,Weizhe Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本研究定义并检测DeFi质押中的逻辑缺陷，提出基于大语言模型的静态分析工具SSR，有效发现合约中的安全问题。


<details>
  <summary>Details</summary>
Motivation: DeFi质押存在逻辑缺陷，攻击者可通过操纵奖励金额或重复领取奖励等手段获取不当利益，亟需针对DeFi质押逻辑缺陷的检测方法。

Method: 分析64起安全事件与144份审计报告，归纳出六种DeFi质押逻辑缺陷类型，设计并实现基于大语言模型的静态分析工具SSR，通过构建DeFi质押模型并结合语义特征检测逻辑缺陷。

Result: SSR工具在基于真实数据集的评估中达到92.31%准确率、87.92%召回率和88.85% F1值；对约16,000份合约检测发现约22.24%存在逻辑缺陷。

Conclusion: SSR工具能高效准确地检测DeFi质押逻辑缺陷，揭示该领域存在较多安全隐患，促进DeFi生态更安全发展。

Abstract: Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.

</details>
