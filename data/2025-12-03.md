<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 43]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review](https://arxiv.org/abs/2512.02024)
*Yan Yang,Mouxiao Bian,Peiling Li,Bingjian Wen,Ruiyao Chen,Kangkun Mao,Xiaojun Ye,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Kaifeng Qiu,Junyan Wu*

Main category: cs.CL

TL;DR: RxBench基准测试评估了大型语言模型在处方审核中的表现，顶尖模型成绩优异甚至超越药剂师，微调中游模型提升效果显著。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型快速进步，急需系统且细致的标准评估框架来验证其在处方审核等临床支持任务中的能力和局限。

Method: 开发包含多种题型的综合处方审核题库，由经验丰富的临床药剂师审核题目，评测多种先进大型语言模型，并针对中游模型进行微调优化。

Result: 本论文开发了RxBench，一个涵盖处方审核中14种常见错误类型的综合基准测试，包含1,150个单选题、230个多选题和879个简答题，所有题目均由临床药剂师审核。对18个先进的大型语言模型进行了评测，发现模型表现有明显层次划分，顶尖模型在准确性和稳健性方面超过其他模型，且部分模型的表现与有执照药剂师相当甚至更优。基于评测结果，作者对中游模型进行了针对性微调，获得了可匹敌领先模型的专用模型。RxBench为评估和提升大型语言模型在处方审核任务中的能力提供了标准化、面向错误类型的框架，同时为构建更可靠的临床辅助工具奠定基础。

Conclusion: RxBench建立了标准化的多错误类型基准，用以系统评测和提升大型语言模型在处方审核任务中的能力，推动临床辅助工具的发展。

Abstract: The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.

</details>


### [2] [Deep Research: A Systematic Survey](https://arxiv.org/abs/2512.02038)
*Zhengliang Shi,Yiqun Chen,Haitao Li,Weiwei Sun,Shiyu Ni,Yougang Lyu,Run-Ze Fan,Bowen Jin,Yixuan Weng,Minjun Zhu,Qiujie Xie,Xinyu Guo,Qu Yang,Jiayi Wu,Jujia Zhao,Xiaqiang Tang,Xinbei Ma,Cunxiang Wang,Jiaxin Mao,Qingyao Ai,Jen-Tse Huang,Wenxuan Wang,Yue Zhang,Yiming Yang,Zhaopeng Tu,Zhaochun Ren*

Main category: cs.CL

TL;DR: 本文全面综述了结合大语言模型与外部工具的深度研究系统，构建了完整框架与技术体系，指明研究路径和未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在解决复杂任务时，单次提示或标准检索增强生成方法已难以满足多源、可验证和批判性思维的需求，因此需要结合外部工具提升其研究和推理能力。

Method: 通过划分三阶段路线图，明确查询规划、信息获取、记忆管理及答案生成四大组件，结合提示、微调和强化学习等优化方法，系统构建深度研究体系。

Result: 本文系统综述了深度研究系统，提出了三阶段发展路线图，明确核心组件及其细化分类，总结了优化技术及评估标准，梳理了当前挑战并展望未来发展方向。

Conclusion: 深度研究作为结合大语言模型与多源工具的全新范式，具备强大潜力，但仍面临多项挑战，需持续优化和完善。

Abstract: Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.

</details>


### [3] [Mirror, Mirror on the Wall -- Which is the Best Model of Them All?](https://arxiv.org/abs/2512.02043)
*Dina Sayed,Heiko Schuldt*

Main category: cs.CL

TL;DR: 本文针对大模型快速迭代和多样化的现状，研究了基于排行榜的量化模型评估，结合医学领域案例，提出了一种系统化模型选择方法，帮助用户选择最适合任务的模型。


<details>
  <summary>Details</summary>
Motivation: 由于基础大模型的快速发展和多样化，选择适合特定领域或任务的模型变得复杂，需要综合考虑定性和定量两个维度进行选择。

Method: 通过分析当前的基准排行榜，特别是在医学领域的应用案例，评估模型的定量性能，结合模型卡片等定性信息，形成综合的模型选择框架。

Result: 本文展示了医学领域内模型性能的演变和现状，验证了基于排行榜的量化评估对于实际模型选择的重要性，并提出了指导性的模型选择方法。

Conclusion: 本文提出了一个系统化的模型选择方法（MSM），帮助用户在众多基础模型中选择最适合特定应用场景的模型。

Abstract: Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.

</details>


### [4] [Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models](https://arxiv.org/abs/2512.02044)
*Kecheng Chen,Ziru Liu,Xijia Tao,Hui Liu,Xinyu Fu,Suiyun Zhang,Dandan Tu,Lingpeng Kong,Rui Liu,Haoliang Li*

Main category: cs.CL

TL;DR: 通过历史上下文修正生成轨迹和自适应采样预算，提出CCD方法提升扩散语言模型推理速度与质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型推理依赖局部步长指标，导致采样轨迹不一致且生成质量不佳，亟需一种更可靠的一致性评估方法和更高效的采样策略。

Method: 提出轨迹修正机制利用条件互信息理论评估步骤一致性，并设计自适应采样策略动态调整解码预算。

Result: 本文提出了一种称为Coherent Contextual Decoding（CCD）的新颖推理框架，通过利用历史上下文进行轨迹修正和条件互信息理论，提高了扩散语言模型的序列一致性和生成质量。同时引入自适应采样策略，动态调整解码预算，显著提升了采样速度和性能。实验结果表明，CCD在Dream和LLaDA基准测试上实现了最高3.48倍的加速和3.91%的性能提升。

Conclusion: CCD框架通过轨迹修正和自适应采样提升了扩散语言模型的生成质量和推理效率，实现了速度与性能的双重提升。

Abstract: Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.

</details>


### [5] [Reversing Large Language Models for Efficient Training and Fine-Tuning](https://arxiv.org/abs/2512.02056)
*Eshed Gal,Moshe Eliasof,Javier Turek,Uri Ascher,Eran Treister,Eldad Haber*

Main category: cs.CL

TL;DR: 提出利用时间可逆动力学设计节省内存的LLM可逆架构，通过微调可将现有模型转换，提升训练效率与性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）训练成本高，时间长，需节省内存以处理更大批量数据。

Method: 引入基于对称和辛微分方程的可逆架构，通过时间可逆动力学在反向传播中恢复隐藏状态，减少激活值存储。同时提出将现有非可逆LLM转换为可逆架构的微调方法。

Result: 所提可逆架构在多个数据集和基准测试上表现可比或优于标准模型，显著降低内存消耗，提高训练吞吐量，支持更大批量训练。

Conclusion: 该可逆架构为LLM训练与微调提供了一条高效、可扩展的路径，显著减少训练内存和计算开销，促进了大模型的实用性。

Abstract: Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.

</details>


### [6] [Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074)
*Zirui Lin,Haris Gulzar,Monnika Roslianna Busto,Akiko Masaki,Takeharu Eda,Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: 本文提出将内存高效微调方法应用于语音模型方言识别任务，有效降低资源消耗并加速训练，且保持准确率。


<details>
  <summary>Details</summary>
Motivation: 传统微调语音模型进行方言识别计算成本和内存需求高昂，且现有参数高效微调方法在内存效率和训练速度方面提升有限，需要探索新的高效微调策略。

Method: 将源于语言处理领域的MEFT方法应用于通用预训练语音模型，在测试中对Whisper模型进行微调实现方言识别，并综合分析不同MEFT方法在GPU内存使用和训练速度上的表现。

Result: 在KeSpeech数据集上识别六种普通话方言时，MEFT方法最大减少73.25%的GPU内存使用，训练速度提升2.1倍，识别准确率与常规微调及PEFT方法相当。

Conclusion: 通过应用内存高效微调（MEFT）方法，可以显著降低预训练语音模型在方言识别任务中的GPU内存使用量和提升训练速度，同时保持与传统微调和参数高效微调（PEFT）相当的识别准确率。

Abstract: Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.

</details>


### [7] [Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation](https://arxiv.org/abs/2512.02141)
*Pritish N. Desai,Tanay Kewalramani,Srimanta Mandal*

Main category: cs.CL

TL;DR: 本文提出通过TF-IDF样本筛选和分词器词汇扩充，在减少训练数据的前提下，提升BERT在仇恨言论检测中的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上滥用性言论不断演变，新兴俚语和隐晦用词使检测系统难以应对，需要在减少训练数据规模的同时保持性能。

Method: 采用基于TF-IDF的样本选择机制，只保留最具信息量的75%训练样本，同时对BERT分词器进行扩充，加入领域特定的俚语和词汇变体。

Result: 在广泛使用的仇恨言论数据集上，方法在保持竞争性能的同时，提高了计算效率，体现出良好的扩展性和适应性。

Conclusion: 该方法有效减少了训练集规模，提高了模型效率，并能适应不断变化的仇恨言论用语，适合大规模滥用内容的自动化监测。

Abstract: Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.

</details>


### [8] [Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models](https://arxiv.org/abs/2512.02185)
*Ziyan Wang,Enmao Diao,Qi Le,Pu Wang,Guanchu Wang,Minwoo Lee,Shu-ping Yeh,Li Yang*

Main category: cs.CL

TL;DR: 针对推理大型语言模型剪枝破坏性能的问题，本文提出RESP框架，利用模型自生成推理轨迹校准剪枝过程，大幅提升剪枝后模型的推理准确率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型（RLM）因其模型庞大和推理输出冗长，导致部署成本高且不适合资源受限环境。通过剪枝减少计算和内存成本是潜在的解决方案，但现有剪枝方法会严重损害RLM的性能。

Method: 分析现有剪枝方法对RLM效果不佳的原因，发现主要是校准数据、剪枝目标和推理行为之间的不匹配。提出RESP框架，通过自我生成的推理轨迹作为校准信号，结合只解码的梯度重要性估计和逐步再生成，确保剪枝决策与模型推理动态一致。

Result: 在Qwen3-8B模型上，RESP方法在GSM8K和MathQA测试集表现优异，在20-30%稀疏度下保持接近密集模型的准确率。即使在40%稀疏度时，准确率仍分别达到81.3%和59.6%，显著优于现有最强剪枝基线。

Conclusion: RESP通过利用模型自生成的推理数据有效解决了RLM剪枝中的性能崩溃问题，显著提升了剪枝后模型的推理能力和稳定性，实现了高稀疏率下的高准确率。

Abstract: Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.

</details>


### [9] [Spoken Conversational Agents with Large Language Models](https://arxiv.org/abs/2512.02593)
*Chao-Han Huck Yang,Andreas Stolcke,Larry Heck*

Main category: cs.CL

TL;DR: 本文综述了语音对话代理向语音本地大语言模型的发展路径，涵盖适配方法、设计选择、数据与评测，结合工业应用，指出隐私、安全等挑战并提供系统性方案。


<details>
  <summary>Details</summary>
Motivation: 随着语音对话代理需求增长，现有技术亟需向更高效、更鲁棒、更智能的语音本地大模型转型以满足多模态、实时交互及工业应用的挑战。

Method: 本文系统回顾了从级联ASR/NLU到端到端、检索与视觉辅助系统的技术演变，包括文本LLMs音频适配、跨模态对齐、联合训练技术，并比较了多种系统架构与设计方案。

Result: 本文综述了语音对话代理从传统的级联ASR/NLU系统向端到端、结合检索与视觉信息的语音本地大规模语言模型(LLMs)发展的过程。文章探讨了将文本LLMs适配至音频输入、跨模态对齐以及语音-文本联合训练方法，介绍了相关数据集、评价指标及不同口音的鲁棒性，并比较了多种设计选择（如级联系统与端到端系统、ASR后修正、流式处理）。文章还连接了工业助理与当前的开放域及任务导向代理，强调了可复现的基线方法，并指出隐私、安全与评估中的开放问题。最后，参会者能获得实用的技术方案和系统级发展路线图。

Conclusion: 语音对话代理正逐步向集成音频、视觉和文本的端到端大模型转变，现有技术和工业实践为构建高鲁棒性、多模态、实时系统提供了基础，但仍需解决隐私、安全和评估方面的挑战以推动应用发展。

Abstract: Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.

</details>


### [10] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: MODOMA系统通过两代理互动模型，成功模拟了离散语法类别的语言习得，验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一个可控、参数化的计算多代理实验环境，以模拟语言习得过程并显式表示习得的语法知识，检验模型的有效性。

Method: 利用MODOMA系统通过两个语言模型（成人代理和儿童代理）的交互进行无监督语言习得实验，结合统计和基于规则的方法，构建基于知识的语言模型。

Result: 儿童代理基于成人代理生成的训练和测试数据成功习得并表示了功能词类和内容词类，且习得模式与人类语言习得规律相似。

Conclusion: MODOMA系统为计算语言习得实验提供了新的可能性，实验结果支持该系统能够有效模拟离散语法类别的学习过程。

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [11] [Swivuriso: The South African Next Voices Multilingual Speech Dataset](https://arxiv.org/abs/2512.02201)
*Vukosi Marivatee,Kayode Olaleye,Sitwala Mundia,Andinda Bakainga,Unarine Netshifhefhe,Mahmooda Milanzie,Tsholofelo Hope Mogale,Thapelo Sindane,Zainab Abdulrasaq,Kesego Mokgosi,Chijioke Okorie,Nia Zion Van Wyk,Graham Morrissey,Dale Dunbar,Francois Smit,Tsosheletso Chidi,Rooweither Mabuya,Andiswa Bukula,Respect Mlambo,Tebogo Macucwa,Idris Abdulmumin,and Seani Rananga*

Main category: cs.CL

TL;DR: 本文提出了Swivuriso多语言语音数据集，填补南非七种语言ASR数据缺口，推动相关技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别数据集中缺少对南非七种语言的充分覆盖，且应用领域限制，难以支持多样化的ASR技术研究和应用。

Method: 构建一个包含3000小时多语言语音数据的Swivuriso数据集，涵盖农业、医疗保健及一般领域，支持七种南非语言的自动语音识别技术开发和基准测试。

Result: 成功构建了Swivuriso多语言语音数据集，并通过训练和微调ASR模型取得了基线性能，同时与其他相关语言的ASR数据集进行了对比。

Conclusion: Swivuriso数据集为南非多语言ASR研究提供了坚实基础，通过实验证明其有效性，助力相关技术的进一步发展。

Abstract: This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.

</details>


### [12] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung,Nikolay Malkin,Mirella Lapata*

Main category: cs.CL

TL;DR: 本文提出了LiteReason方法，通过一个轻量级的推理投影模块在进行强化学习时动态切换潜在和离散推理步骤，从而大幅减少计算量。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的推理优化计算代价高，尤其是在叙事类任务中需要处理大量token，需提升推理效率。

Method: LiteReason使用一个轻量级的推理投影模块，训练生成连续的潜在令牌，RL过程中模型动态激活该模块以跳过部分推理步骤。

Result: 在情节漏洞检测和章节生成任务上，LiteReason优于潜在推理基线，推理效果接近非潜在RL训练，同时大幅减少推理长度。

Conclusion: LiteReason在保持推理效果接近非潜在强化学习训练的同时，将推理长度缩短了77-92%，有效提升了计算效率。

Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [13] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

Main category: cs.CL

TL;DR: 提出DETAIL框架评估提示词具体性对LLM推理性能的影响，发现更具体的提示词能提升准确率。


<details>
  <summary>Details</summary>
Motivation: 探究提示词的具体程度对大型语言模型推理性能的影响，这一领域尚未被充分研究。

Method: 利用GPT-4生成多级具体程度的提示词，使用困惑度衡量提示词具体性，通过基于GPT的语义等价性评估回答正确性，在30个推理任务上对GPT-4和O3-mini进行实验。

Result: 不同具体程度的提示词会影响模型的准确率，尤其对小型模型和程序类任务影响更大。

Conclusion: 需要采用自适应的提示设计策略，以提升不同任务和模型的表现，并提供相应的工具和数据支持进一步研究。

Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [14] [CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering](https://arxiv.org/abs/2512.02251)
*Liangji Kong,Aditya Joshi,Sarvnaz Karimi*

Main category: cs.CL

TL;DR: CAIRNS框架提升了农业气候适应问答的可信度和可读性，无需微调，通过结构化提示和混合评估实现优异表现。


<details>
  <summary>Details</summary>
Motivation: 为农业专家提供一个能够从复杂和异构数据源中提取可信且易读的气候适应相关问答的工具，应对因气候变化带来的农业挑战。

Method: 采用结构化的ScholarGuide提示生成答案，结合一致性加权的混合评估器评估答案质量，无需微调或强化学习。

Result: 该论文提出了一个名为CAIRNS的气候适应问答框架，旨在帮助农业专家（如农民顾问）从复杂的网络证据中获取可信的初步答案。该框架通过结构化的ScholarGuide提示提升答案的可读性和引用的可靠性，并采用一种基于一致性加权的混合评估器结合模型间一致性和专家意见，确保评估的稳健性。CAIRNS无需额外微调或强化学习，即可实现可读、可验证且基于领域的问答。通过使用现有的专家策划问答数据集进行评测，CAIRNS在大多数指标上优于基线方法，消融实验也支持该结论。此外，论文通过与人类评判的相关性分析验证了基于大型语言模型的评估方法的有效性。

Conclusion: CAIRNS能有效提供可信、可读的气候适应领域问答，验证了其评估机制的可靠性和方法优势。

Abstract: Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.

</details>


### [15] [HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models](https://arxiv.org/abs/2512.02299)
*Boya Zhang,Alban Bornet,Rui Yang,Nan Liu,Douglas Teodoro*

Main category: cs.CL

TL;DR: 这篇论文提出了HealthContradict数据集，用于评估语言模型在医学问答中处理长且矛盾上下文信息的能力，重点考察模型在正确、错误或矛盾上下文下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型如何利用上下文信息回答健康问题及其面对矛盾上下文时的反应，弥补现有医学问答评测中对模型上下文推理能力区分不足的缺陷。

Method: 构建并使用包含920个实例的HealthContradict数据集，每个实例包含健康相关问题、基于科学证据的事实答案及两份相互矛盾的文档，采用多种提示设置（正确、错误及矛盾上下文）测试模型表现。

Result: 实验表明，为生物医学领域进行细化训练的语言模型在利用正确上下文和抵抗错误上下文方面表现优异，显示了超越单纯参数记忆的上下文推理能力。

Conclusion: 细化训练的生物医学语言模型不仅依赖预训练的参数知识，还能利用正确上下文并抵抗错误上下文影响，表现出较强的上下文推理能力。

Abstract: How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.

</details>


### [16] [When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers](https://arxiv.org/abs/2512.02304)
*Jack Lu,Ryan Teehan,Jinran Jin,Mengye Ren*

Main category: cs.CL

TL;DR: 本文系统研究了大语言模型(LLMs)中解题器与验证器的互动，比较了自我验证及跨模型族验证的效果，揭示了后训练对验证性能的影响。


<details>
  <summary>Details</summary>
Motivation: 验证器能够从候选答案中筛选出高质量解答，提升解题模型表现，但现有研究只关注自我验证，且未明确后训练对验证性能的影响。

Method: 通过37个不同模型，涵盖多个模型族、尺寸及后训练版本，在9个涵盖逻辑推理、结构谜题、符号计算、数学、常识、事实回忆及领域知识的基准任务上，比较自我验证、同族和跨族模型验证效果，并引入并验证了预测性能提升的度量指标“验证器增益”。

Result: 验证器增益指标有效预测性能提升；跨模型族验证效果最佳；后训练减少自我提升但增强跨族提升；数学和逻辑任务可验证性最高。

Conclusion: 跨模型族的验证器更有助于提升解题性能，后训练虽降低自我提升但增强了跨族提升，数学和逻辑任务具有最高的可验证性。

Abstract: Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.

</details>


### [17] [Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering](https://arxiv.org/abs/2512.02363)
*Lei Fu,Xiang Chen,Kaige Gao Xinyue Huang,Kejian Tong*

Main category: cs.CL

TL;DR: KARMA框架通过集成多源知识和安全控制机制，提升了服务领域问答系统的可信度和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在特定服务领域如医疗政策和政府福利中面临事实一致性和上下文对齐的挑战，亟需提高准确性与安全性。

Method: 提出了KARMA框架，结合双编码器融合结构化与非结构化知识，门控记忆单元动态调节知识整合，以及安全感知可控解码器减少不安全输出。

Result: 在专有问答数据集上的大量实验表明，KARMA在答案质量和安全性上均优于其他强基线方法。

Conclusion: KARMA框架在服务领域的特定问答系统中显著提升了答案的准确性和安全性，优于现有强基线模型。

Abstract: Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.

</details>


### [18] [TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models](https://arxiv.org/abs/2512.02402)
*Yunchao Wang,Guodao Sun,Zihang Fu,Zhehao Liu,Kaixing Du,Haidong Gao,Ronghua Liang*

Main category: cs.CL

TL;DR: 本文提出了TaleFrame系统，结合大语言模型和人机交互，通过结构化信息实现对故事生成的精细控制。该系统将故事结构分解为实体、事件、关系和故事大纲四个基本单元，利用JSON格式数据训练模型，实现从结构化数据到连贯故事的转换。用户通过直观界面操作故事元素，系统支持多维度评估并提供优化建议，提升用户故事创作体验。


<details>
  <summary>Details</summary>
Motivation: 现有创意故事生成系统难以细粒度控制用户意图，输入不明确，导致生成故事无法满足用户需求，限制了系统应用。因而亟需一个结合结构化信息和人机交互的系统，实现对故事生成过程的精准控制。

Method: 本文采用将故事结构拆分为四个基本单元的方式，基于Tinystories数据集构建JSON格式偏好数据集，微调本地Llama大语言模型，利用JSON2Story方法进行故事生成。系统设计了支持拖拽、连接等交互的界面，并通过七个维度评估生成故事质量，提供改进建议，支持用户迭代修改。

Result: 系统成功将结构化信息转化为连贯故事，用户通过交互界面能够实现细节和进程控制。七个维度的评估机制和优化建议增强了故事质量。定量实验和用户调查均显示TaleFrame具备显著的实用效果和创新价值。

Conclusion: TaleFrame有效提升了故事生成系统对用户意图的精准理解与控制，通过结构化的工具和交互设计，使用户能够更便捷地创作满意的故事。定量评估和用户研究表明该系统有显著实用价值。

Abstract: With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.

</details>


### [19] [A Concise Review of Hallucinations in LLMs and their Mitigation](https://arxiv.org/abs/2512.02527)
*Parth Pulkundwar,Vivek Dhanawade,Rohit Yadav,Minal Sonkar,Medha Asurlekar,Sarita Rathod*

Main category: cs.CL

TL;DR: 本文总结了传统语言模型中幻觉现象的种类、成因及其缓解方法。


<details>
  <summary>Details</summary>
Motivation: 幻觉现象对自然语言处理领域构成严重挑战，亟需系统理解和解决该问题。

Method: 通过分析和归纳现有研究，对幻觉的分类和成因进行总结，并探讨相应的缓解策略。

Result: 本文提供了关于幻觉现象的全面概述及其缓解方法的总结，成为相关研究的参考资源。

Conclusion: 了解幻觉的类型和来源并采取有效措施能够减少语言模型中的幻觉现象。

Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.

</details>


### [20] [What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints](https://arxiv.org/abs/2512.02552)
*Francesco Paolo Savatteri,Chahan Vidal-Gorène,Florian Cafiero*

Main category: cs.CL

TL;DR: 本研究评估了在线虚假新闻检测和病毒传播预测任务，比较了文本嵌入与数值特征模型，发现文本内容在虚假新闻识别中效果显著，病毒预测更具挑战性并受标签构造影响大。


<details>
  <summary>Details</summary>
Motivation: 探讨在实际操作场景中，面对网络虚假信息的快速反应需求下，如何有效识别虚假新闻和预测信息传播的病毒性。

Method: 使用EVONS和FakeNewsNet数据集，比较了基于文本的RoBERTa和Mistral嵌入，以及基于数值特征的轻量模型和序列模型（GRU、门控架构、Transformer编码器）。

Result: 文本嵌入在虚假新闻检测中表现出较强的区分能力，数值特征预测模型在受限环境下仍有效；病毒预测任务较难，对标签和时间特征的要求较高；降维分析显示病毒性传播在非线性结构下信息更丰富。

Conclusion: 文本内容是虚假新闻检测的强有力区分特征，而在语言模型不可用或计算资源受限时，基于数值特征的轻量级模型仍可行。病毒式传播预测比虚假新闻检测更复杂，且对标签构造非常敏感。

Abstract: We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.

</details>


### [21] [ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce](https://arxiv.org/abs/2512.02555)
*Zheng Fang,Donghao Xie,Ming Pang,Chunyuan Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo*

Main category: cs.CL

TL;DR: 本文提出了ADORE框架，通过规则感知、错误类型生成对抗样本和关键属性知识蒸馏三大模块，解决电商搜索中语义鸿沟和样本稀缺问题，提升相关性建模的鲁棒性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 电商搜索相关性建模面临术语匹配方法的语义鸿沟和神经模型依赖少量领域特定难样本的挑战，需解决数据稀缺与推理能力问题。

Method: ADORE包含：1）规则感知相关性判别模块，利用大语言模型生成并通过KTO优化的训练数据；2）错误类型感知数据合成模块，自动生成对抗样本提升鲁棒性；3）关键属性增强知识蒸馏模块，将领域属性层次结构注入学生模型。

Result: 大规模实验和在线A/B测试表明ADORE显著提升了相关性建模效果，展现出资源高效且认知对齐的工业应用潜力。

Conclusion: ADORE框架有效地自动化了标注、对抗样本生成与知识蒸馏过程，显著改善了电商搜索中相关性建模的性能和鲁棒性，验证了其实用性和优越性。

Abstract: Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.

</details>


### [22] [DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)
*DeepSeek-AI,Aixin Liu,Aoxue Mei,Bangcai Lin,Bing Xue,Bingxuan Wang,Bingzheng Xu,Bochao Wu,Bowei Zhang,Chaofan Lin,Chen Dong,Chengda Lu,Chenggang Zhao,Chengqi Deng,Chenhao Xu,Chong Ruan,Damai Dai,Daya Guo,Dejian Yang,Deli Chen,Erhang Li,Fangqi Zhou,Fangyun Lin,Fucong Dai,Guangbo Hao,Guanting Chen,Guowei Li,H. Zhang,Hanwei Xu,Hao Li,Haofen Liang,Haoran Wei,Haowei Zhang,Haowen Luo,Haozhe Ji,Honghui Ding,Hongxuan Tang,Huanqi Cao,Huazuo Gao,Hui Qu,Hui Zeng,Jialiang Huang,Jiashi Li,Jiaxin Xu,Jiewen Hu,Jingchang Chen,Jingting Xiang,Jingyang Yuan,Jingyuan Cheng,Jinhua Zhu,Jun Ran,Junguang Jiang,Junjie Qiu,Junlong Li,Junxiao Song,Kai Dong,Kaige Gao,Kang Guan,Kexin Huang,Kexing Zhou,Kezhao Huang,Kuai Yu,Lean Wang,Lecong Zhang,Lei Wang,Liang Zhao,Liangsheng Yin,Lihua Guo,Lingxiao Luo,Linwang Ma,Litong Wang,Liyue Zhang,M. S. Di,M. Y Xu,Mingchuan Zhang,Minghua Zhang,Minghui Tang,Mingxu Zhou,Panpan Huang,Peixin Cong,Peiyi Wang,Qiancheng Wang,Qihao Zhu,Qingyang Li,Qinyu Chen,Qiushi Du,Ruiling Xu,Ruiqi Ge,Ruisong Zhang,Ruizhe Pan,Runji Wang,Runqiu Yin,Runxin Xu,Ruomeng Shen,Ruoyu Zhang,S. H. Liu,Shanghao Lu,Shangyan Zhou,Shanhuang Chen,Shaofei Cai,Shaoyuan Chen,Shengding Hu,Shengyu Liu,Shiqiang Hu,Shirong Ma,Shiyu Wang,Shuiping Yu,Shunfeng Zhou,Shuting Pan,Songyang Zhou,Tao Ni,Tao Yun,Tian Pei,Tian Ye,Tianyuan Yue,Wangding Zeng,Wen Liu,Wenfeng Liang,Wenjie Pang,Wenjing Luo,Wenjun Gao,Wentao Zhang,Xi Gao,Xiangwen Wang,Xiao Bi,Xiaodong Liu,Xiaohan Wang,Xiaokang Chen,Xiaokang Zhang,Xiaotao Nie,Xin Cheng,Xin Liu,Xin Xie,Xingchao Liu,Xingkai Yu,Xingyou Li,Xinyu Yang,Xinyuan Li,Xu Chen,Xuecheng Su,Xuehai Pan,Xuheng Lin,Xuwei Fu,Y. Q. Wang,Yang Zhang,Yanhong Xu,Yanru Ma,Yao Li,Yao Li,Yao Zhao,Yaofeng Sun,Yaohui Wang,Yi Qian,Yi Yu,Yichao Zhang,Yifan Ding,Yifan Shi,Yiliang Xiong,Ying He,Ying Zhou,Yinmin Zhong,Yishi Piao,Yisong Wang,Yixiao Chen,Yixuan Tan,Yixuan Wei,Yiyang Ma,Yiyuan Liu,Yonglun Yang,Yongqiang Guo,Yongtong Wu,Yu Wu,Yuan Cheng,Yuan Ou,Yuanfan Xu,Yuduan Wang,Yue Gong,Yuhan Wu,Yuheng Zou,Yukun Li,Yunfan Xiong,Yuxiang Luo,Yuxiang You,Yuxuan Liu,Yuyang Zhou,Z. F. Wu,Z. Z. Ren,Zehua Zhao,Zehui Ren,Zhangli Sha,Zhe Fu,Zhean Xu,Zhenda Xie,Zhengyan Zhang,Zhewen Hao,Zhibin Gou,Zhicheng Ma,Zhigang Yan,Zhihong Shao,Zhixian Huang,Zhiyu Wu,Zhuoshu Li,Zhuping Zhang,Zian Xu,Zihao Wang,Zihui Gu,Zijia Zhu,Zilin Li,Zipeng Zhang,Ziwei Xie,Ziyi Gao,Zizheng Pan,Zongqing Yao,Bei Feng,Hui Li,J. L. Cai,Jiaqi Ni,Lei Xu,Meng Li,Ning Tian,R. J. Chen,R. L. Jin,S. S. Li,Shuang Zhou,Tianyu Sun,X. Q. Li,Xiangyue Jin,Xiaojin Shen,Xiaosha Chen,Xinnan Song,Xinyi Zhou,Y. X. Zhu,Yanping Huang,Yaohui Li,Yi Zheng,Yuchen Zhu,Yunxian Ma,Zhen Huang,Zhipeng Xu,Zhongyu Zhang,Dongjie Ji,Jian Liang,Jianzhong Guo,Jin Chen,Leyi Xia,Miaojun Wang,Mingming Li,Peng Zhang,Ruyi Chen,Shangmian Sun,Shaoqing Wu,Shengfeng Ye,T. Wang,W. L. Xiao,Wei An,Xianzu Wang,Xiaowen Sun,Xiaoxiang Wang,Ying Tang,Yukun Zha,Zekai Zhang,Zhe Ju,Zhen Zhang,Zihua Qu*

Main category: cs.CL

TL;DR: DeepSeek-V3.2采用稀疏注意力和强化学习，提升计算效率和推理表现，超越GPT-5并在国际竞赛中获金牌。


<details>
  <summary>Details</summary>
Motivation: 提升模型在长上下文场景中的计算效率和推理能力，同时增强代理性能。

Method: 引入DeepSeek稀疏注意力机制（DSA）以降低计算复杂度；采用可扩展的强化学习框架与后训练计算扩展；开发大规模代理任务合成流水线以生成训练数据。

Result: DeepSeek-V3.2表现出与GPT-5相当的性能，特高计算版本DeepSeek-V3.2-Speciale超越GPT-5，在2025年IMO和IOI竞赛中取得金牌成绩。

Conclusion: 通过技术创新，DeepSeek-V3.2实现了高效计算与卓越推理能力的平衡，显著提升了模型在复杂交互环境中的泛化和执行力。

Abstract: We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.

</details>


### [23] [From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks](https://arxiv.org/abs/2512.02580)
*Changpeng Yang,Jinyang Wu,Yuchen Liu,Shuai Zhang,Yang Li,Qiliang Liang,Hongzhen Wang,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CL

TL;DR: 通过逐步引入正负优势信号的课程策略，CAPO改善了强化学习在大语言模型后训练中的训练效果，有效提升了推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强化学习中对正负信号的混合使用，特别是在训练早期，可能导致指导信息模糊且效果有限。

Method: 提出了CAPO（Curriculum Advantage Policy Optimization）机制，该方法基于优势信号实现自适应课程学习。方法先利用仅含正向优势样本的模仿学习建立稳固基础，然后引入负向信号培养判别能力。兼容多种强化学习优化方法如GRPO、PPO、RLOO和Reinforce++。

Result: CAPO在数学推理任务中实现了稳定且显著的性能提升，并在多模态图形用户界面（GUI）推理场景中表现出良好的泛化能力。

Conclusion: CAPO作为一种基于优势信号的自适应课程优化框架，能稳定提升语言模型推理性能，且在多场景下具备良好适用性与鲁棒性。

Abstract: Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.

</details>


### [24] [Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization](https://arxiv.org/abs/2512.02665)
*Jing Ma*

Main category: cs.CL

TL;DR: 研究发现大语言模型生成多文档摘要时，优先考虑首个文档，存在输入顺序偏向风险。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在多文档摘要任务中是否均等权重不同来源文档，避免潜在的不公正偏向。

Method: 构建40组三方立场的文章，改变输入顺序后用Gemini 2.5 Flash生成中性概述，并通过ROUGE-L、BERTScore和SummaC进行评估，采用单因素方差分析和配对比较分析输入顺序对摘要语义的影响。

Result: BERTScore显示显著的首因效应，意味着摘要与第一个文档语义更相似，且第二和第三位置文档之间无显著差异。

Conclusion: 大语言模型在生成多文档摘要时存在显著的首因效应，即模型更倾向于对首个输入文档赋予更大权重。

Abstract: Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.

</details>


### [25] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji,Tatsuhiko Saito,Yasutomo Kimura*

Main category: cs.CL

TL;DR: 本文比较七种模型合并算法在减少大语言模型社会偏见中的效果，发现偏见减少与性能下降存在权衡，SLERP算法表现最优。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因训练数据中的社会偏见而存在公平性和信任问题，需寻找有效的参数编辑方法来缓解这些偏见。当前模型合并方法虽被提出，但缺乏系统的实证比较。

Method: 本文选择了Linear、Karcher Mean、SLERP、NuSLERP、TIES、DELLA和Nearswap七种模型合并算法，在GPT、LLaMA和Qwen模型家族中应用13个公开权重模型，利用BBQ、BOLD和HONEST三个偏见数据集评估偏见缓解效果，并通过SuperGLUE基准测试下游任务性能。

Result: 方法在减少偏见的同时会影响下游任务准确率，尤其是在阅读理解、常识及因果推理任务上。Linear、SLERP和Nearswap能较好减少偏见且维护整体性能，其中SLERP在适中插值权重下表现最为均衡。

Conclusion: 模型合并算法能有效减轻大语言模型中的社会偏见，但过度去偏或不恰当的方法会损害模型的语言理解能力。SLERP算法在平衡偏见减少和性能保持方面表现最佳。

Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [26] [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711)
*Lavish Bansal,Naman Mishra*

Main category: cs.CL

TL;DR: CREST通过利用跨语言迁移策略，低参数成本实现了涵盖100种语言的内容安全分类，有效解决了低资源语言的安全保障问题。


<details>
  <summary>Details</summary>
Motivation: 现有的内容安全措施主要集中于高资源语言，忽视了大量使用低资源语言的人群，亟需一种能够覆盖更多语言的普适安全保障机制。

Method: 提出CREST，一种参数高效的多语言安全分类模型，通过在13种高资源语言上训练，利用基于聚类的跨语言迁移技术，实现对100种语言（包括低资源语言）的支持。

Result: CREST在六个安全基准测试中表现优于同等规模的现有安全护栏，并在参数远超（2.5B及以上）模型中也表现出竞争力。

Conclusion: 语言特定的安全措施存在局限，应开发通用、语言无关的安全系统以满足全球用户需求。

Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.

</details>


### [27] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma,Jun Wang,Zafeirios Fountas*

Main category: cs.CL

TL;DR: 本文通过设计基于心理物理学的多模态贝叶斯估算任务，评测大型语言模型在不确定性处理和多模态信息整合中的贝叶斯行为，发现准确率高不代表策略合理，提出新的基准与评价指标推动多模态模型发展。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）是否能在未经过明确训练或指导的情况下，表现出接近最优的贝叶斯多模态信号整合能力，这一能力在人类感知任务中是直观存在的。

Method: 采用心理物理学范式，设计了BayesBench行为基准，包括长度、位置、距离和持续时间四个跨文本和图像的估算任务，通过对九个不同LLMs与人类判断的系统性行为研究，进行噪声、上下文和指令提示的对照消融实验，评估模型的准确性、行为和效率。同时引入贝叶斯一致性评分来捕捉行为变化。

Result: 结果表明，虽然部分大型模型表现出贝叶斯一致的适应行为，但准确度高并不意味着鲁棒性强。例如GPT-5 Mini在文本准确度上表现完美，但未能有效整合视觉线索。准确率和贝叶斯倾向之间存在相关性，但存在能力与策略的显著分离。

Conclusion: 性能指标过于关注准确率可能忽视模型在处理不确定性方面的脆弱性。发现LLMs在不确定性处理上表现出新兴的贝叶斯原则行为，贝叶斯一致性评分为评估模型多模态整合策略提供了新视角。公开发布的心理物理学基准和评估工具将有助于未来多模态架构设计。

Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

</details>


### [28] [SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys](https://arxiv.org/abs/2512.02763)
*Jiahao Zhao,Shuaixing Zhang,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: 本文提出了SurveyEval基准，用于评估LLM自动生成的综合性调研报告，涵盖整体质量、结构连贯性和引用准确性三个维度。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏有效方法评估复杂的LLM自动调研系统，需构建全面的评测标准。

Method: 设计了基于7个学科的SurveyEval评测基准，并结合LLM作为评判者的方法，引入人工参考增强评估与人类判断的对齐。

Result: 通过SurveyEval评估发现专门调研系统在质量上明显优于一般长文本或论文写作系统。

Conclusion: 专门的调研生成系统在生成高质量调研报告方面表现优于通用长文本生成系统。

Abstract: LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.

</details>


### [29] [PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](https://arxiv.org/abs/2512.02764)
*Robert Belanec,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: 本文提出了PEFT-Factory框架，用于高效微调大型语言模型，整合并标准化了多种参数高效微调方法，提升了方法的复现性和对比性。


<details>
  <summary>Details</summary>
Motivation: 当前很多PEFT方法难以复现、部署及相互比较，亟需一个统一且稳定的框架来解决这些问题。

Method: PEFT-Factory采用模块化设计，支持现成及自定义的PEFT方法，集成了丰富的分类和文本生成数据集及评测指标，提供标准化的测试环境。

Result: 实现了19种代表性PEFT方法的集成，支持27个数据集和12个任务，提供标准和PEFT特有评测指标，提升了方法复现性和基准测试的可控性。

Conclusion: PEFT-Factory作为一个统一平台，支持19种PEFT方法和多种任务及评测，显著改善了PEFT方法的复现、部署和比较难题，促进了相关研究的发展。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory

</details>


### [30] [Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](https://arxiv.org/abs/2512.02772)
*Weihang Su,Jianming Long,Changyue Wang,Shiyu Lin,Jingyan Xu,Ziyi Ye,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: 该论文提出UniFact统一评估框架，弥合幻觉检测与事实验证的研究鸿沟，证明融合方法优于单一范式，推动大型语言模型事实性检测研究整合与提升。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）频繁产生幻觉，即生成内容流畅但事实错误，降低了信任度和应用效果。现有两种独立的发展范式——幻觉检测（HD）和事实验证（FV），由于假设、数据集和评估协议不同，导致研究割裂，阻碍共同进步。

Method: 提出了UniFact统一评估框架，实现FV和HD的实例级比较，动态生成模型输出及事实性标签，涵盖多种LLM和检测方法的大规模实验。

Result: 发现（1）无单一范式绝对优越，（2）HD与FV捕获事实错误的互补方面，（3）融合两者的混合方法达到最优性能。

Conclusion: 提出融合幻觉检测与事实验证的统一研究议程，推动LLM领域整体进步，并开源了相关代码、数据和基线实现。

Abstract: Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/

</details>


### [31] [Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension](https://arxiv.org/abs/2512.02791)
*Juexi Shao,Siyou Li,Yujian Gan,Chris Madge,Vanja Karan,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文设计了三层次数据合成策略以提升对话式泛化指代表达的视觉定位性能，解决数据稀缺和领域偏移难题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在训练与评估领域分布差异下表现欠佳，加之缺乏带注释的对话定位数据，影响泛化能力，因此需要新的数据合成策略来提升性能。

Method: 采用三层数据合成策略，兼顾真实性和可控性，生成大规模对话条件视觉定位监督数据，随后对模型进行微调。

Result: 该论文提出了一种针对对话条件下的泛化指代表达理解（GREC）任务的三阶数据合成方法，旨在解决训练与评估域分布偏移和缺乏带注释对话数据的问题。通过在合成数据上微调，模型在标准评价指标上显著优于之前的方法。

Conclusion: 三阶数据合成方法有效提升了模型在复杂视觉场景下根据对话背景进行目标定位的能力，缓解了数据稀缺和分布偏移问题。

Abstract: Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.

</details>


### [32] [TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages](https://arxiv.org/abs/2512.02799)
*Mike Nkongolo,Hilton Vorster,Josh Warren,Trevor Naick,Deandre Vanmali,Masana Mashapha,Luke Brand,Alyssa Fernandes,Janco Calitz,Sibusiso Makhoba*

Main category: cs.CL

TL;DR: 本文提出TriLex框架，通过词典扩展提升低资源非洲语言情感分析效果，显著改善了两种非洲语言预训练模型的性能，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 低资源非洲语言在情感分析中的代表性不足，限制了词汇覆盖率和多语言自然语言处理系统的性能。

Method: 提出了TriLex，这是一种三阶段增强检索框架，结合基于语料库的提取、跨语言映射和基于检索增强生成(RAG)的词汇细化，系统地扩展低资源语言的情感词典。

Result: 基于扩展的词典，评估了两个非洲预训练语言模型(AfroXLMR和AfriBERTa)的性能。AfroXLMR在isiXhosa和isiZulu语言中F1分数超过80%，表现优异且跨语言稳定性强。AfriBERTa虽未在目标语言预训练，但仍能达到约64%的F1分数。两者均优于传统机器学习基线，集成分析进一步提高了精准度和稳健性。

Conclusion: TriLex被证明是一个可扩展且有效的框架，适用于低资源南非语言的多语言情感词典扩展和情感建模。

Abstract: Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.

</details>


### [33] [SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment](https://arxiv.org/abs/2512.02807)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 本文提出无需外部标注的stable rank指标，通过内部表征的有效维数衡量模型质量，结合强化学习显著提升语言模型性能，实现了无监督的有效模型对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于外部监督的LLM对齐方法面临标注稀缺、主观性强和奖励模型易被攻击等问题，需探索无监督且稳健的质量衡量手段。

Method: 提出stable rank指标量化隐藏状态的有效维度，衡量信息在表示维度上的分布质量，并基于此设计Stable Rank Group Relative Policy Optimization (SR-GRPO)用于强化学习优化。

Result: stable rank在RewardBench上达到84.04%准确率，相较贪婪解码提升11.3个百分点准确率；SR-GRPO无监督条件下提升Qwen2.5-1.5B模型在STEM任务10%和数学推理19%的表现，优于学习式奖励模型和自我评估方法。

Conclusion: 通过引入stable rank作为内在的、无需标注的质量信号，本文实现了无需外部监督的语言模型性能提升，验证了从模型内部表示中提取质量信号的有效性。

Abstract: Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.

</details>


### [34] [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/abs/2512.02816)
*Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong*

Main category: cs.CL

TL;DR: 该论文构建了一个面向中医领域的大型语言模型的多任务临床评估基准，综合考察知识、伦理、安全及辨证施治能力，推动智能中医研究。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在中医领域的兴起，迫切需要一种能够综合评估其临床应用能力，尤其是在辨证施治等关键中医核心环节上的表现的评估体系。

Method: 本研究由中医专家主导，采用严密的数据注释流程和专门的奖励模型，结合选择响应、判断模型及奖励模型三种评估机制，对中医LLM进行多任务综合评价。

Result: 该论文提出了一个专门针对中医领域大型语言模型（LLM）的综合临床案例评估基准（TCM-BEST4SDT），以解决中医辨证施治的个性化、多样性及整体性特点所带来的评估挑战。该基准包含四个任务：中医基础知识、医学伦理、内容安全和辨证施治，采用专家领导的数据注释流程和专门的奖励模型来评估处方与辨证的一致性。评估框架结合了多种机制，包括选择响应、判断模型和奖励模型。通过对15个主流大模型的实验验证了该基准的有效性，并已公开发布以促进智能中医研究发展。

Conclusion: 该研究成功设计并验证了一个多维度的中医大型语言模型评估基准，对促进智能中医技术进步具有重要意义。

Abstract: The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.

</details>


### [35] [BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion](https://arxiv.org/abs/2512.02817)
*Sai Koneru,Fabian Retkowski,Christian Huber,Lukas Hilgert,Seymanur Akti,Enes Yavuz Ugan,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: 提出多模态多语言讲座助手BOOM，实现音频和幻灯片同步翻译，提升教育内容的本地化和理解效果。


<details>
  <summary>Details</summary>
Motivation: 全球化教育和在线学习的快速发展使得本地化教育内容成为关键挑战，特别是多模态的讲座材料需要处理多种输入模态。

Method: 提出BOOM，一个多模态多语言讲座助手，联合翻译讲座音频和幻灯片，实现文本翻译、幻灯片本地化及语音合成的同步输出。

Result: 实验表明，幻灯片感知的转录不仅支持多模态内容完整保留，还对下游任务如摘要和问答带来连锁收益。

Conclusion: BOOM方法促进了讲座内容的跨语言获取和理解，提高了多模态教育资源的可访问性和完整保真度。

Abstract: The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.

</details>


### [36] [promptolution: A Unified, Modular Framework for Prompt Optimization](https://arxiv.org/abs/2512.02840)
*Tom Zehle,Timo Heiß,Moritz Schlager,Matthias Aßenmacher,Matthias Feurer*

Main category: cs.CL

TL;DR: 提出了promptolution，一个统一的开源框架，解决了prompt优化实践中代码分散和维护困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有prompt优化实现分散且依赖于难以维护的研究代码库，限制了其实际应用。

Method: 设计并实现了一个集成多种离散prompt优化器的扩展系统，该系统独立于底层大语言模型，支持多种实现。

Result: 开发了promptolution框架，整合了多种优化器，简化了prompt优化的流程，便于研究者和实践者使用。

Conclusion: promptolution成功地提供了一个统一且模块化的开源框架，极大地方便了prompt优化的应用与研究。

Abstract: Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.

</details>


### [37] [Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages](https://arxiv.org/abs/2512.02841)
*Lechen Zhang,Yusheng Zhou,Tolga Ergen,Lajanugen Logeswaran,Moontae Lee,David Jurgens*

Main category: cs.CL

TL;DR: 本研究提出了一个多语言评估框架和提示词优化方法，系统地提升大型语言模型的多语言推理性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在仅支持英语的系统提示，而现实应用中需要单一提示词在多语言环境中稳定可靠地运作，因此探讨系统提示如何引导模型实现准确且稳健的跨语言行为。

Method: 通过提出一个统一的四维评估框架，结合五种语言、三种大型语言模型和三个基准测试，进行大规模实验，研究系统提示词对跨语言模型表现的影响；并开发多语言设置下的提示词优化框架，以自动发现提升模型性能的提示词。

Result: 发现某些提示词组成部分如链式思维（CoT）、情绪和场景与稳健的多语言表现相关，提示词优化框架能自动提升所有指标5-10%；深入分析1000多万条推理单元，发现更高效提示能促进更具结构性和一致性的推理，减少无谓的语言切换。

Conclusion: 系统提示词优化是一条可扩展的路径，可有效提升大型语言模型在多语言环境下的准确性和稳定性，促进更结构化和一致的推理过程，减少语言切换，提高模型跨语言表现。

Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.

</details>


### [38] [Bangla Hate Speech Classification with Fine-tuned Transformer Models](https://arxiv.org/abs/2512.02845)
*Yalda Keivan Jafari,Krishno Dey*

Main category: cs.CL

TL;DR: 针对孟加拉语仇恨言论识别，BanglaBERT作为专门预训练的语言模型，效果优于多语言及传统模型，表明低资源语言需重视语言专属的预训练模型。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语拥有超过2.3亿使用者，但在计算资源上严重不足，且现有数据集、正字法及语言多样性影响仇恨言论识别，急需有效的自动化审核工具。

Method: 复现官方基线方法（Majority、Random、支持向量机等），同时引入逻辑回归、随机森林、决策树以及多种基于Transformer的模型（DistilBERT、BanglaBERT、m-BERT、XLM-RoBERTa）进行仇恨言论分类。

Result: 所有基于Transformer的模型均优于传统基线，BanglaBERT在两个子任务中表现最佳，显示特定语言预训练模型的重要性和潜力。

Conclusion: 针对低资源语言孟加拉语的仇恨言论识别问题，基于预训练语言模型的孟加拉语专用模型BanglaBERT表现最佳，优于多语言模型和传统基线方法。

Abstract: Hate speech recognition in low-resource languages remains a difficult problem due to insufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the growing need for automated moderation on social media platforms, Bangla is significantly under-represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official baselines (e.g., Majority, Random, Support Vector Machine) and also produce and consider Logistic Regression, Random Forest, and Decision Tree as baseline methods. We also utilized transformer-based models such as DistilBERT, BanglaBERT, m-BERT, and XLM-RoBERTa for hate speech classification. All the transformer-based models outperformed baseline methods for the subtasks, except for DistilBERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language-specific pre-training is very important. Our results highlight the potential and need for pre-trained language models for the low-resource Bangla language.

</details>


### [39] [Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning](https://arxiv.org/abs/2512.02874)
*Haonan Wang,Chao Du,Kenji Kawaguchi,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出ThinkMerge，通过并行推理轨迹的概率平均提升开放式推理和代码生成任务表现，优于多数投票方法且支持多种解码策略。


<details>
  <summary>Details</summary>
Motivation: 多数投票在封闭式问题回答中有效，但不适用于开放式推理任务，如代码生成和基于网络的深度研究，这些任务中完整解的"多数"难以定义。

Method: 提出ThinkMerge，一种无需训练的即插即用解码策略，通过运行K条并行推理轨迹并在同步点平均预测下一个词的概率，从而生成单一连贯的输出。

Result: ThinkMerge在AIME和GPQA任务上匹配或优于多数投票，在开放式代码任务（LiveCodeBench）中pass@1提升显著（分别提升8.28%和7.58%），且在多种基于网络的深度研究任务中也表现优异。

Conclusion: ThinkMerge有效利用并行推理提升开放式推理任务性能，无需依赖对完整输出的多数投票，可无缝集成现有解码技术。

Abstract: Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a "majority" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.

</details>


### [40] [Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules](https://arxiv.org/abs/2512.02892)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: SchED通过无训练早期退出策略，显著加速扩散大语言模型采样，在保持高性能的同时实现3.8-4倍速度提升。


<details>
  <summary>Details</summary>
Motivation: 扩散大型语言模型虽有潜力，但采样过程因迭代慢导致实际应用受限，急需一种无需训练且适用多模型的加速策略来提升其解码效率。

Method: 设计了一个基于全范围logit边缘的早期退出算法SchED，无需额外训练，结合平滑的进度相关置信阈值动态停止解码过程。使用多个dLLMs及丰富任务进行大规模评测。

Result: 提出SchED，一种无训练、模型无关的早期退出算法，通过聚合全范围logit边缘并在达到平滑、进度相关的置信阈值时停止解码，实现加速扩散大型语言模型(dLLMs)的采样过程。实验证明SchED在多种dLLM及任务中实现了3.8-4.0倍速度提升，同时保持近乎100%的性能。SchED对比先前基于置信度的早期退出方法表现更佳，尤其在长文本生成中表现稳定。通过熵分析发现指令调优加快了预测熵的衰减，使得SchED能有效平衡效率与质量。

Conclusion: SchED有效地将置信度稳定性转化为计算节省，显著提升dLLM解码效率，且优于现有早期退出方法，尤其适用于长文本生成任务。

Abstract: Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.

</details>


### [41] [AutoNeural: Co-Designing Vision-Language Models for NPU Inference](https://arxiv.org/abs/2512.02924)
*Wei Chen,Liangmin Wu,Yunhai Hu,Zhiyuan Li,Zhiyuan Cheng,Yicheng Qian,Lingyue Zhu,Zhipeng Hu,Luoyi Liang,Qiang Tang,Zhen Liu,Han Yang*

Main category: cs.CL

TL;DR: 针对NPU硬件限制，提出AutoNeural视觉语言模型，通过结构创新和整数推理，大幅提升量化稳定性和推理效率，实现边缘AI实时多模态应用。


<details>
  <summary>Details</summary>
Motivation: 当前为GPU设计的视觉语言模型在NPU上表现不佳，主要因ViTs的量化脆弱性及自回归注意力机制带来的I/O瓶颈，导致硬件资源未被充分利用。

Method: 提出AutoNeural架构，采用MobileNetV5样式的深度可分离卷积替换传统ViT编码器，结合基于状态空间模型和Transformer层的混合语言骨干，通过整数推理和高效门控卷积实现低复杂度和低I/O开销。

Result: AutoNeural显著提升量化稳定性，视觉编码器量化误差降低7倍，端到端延迟降低14倍，解码速度提高3倍，支持4倍更长上下文窗口，在高通SA8295P SoC上实现真实车载应用的实时性能。

Conclusion: 重新设计适配NPU硬件的视觉语言模型架构对实现高效边缘AI推理至关重要。

Abstract: While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.

</details>


### [42] [Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic](https://arxiv.org/abs/2512.02987)
*Muyu Pan,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

TL;DR: 本论文提出了一种结合经典NLP技术、自定义语法、符号计算库和微调语言模型的新框架，以实现将英文句子自动翻译为形式逻辑表达式并转为CNF形式，从而减少大语言模型在逻辑翻译任务中的幻觉现象，提高翻译的准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在将自然语言语句自动翻译成形式逻辑时存在幻觉问题，影响逻辑推理的准确性和可靠性，亟需一种减少幻觉并保证精度的解决方案。

Method: 结合经典NLP技术、自定义语法规则、符号计算库和经过微调的语言模型，实现从英语到形式逻辑再到CNF的自动转换流程。

Result: 微调后的模型在不同语法设置下能够主动纠正原模型产生的同类型幻觉，显著提升逻辑表达及CNF形式的准确生成。

Conclusion: 该框架能够有效减少大语言模型在自然语言到形式逻辑转换中的错误输出，提供可靠的CNF生成，验证了微调模型在纠正幻觉方面的能力。

Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.

</details>


### [43] [The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models](https://arxiv.org/abs/2512.03026)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh*

Main category: cs.CL

TL;DR: 本文提出了MoCoP，一个无数据集闭环框架，持续评估大型语言模型的道德一致性，揭示了伦理稳定性和语言安全性是模型的长期特征，推动了计算伦理学的发展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展需要实现道德一致性，即在不同背景下保持伦理一致的推理能力。现有的对齐框架依赖静态数据集和事后评估，缺乏对伦理推理如何随场景或时间变化的洞察。

Method: 提出了Moral Consistency Pipeline (MoCoP)，一个无数据集、闭环的框架，结合词汇完整性分析、语义风险估计和基于推理的判断建模，自主生成、评估和完善伦理场景，不依赖外部监督。

Result: 通过对GPT-4-Turbo和DeepSeek的实证研究，MoCoP有效捕捉了长期的伦理行为，发现伦理性与毒性之间存在显著负相关关系(rET = -0.81，p < 0.001)，且与响应延迟几乎无关，表明道德一致性和语言安全是模型行为的稳定特征。

Conclusion: MoCoP将伦理评估重新定义为动态、模型无关的道德反思，为规模化、持续审计提供了可复现的基础，推动了自主AI系统中计算伦理的研究。

Abstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [44] [Bin2Vec: Interpretable and Auditable Multi-View Binary Analysis for Code Plagiarism Detection](https://arxiv.org/abs/2512.02197)
*Moussa Moussaoui,Tarik Houichime,Abdelalim Sadiq*

Main category: cs.SE

TL;DR: Bin2Vec综合程序静态和动态信息，生成可视化的多视图表示，提升软件相似性比较的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有软件比较方法往往只侧重单一类型信息，导致相似性判断不全面且缺乏解释性，Bin2Vec旨在通过多角度信息融合提升比较的可靠性和可解释性。

Method: Bin2Vec通过提取程序内置函数、导入导出、运行时指令和内存使用等多种特征，生成可单独查看的多视角数据视图，最终融合成整体相似度分数，支持机器学习处理。

Result: Bin2Vec框架通过结合程序的外观信息（内置函数、导入和导出）与行为信息（指令和内存使用）来实现软件相似性比较，生成易于解释的多视图表示和整体相似度评分。

Conclusion: Bin2Vec方法不仅提供了更加完整和解释友好的软件比较结果，还因其模块化设计适合多种安全审计和逆向工程应用。

Abstract: We introduce Bin2Vec, a new framework that helps compare software programs in a clear and explainable way. Instead of focusing only on one type of information, Bin2Vec combines what a program looks like (its built-in functions, imports, and exports) with how it behaves when it runs (its instructions and memory usage). This gives a more complete picture when deciding whether two programs are similar or not. Bin2Vec represents these different types of information as views that can be inspected separately using easy-to-read charts, and then brings them together into an overall similarity score. Bin2Vec acts as a bridge between binary representations and machine learning techniques by generating feature representations that can be efficiently processed by machine-learning models. We tested Bin2Vec on multiple versions of two well-known Windows programs, PuTTY and 7-Zip. The primary results strongly confirmed that our method compute an optimal and visualization-friendly representation of the analyzed software. For example, PuTTY versions showed more complex behavior and memory activity, while 7-Zip versions focused more on performance-related patterns. Overall, Bin2Vec provides decisions that are both reliable and explainable to humans. Because it is modular and easy to extend, it can be applied to tasks like auditing, verifying software origins, or quickly screening large numbers of programs in cybersecurity and reverse-engineering work.

</details>


### [45] [Towards autonomous normative multi-agent systems for Human-AI software engineering teams](https://arxiv.org/abs/2512.02329)
*Hoa Khanh Dam,Geeta Mahala,Rashina Hoda,Xi Zheng,Cristina Conati*

Main category: cs.SE

TL;DR: 本文提出了一种由具备人类类推理能力的自主智能代理主导的软件工程范式，这些代理基于大型语言模型，具备信念、欲望、意图和记忆，与人类及其他代理协作完成软件设计、实现、测试和部署，实现高效、可靠和适应性强的软件开发。


<details>
  <summary>Details</summary>
Motivation: 当前软件开发过程在速度、可靠性及适应性方面存在局限，本文旨在通过引入具有人类类推理能力的自主智能代理，提升软件开发的效率与质量。

Method: 利用大型语言模型赋能的软件工程代理，具备信念、欲望、意图和记忆，以模仿人类推理过程；这些代理之间的协作通过德性模态规范（承诺、义务、禁止和许可）进行管理，以保证交互的合规性。

Result: 构建了一个支持多代理和人类协作的软件工程系统，该系统通过规范化的交互管理，实现了远超现有开发流程的速度和可靠性，同时保证了透明性和信任度。

Conclusion: 通过引入具备人类思维特征和受规范约束的智能代理，建立了一个可扩展、透明且值得信赖的人机协作软件工程框架。

Abstract: This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.

</details>


### [46] [Process-Centric Analysis of Agentic Software Systems](https://arxiv.org/abs/2512.02393)
*Shuyang Liu,Yang Chen,Rahul Krishna,Saurabh Sinha,Jatin Ganhotra,Reyhan Jabbarvand*

Main category: cs.SE

TL;DR: 提出Graphectory对智能代理系统时间语义关系建模，分析4000条轨迹揭示复杂策略和流程效率问题，超越以结果为中心的评价方法。


<details>
  <summary>Details</summary>
Motivation: 传统软件系统评价多关注执行结果，忽视了智能代理系统的运行过程与策略变化，难以深入理解其推理和行为机制。

Method: 提出Graphectory方法，将智能代理系统的时间和语义关系结构化编码，支持基于过程的质量分析指标设计。利用该方法自动分析4000条基于四种大型语言模型的两大主流智能代理编程工作流轨迹。

Result: 发现使用更丰富提示词或更强语言模型的代理表现出更复杂的轨迹结构，反映出更充分的探索和验证；问题解决策略与问题难度及语言模型相关，成功问题具有连贯步骤，而未解决问题表现为混乱反复；即使成功，流程中仍存在效率低下的问题，轨迹不必要的冗长。

Conclusion: Graphectory方法有效揭示智能代理系统运行过程中的行为特征和策略差异，提供比结果导向更细致的质量评估视角，有助优化智能代理软件系统设计与评估。

Abstract: Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.
  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.

</details>


### [47] [Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System](https://arxiv.org/abs/2512.02567)
*Martin Weiss,Jesko Hecking-Harbusch,Jochen Quante,Matthias Woehrle*

Main category: cs.SE

TL;DR: 通过自动反馈循环优化的C到Rust代码翻译系统，减少了模型差异影响并提升翻译成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 强生成式人工智能对软件工程任务有重大影响，但自动化工具在工业应用中需更高可靠性，研究自动反馈循环、LLM选择和行为保持代码变更对质量的影响。

Method: 采用生成-检查模式的自动C到Rust代码翻译系统，利用大语言模型(LLM)生成Rust代码，并通过自动化反馈循环修复生成结果。

Result: 未使用反馈循环时，LLM的选择显著影响翻译成功率；使用反馈循环后，不同模型间差异减小，系统平均性能和对代码扰动的鲁棒性均提升；代码扰动多样性甚至促进性能改进。

Conclusion: 自动反馈循环能显著提高代码翻译系统的稳定性和效果，减小对具体大语言模型的依赖，且适度代码扰动有助于性能提升。

Abstract: The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.
  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.
  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.

</details>


### [48] [Empirical Assessment of the Perception of Software Product Line Engineering by an SME before Migrating its Code Base](https://arxiv.org/abs/2512.02707)
*Thomas Georges,Marianne Huchard,Mélanie König,Clémentine Nebut,Chouki Tibermacine*

Main category: cs.SE

TL;DR: 本研究通过访谈中小企业软件团队，发现迁移软件产品线虽挑战大，但有效沟通和保留现有良好做法有助于顺利过渡。


<details>
  <summary>Details</summary>
Motivation: 帮助中小企业评估并顺利实施向软件产品线的迁移。

Method: 通过采访关键利益相关者并分析其反馈，评估迁移过程中的利弊及风险。

Result: 发现所有参与者均认可迁移带来的利益，且主动沟通与保障良好现有实践为减少阻力的关键。

Conclusion: 迁移到软件产品线对公司有益，但需谨慎管理以减少风险。

Abstract: Migrating a set of software variants into a software product line (SPL) is an expensive and potentially challenging endeavor. Indeed, SPL engineering can significantly impact a company's development process and often requires changes to established developer practices. The work presented in this paper stems from a collaboration with a Small and Medium-sized Enterprise (SME) that decided to migrate its existing code base into an SPL. In this study, we conducted an in-depth evaluation of the company's current development processes and practices, as well as the anticipated benefits and risks associated with the migration. Key stakeholders involved in software development participated in this evaluation to provide insight into their perceptions of the migration and their potential resistance to change. This paper describes the design of the interviews conducted with these stakeholders and presents an analysis of the results. Among the qualitative findings, we observed that all participants, regardless of their role in the development process, identified benefits of the migration relevant to their own activities. Furthermore, our results suggest that an effective risk mitigation strategy involves keeping stakeholders informed and engaged throughout the process, preserving as many good practices as possible, and actively involving them in the migration to ensure a smooth transition and minimize potential challenges.

</details>


### [49] [Integrative Analysis of Risk Management Methodologies in Data Science Projects](https://arxiv.org/abs/2512.02728)
*Sabrina Delmondes da Costa Feitosa*

Main category: cs.SE

TL;DR: 本文比较分析数据科学风险管理方法，发现传统方法覆盖不足，现代框架包括伦理和治理，提出混合框架以提升项目成功率。


<details>
  <summary>Details</summary>
Motivation: 数据科学项目高失败率，受限于技术、组织及风险管理不足。现有文献指出数据成熟度低、治理缺乏、技术与业务脱节，以及缺少应对伦理和社会技术风险的机制。

Method: 通过指数数据库进行整合性文献回顾，分析并对比主要风险管理方法，包括ISO 31000、PMBOK风险管理、NIST RMF，及数据科学特定框架CRISP DM和DS EthiCo RMF。

Result: 发现传统方法对新兴风险覆盖有限，现代模型能整合伦理监督、治理及持续监控。

Conclusion: 提出发展结合技术效率、组织协调和责任数据实践的混合框架，并指出研究空白以指导未来研究。

Abstract: Data science initiatives frequently exhibit high failure rates, driven by technical constraints, organizational limitations and insufficient risk management practices. Challenges such as low data maturity, lack of governance, misalignment between technical and business teams, and the absence of structured mechanisms to address ethical and sociotechnical risks have been widely identified in the literature. In this context, the purpose of this study is to conduct a comparative analysis of the main risk management methodologies applied to data science projects, aiming to identify, classify, and synthesize their similarities, differences and existing gaps. An integrative literature review was performed using indexed databases and a structured protocol for selection and content analysis. The study examines widely adopted risk management standards ISO 31000, PMBOK Risk Management and NIST RMF, as well as frameworks specific to data science workflows, such as CRISP DM and the recently proposed DS EthiCo RMF, which incorporates ethical and sociotechnical dimensions into the project life cycle. The findings reveal that traditional approaches provide limited coverage of emerging risks, whereas contemporary models propose multidimensional structures capable of integrating ethical oversight, governance and continuous monitoring. As a contribution, this work offers theoretical support for the development of hybrid frameworks that balance technical efficiency, organizational alignment and responsible data practices, while highlighting research gaps that can guide future investigations.

</details>


### [50] ["Can you feel the vibes?": An exploration of novice programmer engagement with vibe coding](https://arxiv.org/abs/2512.02750)
*Kiev Gama,Filipe Calegario,Victoria Jackson,Alexander Nolte,Luiz Augusto Morais,Vinicius Garcia*

Main category: cs.SE

TL;DR: 本文通过一场教育黑客松研究vibe coding在新手程序员中的应用，发现其促进跨学科合作和技能提升，但需配合引导和评估以充分发挥教育效果。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能和AI辅助编码的发展，vibe coding作为一种通过自然语言提示代替直接编码的方法出现，其教育影响尚未充分研究。

Method: 在巴西一所公立大学举办了一天的教育黑客松，31名不同背景的本科生组成9支队伍，通过观察、问卷调查和半结构化访谈分析参与者的创意过程、工具使用、协作动态及学习成果。

Result: vibe coding促进了快速原型设计和跨学科合作，提升了提示工程技能，但存在创意早期收敛、代码质量参差不齐和对核心软件工程实践参与不足的问题。参与团队采用多种AI工具组合流程，依赖人工判断进行关键优化。短时间形式有效增强初学者信心，适合时间有限的参与者。

Conclusion: vibe coding黑客松在配合发散思维引导、AI输出批判性评估和合理质量预期的情况下，可以成为低风险的有效学习环境。

Abstract: Emerging alongside generative AI and the broader trend of AI-assisted coding, the term "vibe coding" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.

</details>


### [51] [Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior](https://arxiv.org/abs/2512.02795)
*Marcus Kessel*

Main category: cs.SE

TL;DR: 该论文提出了观察湖仓库（observation lakehouses）框架，用于持续存储和分析代码执行行为数据，实现高效动态行为挖掘，支持多版本评估和行为聚类。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成大模型训练多基于静态代码和注释，难以获取准确的运行时行为信息，因而容易学习到错误代码。动态观测执行是获得代码真实语义行为的唯一可行手段。

Method: 利用Apache Parquet、Iceberg和DuckDB构建存储系统，搭配持续输入的数据管线（如LASSO和持续集成单元测试），通过SQL查询实时生成刺激-响应矩阵和立方体的切片进行分析。

Result: 在包含509个问题的基准测试中，系统成功摄取了约860万条行为观测数据，重建行为视图和执行聚类均在100毫秒以内完成，证明了持续行为挖掘的实用性且无需分布式集群。

Conclusion: 观察湖仓库结合持续的刺激-响应立方体（SRC），实现了高效的行为数据存储、查询和分析，能够在非分布式环境下快速重建行为视图，实现行为驱动的代码评估和训练。

Abstract: Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse

</details>


### [52] [Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits](https://arxiv.org/abs/2512.02898)
*Pedro Orvalho,Marta Kwiatkowska,Mikoláš Janota,Vasco Manquinho*

Main category: cs.SE

TL;DR: CFaults是一种用于C软件和布尔电路多故障定位的新工具，通过将所有失败测试整合成一个最大可满足性(MaxSAT)公式，提高了定位的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于公式的故障定位方法无法保证针对所有失败测试的诊断一致性，且会产生冗余且非最小子集的诊断结果，特别是在多故障程序/电路中。

Method: 利用基于模型诊断（MBD）结合多个观察，将所有失败测试用例合并为统一的MaxSAT公式，实现一致性保证和故障定位简化。

Result: 在三个基准测试集上，CFaults在C程序故障定位速度优于BugAssist、SNIPER和HSD；在ISCAS85布尔电路基准中，定位数量稍少但竞争力强，同时只输出子集最小诊断，避免冗余。

Conclusion: CFaults在C软件故障定位上比现有FBFL方法更快且只产生子集最小的诊断结果，在布尔电路定位上虽稍慢，但依然具有竞争力。

Abstract: Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.
  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).

</details>


### [53] [The Evolutionary Ecology of Software: Constraints, Innovation, and the AI Disruption](https://arxiv.org/abs/2512.02953)
*Sergi Valverde,Blai Vidiella,Salva Duran-Nebreda*

Main category: cs.SE

TL;DR: 本文从生态和进化视角研究软件与创新的共生关系，利用模型和案例揭示AI工具对软件及文化进化的影响，警示文化停滞风险。


<details>
  <summary>Details</summary>
Motivation: 探索软件如何在新颖性生成与模仿的竞争力量下进化，以及AI驱动工具对该进化的影响。

Method: 结合基于代理的建模、复杂网络分析、进化理论和案例研究的方法。

Result: 揭示了编程语言及技术工具与社会规范、文化动态和人类互动的共进化关系，强调AI工具可能带来的文化多样性降低风险。

Conclusion: 软件与创新的共生关系驱动其复杂的进化轨迹，AI工具的引入带来新的进化压力，可能导致文化停滞。

Abstract: This chapter investigates the evolutionary ecology of software, focusing on the symbiotic relationship between software and innovation. An interplay between constraints, tinkering, and frequency-dependent selection drives the complex evolutionary trajectories of these socio-technological systems. Our approach integrates agent-based modeling and case studies, drawing on complex network analysis and evolutionary theory to explore how software evolves under the competing forces of novelty generation and imitation. By examining the evolution of programming languages and their impact on developer practices, we illustrate how technological artifacts co-evolve with and shape societal norms, cultural dynamics, and human interactions. This ecological perspective also informs our analysis of the emerging role of AI-driven development tools in software evolution. While large language models (LLMs) provide unprecedented access to information, their widespread adoption introduces new evolutionary pressures that may contribute to cultural stagnation, much like the decline of diversity in past software ecosystems. Understanding the evolutionary pressures introduced by AI-mediated software production is critical for anticipating broader patterns of cultural change, technological adaptation, and the future of software innovation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [54] [Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading](https://arxiv.org/abs/2512.02227)
*Jifeng Li,Arnav Grover,Abraham Alpuerto,Yupeng Cao,Xiao-Yang Liu*

Main category: cs.MA

TL;DR: 本文提出一种金融代理编排框架，通过代理化分解传统算法交易系统，实现优于市场的股票和比特币交易表现，代码开源。


<details>
  <summary>Details</summary>
Motivation: 金融市场因其时间动态性和低信噪比，是AI代理的关键应用领域。构建高效算法交易系统通常需要专业团队长年开发测试。

Method: 提出了一种金融代理编排框架，将传统算法交易系统的各个组件映射为不同的代理，包括规划者、编排者、阿尔法代理、风险代理、投资组合代理、回测代理、执行代理、审计代理和记忆代理。

Result: 在股票交易任务中，实现收益20.42%，夏普比率2.63，最大回撤-3.59%，优于同期S&P 500指数表现。在比特币交易任务中，实现收益8.39%，夏普比率0.38，最大回撤-2.80%，表现优于比特币价格涨幅。代码已开源。

Conclusion: 所提出的代理编排框架能够有效提升算法交易表现，降低专业门槛，助力大众获取金融智能。

Abstract: The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\%$, while the S&P 500 index yielded a return of $15.97\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\%$, whereas the BTC price increased by $3.80\%$. Our code is available on \href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.

</details>


### [55] [Decentralized Multi-Agent System with Trust-Aware Communication](https://arxiv.org/abs/2512.02410)
*Yepeng Ding,Ahmed Twabi,Junwei Yu,Lingfeng Zhang,Tohru Kondo,Hiroyuki Sato*

Main category: cs.MA

TL;DR: 本文提出一种基于区块链的去中心化多智能体系统，解决传统集中式架构的信任和扩展性问题，确保安全可信且性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统集中式多智能体系统存在单点故障、审查脆弱性、扩展性限制和信任问题，亟需一种可扩展、抗审查且可信的系统架构。

Method: 提出一种基于区块链的去中心化多智能体系统(DMAS)架构，通过加密原语和链上操作实现可信通信协议，确保交互可验证性、通信完整性、真实性、不可否认性和条件保密性。

Result: 设计并实现了支持可信通信的去中心化多智能体系统，安全分析证明其满足多项安全属性，性能分析显示系统具有良好的扩展性和效率。

Conclusion: 去中心化多智能体系统通过区块链技术实现了安全、可信且高效的多智能体交互，克服了传统集中式系统的关键缺陷，是构建可信任多智能体系统的有效方案。

Abstract: The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.

</details>


### [56] [EZYer: A simulacrum of high school with generative agent](https://arxiv.org/abs/2512.02561)
*Jinming Yang,Zimu Ji,Weiqi Luo,Gaoxi Wang,Bin Ma,Yueling Deng*

Main category: cs.MA

TL;DR: EZYer是一个集自动教材生成、协作互动笔记和内容质量控制于一体的智能教学代理，经过多维度评估显示生成内容质量高，应用前景良好。


<details>
  <summary>Details</summary>
Motivation: 当前在线教育与大型语言模型发展迅速，但在课件生成、互动笔记和内容质量保障方面仍存在服务不全、性能不足和交互性弱等问题，亟需开发一套综合性智能教学工具以提升教学内容生成的质量与互动体验。

Method: 通过文本语料库检索与深度生成技术自动生成结构化教材和LaTeX课件；设计四角色的协同交互机制生成学术笔记；构建关键词过滤、内容评分、共验证和动态纠正等质量控制机制；采用五维度评估指标和多大型语言模型进行系统评测。

Result: 本文提出了生成性智能代理EZYer，该系统包括教师模块、学生模块和控制器，分别实现教学材料的自动生成、协作互动学习笔记的生成以及内容质量的严格把控。教师模块基于文本语料检索与深度生成技术，自动生成符合高中数学教学大纲的结构化教材和LaTeX Beamer课件。学生模块通过师、助、优生和困难生四角色的协作，生成学术笔记以提升学习深度和趣味性。控制器包括关键词过滤、内容评分、角色共验证和动态内容纠正，确保内容的学术严谨性和教学合适性。该系统设计了内容准确性、知识覆盖度、易用性、格式正确性和视觉设计五维度评估指标，并由五个大型语言模型对生成的100份Beamer和笔记进行评分，结果显示EZYer生成的内容质量优异，具有良好的应用前景。

Conclusion: EZYer系统能有效自动生成高质量的符合教学大纲的数学教材和互动笔记，且通过严格的多维度质量控制保障内容的学术性和适用性，具有较好的推广应用潜力。

Abstract: With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in terms of courseware generation, interactive notes and quality assurance of content. In particular, the proposed generative agent EZYer : 1) Teacher Module: Integrating the Text Corpus retrieval and in-depth generation technologies, it automatically generates structured teaching materials and LaTeX Beamer courseware in line with the high school mathematics syllabus and supports user-defined image insertion. 2) Student Module: Throughout the collaborative interaction of the four roles of Teacher, Assistant, Top Student and Struggling Student, Note Taker summarizes and generates academic notes to enhance the depth and interest of learning. 3) Controller: set up keyword filtering system, content scoring system, role co-validation system, and dynamic content correction system. This ensure academic strictness and pedagogical propriety of EZYer inputs and outputs. In order to evaluate EZYer, this paper designs five-dimensional evaluation indexes of content accuracy, knowledge coverage, usability, formatting correctness and visual design and appeal, and scores 100 Beamer and Notes generated by EZYer by five large language models, separately, and the results show that the quality of EZYer-generated content is excellent and has a good application prospect.

</details>


### [57] [Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions](https://arxiv.org/abs/2512.02682)
*Piercosma Bisconti,Marcello Galisai,Federico Pierucci,Marcantonio Bracale,Matteo Prandi*

Main category: cs.MA

TL;DR: 现有安全方法适用于单一模型，人机交互环境，无法适应多模型交互系统。本文提出系统性安全框架和设计方案，应对大语言模型生态的集体风险。


<details>
  <summary>Details</summary>
Motivation: 当前安全机制只针对单模型与用户的交互，忽略了多模型相互作用带来的风险，导致系统整体可能出现失效。

Method: 通过理论分析和设计框架提出，研究大语言模型(LLM)互相交互时的安全机制。

Result: 提出了Emergent Systemic Risk Horizon(ESRH)框架，将安全从单模型层面上升到系统层面，系统性风险源于交互结构；并构建了微观、中观、宏观层面的故障分类及InstitutionalAI设计方案。

Conclusion: 有效治理多模型生态需要从单模型安全转向系统安全，采用ESRH框架识别交互引发的不稳定，并通过InstitutionalAI架构实现自适应监督。

Abstract: This paper examines why safety mechanisms designed for human-model interaction do not scale to environments where large language models (LLMs) interact with each other. Most current governance practices still rely on single-agent safety containment, prompts, fine-tuning, and moderation layers that constrain individual model behavior but leave the dynamics of multi-model interaction ungoverned. These mechanisms assume a dyadic setting: one model responding to one user under stable oversight. Yet research and industrial development are rapidly shifting toward LLM-to-LLM ecosystems, where outputs are recursively reused as inputs across chains of agents. In such systems, local compliance can aggregate into collective failure even when every model is individually aligned. We propose a conceptual transition from model-level safety to system-level safety, introducing the framework of the Emergent Systemic Risk Horizon (ESRH) to formalize how instability arises from interaction structure rather than from isolated misbehavior. The paper contributes (i) a theoretical account of collective risk in interacting LLMs, (ii) a taxonomy connecting micro, meso, and macro-level failure modes, and (iii) a design proposal for InstitutionalAI, an architecture for embedding adaptive oversight within multi-agent systems.

</details>
