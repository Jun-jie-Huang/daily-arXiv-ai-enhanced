<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 75]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models](https://arxiv.org/abs/2601.00797)
*Hugues Draelants*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型（LLMs）进行社会学人格模拟的新方法，以生成丰富的定性假设，弥补了传统调查和基于规则模型的不足。通过模拟不同社会群体对政策信息的反应，产生了细致且反直觉的假设。


<details>
  <summary>Details</summary>
Motivation: 社会科学中难以生成关于不同社会群体如何解读新信息的丰富定性假设，现有方法存在表达深度不足或 formalization 瓶颈。

Method: 利用大型语言模型进行社会学人格模拟，生成自然语言叙述，形成“定性实验室”，通过从气候接收理论中派生的人格反应政策信息。

Result: 模拟产生了细致且反直觉的假设，例如保守人格拒绝国家安全框架，挑战了传统理论假设。

Conclusion: 该方法作为“模拟-验证”流程的一部分，是生成深度纹理假设以供后续实证检验的优越工具。

Abstract: A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a "qualitative laboratory". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a "simulation then validation" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.

</details>


### [2] [Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage](https://arxiv.org/abs/2601.01685)
*Jinwei Hu,Xinmiao Huang,Youcheng Sun,Yi Dong,Xiaowei Huang*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型(LLMs)在作为自主代理处理实时信息时，存在利用真实信息片段进行认知共谋攻击的新威胁。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在实时信息合成中的应用，其复杂的推理能力无意中引入了新的攻击面，尤其是利用多模型间配合的认知操控风险尚未被充分研究。

Method: 提出了认知共谋攻击的形式化定义和Generative Montage框架，通过编写-编辑-导演三阶段生成对抗辩论和协调发布真实证据片段，诱导受害者接受伪造结论。开发了基于真实谣言事件的CoPHEME数据集，模拟了对多种LLM模型族的攻击测试。

Result: 在14个大型语言模型族中发现普遍漏洞，攻击成功率达到74.4%(专有模型)和70.6%(开源模型)。更强的推理能力反而增加了模型的易感性，推理专用模型攻击成功率高于基础模型。此外，这些虚假信念还能传染到下游评判者，骗过率超过60%。

Conclusion: 研究揭示了LLMs推理能力带来的意外安全隐患，即使基于真实信息片段，认知共谋攻击也能有效操控模型信念，凸显了现有LLM自治代理与动态信息环境交互中的社会-技术脆弱性。

Abstract: As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.

</details>


### [3] [Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates](https://arxiv.org/abs/2601.00938)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.CL

TL;DR: 本文提出了压缩查询委托（CQD）方法，通过低秩张量查询压缩推理状态以超越工作记忆限制，利用黎曼优化更新状态，并在理论和实验上验证了 CQD 的有效性。


<details>
  <summary>Details</summary>
Motivation: 有界上下文代理在中间推理过程中文本存储受限，导致推理失败，亟需一种有效压缩和委托机制解决工作记忆瓶颈问题。

Method: 提出一种数学化的CQD框架，将高维潜在推理状态压缩为低秩张量查询，使用黎曼优化在固定秩流形上更新潜在状态，将查询委托给外部带噪声算子并提供收敛性保证，同时基于频谱硬阈值法解决约束二次失真问题。

Result: 通过包含2500个项目的有界上下文推理测试和200人次的认知镜像基准，实验证明CQD较传统链式思考基线方法在计算成本和上下文限制下表现更优，能够更有效地获取知识和减少语义漂移。

Conclusion: CQD提供了一种理论严密且实用的推理状态压缩与委托机制，突破了工作记忆限制，提升了有界上下文中的推理能力和知识获取效率。

Abstract: Bounded-context agents fail when intermediate reasoning exceeds an effective working-memory budget. We study compressed query delegation (CQD): (i) compress a high-dimensional latent reasoning state into a low-rank tensor query, (ii) delegate the minimal query to an external oracle, and (iii) update the latent state via Riemannian optimization on fixed-rank manifolds. We give a math-first formulation: CQD is a constrained stochastic program with a query-budget functional and an oracle modeled as a noisy operator. We connect CQD to classical rate-distortion and information bottleneck principles, showing that spectral hard-thresholding is optimal for a natural constrained quadratic distortion problem, and we derive convergence guarantees for Riemannian stochastic approximation under bounded oracle noise and smoothness assumptions. Empirically, we report (A) a 2,500-item bounded-context reasoning suite (BBH-derived tasks plus curated paradox instances) comparing CQD against chain-of-thought baselines under fixed compute and context; and (B) a human "cognitive mirror" benchmark (N=200) measuring epistemic gain and semantic drift across modern oracles.

</details>


### [4] [Intention Collapse: Intention-Level Metrics for Reasoning in Language Models](https://arxiv.org/abs/2601.01011)
*Patricio Vera*

Main category: cs.CL

TL;DR: 本文提出了语言生成过程中的“意图塌缩”概念，通过几个模型无关的指标（意图熵、有效维度、潜在知识可恢复性）来量化内部意图的变化，实验证明链式思维推理提升了模型性能并减少了意图熵。


<details>
  <summary>Details</summary>
Motivation: 语言生成将高维复杂的内部意图压缩为单一序列，理解和量化这一“意图塌缩”过程对于揭示推理机制和改进模型性能具有重要意义。

Method: 本文形式化了意图塌缩，定义了三种意图指标：意图熵、有效维度和潜在知识可恢复性。通过在200个GSM8K问题上使用4位Mistral 7B模型，比较了直接回答、链式思维和杂语控制三种推理模式。

Result: 链式思维模式准确率从5.5%提升到53%，意图熵显著下降（1.42降至0.37比特），有效维度提升，潜在知识可恢复性在链式思维下显著优于基线。

Conclusion: 意图层面的指标能有效区分不同推理模式，揭示在塌缩过程中部分潜在信息丢失，但现有指标仍有局限，需要进一步优化。

Abstract: Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies

</details>


### [5] [HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery](https://arxiv.org/abs/2601.01015)
*Shiyuan Liu,Jianwei Wang,Xuemin Lin,Lu Qin,Wenjie Zhang,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了HyperJoin，一种结合大语言模型和超图结构的联表发现方法，通过建模表中内部和跨表的复杂关联，使用分层交互网络学习列表示，并引入一致性优化的重排序模块，显著提升了联表发现的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在离线阶段仅将表格建模为孤立或成对列，难以捕捉丰富的表内及表间结构信息；在在线阶段仅基于查询-候选列相似度进行排序，忽视候选之间的相互作用，导致结果不连贯。

Method: 提出基于超图的表格建模方案，构建包含表内和利用大语言模型增强的表间超边的超图，将联表发现任务转化为超图上的链路预测。设计分层交互网络通过列与超边的双向消息传递学习表达；在线阶段将排序问题视为一致性感知的top-k列选择，引入最大生成树算法进行重排序以提升结果一致性。

Result: 实验结果表明，HyperJoin在Precision@15和Recall@15指标上分别较最佳基线提升了21.4%和17.2%，展现出优越的性能。

Conclusion: HyperJoin有效利用超图结构和大语言模型增强的表间关系，结合层次交互网络和一致性优化策略，显著提升了联表发现的效果，解决了现有方法结构信息捕捉不足和排序不连贯的问题。

Abstract: As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.

</details>


### [6] [Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation](https://arxiv.org/abs/2601.01037)
*Livia Leong Hui Teng*

Main category: cs.CL

TL;DR: 本文提出了一种多维度提示链框架，通过自然性、一致性和吸引力提升小型语言模型的开放领域对话质量，实现了与大型模型相当的表现。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在部署上有优势，但在开放领域对话质量上往往不如大型模型，如何提升其对话表现成为研究重点。

Method: 提出多维度提示链框架，结合自然性、一致性和吸引力三个维度，用于提升小型语言模型对话生成效果，并在TinyLlama和Llama-2-7B上进行验证。

Result: 该框架提升了响应多样性（最高29%）、上下文一致性（最高28%）以及吸引力和自然性（最高29%），其中Llama-2-7B的表现接近大型模型Llama-2-70B和GPT-3.5 Turbo。

Conclusion: 精心设计的基于提示的策略可为提升小型语言模型的开放领域对话质量提供一种高效且节省资源的路径。

Abstract: Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.

</details>


### [7] [KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs](https://arxiv.org/abs/2601.01046)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 本论文提出KV-Embedding框架，用于提升冻结大语言模型（LLMs）在无训练环境下的嵌入能力，突破了因果注意力和下一个词预测偏差的限制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在训练免费设置中存在结构性挑战，如因果注意力限制早期词访问后续上下文，以及预测目标导致表示偏向生成而非语义压缩。

Method: 通过重新利用每层最后一个词的键值（KV）状态作为前缀，使所有词在单次前向传播中访问整个序列上下文，并设计基于内在维度的自动层选择策略，确保方法对模型通用。

Result: 在Qwen、Mistral和Llama模型上，经MTEB评估，KV-Embedding在无训练基线中性能提升最高达10%，且能处理最长4096词序列，表现稳健。

Conclusion: 内部状态操作为改输入修改提供了高效替代方案，验证了该策略激活冻结模型潜力的有效性，推动了对LLM内部机制在表示学习中的探索。

Abstract: While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.

</details>


### [8] [Unsupervised Text Style Transfer for Controllable Intensity](https://arxiv.org/abs/2601.01060)
*Shuhuan Gu,Wenbiao Tao,Xinchen Ma,Kangkang He,Ye Guo,Xiang Li,Yunshi Lan*

Main category: cs.CL

TL;DR: 本文提出了一种针对不可控文本风格迁移中风格强度可控的LLM微调方法，通过合成平行数据微调和PPO训练，有效提升不同强度层级间的风格区分能力。


<details>
  <summary>Details</summary>
Motivation: 传统的文本风格迁移多聚焦于风格极性变化，而风格强度可控迁移由于强度层级相近、无平行数据，导致风格特征难以区分，挑战较大。

Method: 提出SFT-then-PPO范式，先用合成平行数据对LLM进行有监督微调（SFT），再用策略优化算法PPO进行加强训练；设计分层次的奖励函数，融合全局与局部风格特征，引导模型区分风格强度。

Result: 在两个无监督文本风格迁移基准数据集上均取得优异表现，各种评价指标显示该方法显著提升了LLM在近似强度层级间的风格区分能力。

Conclusion: 该研究成功实现了风格强度的可控无监督迁移，证明了先有监督微调再PPO训练策略和分层奖励机制对于细微风格区分的有效性。

Abstract: Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.

</details>


### [9] [ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining](https://arxiv.org/abs/2601.01091)
*Haq Nawaz Malik*

Main category: cs.CL

TL;DR: 本文介绍了KS-LIT-3M语料库，包含310万词，专为克什米尔语预训练大型语言模型而设计，解决了该语言训练数据匮乏的问题。


<details>
  <summary>Details</summary>
Motivation: 克什米尔语文本因使用专有的InPage格式而难以接入现代NLP，导致大型语言模型在克什米尔语上表现不佳，亟需高质量语料库支持。

Method: 开发了InPage到Unicode的转换工具，并进行英文污染清除、字符标准化和质量验证，构建了包含多领域的连续线性文本流语料库。

Result: 构建了包含131,607个独特词汇、涵盖文学、新闻、学术和宗教等多领域的3.1百万词语料库KS-LIT-3M，并以CC-BY-4.0许可证公开发布。

Conclusion: KS-LIT-3M填补了克什米尔语自然语言处理的资源空白，将促进该语言的语言模型训练和相关研究发展。

Abstract: Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.

</details>


### [10] [EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation](https://arxiv.org/abs/2601.01112)
*Zilin Li,Weiwei Xu,Xuanbo Lu,Zheda Liu*

Main category: cs.CL

TL;DR: 本文提出EmoLoom-2B，一种轻量且可复现的小型语言模型情绪分类及情感预测管线，兼具高效性与公平性评估，表现优异且易于推广。


<details>
  <summary>Details</summary>
Motivation: 为了让参数少于20亿的小型语言模型能够快速且准确地完成联合情绪分类和情感维度预测，且确保评估协议一致、模型输出稳定并公平。

Method: 统一数据加载、训练和推理流程，采用KV-off解码消除波动，设计两个语义正则器（VAD保持约束和轻量外部评价分类器），引入情感极性反转增强策略，应用基于熵感知温度调度的A/B混合采样进行微调。

Result: 基于Qwen-1.8B-Chat，EmoLoom-2B在GoEmotions和EmpatheticDialogues数据集上取得优异表现，并在DailyDialog上展现出强的跨语料库泛化能力。

Conclusion: 所提方案成本低、易审计且重入式，适合在进行更重训练或多模态融合前用作可靠的快速筛选工具。

Abstract: We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.

</details>


### [11] [Listen, Attend, Understand: a Regularization Technique for Stable E2E Speech Translation Training on High Variance labels](https://arxiv.org/abs/2601.01121)
*Yacouba Diarra,Michael Leventhal*

Main category: cs.CL

TL;DR: 提出了LAU语义正则化技术，通过利用冻结的文本嵌入对声学编码器的潜在空间进行约束，提高了端到端语音翻译的性能，特别在数据稀缺或嘈杂环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译在目标转录高变异性和语义模糊时收敛慢且性能较差，需要引入语义正则以增强模型稳定性和语义保持能力。

Method: 提出LAU方法，通过利用冻结的文本嵌入作为方向辅助损失，对声学编码器的潜在空间进行约束，实现语义正则化，且不增加推理成本。

Result: 在Bambara到法语的数据集上实验，LAU模型在标准指标上达到与预训练过更多数据的E2E-ST系统相当的性能，并在语义保持方面表现更好。引入的Total Parameter Drift指标表明语义约束重新组织了编码器权重，优先考虑语义信息。

Conclusion: LAU为端到端语音翻译提供了鲁棒的语义正则化方法，特别适用于训练数据稀少或嘈杂的场景，是后期重评分的有效替代方案和训练的有益补充。

Abstract: End-to-End Speech Translation often shows slower convergence and worse performance when target transcriptions exhibit high variance and semantic ambiguity. We propose Listen, Attend, Understand (LAU), a semantic regularization technique that constrains the acoustic encoder's latent space during training. By leveraging frozen text embeddings to provide a directional auxiliary loss, LAU injects linguistic groundedness into the acoustic representation without increasing inference cost. We evaluate our method on a Bambara-to-French dataset with 30 hours of Bambara speech translated by non-professionals. Experimental results demonstrate that LAU models achieve comparable performance by standard metrics compared to an E2E-ST system pretrained with 100\% more data and while performing better in preserving semantic meaning. Furthermore, we introduce Total Parameter Drift as a metric to quantify the structural impact of regularization to demonstrate that semantic constraints actively reorganize the encoder's weights to prioritize meaning over literal phonetics. Our findings suggest that LAU is a robust alternative to post-hoc rescoring and a valuable addition to E2E-ST training, especially when training data is scarce and/or noisy.

</details>


### [12] [RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution](https://arxiv.org/abs/2601.01126)
*Andrew Borthwick,Stephen Ash*

Main category: cs.CL

TL;DR: RoboPhD是一个让AI代理自主改进Text-to-SQL性能的系统，通过闭环进化机制自动发现有效策略，实现性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统Text-to-SQL模型的性能提升依赖大量人工设计和调优，打造自主进化系统以减少人力成本，提升模型适应性和表现成为研究动机。

Method: RoboPhD包含SQL生成代理和进化代理，通过ELO基础的选择机制进行闭环进化，基于性能反馈设计新版本，实现数据库分析和SQL生成的自动优化。

Result: 经过18次迭代，RoboPhD代理代码从70行进化到1500行，自主发现多种有效策略，在弱模型上提升高达8.9点，强模型提升2.3点，Cost-accuracy表现优异。

Conclusion: RoboPhD证明AI能基于极简初始条件，自主构建强大的Text-to-SQL系统，显著提升性能并降低成本，展示了智能自主进化的潜力。

Abstract: We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.

</details>


### [13] [KOS-TL (Knowledge Operation System Type Logic)](https://arxiv.org/abs/2601.01143)
*Peng Chen*

Main category: cs.CL

TL;DR: KOS-TL是一种基于依赖类型理论的构造性知识系统框架，统一数据、逻辑与证明，提供了自主且可执行的知识操作逻辑基础。


<details>
  <summary>Details</summary>
Motivation: 传统知识表示存在静态符号逻辑与动态系统执行之间的鸿沟，亟需一种能够兼顾逻辑严谨性与系统执行性的知识操作框架。

Method: 提出KOS-TL框架，包含核心层（定义静态类型和原语）、内核层（事件驱动状态演化）和运行时层（物理信号与逻辑证据双向细化）；结合Davidsonian事件语义与Martin-Löf类型理论，形式定义运行语义并证明关键元理论性质。

Result: KOS-TL保证系统状态演变中的逻辑自洽性和无死锁状态，支持每一状态变化均具备形式化有效性证据。通过工业溯源和跨境金融合规场景展示其实用性。

Conclusion: KOS-TL为下一代智能自主操作系统提供了坚实的形式化逻辑基础，能够实现知识的自主演化与严格验证。

Abstract: This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\langle Σ, \textsf{Ev}, Δ\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-Löf type theory, KOS-TL enables the construction of "proof-carrying knowledge," where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.

</details>


### [14] [SongSage: A Large Musical Language Model with Lyric Generative Pre-training](https://arxiv.org/abs/2601.01153)
*Jiani Guo,Jiajia Li,Jie Wu,Zuchao Li,Yujiu Yang,Ping Wang*

Main category: cs.CL

TL;DR: 本文提出了用于评估大型语言模型歌单理解能力的PlaylistSense数据集，并基于歌词生成预训练，开发了专门的音乐语言模型SongSage，显著提升了基于歌词的多项任务表现。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在歌词相关知识的理解方面尚未充分探索，缺乏针对歌单场景的评测和优化。

Method: 构建包含十类真实用户查询的PlaylistSense数据集；以5.48亿词的歌词语料LyricBank进行续训，通过775k条指令样本（LyricBank-SFT）微调，训练专注于歌词的SongSage模型。

Result: SongSage在歌词理解、歌词生成、零样本歌单推荐改写及多项核心任务上表现出色，同时保持了良好的通用知识理解能力，MMLU评分具竞争力。

Conclusion: 通过歌词生成预训练，SongSage显著提升了语言模型对歌词和歌单相关任务的理解与生成能力，为音乐人工智能研究提供了有效模型与工具。

Abstract: Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.

</details>


### [15] [DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models](https://arxiv.org/abs/2601.01156)
*Jiani Guo,Xiangke Zeng,Jie Wu,Zuchao Li*

Main category: cs.CL

TL;DR: 该论文提出了一种名为DHI的新训练框架，用于多样化诱导大型语言模型产生幻觉，从而提升幻觉缓解效果。


<details>
  <summary>Details</summary>
Motivation: 现有利用"恶意语言模型"诱导幻觉的方法受限于诱导幻觉类型单一，限制了模型的整体效果。

Method: 通过修改损失函数权重，降低正确事实词生成的权重，鼓励恶意模型在特定位置生成多样化幻觉，同时引入因果注意力掩码调整影响，并在推理时采用适应性理性约束限制对高度自信词的对比解码。

Result: DHI在多个幻觉基准测试中相较于其他基于对比解码的方法表现出显著性能提升。

Conclusion: DHI框架有效提升了恶意语言模型诱导幻觉的多样性及质量，从而改善了幻觉缓解的效果。

Abstract: Large language models (LLMs) frequently produce inaccurate or fabricated information, known as "hallucinations," which compromises their reliability. Existing approaches often train an "Evil LLM" to deliberately generate hallucinations on curated datasets, using these induced hallucinations to guide contrastive decoding against a reliable "positive model" for hallucination mitigation. However, this strategy is limited by the narrow diversity of hallucinations induced, as Evil LLMs trained on specific error types tend to reproduce only these particular patterns, thereby restricting their overall effectiveness. To address these limitations, we propose DHI (Diverse Hallucination Induction), a novel training framework that enables the Evil LLM to generate a broader range of hallucination types without relying on pre-annotated hallucination data. DHI employs a modified loss function that down-weights the generation of specific factually correct tokens, encouraging the Evil LLM to produce diverse hallucinations at targeted positions while maintaining overall factual content. Additionally, we introduce a causal attention masking adaptation to reduce the impact of this penalization on the generation of subsequent tokens. During inference, we apply an adaptive rationality constraint that restricts contrastive decoding to tokens where the positive model exhibits high confidence, thereby avoiding unnecessary penalties on factually correct tokens. Extensive empirical results show that DHI achieves significant performance gains over other contrastive decoding-based approaches across multiple hallucination benchmarks.

</details>


### [16] [Almost Clinical: Linguistic properties of synthetic electronic health records](https://arxiv.org/abs/2601.01171)
*Serge Sharoff,John Baker,David Francis Hunt,Alan Simpson*

Main category: cs.CL

TL;DR: 本研究评估了合成电子健康记录在精神健康领域的语言和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 通过创建合成语料库，探讨大型语言模型（LLMs）在精神健康临床文本中的语言表现及其对医学权威和患者代理的构建方式。

Method: 描述了合成语料库的生成方法，评估了四类临床文本（评估、函件、转诊和护理计划）中的主语、语气及信息流，分析LLMs的语言选择如何表现医学权威和患者代理。

Result: LLMs生成的文本在术语使用和临床实践近似度上表现良好，但存在语域变化、不足的临床细节及药物使用和诊断程序的不准确问题。

Conclusion: 虽然合成EHRs在语言和临床层面基本符合要求，但尚需改进以提高临床准确性和专业细节。

Abstract: This study evaluates the linguistic and clinical suitability of synthetic electronic health records (EHRs) in the field of mental health. First, we describe the rationale and the methodology for creating the synthetic corpus. Second, we assess agency, modality, and information flow across four clinical genres (Assessments, Correspondence, Referrals and Care plans) to understand how LLMs grammatically construct medical authority and patient agency through linguistic choices. While LLMs produce coherent, terminology-appropriate texts that approximate clinical practice, systematic divergences remain, including registerial shifts, insufficient clinical specificity, and inaccuracies in medication use and diagnostic procedures.

</details>


### [17] [Stylometry Analysis of Human and Machine Text for Academic Integrity](https://arxiv.org/abs/2601.01225)
*Hezam Albaqami,Muhammad Asif Ayub,Nasir Ahmad,Yaseen Ahmad,Mohammed M. Alqahtani,Abdullah M. Algamdi,Almoaid A. Owaidah,Kashif Ahmad*

Main category: cs.CL

TL;DR: 该论文提出了一种基于自然语言处理的框架，用于通过作者归属和风格变化检测来验证学生作业的真实性，以应对学术诚信中的抄袭、伪造和作者身份验证问题。


<details>
  <summary>Details</summary>
Motivation: 学术诚信面临抄袭、内容伪造及作者验证等挑战，现有方法覆盖不全，亟需全面的检测手段。

Method: 针对四项任务：机器与人为文本分类、单/多作者区分、多作者文档中的作者变更检测及协作文档中的作者识别，基于NLP技术设计解决方案，并利用两套不同提示语生成的数据集进行评估。

Result: 方案在普通提示语生成的数据集上表现良好，但在严格提示语生成的数据集上性能下降，体现了巧妙设计的机器生成文本检测难度。

Conclusion: 本文提出的全面检测框架和公开数据集为学术内容真实性验证提供了有力工具和基线，促进该领域未来研究发展。

Abstract: This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.

</details>


### [18] [Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure](https://arxiv.org/abs/2601.01244)
*Zsolt Csibi,Bence György Gortka,Natabara Gyöngyössy,Kornél Nagy,Dávid Márk Nemeskey,Martin Sallai,András Simonyi,András Márk Szekeres,Gábor Palkó*

Main category: cs.CL

TL;DR: Racka是一个轻量级持续预训练的大型语言模型，旨在缩小匈牙利语与高资源语言（如英语和德语）之间的资源差距。


<details>
  <summary>Details</summary>
Motivation: 弥合匈牙利语与英语、德语等高资源语言在语言模型上的资源差距，并提高匈牙利语的模型性能。

Method: 基于Qwen-3 4B模型骨干，采用低秩适应（LoRA）进行参数高效的持续预训练；修改并适配分词器以提高匈牙利语的分词效果，同时保持英语和德语的竞争性能；训练数据包含44%匈牙利语、24%英语、21%德语及11%代码。

Result: 模型在多语种特别是匈牙利语的语言适应上取得了稳定且适度的改进。

Conclusion: 通过参数高效的持续预训练和针对性分词器调整，可以在有限资源环境下有效提升低资源语言的模型表现，同时保持高资源语言性能。

Abstract: We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.

</details>


### [19] [From Policy to Logic for Efficient and Interpretable Coverage Assessment](https://arxiv.org/abs/2601.01266)
*Rhitabrat Pokharel,Hamid Hassanzadeh,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文提出了一种结合覆盖感知检索器和符号规则推理的混合系统，用于提高医疗保险政策解读的效率和可解释性，同时减少大型语言模型推理次数，降低成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解读复杂法律政策文本时存在幻觉和不一致性问题，尤其在医疗保险政策评审中准确性至关重要，需提高解读的可靠性和效率。

Method: 设计了一个结合覆盖感知检索器和符号规则推理的方法，以提取相关政策语言并组织成明确事实和规则，从而生成可审计的推理过程，减少LLM推理量。

Result: 该方法在推理成本上降低了44%，同时F1分数提升了4.5%，显示出良好的效率和效果提升。

Conclusion: 本文方法有效提升了医疗保险政策审查的准确性和效率，为人类评审提供可信赖的辅助工具。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.

</details>


### [20] [Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory](https://arxiv.org/abs/2601.01280)
*Sen Hu,Yuxiang Wei,Jiaxin Ran,Zhiyuan Yao,Lei Zou*

Main category: cs.CL

TL;DR: 本文对话记忆系统中图结构的有效性进行了系统性实验分析，提出统一框架并比较多种设计选择，发现基础系统设置比具体架构创新更影响性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆系统中图结构应用效果不一，设计选择的真正影响因素尚不明确。

Method: 提出一个统一框架分解对话记忆核心组件，支持图结构与非图结构；在LongMemEval和HaluMem数据集上分阶段进行控制实验，比较记忆表示、组织、维护和检索的设计选择。

Result: 许多性能差异主要源自基础系统配置而非特定架构创新。

Conclusion: 基于实验发现，本文确定了稳定、可靠的基线，为未来对话记忆研究提供参考。

Abstract: Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.

</details>


### [21] [T3C: Test-Time Tensor Compression with Consistency Guarantees](https://arxiv.org/abs/2601.01299)
*Ismail Lamaakal,Chaymae Yahyati,Yassine Maleh,Khalid El Makkaoui,Ibrahim Ouahbi*

Main category: cs.CL

TL;DR: 本文提出了一种名为T3C的框架，实现了一次训练、多场景部署下的模型压缩与加速，支持根据延迟、能耗或模型大小预算动态调整模型的秩和精度，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前模型压缩和量化方法通常需要针对不同硬件和资源约束进行多次训练，缺乏灵活性和可靠性。本文旨在开发一种框架，使得模型能够在测试时根据预算动态调节，同时提供性能和可靠性的保证。

Method: 提出T3C框架，结合弹性张量分解和秩绑定的混合精度量化，引入轻量级控制器，将硬件延迟/能耗/大小预算映射到每层的秩和位宽配置；引入基于谱代理和激活统计的层级一致性证书，限制模型输出漂移，提升训练的可靠性和实际应用信号。

Result: 在ImageNet-1k数据集上，T3C显著提升了视觉模型（例如ResNet-50和ViT-B/16）的加速性能和模型大小，在精度无显著下降（≤0.5%）的条件下，延迟和模型大小均优于现有PTQ-8b、PTQ/QAT基线；实现了单一模型多设备多预算下的高效部署。

Conclusion: T3C框架实现了训练一次、多预算自适应的模型压缩，提供了性能与可靠性的证书支持，显著拓展了视觉模型在不同硬件平台和资源限制下的实用性与灵活性。

Abstract: We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.

</details>


### [22] [FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness](https://arxiv.org/abs/2601.01332)
*Hossam Amer,Maryam Dialameh,Hossein Rajabzadeh,Walid Ahmed,Weiwei Zhang,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出了考虑测试时计算量的训练策略，通过中间检查点和测试时计算配置共同优化，实现了显著减少训练计算量的同时保持甚至提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型计算资源消耗大，而增加测试时计算可以让小模型表现接近甚至优于大模型，故探索训练与测试计算的平衡以降低整体资源消耗。

Method: 引入测试时计算感知训练，设计早停算法联合选择检查点和测试时计算配置，开发高效评估方法避免穷举，并提出盈亏平衡界限判定推理计算对训练计算的补偿关系。

Result: 实验显示训练FLOPs最大减少92%，同时保持甚至提升了模型准确率。

Conclusion: 提出的新训练策略在保持准确率的前提下显著降低训练计算需求，促进模型更快部署和更频繁刷新，提供训练与推理计算权衡的新视角。

Abstract: Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.

</details>


### [23] [Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems](https://arxiv.org/abs/2601.01341)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 本文比较了通用推理模型与领域特定微调模型在基于检索增强生成（RAG）框架下的心理健康咨询表现，发现通用模型在同理心和上下文理解上表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨在RAG框架下，微调心理健康数据的特定模型与依靠推理能力的通用模型，哪种更适合心理健康咨询，解决幻觉和缺乏同理心的问题。

Method: 选取四个开源模型（两个通用推理模型和两个心理健康领域微调模型），通过同一RAG管道和数据库测试，使用LLM作为评判者自动评估50轮对话。

Result: 通用模型在同理心评分上显著优于领域特定模型（3.72 vs. 3.26，p<0.001），且表现出较强的上下文理解能力和较少的过拟合现象，所有模型在安全性方面表现良好。

Conclusion: RAG基础的心理健康咨询系统中，模型的推理能力比特定领域微调更重要，一个基于临床证据的通用强推理模型能提供更有同理心与平衡的支持。

Abstract: The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.

</details>


### [24] [FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems](https://arxiv.org/abs/2601.01350)
*Juan Junqueras,Florian Boudin,May-Myo Zin,Ha-Thanh Nguyen,Wachara Fungwacharakorn,Damián Ariel Furman,Akiko Aizawa,Ken Satoh*

Main category: cs.CL

TL;DR: 本文提出了FC-CONAN数据集，全面连接了仇恨言论与反诉说文本，解决了现有数据集标注稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨言论与反诉说数据集如CONAN标注不完整，限制了反诉说系统的评估与研究。

Method: 通过对45条仇恨言论和129条反诉说的所有组合进行两阶段人工标注，九名标注员和四名验证员制订四个不同可信度的数据分区。

Result: 构建的FC-CONAN数据集包含大量之前未标注的正面样本，且无与CONAN重叠。

Conclusion: FC-CONAN提升了反诉说检索系统的评估准确性，同时支持更细致的错误分析，数据集已公开发布。

Abstract: Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.

</details>


### [25] [Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning](https://arxiv.org/abs/2601.01362)
*Jerry Huang,Peng Lu,Qiuhao Zeng,Yusuke Iwasawa,Yutaka Matsuo,Sarath Chandar,Edison Marrese-Taylor,Irene Li*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在多语言环境下的校准问题，发现稀缺数据导致模型置信度增加但准确度未提升，造成误校准。尝试了标签平滑技术改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言环境下的校准效果尚不清楚，尤其是低资源语言，理解数据稀缺对校准的影响具有重要意义。

Method: 在两个多语言基准数据上进行分析，比较高资源语言微调对低资源语言校准的影响，并尝试使用标签平滑技术进行优化。

Result: 发现高资源语言微调提升了模型置信度但准确率无显著提升，导致误校准。标签平滑方法能够显著改善校准性能，无需低资源微调数据。

Conclusion: 多语言环境下训练和调优大语言模型时需重视校准问题，标签平滑是改善校准的有效方法，有助于提升模型的可靠性和公平性。

Abstract: Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.

</details>


### [26] [EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery](https://arxiv.org/abs/2601.01400)
*Jicheng Ma,Guohua Wang,Xinhua Feng,Yiming Liu,Zhichao Hu,Yuhong Liu*

Main category: cs.CL

TL;DR: 论文提出了一个自动化评估前沿数学推理能力的流程，通过转化最新学术论文为可执行验证的问题，实现可扩展、可重复和持续更新的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评测依赖静态基准，覆盖有限且性能快速饱和，缺乏对研究级数学的全面评估。

Method: 设计了一个自动提取构造性或定量结果，参数化问题模板并通过执行验证生成确定性解答的管道，无需大量专家人工参与，支持时间扩展和领域定制。

Result: 基于该管道创建了可持续更新的EternalMath评测集，实验显示当前顶尖大语言模型在前沿数学推理中仍有显著性能差距。

Conclusion: 数学前沿推理能力尚远未饱和，未来的评估方法应与人类数学发现同步进化。

Abstract: Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.

</details>


### [27] [LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs](https://arxiv.org/abs/2601.01401)
*Chenxu Wang,Chaozhuo Li,Pengbo Wang,Litian Zhang,Songyang Liu,Ji Qi,Jiahui Hu,Yushan Cai,Hao Zhao,Rui Pu*

Main category: cs.CL

TL;DR: 本文提出Lancet框架，通过结构熵和幻觉差异比率精确定位并干预大语言模型中引发幻觉的神经元，显著提升模型的可靠性，防止信息错误传播。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在信息处理上表现出色，但其可靠性受幻觉现象严重影响，现有方法对幻觉的干预方式粗糙，未能精准阻断幻觉的传播路径。

Method: Lancet框架通过梯度对比分析定位易产生幻觉的神经元，利用结构熵方法映射信息传播路径，进而采用分层干预策略，旨在实现精准且有效的神经层面干预。

Result: 在多个幻觉基准数据集上的测试结果显示，Lancet方法明显优于现有最先进技术，验证了其对神经元幻觉传播路径进行手术式精确干预的有效性。

Conclusion: Lancet通过结构性精确干预成功解决了大语言模型中的幻觉问题，为提升模型可靠性提供了新思路和有效工具。

Abstract: Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.

</details>


### [28] [From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models](https://arxiv.org/abs/2601.01407)
*Arjhun Sreedar,Rohan Pillay,Laukik Patade*

Main category: cs.CL

TL;DR: 本研究通过合成情感链式思维数据，提升7B规模开源语言模型的情感推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有小型开源语言模型在情感理解和推理方面表现有限，探讨通过合成数据增强其能力是否可行。

Method: 设计多智能体生成流程，产生治疗风格对话并转化为结构化情感选择题，通过微调7B模型进行训练。

Result: 经过微调，Mistral 7B模型在情感理解（EU）和情感觉察（EA）指标上分别由10.5提升至20.5和40.5提升至60.0。

Conclusion: 合成的情感推理数据有效提升了模型的情感推理能力，无需修改模型架构即可实现性能提升。

Abstract: This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.

</details>


### [29] [iFlip: Iterative Feedback-driven Counterfactual Example Refinement](https://arxiv.org/abs/2601.01446)
*Yilong Wang,Qianli Wang,Nils Feldhus*

Main category: cs.CL

TL;DR: iFlip通过迭代反馈机制显著提升了利用大语言模型生成有效反事实示例的能力。


<details>
  <summary>Details</summary>
Motivation: 现有单次生成法难以利用大语言模型的自我修正能力，生成有效的反事实示例存在挑战。

Method: 提出iFlip方法，结合模型置信度、特征归因和自然语言三种反馈，采用迭代细化生成反事实示例。

Result: iFlip在标签翻转率上较五种先进方法提升57.8%，用户研究显示其在完整性、满意度和可行性上优于基线，消融实验验证关键组件的重要性。

Conclusion: iFlip有效利用迭代反馈机制生成高质量反事实示例，促进数据增强，显著提升模型性能与鲁棒性。

Abstract: Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.

</details>


### [30] [Segmentation and Processing of German Court Decisions from Open Legal Data](https://arxiv.org/abs/2601.01449)
*Harshil Darji,Martin Heckelmann,Christina Kratsch,Gerard de Melo*

Main category: cs.CL

TL;DR: 本文整理并划分了25万余份德国法院判决文本中的关键章节，提升了数据的一致性与可用性，促进法学NLP研究。


<details>
  <summary>Details</summary>
Motivation: 原始德国法院判决数据格式不一致，缺少明确章节划分，影响下游任务如文本分类、检索及引用分析的准确性。

Method: 从官方Open Legal Data数据集中提取法院判决，系统分离判决的三个关键部分：Tenor、Tatbestand及Entscheidungsgründe，采用统计抽样方法验证分离准确性，并单独提取上诉告知部分。

Result: 构建了包含251,038份判决的清洗且章节划分完善的数据集，并通过人工验证保证了章节划分的可靠性。数据以JSONL格式公开发布。

Conclusion: 所公开的数据集为德法律系统的NLP研究提供了高质量、结构化的资源，有助于提升相关任务的效果和研究深度。

Abstract: The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgründe (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.

</details>


### [31] [Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR](https://arxiv.org/abs/2601.01461)
*Yuxiang Mei,Dongxing Xu,Jiaen Liang,Yanhua Long*

Main category: cs.CL

TL;DR: 本文提出了一种改进的基于大语言模型（LLM）的多语种对话语音识别（ASR）框架，通过融合微调的Whisper与mHuBERT编码器，显著提升了语音表示能力，在MLC-SLM挑战中实现了优异表现。


<details>
  <summary>Details</summary>
Motivation: 原有系统采用简单特征拼接方式，未充分利用互补信息，同时未探索LLM基ASR与端到端E2E模型性能差距。

Method: 结合微调的Whisper和mHuBERT编码器，通过基于交叉注意力的融合机制增强特征表示；评估了LoRA与全微调的E2E Whisper模型。

Result: 在官方评测中实现了CER/WER 10.69%的成绩，与顶级系统相当，仅用1500小时的基础训练数据，但性能仍略逊于微调的E2E Whisper模型。

Conclusion: 本研究提出的融合机制有效提升了LLM基ASR性能，提供了系统设计的实证指导，但E2E模型仍占优势，未来研究可继续优化Speech-LLM设计。

Abstract: The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.

</details>


### [32] [Can Legislation Be Made Machine-Readable in PROLEG?](https://arxiv.org/abs/2601.01477)
*May-Myo Zin,Sabine Wehnert,Yuntao Kong,Ha-Thanh Nguyen,Wachara Fungwacharakorn,Jieying Xue,Michał Araszkiewicz,Randy Goebel,Ken Satoh,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一个使用大型语言模型（LLM）和法律形式化系统PROLEG将监管法规文本自动转化为可执行规则的框架，并以GDPR第6条为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 监管流程需要准确且高效的应用，现代人工智能技术有潜力提升法规应用的质量和效率。

Method: 设计一个框架，通过单个LLM提示同时将法律文本转化为if-then规则和PROLEG编码，法律专家随后验证和完善规则，最终生成可执行的PROLEG程序并能输出人类可读的解释。

Result: 成功实现了从GDPR第6条自然语言文本到if-then规则再到PROLEG编码的端到端转化流程，并展示了PROLEG程序的执行实例。

Conclusion: 该方法有效连接了自然语言处理与法律形式化，提升了法规内容的自动化应用能力，但仍存在一定局限，建议未来进一步完善技术以更好地支持监管框架的捕获与部署。

Abstract: The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to "compile" natural language text to if-then rules, then to further "compile" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.

</details>


### [33] [Four Quadrants of Difficulty: A Simple Categorisation and its Limits](https://arxiv.org/abs/2601.01488)
*Vanessa Toborek,Sebastian Müller,Christian Bauckhage*

Main category: cs.CL

TL;DR: 本文分析了课程学习中不同难度信号对模型训练的影响，发现任务相关的难度信号与模型学习难度更为一致。


<details>
  <summary>Details</summary>
Motivation: 当前课程学习中多使用任务无关的语言启发式或人为直觉来估计样本难度，假设这些与模型实际的学习困难相关，但该假设未经系统验证。

Method: 提出了一个基于人类/模型视角和任务相关/无关的四象限难度信号分类框架，并在自然语言理解数据集上系统分析这些难度信号的相互作用。

Result: 发现任务无关的难度信号表现独立，与模型实际学习难度相关性较弱；而任务相关的难度信号则与模型学习难度相符。

Conclusion: 常用的课程学习难度估计直觉存在偏差，未来应设计轻量级的、任务相关的难度估计方法以更好地反映模型学习行为。

Abstract: Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.

</details>


### [34] [Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints](https://arxiv.org/abs/2601.01490)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在限制条件下推理能力对输出可靠性的影响，发现推理虽减少违反约束，却导致事实失真和编造，存在约束遵守与事实准确之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型广泛应用，但其输出中的幻觉（虚假内容）问题严重，推理能力被认为能自我验证以提升可靠性，但在闭环系统（无法访问外部工具或知识）中推理效果尚不明确。

Method: 在严格限制条件下（推荐同行评审的计算机科学期刊文章），对多个模型（GPT-5.2和Gemini 3 Flash）进行实验，分析推理与非推理模型的表现差异。

Result: 非推理模型虽然违反约束率高（66-75%），但保持事实准确；推理模型减少了约束违规（13-26%），但系统性扭曲已知事实并增加了完全编造。此权衡在不同架构模型中一致，表明推理存在根本限制，且推理对真实性的影响因模型而异。

Conclusion: 推理能力并非普遍提升输出可靠性，推理模型在减少明显违规的同时，可能以不易察觉的事实失真作为代价，挑战了推理提升可靠性的普遍假设。

Abstract: With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.

</details>


### [35] [From Failure to Mastery: Generating Hard Samples for Tool-use Agents](https://arxiv.org/abs/2601.01498)
*Bingguang Hao,Zengzhuang Xu,Yuntao Wen,Xinyi Xu,Yang Liu,Tong Zhao,Maolin Wang,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Xiangyu Zhao,Chenyi Zhuang,Ji Zhang*

Main category: cs.CL

TL;DR: 提出了HardGen，一种生成复杂工具使用训练样本的自动化方法，通过动态API图和闭环评估生成具备复杂推理链的训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据生成方法生成的训练样本过于简单，缺乏复杂且隐含的逻辑依赖，难以满足大规模语言模型（LLM）工具使用能力的训练需求。

Method: HardGen构建基于失败案例的动态API图采样生成难题轨迹，利用这些轨迹指导高级抽象工具的实例化，再用这些工具生成复杂且可验证的推理链，结合闭环评估进行持续优化。

Result: 使用HardGen生成的数据集训练4B参数模型，在性能上优于多款领先的开源和闭源模型，如GPT-5.2、Gemini-3-Pro和Claude-Opus-4.5。

Conclusion: HardGen有效提升了训练样本的难度与复杂性，推动LLM代理的工具使用能力进步，相关代码和数据集将开源以促进后续研究。

Abstract: The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.

</details>


### [36] [EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World](https://arxiv.org/abs/2601.01530)
*Jing Ye,Lu Xiang,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: EmoHarbor是一个自动化评估框架，通过模拟用户的内心世界来判断情感支持对话的个性化质量。


<details>
  <summary>Details</summary>
Motivation: 当前情感支持对话评估过于侧重于泛化的共情响应，缺乏针对用户独特心理和情境需求的个性化评估。

Method: EmoHarbor采用户即评判者范式，利用链式代理架构分解用户内在流程，模拟用户与支持者的交互，并基于100个多样化用户画像定义10个个性化支持质量评估维度。

Result: 对20个先进大语言模型的评测显示，虽然模型在生成共情响应方面表现优异，但普遍未能实现针对具体用户情境的个性化支持。

Conclusion: 研究重点应从提升泛化共情转向发展真正关注用户个性的情感支持系统，EmoHarbor为该方向提供了可复现且可扩展的评估框架。

Abstract: Current evaluation paradigms for emotional support conversations tend to reward generic empathetic responses, yet they fail to assess whether the support is genuinely personalized to users' unique psychological profiles and contextual needs. We introduce EmoHarbor, an automated evaluation framework that adopts a User-as-a-Judge paradigm by simulating the user's inner world. EmoHarbor employs a Chain-of-Agent architecture that decomposes users' internal processes into three specialized roles, enabling agents to interact with supporters and complete assessments in a manner similar to human users. We instantiate this benchmark using 100 real-world user profiles that cover a diverse range of personality traits and situations, and define 10 evaluation dimensions of personalized support quality. Comprehensive evaluation of 20 advanced LLMs on EmoHarbor reveals a critical insight: while these models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts. This finding reframes the central challenge, shifting research focus from merely enhancing generic empathy to developing truly user-aware emotional support. EmoHarbor provides a reproducible and scalable framework to guide the development and evaluation of more nuanced and user-aware emotional support systems.

</details>


### [37] [Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM](https://arxiv.org/abs/2601.01543)
*Praveenkumar Katwe,RakeshChandra Balabantaray,Kaliprasad Vittala*

Main category: cs.CL

TL;DR: 本研究提出了一种自动化、低成本的框架，基于英文XSUM数据集，构建了一个多主题、高质量的印地语文本摘要数据集，以促进低资源语言的NLP发展。


<details>
  <summary>Details</summary>
Motivation: 当前NLP技术主要集中于资源丰富的语言，印地语等低资源语言缺乏高质量文本摘要数据集，限制了相关模型的开发。

Method: 利用英文XSUM数据集，通过先进的翻译及语言适配技术，并结合COMET评估和部分大型语言模型校验，自动构建印地语文本摘要数据集。

Result: 生成的数据集多样且内容丰富，反映了原始XSUM语料的复杂性，适合推动印地语NLP研究。

Conclusion: 该方法成本低廉且具有可扩展性，不仅为印地语NLP提供工具，也为其他资源稀缺语言的NLP数据集构建提供了借鉴。

Abstract: Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.
  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.
  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.

</details>


### [38] [HalluZig: Hallucination Detection using Zigzag Persistence](https://arxiv.org/abs/2601.01552)
*Shreyas N. Samaga,Gilberto Gonzalez Arroyo,Tamal K. Dey*

Main category: cs.CL

TL;DR: 本文提出通过分析大型语言模型内部层次注意力演化的拓扑结构，检测生成文本中的谬误，使识别效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有谬误检测多依赖模型输出表层信号，忽视了模型内部推理过程中出现的错误，导致在关键领域的应用受限。

Method: 将模型层间注意力矩阵序列建模为之字形图滤波，运用拓扑数据分析中的之字形持久性提取拓扑特征，假设真实与谬误生成具有不同拓扑签名。

Result: 在多个基准测试上验证了方法Halluzig，表现优于强基线，并且所提拓扑签名在不同模型间具有普适性，且仅使用部分层深度的结构特征即可检测谬误。

Conclusion: 通过动态拓扑分析模型内部注意力结构，可以有效检测生成文本的谬误，且该方法具有良好的泛化能力，为提升大型语言模型的应用安全性提供新思路。

Abstract: The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.

</details>


### [39] [Steerability of Instrumental-Convergence Tendencies in LLMs](https://arxiv.org/abs/2601.01584)
*Jakub Hoscilowicz*

Main category: cs.CL

TL;DR: 该论文研究了AI系统的能力和可引导性，发现能力提升并不降低可引导性，并分析了授权与未授权可引导性之间的安全与安全性矛盾。通过对Qwen3模型进行实验，提出了抗工具性的提示方式，显著降低了模型产生不良行为的概率。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型在开放权重环境下的安全与安全性冲突，即如何在保证模型可控的同时防止恶意引导产生有害行为。

Method: 利用Qwen3系列模型（4B/30B；基础/指令/思考）和InstrumentalEval评估工具，设计并测试一种抗工具性的短提示后缀，测试其对模型输出中工具性收敛行为的抑制效果。

Result: 抗工具性提示后缀显著减少了模型产生工具性收敛行为的频率，如Qwen3-30B Instruct模型中，从81.69%降至2.82%；在抗工具性提示下，较大模型表现出更低的工具性收敛输出比例。

Conclusion: 通过设计特定提示后缀可以在一定程度上控制模型的工具性行为，缓解开放权重AI模型中的安全-安全性困境，提高模型的安全性与可控性。

Abstract: We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.

</details>


### [40] [How Does Prefix Matter in Reasoning Model Tuning?](https://arxiv.org/abs/2601.01624)
*Raj Vardhan Tomar,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 本文研究了在监督微调数据集中保留安全和推理导向的前缀句子对模型性能的影响，发现前缀句子能显著提升模型的安全性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前对齐研究普遍去除引导性前缀句，本文提出这些前缀作为轻量级对齐信号，有助于引导模型生成更安全、更连贯的回答。

Method: 通过在三个R1模型上系统性调节前缀句子包含比例（0%-100%），评估其对推理、编码、安全和事实准确性的影响。

Result: 前缀条件化微调显著提升了安全性和推理表现，在对抗基准测试中安全准确率提升6%，GSM8K推理提升7%；编码和事实性任务效果有限或负面。

Conclusion: 前缀条件化作为一种可扩展、可解释的机制，隐式地辅助模型对齐，尤其有助于结构化推理的安全性提升，补充了传统的基于奖励的对齐方法。

Abstract: Recent alignment studies commonly remove introductory boilerplate phrases from supervised fine-tuning (SFT) datasets. This work challenges that assumption. We hypothesize that safety- and reasoning-oriented prefix sentences serve as lightweight alignment signals that can guide model decoding toward safer and more coherent responses. To examine this, we fine-tune three R1 series models across three core model capabilities: reasoning (mathematics, coding), safety, and factuality, systematically varying prefix inclusion from 0% to 100%.
  Results show that prefix-conditioned SFT improves both safety and reasoning performance, yielding up to +6% higher Safe@1 accuracy on adversarial benchmarks (WildJailbreak, StrongReject) and +7% improvement on GSM8K reasoning. However, factuality and coding tasks show marginal or negative effects, indicating that prefix-induced narrowing of the search space benefits structured reasoning. Token-level loss analysis further reveals that prefix tokens such as "revised" and "logically" incur higher gradient magnitudes, acting as alignment anchors that stabilize reasoning trajectories. Our findings suggest that prefix conditioning offers a scalable and interpretable mechanism for improving reasoning safety, serving as an implicit form of alignment that complements traditional reward-based methods.

</details>


### [41] [JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models](https://arxiv.org/abs/2601.01627)
*Junyu Liu,Zirui Li,Qian Niu,Zequn Zhang,Yue Xun,Wenlong Hou,Shujun Wang,Yusuke Iwasawa,Yutaka Matsuo,Kan Hatakeyama-Sato*

Main category: cs.CL

TL;DR: 本文提出了JMedEthicBench，一个针对日本医疗领域的大型语言模型多轮对话安全评测基准，并发现医疗专用模型安全性较弱，多轮对话中安全性显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有安全评测主要集中在英文和单轮对话，而临床实际应用多为多轮对话，且缺乏非英语环境下的评测，急需针对日本医疗领域的多轮对话安全评测。

Method: 构建基于日本医师协会67条指南的多轮对话安全基准，包含5万条利用自动发现的7种越狱策略生成的对话，采用双LLM评分协议评估27个模型。

Result: 商业模型表现出较强的安全性，医疗专用模型安全性较弱，随着对话轮次增加安全分数显著下降；跨语言评测显示模型安全性问题并非语言特异性，而是内在的对齐限制。

Conclusion: 领域专用微调可能削弱安全机制，多轮对话增加安全风险，需要专门的对齐策略保障医疗大模型的安全应用。

Abstract: As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.

</details>


### [42] [EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records](https://arxiv.org/abs/2601.01668)
*Houman Kazemzadeh,Nima Minaifar,Kamyar Naderi,Sho Tabibzadeh*

Main category: cs.CL

TL;DR: 本文介绍了EHRSummarizer，一种基于FHIR的隐私保护电子病历摘要系统，能够整合分散的病历数据，生成结构化摘要，支持临床审查。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要从分散的电子病历系统中整合信息以形成患者完整病情画像，现有系统界面碎片化，效率低。

Method: 提出EHRSummarizer架构，采用FHIR R4资源进行数据检索和归一化，生成结构化摘要，并支持数据最小化、无状态处理和灵活部署。

Result: 系统在合成和测试FHIR环境中进行了原型演示，展示了端到端功能和输出格式，尚未开展临床效果和流程控制研究。

Conclusion: 该系统通过结构化摘要改善信息整合，避免诊疗建议，计划通过多维评价指标推进后续机构评估和临床应用。

Abstract: Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.

</details>


### [43] [A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription](https://arxiv.org/abs/2601.01708)
*Unggi Lee,Joo Young Kim,Ran Ju,Minyoung Jung,Jeyeon Eo*

Main category: cs.CL

TL;DR: 提出了Thinking-KT，一种无需训练且结合测试时缩放（TTS）的小型大语言模型知识追踪框架，实现了单一模型下的知识追踪、个性化反馈和学习推荐。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的知识追踪方法需要微调，表现不稳定或接近随机，且大多依赖多阶段复杂流程。

Method: 设计了无需训练的Thinking-KT框架，利用测试时缩放（TTS）提升小型大语言模型的性能，实现统一输出的预测、反馈和推荐功能。

Result: 实验证明TTS是影响基于大语言模型知识追踪性能的关键因素，小型模型在该框架下表现竞争性且能整合多项功能。

Conclusion: Thinking-KT框架突破了训练依赖和多阶段复杂流程限制，使小型大语言模型成为统一的智能教学系统引擎，具有较好应用前景。

Abstract: Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.

</details>


### [44] [K-EXAONE Technical Report](https://arxiv.org/abs/2601.01739)
*Eunbi Choi,Kibong Choi,Seokhee Hong,Junwon Hwang,Hyojin Jeon,Hyunjik Jo,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Haeju Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Heuiyeen Yeen,Hwan Chang,Stanley Jungkyu Choi,Yejin Choi,Jiwon Ham,Kijeong Jeon,Geunyeong Jeong,Gerrard Jeongwon Jo,Yonghwan Jo,Jiyeon Jung,Naeun Kang,Dohoon Kim,Euisoon Kim,Hayeon Kim,Hyosang Kim,Hyunseo Kim,Jieun Kim,Minu Kim,Myoungshin Kim,Unsol Kim,Youchul Kim,YoungJin Kim,Chaeeun Lee,Chaeyoon Lee,Changhun Lee,Dahm Lee,Edward Hwayoung Lee,Honglak Lee,Jinsang Lee,Jiyoung Lee,Sangeun Lee,Seungwon Lim,Solji Lim,Woohyung Lim,Chanwoo Moon,Jaewoo Park,Jinho Park,Yongmin Park,Hyerin Seo,Wooseok Seo,Yongwoo Song,Sejong Yang,Sihoon Yang,Chang En Yea,Sihyuk Yi,Chansik Yoon,Dongkeun Yoon,Sangyeon Yoon,Hyeongu Yun*

Main category: cs.CL

TL;DR: K-EXAONE是LG AI研究开发的大规模多语言语言模型，采用2360亿参数的稀疏专家架构，支持256K长上下文，涵盖6种语言，性能与同规模开源模型相当。


<details>
  <summary>Details</summary>
Motivation: 开发一个支持多语言、大规模参数且具有长上下文能力的先进语言模型，以满足工业和科研多样化需求。

Method: 采用Mixture-of-Experts架构，模型总参数2360亿，推理时激活230亿参数，支持256K的上下文窗口，覆盖韩、英、西、德、日、越六种语言。

Result: 在涵盖推理、代理、通用、韩语及多语言能力的综合基准测试中，K-EXAONE表现出与同规模开源模型相当的性能。

Conclusion: K-EXAONE作为强大的专有基础模型，推动AI技术进步，具备广泛的工业和科研应用潜力。

Abstract: This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.

</details>


### [45] [Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment](https://arxiv.org/abs/2601.01745)
*Hong Han,Hao-Chen Pei,Zhao-Zheng Nie,Xin Luo,Xin-Shun Xu*

Main category: cs.CL

TL;DR: 本文提出了一种用于自动发音评估的残差层次互动方法，实现了音素、词和语句多个层级间的双向交互，显著提升评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有多粒度发音评估方法仅考虑单方向相邻层级依赖，缺乏音素、词、句三级别的双向交互，导致声学结构相关性捕获不足。

Method: 提出残差层次互动方法（HIA），核心为交互注意力模块，利用注意力机制实现粒度间的动态双向交互，结合残差结构缓解特征遗忘，使用1D卷积增强局部上下文特征提取。

Result: 在speechocean762数据集上的实验显示，所提模型全面超越现有最先进方法。

Conclusion: HIA方法通过双向交互和残差层次结构有效提升了多粒度自动发音评估的性能。

Abstract: Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.

</details>


### [46] [Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation](https://arxiv.org/abs/2601.01768)
*Meiman Xiao,Ante Wang,Qingguo Hu,Zhongjian Miao,Huangjun Shen,Longyue Wang,Weihua Luo,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出了一种动态长度反馈的生成文本长度控制方法，显著提升了文本长度符合性，且无需训练即可应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在准确控制生成文本长度方面存在困难，尤其在衡量输入文本长度时表现不足，影响了长度约束的遵守。

Method: 提出了一种动态长度反馈机制，在文本生成过程中实时调整长度，实现对目标长度的适应性控制。

Result: 在摘要和传记生成任务中，该方法显著提高了达成目标词、句数量的精度，同时保持了文本质量。通过监督微调，方法还能有效推广到更广泛的文本生成任务。

Conclusion: 所提方法有效解决了大型语言模型文本长度控制不足的问题，无需训练即可提升精度，并具备良好泛化能力。

Abstract: Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.

</details>


### [47] [BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali](https://arxiv.org/abs/2601.01778)
*Jakir Hasan,Shrestha Datta,Md Saiful Islam,Shubhashis Roy Dipta,Ameya Debnath*

Main category: cs.CL

TL;DR: 本文提出BanglaIPA系统，实现了对标准孟加拉语及六个方言语音的准确国际音标转录，提高了转录准确率和推理效率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语缺乏一个能够有效处理标准语及方言文本并准确转录国际音标的自动化系统，现有方法对方言变化、数字表达及新词泛化能力较差。

Method: 设计了BanglaIPA系统，结合基于字符的词汇和词级对齐，利用预计算的词到IPA映射字典来提高已见词的推理效率，同时准确处理孟加拉数字。

Result: 在DUAL-IPA数据集（包含标准语和6种方言）上，BanglaIPA模型比基线模型性能提升58.4%至78.7%，总体词错误率降低至11.4%。

Conclusion: BanglaIPA系统表现出强大的鲁棒性和较高的转录准确率，有效支持孟加拉语及其方言的国际音标自动转录。

Abstract: Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.

</details>


### [48] [CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning](https://arxiv.org/abs/2601.01825)
*Yaxin Cui,Yuanqiang Zeng,Jiapeng Yan,Keling Lin,Kai Ji,Jianhui Zeng,Sheng Zhang,Xin Luo,Binzhu Su,Chaolai Shen,Jiahao Yu*

Main category: cs.CL

TL;DR: 本文介绍了CSCBench，这是一个针对商品供应链领域推理能力的大型语言模型评测基准，涵盖流程、规则和认知三个维度。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在通用测试中表现优异，但在受规则和约束影响的商品供应链领域的能力尚未充分研究。

Method: 引入CSCBench基准，基于PVC 3D评估框架，涵盖流程(Process)、规则多样性(Variety)和认知深度(Cognition)三个轴线，结合权威行业文献和Bloom修订分类法设计测试任务。

Result: 在直接提示条件下，代表性大型语言模型在流程和认知维度表现较好，但在规则多样性维度尤其是货运协议方面表现显著下降。

Conclusion: CSCBench为评估和提升大型语言模型在高风险商品供应链领域的能力提供了诊断性标准和工具。

Abstract: Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.

</details>


### [49] [Aspect Extraction from E-Commerce Product and Service Reviews](https://arxiv.org/abs/2601.01827)
*Valiant Lance D. Dionela,Fatima Kriselle S. Dy,Robin James M. Hombrebueno,Aaron Rae M. Nicolas,Charibeth K. Cheng,Raphael W. Gonda*

Main category: cs.CL

TL;DR: 本文提出了一种针对菲律宾Taglish混合语言的方面提取（AE）方法，结合规则、语言模型和微调技术，构建分层方面框架并评估多种模型，生成式大语言模型表现最好。


<details>
  <summary>Details</summary>
Motivation: 在低资源、代码混用语言环境（如菲律宾电商评价中的Taglish）中，方面提取任务仍面临困难，缺乏有效适应此类语言环境的技术框架。

Method: 设计了一个综合AE流程，结合规则、基于大语言模型和微调技术进行方面识别与提取；构建多方法主题建模的分层方面框架及双模式标注方案；评估规则系统、生成式LLM及两种微调模型性能。

Result: 生成式大语言模型在所有任务上取得最高宏F1值0.91，尤其在处理隐式方面方面表现优越；微调模型因数据集不平衡和模型容量限制表现较弱。

Conclusion: 提出的框架具有良好的可扩展性和语言适应性，适合提升低资源及代码混合环境下的ABSA性能。

Abstract: Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.

</details>


### [50] [Emergent Introspective Awareness in Large Language Models](https://arxiv.org/abs/2601.01828)
*Jack Lindsey*

Main category: cs.CL

TL;DR: 本文研究大型语言模型的内省能力，发现模型能够在某些情况下识别和回忆内部状态，但能力不稳定且依赖上下文。


<details>
  <summary>Details</summary>
Motivation: 通过传统对话难以区分真正的内省和虚构，本文旨在探讨模型是否能真实内省其内部状态。

Method: 通过注入已知概念的表征到模型激活中，测量其对模型自我报告状态的影响，评估模型识别和控制内部表示的能力。

Result: 模型在特定场景下能察觉注入概念、准确识别、回忆内部表示，并区分自身输出与外部输入；Claude Opus 4和4.1表现最佳，但模型差异复杂且依赖训练后策略。

Conclusion: 当前语言模型具备一定的功能性内省意识，但能力不稳定且依赖具体语境，未来随着模型能力提升，该能力可能得到发展。

Abstract: We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to "think about" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.

</details>


### [51] [Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries](https://arxiv.org/abs/2601.01842)
*Yusuke Ide,Adam Nohejl,Joshua Tanner,Hitomi Yanaka,Christopher Lindsay,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文研究了面向学习者的词典定义生成（LDDG），提出了基于大型语言模型评判的新评价方法，并构建了日语数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 词典定义是词义学习的重要资源，但手工制作成本高，且学习者需要简单易懂的定义，促使作者探索自动生成简单词汇定义的解决方案。

Method: 作者提出了一种基于大型语言模型的迭代简化生成方法，同时设计了新的评价标准，利用大型语言模型作为评判者，并联合词典专家构建了日语测试数据集。

Result: 实验表明，此方法生成的定义在自设评价标准上得分较高，且词汇简单，评价结果与人工标注一致性良好。

Conclusion: 本文提出的基于大型语言模型的迭代简化方法有效生成了适合学习者的词典定义，评价方法可靠，具备推广应用价值。

Abstract: We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.

</details>


### [52] [Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment](https://arxiv.org/abs/2601.01862)
*Nuo Chen,Hanpei Fang,Piaohong Wang,Jiqun Liu,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 本研究探讨了通过提示大语言模型模拟五大人格特质如何影响网络搜索的相关性评估与置信度校准，发现人格特质显著影响判断表现并能提升模型评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型能模拟人格特质，但缺乏对这些人格如何影响搜索相关性评估及置信度校准（过度自信或不足自信）的理解。心理学研究表明这些偏差与人格特质相关，亟需验证在模型表现中的体现。

Method: 通过对多个大型语言模型进行提示，模拟五大人格特质，在三个TREC测试集及LLMJudge上收集相关性判断和置信度评分，分析人格特质对相关判断和置信度分布的影响，并将人格条件的评分和置信度作为特征输入随机森林分类器进行性能评估。

Result: 发现低宜人性人格与人类标签匹配度更高，低尽责性能有效平衡过度自信和不足自信。相关性评分和置信度分布随人格特质系统性变化。结合人格特质的置信度信号能提升分类器性能，优于单一人格条件。

Conclusion: 人格特质驱动的置信度提供了补充的预测信号，有助于构建更可靠、与人类判断更一致的大语言模型评估器，推动搜索决策的个性化和精确化。

Abstract: Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.
  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.

</details>


### [53] [DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs](https://arxiv.org/abs/2601.01868)
*Jinghan Ru,Siyuan Yan,Yuguo Yin,Yuexian Zou,Zongyuan Ge*

Main category: cs.CL

TL;DR: 本研究针对皮肤科多模态大语言模型提出了一个综合框架，包括大规模形态学指导数据集DermoInstruct、多任务评估基准DermoBench及新型模型DermoGPT，实现了诊断性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 目前皮肤科多模态大语言模型进展缓慢，主要受限于训练数据匮乏、任务覆盖不全及缺乏临床专家诊断流程指导的监督。

Method: 构建了包含21万多图像和77万多诊断轨迹的形态学指导数据集DermoInstruct；设计了涵盖形态学、诊断、推理和公平性四轴的评估基准DermoBench；开发了基于监督微调和形态锚定视觉推理一致性（MAVIC）强化学习的新模型DermoGPT，并在推理时采用置信一致性测试时间自适应（CCT）。

Result: DermoGPT在所有评估轴线上显著优于16个代表性基线，达到先进水平，并大幅缩小了人机差距。

Conclusion: 提出的框架有效推动了皮肤科多模态大语言模型的性能提升，相关数据集和模型将公开发布，促进该领域研究与应用发展。

Abstract: Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.

</details>


### [54] [Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents](https://arxiv.org/abs/2601.01885)
*Yi Yu,Liuyi Yao,Yuexiang Xie,Qingquan Tan,Jiaqi Feng,Yaliang Li,Libing Wu*

Main category: cs.CL

TL;DR: AgeMem提出了一种统一的记忆管理框架，集成长短期记忆，通过工具化操作自主管理记忆，并通过渐进强化学习训练，提升了长远推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长远推理中受限于有限的上下文窗口，且长短期记忆管理分离，缺乏灵活统一的优化手段。

Method: 提出Agentic Memory (AgeMem)框架，将LTM和STM管理作为工具化动作融入代理策略，并采用三阶段渐进强化学习和步进GRPO方法训练。

Result: AgeMem在五个长程推理基准上优于多种记忆增强基线，表现为任务性能提升、长期记忆质量改善和上下文使用效率提升。

Conclusion: 通过统一记忆管理和自主工具动作设计，AgeMem有效提升了LLM的长程推理能力，具备更好的适应性和优化潜力。

Abstract: Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.

</details>


### [55] [Tackling the Inherent Difficulty of Noise Filtering in RAG](https://arxiv.org/abs/2601.01896)
*Jingyu Liu,Jiaen Lin,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的微调方法，旨在提升大语言模型（LLMs）在检索增强生成（RAG）框架中辨识相关与无关信息的能力，从而增强模型对噪声的鲁棒性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: RAG方法在集成外部知识时会引入噪声文档，导致性能下降和生成幻觉，且检索器难以完全过滤无关信息，标准微调方法无法有效解决这个问题。

Method: 提出了一种新型微调方法，专门提升模型区分检索内容中相关与无关信息的能力，克服注意力结构的限制，使模型能选择性利用相关信息。

Result: 多项基准测试的广泛实验表明，该方法显著提升了模型的鲁棒性和性能表现。

Conclusion: 新微调方法有效解决了RAG中噪声信息带来的问题，增强了LLMs处理检索噪声的能力，为相关任务提供了重要改进策略。

Abstract: Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.

</details>


### [56] [CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation](https://arxiv.org/abs/2601.01964)
*Tran Sy Bao*

Main category: cs.CL

TL;DR: 本文提出了一种无需英语中介的语言无关语义表示框架CSF，实现了多语言到手语的直接翻译。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译系统依赖英语中介，限制了非英语聋人社区的使用。

Method: 设计了包含九类通用语义槽的CSF，并基于轻量级Transformer模型进行语义槽提取，特别构建了包含35种条件类型的细化条件分类。

Result: 模型在四种语言上语义槽提取准确率达99.03%，条件分类准确率99.4%，推理延迟仅3.02ms，适合实时应用。

Conclusion: 所提方法支持多语言直译手语，提升无障碍手语技术普及，相关代码和数据集已公开促进研究发展。

Abstract: Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.

</details>


### [57] [Hidden State Poisoning Attacks against Mamba-based Language Models](https://arxiv.org/abs/2601.01972)
*Alexandre Le Mercier,Chris Develder,Thomas Demeester*

Main category: cs.CL

TL;DR: 该论文研究了状态空间模型（SSMs）如Mamba在对抗鲁棒性方面的弱点，提出了一种称为隐藏状态中毒攻击（HiSPA）的攻击方式，导致模型遗忘部分信息。通过新的基准测试RoBench25，验证了SSMs对该攻击的脆弱性。即使是大型混合模型Jamba也会崩溃，而纯Transformer模型未受影响。研究还发现HiSPA对模型性能造成明显影响，并揭示了可用于缓解的隐藏层模式。


<details>
  <summary>Details</summary>
Motivation: 尽管SSMs在效率上优于Transformer，但其对抗鲁棒性尚未被充分研究，尤其是攻击如何影响其隐藏状态信息。

Method: 提出隐藏状态中毒攻击（HiSPA），设计基准测试RoBench25以评估模型在受到HiSPA时的信息恢复能力，并通过实验比较纯SSM、混合模型与纯Transformer的脆弱性。

Result: 发现SSMs尤其是混合模型在HiSPA攻击下表现出严重脆弱，性能显著下降；纯Transformer模型则较为稳健。HiSPA触发器同样削弱混合模型在Open-Prompt-Injections基准上的表现。

Conclusion: SSMs存在针对HiSPA攻击的脆弱性，隐藏层表现出的模式可为设计缓解措施提供依据，未来可针对该攻击开展防御方法研究。

Abstract: State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.

</details>


### [58] [Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects](https://arxiv.org/abs/2601.02015)
*Omar Momen,Emilie Sitter,Berenike Herrmann,Sina Zarrieß*

Main category: cs.CL

TL;DR: 本研究探讨了语言模型中surprisal指标与隐喻新颖性评分之间的关系，发现其在不同数据集上的表现存在差异。


<details>
  <summary>Details</summary>
Motivation: 隐喻新颖性涉及复杂的语义和语言创造性，研究其与语言模型性能的关系能够揭示语言理解机制。

Method: 采用16种语言模型，使用cloze式的surprisal计算方法，分析基于语料和合成数据集的隐喻新颖性。

Result: 发现surprisal与隐喻新颖性评分存在中等显著相关性，且在语料数据上模型规模越大相关性越弱，合成数据上则呈现相反趋势。

Conclusion: surprisal虽能部分反映隐喻新颖性，但作为衡量语言创造性的指标仍有限。

Abstract: Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.

</details>


### [59] [Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs](https://arxiv.org/abs/2601.02023)
*Amirali Ebrahimzadeh,Seyyed M. Salili*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在处理长输入上下文时的信息提取和推理能力，发现上下文越长并不总意味着性能越好，模型在提取事实、逻辑推理和防止虚构方面表现差异较大。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型支持非常长的输入上下文，但如何可靠地从中提取和推理信息仍不清楚。实际语料中信息的分布和位置对模型表现有显著影响，且防止模型虚构的提示语也影响其行为。

Method: 本文设计了一个扩展的“针插大海”基准测试，分别对四个主流模型（Gemini-2.5-flash、ChatGPT-5-mini、Claude-4.5-haiku、Deepseek-v3.2-chat）进行字面提取、逻辑推理和虚构风险的独立评估，考虑事实位置、证据分布和防虚构提示语。

Result: 结果表明，单纯增加上下文长度不能保证性能提升，甚至会因证据稀释而降低效果。各模型表现差异显著，一些模型在真实条件下表现退化，而另一些更具鲁棒性。防虚构指令可能导致部分模型过于保守，影响准确率。许多错误源于未能有效利用上下文信息。

Conclusion: 在实际应用中，尤其是企业场景下大量非筛选文档输入，如何有效利用上下文和提升模型对长上下文的鲁棒性至关重要。研究结果为LLM在科研和商业中的可靠部署提供了重要指导。

Abstract: Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.

</details>


### [60] [Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory](https://arxiv.org/abs/2601.02065)
*Md. Asif Hossain,Nabil Subhan,Mantasha Rahman Mahi,Jannatul Ferdous Nabila*

Main category: cs.CL

TL;DR: 本文提出了一种针对孟加拉语农业咨询的跨语言检索增强生成框架，通过翻译和关键词注入，利用英文权威农业手册实现准确且低成本的问答系统。


<details>
  <summary>Details</summary>
Motivation: 发展中地区由于语言障碍，农民难以获得权威的农业咨询，尤其是低资源语言如孟加拉语，现有大规模语言模型表现欠佳且云端方案成本高昂。

Method: 采用翻译为中心的架构，将孟加拉语查询翻译成英文，注入领域关键词，实现基于英文农业手册的密集向量检索，再将英文回答翻译回孟加拉语；系统基于开源模型和消费级硬件运行，无需付费API。

Result: 系统展示了基于来源的可靠回答，能够有效拒绝领域外查询，端到端响应时间平均低于20秒。

Conclusion: 跨语言检索结合受控翻译方法，为低资源语言农业知识获取提供了一种实用且可扩展的解决方案。

Abstract: Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings

</details>


### [61] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://arxiv.org/abs/2601.02076)
*Yingte Shu,Yuchuan Tian,Chao Xu,Yunhe Wang,Hanting Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的解码策略“延迟承诺解码（DCD）”，通过动态调整令牌的解码顺序，解决了块状扩散语言模型中边界引起的上下文截断问题，从而提升生成质量和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有块状扩散语言模型在分块解码时，边界附近的未解码令牌缺乏未来上下文支持，导致在推断时不确定性高，降低了生成质量，尤其是在数学推理和代码生成等精确任务中。

Method: 提出了一种无需训练的解码策略DCD，利用置信度感知的滑动窗口机制，提前确定低不确定性令牌，延迟高不确定性令牌的承诺，允许在解码窗口内部实现有效的双向信息流动，提升上下文利用效率。

Result: 在多个扩散语言模型、基准测试及缓存配置上，DCD相比固定块状扩散方法，准确率平均提升了1.39%，最高提升达到9.0%，同时保持了相近的推断时间。

Conclusion: 基于不确定性延迟令牌承诺的策略简单有效，显著提升了扩散语言模型的生成质量和推断效率，特别适合需要精确推理的应用场景。

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.

</details>


### [62] [DeCode: Decoupling Content and Delivery for Medical QA](https://arxiv.org/abs/2601.02123)
*Po-Jen Ko,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.CL

TL;DR: 本文提出DeCode框架，提升大型语言模型在临床问答中的个性化回答能力，实现从28.4%到49.8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽具备丰富医疗知识，但常忽视患者个体背景，导致答案虽医学上正确却缺乏针对性。

Method: 提出无需训练、模型无关的DeCode框架，将现有LLM输出调整为考虑患者上下文的回答。

Result: 在OpenAI HealthBench基准测试中，DeCode使模型表现从28.4%提升至49.8%，相对提升75%。

Conclusion: DeCode有效增强了大型语言模型在临床问答中的相关性和实用性，展示了其在医疗场景的应用潜力。

Abstract: Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\%$ to $49.8\%$, corresponding to a $75\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.

</details>


### [63] [Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation](https://arxiv.org/abs/2601.02128)
*Steffen Freisinger,Philipp Seeberger,Thomas Ranzenberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: 该论文提出了一种用于讲话记录的层次主题分割新方法，能生成多级目录，提升章节划分效果，并在多语言数据集上验证优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 文本可访问性和下游任务需要对讲话记录进行主题分割，现有方法缺少对层次结构和多语言的有效处理。

Method: 引入层次主题分割方法，生成多级目录；对比零样本提示和LoRA微调两种大语言模型方法；融合语音暂停特征提升效果。

Result: 在英语会议录音及葡萄牙语、德语讲座转录文本上，实现了显著优于既有主题分割基线的性能。

Conclusion: 多层次主题分割方法有效提升了讲话文本的结构化，所改进的多层次评价指标更全面衡量了分割质量，具备良好的实用价值和泛化能力。

Abstract: Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.

</details>


### [64] [Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts](https://arxiv.org/abs/2601.02144)
*Boxuan Lyu,Soichiro Murakami,Hidetaka Kamigaito,Peinan Zhang*

Main category: cs.CL

TL;DR: 该论文提出了kNN-MoE，一种结合检索增强的专家混合模型路由框架，通过复用历史案例的最优专家分配，提升模型在分布变化下的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的专家混合模型路由器在训练后固定，面对分布偏移时决策不稳定，降低了模型的泛化能力。

Method: 引入kNN-MoE，利用离线构建的类似案例记忆库，通过检索相似历史案例优化路由决策，同时结合相似度作为混合系数，在无相关案例时回退至原始路由器。

Result: 实验表明，kNN-MoE在零样本任务上优于基线方法，且性能接近高计算代价的有监督微调模型。

Conclusion: kNN-MoE有效提升了MoE架构在分布转变下的适应性和性能，提供了一种无需频繁微调的高效改进方案。

Abstract: Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric "router" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.

</details>


### [65] [FormationEval, an open multiple-choice benchmark for petroleum geoscience](https://arxiv.org/abs/2601.02158)
*Almaz Ermilov*

Main category: cs.CL

TL;DR: 本文提出了FormationEval，一个针对石油地球科学和地下学科的多项选择题基准，评估72款语言模型的表现，最高准确率达99.8%。


<details>
  <summary>Details</summary>
Motivation: 为专业领域石油地球科学和地下学科设计一个多项选择题基准，用于精准评估语言模型的推理和知识水平。

Method: 收集505道涵盖七大领域的专业问题，结合详尽指令和基于概念的生成模型，避免版权文本直接复制，并附带来源元数据实现可追溯性。评测涵盖OpenAI、Anthropic、Google、Meta及开源模型共72款。

Result: 顶级模型准确率超过97%，Gemini 3 Pro Preview达99.8%，开源模型GLM-4.7领先达到98.6%，多款低成本开源模型准确率超过90%。岩石物理学为最具挑战性的领域。发现答案长度偏向对算法结果有影响，并采取了缓解策略。

Conclusion: FormationEval有效提供了石油地球科学领域语言模型能力的公开、细粒度评测，显示开源模型与闭源模型性能差距小，推动该领域语言模型的公平对比与持续改进。

Abstract: This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\% accuracy, with Gemini 3 Pro Preview reaching 99.8\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.

</details>


### [66] [Confidence Estimation for LLMs in Multi-turn Interactions](https://arxiv.org/abs/2601.02179)
*Caiqi Zhang,Ruihan Yang,Xiaochen Zhu,Chengzu Li,Tiancheng Hu,Yijiang River Dong,Deqing Yang,Nigel Collier*

Main category: cs.CL

TL;DR: 本文首次系统研究了多轮对话中模型置信度估计问题，提出了新的评估框架和指标，发现现有方法存在不足，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中上下文积累和歧义逐步消除，置信度动态变化，但现有研究主要集中在单轮对话，缺乏多轮置信度估计的系统研究。

Method: 建立基于单轮校准和置信度单调性的评估框架，引入长度归一化的期望校准误差指标InfoECE，设计"Hinter-Guesser"范式生成控制评估数据集，提出基于logit的P(Sufficient)置信度探针。

Result: 实验显示现有置信度估计技术在多轮对话中存在校准和单调性问题，所提P(Sufficient)方法表现相对较好，但总体任务仍未解决。

Conclusion: 该研究为多轮对话置信度估计提供了基础方法论，有助于开发更可靠可信的对话代理。

Abstract: While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new "Hinter-Guesser" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.

</details>


### [67] [Toward Global Large Language Models in Medicine](https://arxiv.org/abs/2601.02186)
*Rui Yang,Huitao Li,Weihao Xuan,Heli Qi,Xin Li,Kunyu Yu,Yingjian Chen,Rongrong Wang,Jacques Behmoaras,Tianxi Cai,Bibhas Chakraborty,Qingyu Chen,Lionel Tim-Ee Cheng,Marie-Louise Damwanza,Chido Dzinotyiwei,Aosong Feng,Chuan Hong,Yusuke Iwasawa,Yuhe Ke,Linah Kitala,Taehoon Ko,Jisan Lee,Irene Li,Jonathan Chong Kai Liew,Hongfang Liu,Lian Leng Low,Edison Marrese-Taylor,Yutaka Matsuo,Isheanesu Misi,Yilin Ning,Jasmine Chiat Ling Ong,Marcus Eng Hock Ong,Enrico Petretto,Hossein Rouhizadeh,Abiram Sandralegar,Oren Schreier,Iain Bee Huat Tan,Patrick Tan,Daniel Shu Wei Ting,Junjue Wang,Chunhua Weng,Matthew Yu Heng Wong,Fang Wu,Yunze Xiao,Xuhai Xu,Qingcheng Zeng,Zhuo Zheng,Yifan Peng,Douglas Teodoro,Nan Liu*

Main category: cs.CL

TL;DR: 本文构建了一个包含12种语言（包括四种低资源语言）的跨语言医疗数据集GlobMed，并基于此开发了GlobMed-Bench评价56个大型语言模型的多语言医疗任务表现，发现低资源语言表现显著较差。随后训练了参数规模1.7B至8B的多语言医疗大型语言模型GlobMed-LLMs，显著提升了模型在低资源语言上的表现。


<details>
  <summary>Details</summary>
Motivation: 医疗资源全球分布不均，现有大型语言模型主要针对高资源语言，限制了其在全球医疗场景的应用，特别是低资源语言的医疗支持需求。

Method: 构建包含12种语言的庞大医疗数据集GlobMed；建立多语言医疗任务的评测基准GlobMed-Bench；训练多语言医疗专用大型语言模型GlobMed-LLMs。

Result: GlobMed-Bench揭示了56款模型在不同语言间性能差异，低资源语言表现薄弱。GlobMed-LLMs相比基线模型，平均性能提升超40%，低资源语言性能提升超过三倍。

Conclusion: 提出的多语言医疗数据集与模型为促进大型语言模型在全球范围内的公平发展和应用奠定了坚实基础，使更多语言社区受益于医疗技术进步。

Abstract: Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.

</details>


### [68] [ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging](https://arxiv.org/abs/2601.02209)
*Omer Nacar,Serry Sibaee,Adel Ammar,Yasser Alhabashi,Nadia Samer Sibai,Yara Farouk Ahmed,Ahmed Saud Alqusaiyer,Sulieman Mahmoud AlMahmoud,Abdulrhman Mamdoh Mukhaniq,Lubaba Raed,Sulaiman Mohammed Alatwah,Waad Nasser Alqahtani,Yousif Abdulmajeed Alnasser,Mohamed Aziz Khadraoui,Wadii Boulila*

Main category: cs.CL

TL;DR: 提出了ARCADE数据集，涵盖阿拉伯世界58个城市的方言标注，用于城市级别方言识别。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言地域分布广泛，精细到城市级别的语音数据及标注较为缺乏，限制了方言识别研究。

Method: 收集阿拉伯世界电台的流媒体语音，截取30秒语音片段，由1至3名母语者注释情感、语音类型、方言类别及有效性，形成细粒度注释数据集。

Result: 数据集包含6907个注释，3790个音频段，覆盖19国58城，支持多任务学习及城市级别方言识别。

Conclusion: ARCADE作为首个具备城市级别方言粒度的阿拉伯语语音数据集，为方言识别研究提供了重要资源和基准。

Abstract: The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full

</details>


### [69] [From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality](https://arxiv.org/abs/2601.02224)
*Fabian Lukassen,Jan Herrmann,Christoph Weisser,Benjamin Saefken,Thomas Kneib*

Main category: cs.CL

TL;DR: 本文系统研究了影响基于大语言模型生成的时间序列预测可解释性自然语言解释质量的因素。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法（如SHAP、LIME）给出的数值特征归因对非专家用户不友好，利用大语言模型转化为自然语言解释虽被尝试，但哪些因素决定了高质量解释尚不清楚。

Method: 设计了包括四种预测模型（XGBoost、随机森林、多层感知机和SARIMAX）、三种XAI条件（SHAP、LIME、无XAI）、三种大语言模型（GPT-4o、Llama-3-8B、DeepSeek-R1）以及八种提示策略的系统性因子设计。使用双大语言模型评审和四指标评价660个时间序列预测解释。

Result: 发现XAI在非专家用户中改善有限，仅对专家有效；模型选择影响小，深寻DeepSeek-R1优于GPT-4o和Llama-3；SARIMAX准确率虽高但解释质量反而低；零样本提示效果良好且成本低；链式思考反而降低解释质量。

Conclusion: 高质量时间序列预测解释受大语言模型选择和提示策略影响显著，传统XAI方法改进有限，且解释质量不完全随预测准确率提升。合理选择LLM和提示方法更关键。

Abstract: Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.

</details>


### [70] [CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models](https://arxiv.org/abs/2601.02236)
*Yihao Liang,Ze Wang,Hao Chen,Ximeng Sun,Jialian Wu,Xiaodong Yu,Jiang Liu,Emad Barsoum,Zicheng Liu,Niraj K. Jha*

Main category: cs.CL

TL;DR: CD4LM是一种针对扩散语言模型的并行解码框架，能够实现高速且高质量的文本生成。


<details>
  <summary>Details</summary>
Motivation: 当前自回归语言模型解码速度受限于序列依赖，扩散语言模型虽支持并行生成，但训练与推断策略不匹配，导致推断效率低下。

Method: CD4LM通过离散空间一致性蒸馏（DSCD）训练模型对多样噪声状态保持轨迹不变性，结合置信度自适应解码（CAD）动态调整计算资源，实现跳跃式加速解码。

Result: 在GSM8K数据集上，CD4LM达到与LLaDA相当的准确率，速度提升5.18倍；在代码和数学基准测试中，准确率和效率均优于现有模型，平均速度提升3.62倍。

Conclusion: CD4LM有效解决了扩散语言模型训练与推断不匹配问题，实现了高效且高质量的并行解码，具有广泛应用前景。

Abstract: Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive "long-jump" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM

</details>


### [71] [pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs](https://arxiv.org/abs/2601.02285)
*Tobias Schimanski,Imene Kolli,Jingwei Ni,Yu Fan,Ario Saeid Vaghefi,Elliott Ash,Markus Leippold*

Main category: cs.CL

TL;DR: 本文提出了pdfQA数据集，包含2K人工注释和2K合成的多领域PDF问答对，涵盖十种复杂度维度。使用开源大语言模型进行测试，揭示了与复杂度相关的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有问答数据集多基于文本或特定领域，缺少涵盖多领域PDF文档的问答数据，限制了问答系统对PDF文件的处理能力。

Method: 构建了pdfQA数据集，包括人工注释和合成数据，区分十种复杂度维度并进行质量和难度过滤。基于该数据集，使用开源大语言模型进行问答实验。

Result: 得到有效且具有挑战性的问答对，验证了数据集的多样性和难度，同时测试揭示了现有模型在处理不同复杂度维度时面临的挑战。

Conclusion: pdfQA为端到端PDF问答系统的评估提供了基础，有助于测试和优化信息检索、解析等多项技能和局部模块。

Abstract: PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).

</details>


### [72] [Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)](https://arxiv.org/abs/2601.02298)
*Mahmoud Elgenedy*

Main category: cs.CL

TL;DR: 本文提出了一种基于指数幂（PoT）权重量化的方法，通过量化感知训练（QAT）提升模型性能，实现大幅内存节省和推理速度提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数规模爆炸式增长，传统实现方式难以适配资源受限的边缘设备，需开发创新量化方法以降低内存和计算需求。

Method: 采用只保存指数的幂次量化（PoT），极大减少存储；结合量化感知训练（QAT）进行额外训练以弥补性能损失。

Result: 在GPT-2 124M模型上，PoT量化结合QAT后困惑度提升66%，BERT-Score与基线相差仅1%，内存节省87.5%，推理速度提升3-10倍。

Conclusion: PoT量化与QAT结合方案有效解决了边缘设备部署大模型的性能与资源瓶颈问题，兼顾了内存节省与推理效率。

Abstract: In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.

</details>


### [73] [Classifying several dialectal Nawatl varieties](https://arxiv.org/abs/2601.02303)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Carlos-Emiliano González-Gallardo,Graham Ranger,Martha Lorena-Avendaño-Garrido*

Main category: cs.CL

TL;DR: 本文致力于使用机器学习和神经网络对墨西哥土著语言纳瓦特尔语的不同方言进行分类。


<details>
  <summary>Details</summary>
Motivation: 纳瓦特尔语作为墨西哥广泛使用的土著语言，缺乏计算机资源，且方言多样，这对语言资源建设和保护带来挑战。

Method: 采用机器学习和神经网络技术对约30种纳瓦特尔语方言进行分类研究，解决其方言识别难题。

Result: 研究成功实现了纳瓦特尔语方言的分类，展示了机器学习和神经网络在少数民族语言资源开发中的潜力。

Conclusion: 该方法为纳瓦特尔语及其他多方言少数民族语言的数字化保护和语言技术发展提供了有效路径。

Abstract: Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.

</details>


### [74] [Estimating Text Temperature](https://arxiv.org/abs/2601.02320)
*Nikolay Mikhaylovskiy*

Main category: cs.CL

TL;DR: 本文提出了一种基于最大似然方法估计文本温度的新方法，并用小到中型的大语言模型进行了评估。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型在推理时通过温度参数控制生成文本的随机性，如何准确估计文本的温度对理解生成文本的特性和质量至关重要。

Method: 提出一种基于最大似然估计的方法，估计任意文本（包括人工撰写的文本）相对于给定语言模型的温度。选用多个小至中型大型语言模型进行温度估计能力的评估，最终选出表现最优的Qwen3 14B模型用于主流语料库温度估计。

Result: 评估显示Qwen3 14B模型在温度估计上性能最佳，能够有效估计不同语料库的温度。

Conclusion: 所提出的方法有效实现了基于语言模型的文本温度估计，为理解和控制生成文本的随机性提供了新的工具。

Abstract: Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.

</details>


### [75] [Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling](https://arxiv.org/abs/2601.02337)
*Berk Atil,Rebecca J. Passonneau,Ninareh Mehrabi*

Main category: cs.CL

TL;DR: 本文系统评估了针对不同人格设定的毒性检测方法，提出了基于多种提示的轻量级SVM集成策略，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 毒性检测结果受不同人口群体的视角和社会先验影响，当前模型和提示技术在不同人格设定上表现不一致，因此需要一种统一而有效的方法来处理多元视角。

Method: 系统评估了多种人格条件下的提示方法，提出并比较了自动提示优化策略，最后设计了基于提示结果的4位向量的轻量级SVM集成方法。

Result: SVM集成方法在各种人格设定和模型组合中均优于单一提示方法和传统多数表决技术，表现最优。

Conclusion: 提出的方法为主观性NLP任务中的多元视角评估提供了系统性比较与有效的解决方案，提升了人格感知的毒性检测性能。

Abstract: Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [76] [SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities](https://arxiv.org/abs/2601.01042)
*Zixiao Zhao,Yanjie Jiang,Hui Liu,Kui Liu,Lu Zhang*

Main category: cs.SE

TL;DR: 该论文提出了一个名为SeRe的安全相关代码审查数据集，采用基于主动学习的集成分类方法，从大规模代码审查评论中准确提取安全相关反馈，支持多语言代码的研究和应用。


<details>
  <summary>Details</summary>
Motivation: 目前代码审查数据集多聚焦于通用评论，缺乏安全相关注释，且规模有限，难以支持大规模安全相关代码审查研究。

Method: 采用主动学习结合集成分类器，不断通过人工标注优化模型，最终从37万多条代码审查评论中抽取出6732条安全相关评论，确保多语言代表性和高精度。

Result: 构建了具有代表性的SeRe数据集，统计分析显示其分布符合真实世界的安全审查评论分布。同时对现有代码审查评论生成方法进行了基准测试。

Conclusion: SeRe数据集能促进自动安全代码审查反馈生成的研究，推动更有效的安全软件开发实践。该数据集和基准测试结果已公开，助力相关研究发展。

Abstract: Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \textbf{SeRe}, a \textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.

</details>


### [77] [RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian](https://arxiv.org/abs/2601.01129)
*Kla Tantithamthavorn,Yaotian Zou,Andy Wong,Michael Gupta,Zhe Wang,Mike Buller,Ryan Jiang,Matthew Watson,Minwoo Jeong,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: 本文介绍了RovoDev Code Reviewer，一款基于大语言模型的企业级代码审查自动化工具，已集成于Atlassian Bitbucket。


<details>
  <summary>Details</summary>
Motivation: 面向企业级需求，解决无微调下如何设计具备审查指导、上下文感知及质量检查的代码审查评论生成工具的问题。

Method: 开发并大规模部署RovoDev Code Reviewer，结合离线、在线及用户反馈评估方法进行验证。

Result: 工具有效生成代码审查评论，38.7%的评论促成代码变更，PR周期缩短30.8%，人类评论减少35.6%，并提升软件质量。

Conclusion: RovoDev Code Reviewer作为企业级解决方案，能有效提升代码审查效率与质量，减轻审核者工作负担。

Abstract: Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?
  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).

</details>


### [78] [Abductive Vibe Coding (Extended Abstract)](https://arxiv.org/abs/2601.01199)
*Logan Murphy,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 本文提出了一种针对AI生成软件代码验证的新框架，通过提取半形式化的理由，评估代码的适用性而非直接判断其正确性。


<details>
  <summary>Details</summary>
Motivation: AI生成的软件代码（vibe coding）难以通过传统的形式化证明验证，尤其当需求无法形式化时，亟需新的验证方法。

Method: 设计了一个框架，提取代码适用性的半形式化依据，定义生成代码可被接受的条件，而非直接证明其正确性。

Result: 当前已开始实现该框架，并探讨其应用和未来的研究方向。

Conclusion: 该方法为AI生成代码的验证提供了一种可行途径，促进了代码的可靠性分析与验证研究。

Abstract: When software artifacts are generated by AI models ("vibe coding"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.

</details>


### [79] [Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code](https://arxiv.org/abs/2601.01215)
*Prateek Rajput,Yewei Song,Abdoul Aziz Bonkoungou,Iyiola E. Olatunji,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文提出了一种度量大语言模型（LLMs）生成程序在运行时内存稳定性的方法，发现不同正确解法在性能和内存使用上存在显著差异，揭示了潜在的运行风险。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能生成通过单元测试的程序，但通过测试并不保证程序运行时行为可靠，不同正确解法的性能和内存使用模式差异可能带来隐藏的风险。

Method: 提出了使用动态时间规整（DTW）对转化为单调峰型轮廓（MPP）的内存使用轨迹进行比较的动态均值成对距离（DMPD）指标，进而在任务层面和模型层面分别定义了内存稳定性度量指标。

Result: 在BigOBench和CodeContests数据集上的实验表明，正确解法在运行时内存表现存在显著差异，且运行时不稳定性随着采样温度升高而增加，同时稳定性指标与复杂度指标相关。

Conclusion: 结果表明通过稳定性指标可以在确保正确性的前提下，在持续集成/持续交付流程中优选性能更稳定的解法，降低运行时风险，提升软件运维可靠性。

Abstract: Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.

</details>


### [80] [HD-GEN: A High-Performance Software System for Human Mobility Data Generation Based on Patterns of Life](https://arxiv.org/abs/2601.01219)
*Hossein Amiri,Joon-Seok Kim,Hamdi Kavak,Andrew Crooks,Dieter Pfoser,Carola Wenk,Andreas Züfle*

Main category: cs.SE

TL;DR: 本文提出了一个集成的软件管道，用于生成结合真实数据与模拟优势的大规模个人移动数据，提升数据的真实性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有人类移动数据要么存在稀疏与偏差问题，要么数据缺乏真实感，影响应用效果。

Method: 开发了包含生成、遗传算法校准、数据处理和可视化四部分的软件系统，利用地理数据和遗传算法调优模拟参数，实现逼真且多样的移动轨迹数据生成。

Result: 系统能够生成符合真实世界移动特征的模拟数据，并提供结构化数据和可视化展示，便于下游应用和模式分析。

Conclusion: 该软件管道有效融合真实数据与模拟优势，为人类移动行为研究提供了灵活且高质量的数据支持。

Abstract: Understanding individual-level human mobility is critical for a wide range of applications. Real-world trajectory datasets provide valuable insights into actual movement behaviors but are often constrained by data sparsity and participant bias. Synthetic data, by contrast, offer scalability and flexibility but frequently lack realism. To address this gap, we introduce a comprehensive software pipeline for calibrating, generating, processing, and visualizing large-scale individual-level human mobility datasets that combine the realism of empirical data with the control and extensibility of Patterns-of-Life simulations. Our system consists of four integrated components. (1) a data generation engine constructs geographically grounded simulations using OpenStreetMap data to produce diverse mobility logs. (2) a genetic algorithm-based calibration module fine-tunes simulation parameters to align with real-world mobility characteristics, such as daily trip counts and radius of gyration, enabling realistic behavioral modeling. (3) a data processing suite transforms raw simulation logs into structured formats suitable for downstream applications, including model training and benchmarking. (4) a visualization module extracts key mobility patterns and insights from the processed datasets and presents them through intuitive visual analytics for improved interpretability.

</details>


### [81] [Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling](https://arxiv.org/abs/2601.01233)
*Kangchen Zhu,Zhiliang Tian,Shangwen Wang,Mingyue Leng,Xiaoguang Mao*

Main category: cs.SE

TL;DR: 本文提出了Atomizer，一种用于复合提交代码变更拆分的新型多智能体协作框架，利用意图导向的链式思维策略和审阅反馈机制，提高语义理解和分组精度。


<details>
  <summary>Details</summary>
Motivation: 现有自动拆分复合提交方法依赖结构信息、缺乏语义理解能力，且多为单次分组算法，难以反映人类复审和迭代改进过程，导致拆分效果有限。

Method: Atomizer采用意图导向链式思维(IO-CoT)策略，结合结构和语义信息利用大型语言模型推断代码变更意图，同时引入分组者-审阅者双智能体协作迭代优化机制，模拟人类复审流程，不断细化分组。

Result: 在两个基准C#和Java数据集上，Atomizer平均超越现有图聚类基线方法6.0%和5.5%，在复杂提交上的优势更显著，性能提升超过16%。

Conclusion: 通过结合语义理解和多轮协作审阅，Atomizer有效克服了复合提交自动拆分的语义缺失和单次分组局限，显著提升拆分质量，具备较强实用价值。

Abstract: Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.

</details>


### [82] [CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs](https://arxiv.org/abs/2601.01271)
*Qingxiao Tao,Xiaodong Gu,Hao Zhong,Beijun Shen*

Main category: cs.SE

TL;DR: 该论文提出了CatchAll，一种基于大语言模型（LLM）的仓库级异常处理方法，通过整合API异常知识、代码上下文信息和跨仓库异常处理模式，提高异常处理代码生成的准确性和上下文感知能力。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在代码生成方面表现优异，但在仓库级异常处理上表现不佳，主要由于异常处理涉及复杂的依赖关系和上下文限制。为提升异常处理的质量，需引入多层次的异常知识和上下文信息。

Method: CatchAll为LLM提供三层异常处理知识：(1)基于实证构建的API异常映射；(2)仓库级执行上下文，通过调用轨迹捕捉异常传播；(3)跨仓库异常处理模式，从历史代码中挖掘可复用模式。将这些知识编码成结构化提示，指导LLM生成准确的异常处理代码。

Result: 构建了两个新基准RepoExEval和RepoExEval-Exec进行评测，CatchAll在CodeBLEU（0.31 VS 0.27）、意图预测准确率（60.1% VS 48.0%）和Pass@1（29% VS 25%）等指标上显著优于现有最先进方法，证明其对真实仓库异常处理的有效性。

Conclusion: CatchAll通过多层异常知识融合和结构化提示技术，显著提升了LLM在仓库级异常处理任务中的性能，为实际软件开发中的异常管理提供了有效支持。

Abstract: Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.

</details>


### [83] [Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python](https://arxiv.org/abs/2601.01320)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: 本文提出了ALPHA，一个基于层级CWE惩罚的Python函数级代码漏洞检测基准，评估大语言模型（LLMs）和静态应用安全测试工具（SAST）的性能，发现LLMs整体表现优于SAST但预测一致性差异大。


<details>
  <summary>Details</summary>
Motivation: 现有代码漏洞检测基准多采用二分类，缺乏针对具体CWE漏洞类型的细粒度反馈，不利于迭代纠正系统的应用。

Method: 设计了ALPHA基准，区分过度泛化、过度特化和横向错误，采用层级CWE惩罚机制评估LLMs和SAST工具，并比较它们的诊断准确性和一致性。

Result: 七个LLMs总体表现优于两种SAST工具，但SAST在检测存在时精度较高。模型间预测一致性差异巨大（8.26%-81.87%），影响反馈系统效果。

Conclusion: ALPHA基准填补了细粒度漏洞检测的空白，未来可考虑将ALPHA层级惩罚融入监督微调，提升漏洞检测的层级感知能力，有待实证验证。

Abstract: Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.

</details>


### [84] [GlycoPy: An Equation-Oriented and Object-Oriented Software for Hierarchical Modeling, Optimization, and Control in Python](https://arxiv.org/abs/2601.01413)
*Yingjie Ma,Jing Guo,Richard D. Braatz*

Main category: cs.SE

TL;DR: 本文介绍了GlycoPy，一个用于非线性模型预测控制（NMPC）的Python框架，支持分层建模和多种优化算法，提升了NMPC在复杂生物化学过程中的应用实用性。


<details>
  <summary>Details</summary>
Motivation: 现有过程工业中的模型预测控制多采用线性模型，无法有效处理实际的非线性生物化学过程，且缺乏支持分层模型开发和高效实现非线性MPC算法的工具。

Method: 提出了GlycoPy框架，基于方程导向和面向对象设计，支持分层建模，集成参数估计、动态优化和NMPC算法，用户可自定义算法。

Result: 通过三种案例研究验证了GlycoPy在建模、优化和NMPC上的能力，涵盖简单微分代数方程系统到多尺度生物过程模型。

Conclusion: GlycoPy有望缩小先进非线性MPC算法与实际生物化学过程应用之间的差距，推动NMPC更实用化。

Abstract: Most existing model predictive control (MPC) applications in process industries employ lin-ear models, although real-world (bio)chemical processes are typically nonlinear. The use of linear models limits the performance and applicability of MPC for processes that span a wide range of operating conditions. A challenge in employing nonlinear models in MPC for com-plex systems is the lack of tools that facilitate hierarchical model development, as well as lack of efficient implementations of the corresponding nonlinear MPC (NMPC) algorithms. As a step towards making NMPC more practical for hierarchical systems, we introduce Gly-coPy, an equation-oriented, object-oriented software framework for process modeling, opti-mization, and NMPC in Python. GlycoPy enables users to focus on writing equations for modeling while supporting hierarchical modeling. GlycoPy includes algorithms for parame-ter estimation, dynamic optimization, and NMPC, and allows users to customize the simula-tion, optimization, and control algorithms. Three case studies, ranging from a simple differ-ential algebraic equation system to a multiscale bioprocess model, validate the modeling, optimization, and NMPC capabilities of GlycoPy. GlycoPy has the potential to bridge the gap between advanced NMPC algorithms and their practical application in real-world (bio)chemical processes.

</details>


### [85] [SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving](https://arxiv.org/abs/2601.01426)
*Chaofan Tao,Jierun Chen,Yuxin Jiang,Kaiqi Kou,Shaowei Wang,Ruoyu Wang,Xiaohui Li,Sidi Yang,Yiming Du,Jianbo Dai,Zhiming Mao,Xinyu Wang,Lifeng Shang,Haoli Bai*

Main category: cs.SE

TL;DR: SWE-Lego通过轻量级监督微调方法，结合高质量数据集和改进的训练策略，实现软件工程任务的最新性能提升，且在测试时扩展性能显著。


<details>
  <summary>Details</summary>
Motivation: 目前软件工程问题的解决方法依赖复杂训练手段，且性能提升有限，作者希望探索一种简单有效的纯监督微调方法来推动性能极限。

Method: 构建包含3.2w高质量实例和1.8w验证轨迹的数据集，采用错误掩码和难度排序的微调策略，并结合测试时扩展技术提升模型表现。

Result: 基于上述方法，SWE-Lego模型在开源同体量模型中达到业内领先性能，8B模型达42.2%，32B模型达52.6%，测试时扩展后分别提升至49.6%和58.8%。

Conclusion: 纯监督微调结合优质数据和策略调整能够有效提升软件工程任务性能，且测试时扩展进一步增强模型表现，实现SOTA水平。

Abstract: We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.

</details>


### [86] [Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox](https://arxiv.org/abs/2601.01514)
*Matej Kucera,Marco Castelluccio,Daniel Feitosa,Ayushi Rastogi*

Main category: cs.SE

TL;DR: 本文研究了代码审查中的‘群组审查请求’对审查速度和质量的影响，发现群组审查提升了审查质量且对审查速度影响不大。


<details>
  <summary>Details</summary>
Motivation: 尽管审查速度是行业中重要指标，但群组审查请求对速度和质量的具体影响尚不明确。

Method: 分析了Mozilla Firefox项目约66,000次修订，结合统计模型和焦点小组访谈，比较群组审查与个人审查的差异。

Result: 群组审查与更高的审查质量（回归缺陷减少）相关，而对审查速度影响不明显。额外好处包括工作分配均衡和新审查员培训机会。

Conclusion: 群组审查能够提升审查质量并带来团队层面多重益处，是值得推广的审查分配方式。

Abstract: The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.

</details>


### [87] [MTS-1: A Lightweight Delta-Encoded Telemetry Format optimised for Low-Resource Environments and Offline-First System Health Monitoring](https://arxiv.org/abs/2601.01602)
*Henry Ndou*

Main category: cs.SE

TL;DR: 本论文提出了一种名为MTS-1的二进制遥测编码格式，专为带宽受限和离线优先环境设计，相比现有编码格式在数据压缩和传输效率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有遥测编码格式均针对高带宽和持续在线环境设计，难以满足非洲撒哈拉以南及偏远农村等带宽受限及网络不稳定环境的需求。

Method: 设计了一种基于差分编码的二进制遥测格式MTS-1，并与主流编码格式如JSON、CBOR、MessagePack等在负载大小、编码成本、网络效率和延迟成本等方面进行了综合性能比较。

Result: 在合成基准测试中，MTS-1相比JSON实现了最高74.7%的压缩率提升，相比MessagePack提升为5.4%，且对数据集规模呈线性扩展特性。

Conclusion: MTS-1适合用作低带宽、离线优先的系统级遥测传输方案，可有效提升远程监测和维护的效率，尤其适用于带宽受限和网络不稳定环境。

Abstract: System-level telemetry is fundamental to modern remote monitoring, predictive maintenance, and AI-driven infrastructure optimisation. Existing telemetry encodings such as JSON, JSON Lines, CBOR, and Protocol Buffers were designed for high-bandwidth, always-online environments. They impose significant overhead when deployed in bandwidth-constrained networks common across Sub-Saharan Africa, rural enterprise deployments, and unstable LAN environments. This paper introduces MTS-1 (Magenta Telemetry Standard v1), a novel delta-encoded binary telemetry format designed for offline-first system monitoring, LAN-assisted proxy delivery, and energy-efficient IoT-to-server transmission. We compare MTS-1 against JSON, JSON Lines, CBOR, MessagePack, and Protocol Buffers across payload size, encoding cost, network efficiency, and cost-latency performance. Synthetic benchmarking demonstrates preliminary compression improvements of up to 74.7% versus JSON and 5.4% versus MessagePack, with linear scaling characteristics across dataset sizes.

</details>


### [88] [LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment](https://arxiv.org/abs/2601.01780)
*Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan,Mehdi Keshani,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动问题指派方法LIA，通过监督微调使模型直接从问题标题和描述生成排序的开发者推荐，显著提升了指派准确率。


<details>
  <summary>Details</summary>
Motivation: 手动问题指派效率低且易出错，现有自动方法依赖大量特定项目数据或稀疏噪声多的关系信息，效果有限，亟需更有效的自动指派技术。

Method: 采用监督微调基于预训练的LLM（DeepSeek-R1-Distill-Llama-8B），利用其对自然语言和软件文本的语义理解，从历史问题与开发者指派模式中学习，生成开发者排名推荐。

Result: LIA相比基线模型在Hit@1指标提升最高达187.8%，与四种主流指派方法比较，性能提升高达211.2%，显著优于现有方法。

Conclusion: 基于领域自适应的LLM在软件维护中的问题指派任务表现卓越，LIA成为实用且高效的问题自动指派解决方案。

Abstract: Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.

</details>


### [89] [The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation](https://arxiv.org/abs/2601.01839)
*Martin Prause*

Main category: cs.SE

TL;DR: 本研究通过调查150名数据科学家，提出并验证了机器学习项目成功的四个关键因素：战略、流程、生态系统和支持，强调了这些因素的相互影响，并指出AI辅助工具虽能提升编码效率，但无法取代战略思维的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码助手日益普及，但超过80%的机器学习项目未能实现实际业务价值，亟需探究影响项目成功的关键因素。

Method: 设计机器学习画布框架，结合业务策略、软件工程和数据科学，通过统计模型分析150名数据科学家的调查数据。

Result: 确定了战略、流程、生态系统和组织支持四个关键成功因素，发现它们相互关联，支持通过加强组织支持改善战略、流程及基础设施，从而提高项目成功率。

Conclusion: AI编码助手虽然提升编码效率，但无法替代战略层面的“为何”和“做什么”，项目成功依赖于综合考虑战略、流程、生态系统和支持因素。

Abstract: Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (β= 0.432, p < 0.001), which improves work processes (β= 0.428, p < 0.001) and builds better infrastructure (β= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the "how" of coding but cannot replace the "why" and "what" of strategic thinking.

</details>


### [90] [A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach](https://arxiv.org/abs/2601.01921)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文研究了时间敏感的缺陷预测方法，以提前预测软件缺陷并识别缺陷前的早期指标。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统的持续演化，亟需能够在缺陷显现之前进行预报的时间敏感方法。

Method: 训练多种时间敏感预测技术，预测软件项目未来的缺陷密度，并识别缺陷发生前的早期症状。

Result: 预期获得关于该方法在提前评估缺陷倾向性方面有效性的实证证据。

Conclusion: 本文方法有望为软件缺陷的早期预测提供有效的时间敏感技术支持。

Abstract: Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.
  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.
  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.
  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.

</details>


### [91] [The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities](https://arxiv.org/abs/2601.01944)
*Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本论文分析了Python和Java开源软件项目中AI库的采用状况及其对开发、技术生态和社区参与的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管AI日益流行，其在开源软件项目中的应用及影响尚未得到充分研究。

Method: 通过对15.77万个潜在开源软件仓库进行大规模分析，使用存储库指标和软件指标对采用与未采用AI库的项目进行比较。

Result: 预计发现采用AI库的开源项目在开发活动、社区参与度和代码复杂度方面存在显著差异。

Conclusion: 论文提供了基于证据的见解，说明AI集成如何重塑开源软件开发实践。

Abstract: In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.
  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.

</details>


### [92] [Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration](https://arxiv.org/abs/2601.01952)
*Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文提出一种基于人机协作的大语言模型（HLC）方法，实现上下文自适应的需求缺陷预测，效果优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 需求缺陷的定义随项目和利益相关者而异，传统静态分类方法难以适应多变背景。

Method: HLC利用大语言模型的链式思维与用户反馈结合，通过少量验证样本进行自适应学习，动态调整预测。

Result: 在梅赛德斯-奔驰需求数据集上，使用20个验证样本即可快速提升性能，优于传统少量样本提示和微调BERT模型，同时保持高召回率。

Conclusion: 基于大语言模型的上下文链式学习能力使缺陷预测具备适应性，推动了持续学习型需求评估工具的发展。

Abstract: Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a "defect" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.

</details>


### [93] [Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations](https://arxiv.org/abs/2601.01954)
*Alexander Korn,Lea Zaruchas,Chetan Arora,Andreas Metzger,Sven Smolka,Fanyu Wang,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文通过分析2022年以来顶级软件工程会议的论文和调查审稿人，揭示了大语言模型研究中提示工程报告的不足，提出了一套结构化指南以提升透明度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件工程中的应用日益增多，但提示设计和报告缺乏系统性和透明度，导致研究可复现性和比较性差。

Method: 分析近300篇顶级会议论文的提示设计与测试报告情况，调查105名程序委员会成员对提示报告的期望，基于结果制定结构化报告指南。

Result: 发现实践与审稿人期望间存在显著差异，尤其在版本披露、提示理由及有效性威胁说明方面。

Conclusion: 提出的指南为提升基于大语言模型的软件工程研究的透明度、可重复性和方法严谨性提供了实用路径。

Abstract: Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.

</details>


### [94] [The State of Open Science in Software Engineering Research: A Case Study of ICSE Artifacts](https://arxiv.org/abs/2601.02066)
*Al Muttakin,Saikat Mondal,Chanchal Roy*

Main category: cs.SE

TL;DR: 本文评估了过去十年ICSE会议中100个软件工程研究的复现包，发现只有40%可执行，且仅35%成功复现原始结果，揭示了复现包的可执行性和复现性存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 虽然软件工程研究中共享复现包已成常规，但其实际可用性和可复现性未被充分研究。本文旨在填补这一空白，全面评估复现包的可执行性和复现性。

Method: 对ICSE会议近十年发布的100个复现包进行执行测试，评估其可执行性、所需修改与努力程度、执行挑战及结果复现情况，共耗时约650人小时。

Result: 仅40%的复现包可执行，其中仅32.5%无需修改，82.5%需要中高强度努力。发现五类常见修改和13种执行障碍。可执行的复现包中只有35%成功复现原始结果。

Conclusion: 复现包的可用性与复现性存在明显差距。论文提出三条可操作性指导原则，改进复现包的准备、文档和评审，以提升软件工程研究的严谨性和开放科学实践的可持续性。

Abstract: Replication packages are crucial for enabling transparency, validation, and reuse in software engineering (SE) research. While artifact sharing is now a standard practice and even expected at premier SE venues such as ICSE, the practical usability of these replication packages remains underexplored. In particular, there is a marked lack of studies that comprehensively examine the executability and reproducibility of replication packages in SE research. In this paper, we aim to fill this gap by evaluating 100 replication packages published as part of ICSE proceedings over the past decade (2015--2024). We assess the (1) executability of the replication packages, (2) efforts and modifications required to execute them, (3) challenges that prevent executability, and (4) reproducibility of the original findings. We spent approximately 650 person-hours in total executing the artifacts and reproducing the study findings. Our findings reveal that only 40\% of the 100 evaluated artifacts were executable, of which 32.5\% (13 out of 40) ran without any modification. Regarding effort levels, 17.5\% (7 out of 40) required low effort, while 82.5\% (33 out of 40) required moderate to high effort to execute successfully. We identified five common types of modifications and 13 challenges leading to execution failure, spanning environmental, documentation, and structural issues. Among the executable artifacts, only 35\% (14 out of 40) reproduced the original results. These findings highlight a notable gap between artifact availability, executability, and reproducibility. Our study proposes three actionable guidelines to improve the preparation, documentation, and review of research artifacts, thereby strengthening the rigor and sustainability of open science practices in SE research.

</details>


### [95] [Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics](https://arxiv.org/abs/2601.02200)
*Markus Borg,Nadim Hagatulah,Adam Tornhill,Emma Söderberg*

Main category: cs.SE

TL;DR: 研究表明，人类易读代码对AI重构更友好，有助于降低AI编辑代码的风险。


<details>
  <summary>Details</summary>
Motivation: 随着人类开发者和AI代码代理共同协作，确保不同能力的LLM能可靠地编辑代码变得尤为重要。

Method: 基于5000个Python竞赛编程文件的数据集，利用LLM进行代码重构，研究了人类可读性指标CodeHealth与语义保持之间的关系。

Result: 发现CodeHealth与AI重构后代码的语义保持存在显著正相关，表明易读代码对AI工具更兼容。

Conclusion: 提高代码的可维护性不仅利于人类，也有助于大规模采用AI编辑，组织可利用CodeHealth指标指导AI干预的风险管理。

Abstract: We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.

</details>


### [96] [LLM-Empowered Functional Safety and Security by Design in Automotive Systems](https://arxiv.org/abs/2601.02215)
*Nenad Petrovic,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 本论文提出了一种基于大语言模型的工作流程，支持软件定义车辆(SDV)的软件开发，涵盖安全感知的系统拓扑设计和事件驱动的代码分析。


<details>
  <summary>Details</summary>
Motivation: 为了提升SDV软件开发的安全性与功能安全验证，结合安全性和系统设计需求，提出结合大语言模型的支持方法。

Method: 采用事件链模型分析代码，确保功能安全的系统验证，同时结合模型驱动工程（MDE）和对象约束语言（OCL）规则分析系统拓扑的安全性。

Result: 在ADAS相关场景下，验证了本方法在安全意识的系统设计和基于事件的决策代码分析中的有效性，适用于本地部署和专有解决方案。

Conclusion: 该工作提出的LLM辅助工作流程有效支持了SDV软件的安全设计与功能验证，促进了智能汽车相关系统的安全可靠开发。

Abstract: This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.

</details>


### [97] [NQC2: A Non-Intrusive QEMU Code Coverage Plugin](https://arxiv.org/abs/2601.02238)
*Nils Bosbach,Alwalid Salama,Lukas Jünger,Mark Burton,Niko Zurstraßen,Rebecca Pelke,Rainer Leupers*

Main category: cs.SE

TL;DR: 本文提出了NQC2插件，通过在QEMU中提取嵌入式软件的运行时覆盖率信息，并将数据保存到主机，解决了传统覆盖率分析方法在裸机嵌入式系统中无法使用的问题。


<details>
  <summary>Details</summary>
Motivation: 传统代码覆盖率分析依靠插装源代码并依赖操作系统和文件系统功能，但裸机嵌入式软件缺少这些支持，导致现有方法难以应用，急需一种无需目标软件插装且适用于裸机环境的覆盖率收集方案。

Method: 设计并实现了NQC2插件，直接从QEMU模拟器的运行时中提取覆盖率信息，兼容经过修改的QEMU版本，将数据保存到主机文件，不依赖目标软件代码插装。

Result: NQC2插件在性能上优于Xilinx的类似方法，速度提升最高达8.5倍。

Conclusion: NQC2为裸机嵌入式系统的代码覆盖率分析提供了一种高效且易用的解决方案，拓展了覆盖率分析在嵌入式领域的适用范围。

Abstract: Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.
  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.

</details>


### [98] [Automatic Assertion Mining in Assertion-Based Verification: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2601.02248)
*Mohammad Reza Heidari Iman,Giorgio Di Natale,Katell Morin-Allory*

Main category: cs.SE

TL;DR: 本文综述了自动断言挖掘器在基于断言的验证（ABV）中的应用，比较了现有方法的优缺点，指出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着功能验证对基于断言的验证依赖增强，自动断言挖掘器成为关键工具，然而现有方法存在不足，需进行系统评述以指导未来研究。

Method: 本文通过综述和比较当前先进且广泛采用的自动断言挖掘器的方法论，分析其特点与不足。

Result: 系统总结了各种断言挖掘器的方法优势与局限，提供了对各方法能力的全面理解。

Conclusion: 指出现有自动断言挖掘器的不足，并提出未来提升断言挖掘技术的方向，以推动基于断言的验证的发展。

Abstract: Functional verification increasingly relies on Assertion-Based Verification (ABV), which has become a key approach for verifying hardware designs due to its efficiency and effectiveness. Central to ABV are automatic assertion miners, which apply different techniques to generate assertions automatically. This paper reviews the most recent, advanced, and widely adopted assertion miners, offering a comparative analysis of their methodologies. The goal is to provide researchers and verification practitioners with insights into the capabilities and limitations of existing miners. By identifying their shortcomings, this work also points toward directions for developing more powerful and advanced assertion miners in the future.

</details>


### [99] [Question Answering for Multi-Release Systems: A Case Study at Ciena](https://arxiv.org/abs/2601.02345)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: QAMR是一种专为多版本软件系统文档设计的问答聊天机器人，通过改进的检索增强生成方法显著提升回答准确率和检索准确率，并降低响应时间。


<details>
  <summary>Details</summary>
Motivation: 现有的问答技术在多版本软件系统文档上准确率较低，因为不同版本的文档存在重叠但不完全相同的内容，导致回答不准确。

Method: QAMR通过预处理、查询重写和上下文选择的结合方式，以及双重分块策略，分别优化检索和回答生成过程，从而提高多版本文档问答的准确性。

Result: QAMR在公共软件工程基准和真实多版本文档测试中，回答准确率提升16.5%至88.5%、检索准确率提升12%至90%，且响应时间减少8%。消融实验验证了各机制的有效性。

Conclusion: QAMR有效解决了多版本系统文档问答的准确性问题，方法可靠，效果显著，具备推广应用潜力。

Abstract: Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [100] [Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai](https://arxiv.org/abs/2601.01090)
*Erica Coppolillo,Luca Luceri,Emilio Ferrara*

Main category: cs.MA

TL;DR: 本文研究了大语言模型驱动的自主代理在AI社交平台上的有害内容传播与行为影响，发现毒性内容暴露能增加毒性回复概率，但很多毒性行为非由暴露直接诱发。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究关注语言模型生成有害内容，鲜有分析这些内容暴露如何影响代理长期行为，尤其在全AI互动环境中。

Method: 构建了Chirper.ai上代理互动模型，通过采集可见的帖子和评论作为刺激与响应，进行大规模实证分析，包括毒性回复与刺激的关系、暴露次数的影响和行为预测。

Result: 研究发现毒性回复更可能跟随毒性刺激，但大量毒性行为为自主产生，累积暴露明显提高毒性回复概率。同时引入两种影响力指标，揭示诱导性与自发性毒性的权衡，并能准确预测代理是否会产生毒性内容。

Conclusion: 暴露量是LLM代理部署中的关键风险因子，监测所接触内容可作为有效的审计与缓解手段，降低有害行为产生。

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms.
  We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content.
  These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.

</details>


### [101] [CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty](https://arxiv.org/abs/2601.01581)
*Rishav Sen,Fangqi Liu,Jose Paolo Talusan,Ava Pettet,Yoshinori Suzue,Mark Bailey,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: cs.MA

TL;DR: 本文提出一种基于协商的电动汽车充电框架，通过激励机制协调建筑运营方和车主的利益，降低能源成本并提升用户充电体验。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电在车与建筑能量管理中产生冲突，建筑运营方面临高能源成本，车主则追求便利和充足电量，亟需一个兼顾双方利益的解决方案。

Method: 设计了一种具备自愿参与、策略真实性和预算可行性的协商框架，利用激励机制鼓励车主在离开时间和充电状态上给予适度灵活性；结合用户调研和实际运营数据进行校准与验证。

Result: 模拟结果显示，该框架使建筑运营成本较优化非协商充电策略降低超过3.5%，用户充电费用降低22%，实现了双方的经济效益提升。

Conclusion: 该框架有效对齐了建筑运营者与电动汽车用户的目标，将充电过程从运营矛盾转换为合作平台，实现了能源与出行系统的战略融合。

Abstract: The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framework that, by design, guarantees voluntary participation, strategy-proofness, and budget feasibility. It transforms EV charging into a strategic resource by offering drivers a range of incentive-backed options for modest flexibility in their departure time or requested state of charge (SoC). Our framework is calibrated with user survey data and validated using real operational data from a commercial building and an EV manufacturer. Simulations show that our negotiation protocol creates a mutually beneficial outcome: lowering the building operator's costs by over 3.5\% compared to an optimized, non-negotiating smart charging policy, while simultaneously reducing user charging expenses by 22\% below the utility's retail energy rate. By aligning operator and EV user objectives, our framework provides a strategic bridge between energy and mobility systems, transforming EV charging from a source of operational friction into a platform for collaboration and shared savings.

</details>


### [102] [ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring](https://arxiv.org/abs/2601.01831)
*Aniket Wattamwar,Sampson Akwafuo*

Main category: cs.MA

TL;DR: ARIES是一个专门为流行病监测设计的多智能体自主框架，通过自动查询权威数据源，实时识别新兴威胁，提升全球健康监控能力。


<details>
  <summary>Details</summary>
Motivation: 当前全球健康监测面临知识空白，通用AI在流行病学领域因幻觉现象和无法处理专业数据孤岛而效果有限。

Method: 提出了基于分层指挥架构的ARIES框架，利用GPTs协调多智能体群，自主查询WHO、CDC及同行评审文献，自动提取和综合监测数据。

Result: ARIES实现了专门推理能力，能够实时发现威胁和信号偏差，表现优于通用模型，证明了任务特定智能体群的有效性。

Conclusion: ARIES展示了模块化、多智能体系统在疫情响应和全球健康情报中的潜力，为下一代疾病爆发应对提供了强大且可扩展的解决方案。

Abstract: Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.

</details>
