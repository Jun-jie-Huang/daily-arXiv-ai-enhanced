<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 184]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.SE](#cs.SE) [Total: 59]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths](https://arxiv.org/abs/2601.11564)
*Ahilan Ayyachamy Nadar Ponnusamy,Karthic Chandran,M Maruf Hossain*

Main category: cs.CL

TL;DR: 本文研究了大规模上下文窗口对变换器模型性能的影响，揭示了处理大量无关上下文时的非线性性能下降及MoE架构在大规模上下文下的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 了解在扩展大型语言模型上下文窗口时，如何平衡计算开销和模型性能，尤其是应对大规模无关上下文带来的性能挑战。

Method: 通过实验分析稠密变换器架构（如Llama-3.1-70B和Qwen1.5-14B）在大规模无关上下文下的表现，并结合Mixture-of-Experts架构的扩展研究，探讨上下文规模对系统性能的影响。

Result: 发现随着键值缓存(KV cache)增大，模型性能呈非线性下降；MoE架构在不同上下文规模下表现出独特异常，表明高令牌量下架构优势可能被基础设施瓶颈掩盖。

Conclusion: 大规模上下文窗口扩展在提升模型处理复杂任务能力的同时，带来了性能与质量的显著权衡，特别是在处理大量无关上下文时性能非线性下降。

Abstract: The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.

</details>


### [2] [Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings](https://arxiv.org/abs/2601.11565)
*Pakorn Ueareeworakul,Shuman Liu,Jinghao Feng,Ling Hu,Zhantang Shi,Chengqi Sun,Liang Yao,Panyi Ouyang,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: Compass-Embedding v4 专为东南亚电商设计，通过创新训练方法和数据增强，解决低资源语言语义表示难题，实现高效且精准的多语言嵌入。


<details>
  <summary>Details</summary>
Motivation: 东南亚低资源语言缺乏高质量的语义表示，严重影响电商检索和推荐系统的性能，且生产环境对模型效率和准确率有严格要求。

Method: 利用Class-Aware Masking改进对比学习目标，构建多样化训练语料（合成数据生成、跨语言翻译、电商结构化数据），结合大批量训练和球面模型合并技术，采用vLLM和FP8量化优化推理。

Result: Compass-Embedding v4 在多语种基准测试和电商品类任务中表现出色，超越通用嵌入模型，在东南亚低资源语言任务上实现最先进性能，同时保持对高资源语言的竞争力。

Conclusion: Compass-Embedding v4 成功解决了东南亚电商场景中的低资源语言语义表示问题，实现了语义对齐层面的显著改进，并在生产环境中保持高效推理能力。

Abstract: As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.

</details>


### [3] [Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology](https://arxiv.org/abs/2601.11567)
*Vanessa D'Amario,Randy Daniel,Alessandro Zanetti,Dhruv Edamadaka,Nitya Alaparthy,Joshua Tarkoff*

Main category: cs.CL

TL;DR: 本文评估了六种小型开源医学大模型的准确性、一致性和推理表现，揭示了输出稳定性不足和自我评估偏差，强调了现有评估方法的局限及临床应用中的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 现有医学LLM评估多局限于准确率，缺少对一致性、鲁棒性和推理行为的全面评估，且评估的可重复性存在问题，需构建更全面的诊断框架以支持临床决策。

Method: 通过多项选择题结合人工评估和临床审核，针对六个小型开源医学LLM进行儿科内分泌学领域的测试，分析提示变异、自评偏差、一致性与正确性的关系以及系统级扰动对模型表现的影响。

Result: 发现模型在提示微小变动和不同随机设置下输出差异显著，最高性能由HuatuoGPT-o1-8B达成，一致性高不代表结果正确，模型自我评估存在偏差，临床评价发现推理合理性参差不齐，系统环境差异亦导致输出统计学上显著变化。

Conclusion: 小型开源医学大语言模型在不同提示和随机设置下表现出输出变异性和自我评估偏差，高一致性并不保证正确性，存在临床合理性与疏忽并存的现象，系统级变动也影响输出结果。

Abstract: Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.

</details>


### [4] [An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT](https://arxiv.org/abs/2601.11573)
*Muhammad Muneeb,David B. Ascher*

Main category: cs.CL

TL;DR: 本文提出一种高效的九步微调流程，成功将大型语言模型定制为生物信息学领域助手，显著提升其专业表现，并开放大量高质量问答数据集，实现了隐私保护和本地部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂生物信息学应用时往往缺乏专业知识，亟需通过微调提升其领域特定的理解和应用能力，从而更好地支持生物信息学工具和社区问答的需求。

Method: 设计了一个九步微调流程，包括多样数据源集成、结构化预处理、基于提示的QA生成、自然语言推理质量控制、语义去重、聚类数据拆分，以及使用LoRA进行参数高效微调；对三个LLM（LLaMA-3.2-3B、Qwen2.5-7B、Gemma）进行微调并基于14项词汇和语义指标进行评测，选出最佳模型Qwen2.5-7B。

Result: 成功微调了LLM，Qwen2.5-7B在PRSGPT和BioStarsGPT任务中分别实现了BLEU-4提升82%和6%，ROUGE-1提升70%和18%；生成了覆盖28,000个和154,282个问答对的开源数据集；人类评测显示PRSGPT在工具比较任务上准确率达61.9%，BioStarsGPT概念准确率为59%。

Conclusion: 本研究提出了一种可复现的九步流程，用于在生物信息学专门数据上对大型语言模型进行微调，显著提升了模型在复杂生物信息学任务中的表现，实现了可扩展的领域特定微调。研究成功展示了两个应用案例：PRSGPT和BioStarsGPT，并且实现了与Google Gemini相当甚至更优的性能，以及丰富的细节和准确引用。该方法推动了隐私保护和本地部署的生物信息学助手的开发。

Abstract: Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on polygenic risk score (PRS) tools, and BioStarsGPT, trained on community forum discussions. The nine-step pipeline integrates diverse data sources, structured preprocessing, prompt-based question-answer (QA) generation (via Google Gemini), natural language inference (NLI) for quality control, semantic deduplication, clustering-based data splitting, and parameter-efficient fine-tuning using LoRA. We fine-tuned three LLMs (LLaMA-3.2-3B, Qwen2.5-7B, Gemma) and benchmarked them on over 14 lexical and semantic metrics. Qwen2.5-7B emerged as the best performer, with BLEU-4 and ROUGE-1 improvements of 82\% and 70\% for PRSGPT and 6\% and 18\% for BioStarsGPT, respectively. The open-source datasets produced include over 28,000 QA pairs for PRSGPT and 154,282 for BioStarsGPT. Human evaluation of PRSGPT yielded 61.9\% accuracy on the PRS tools comparison task, comparable to Google Gemini (61.4\%), but with richer methodological detail and accurate citations. BioStarsGPT demonstrated 59\% conceptual accuracy across 142 curated bioinformatics questions. Our pipeline enables scalable, domain-specific fine-tuning of LLMs. It enables privacy-preserving, locally deployable bioinformatics assistants, explores their practical applications, and addresses the challenges, limitations, and mitigation strategies associated with their development and use.

</details>


### [5] [ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System](https://arxiv.org/abs/2601.11854)
*Yifei Zhang,Hooshang Nayyeri,Rinat Khaziev,Emine Yilmaz,Gokhan Tur,Dilek Hakkani-Tür,Hari Thadakamalla*

Main category: cs.CL

TL;DR: 本文针对先进任务导向对话系统代理行为不足的评测问题，提出了ATOD基准及ATOD-Eval评估框架和记忆型评估器，实现多目标长期推理对话的系统性评估，验证了其准确性和效率优势。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向对话系统的评估不支持大语言模型驱动的多目标、多任务、多记忆及主动执行等高级代理行为，缺少系统的评测基准。

Method: 设计了ATOD基准和合成对话生成管道以产生需要长期推理的多目标任务对话，并提出ATOD-Eval评估框架，将多个代理维度转化为细粒度度量指标；构建了基于记忆的评估器用于基准测试。

Result: 通过实验验证，ATOD-Eval能够综合评估任务完成度、代理能力和响应质量，并且基于记忆的评估器在本评估环境下较传统方法具有更优的准确率与效率表现。

Conclusion: 本文提出的ATOD基准和ATOD-Eval评估框架有效支持了对先进任务导向对话系统多种代理行为的系统评估，提升了评测的细粒度和全面性。基于ATOD的记忆型评估器在准确性和效率之间实现了更优的平衡。

Abstract: Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.

</details>


### [6] [Concept Attractors in LLMs and their Applications](https://arxiv.org/abs/2601.11575)
*Sotirios Panagiotis Chytas,Vikas Singh*

Main category: cs.CL

TL;DR: 本文解释了大语言模型内部表征形成的机制，提出基于吸引子的无训练干预方法，在多项任务中表现优异，简化了模型调优流程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不同层对语义相关的提示映射到相似的内部表示，揭示了其内部行为规律。

Method: 利用迭代函数系统（IFS）的理论，视层为收缩映射导向概念特定的吸引子，基于吸引子开发无需训练的简单方法。

Result: 基于吸引子的方法在语言翻译、减少幻觉、设定限制和合成数据生成等任务中表现出优异效果，达到或超过专门基线。

Conclusion: 通过理解LLM内部的吸引子结构，提出了一种高效且泛化能力强的干预手段，替代了复杂的微调过程。

Abstract: Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a wide range of practical tasks, including language translation, hallucination reduction, guardrailing, and synthetic data generation. Despite their simplicity, these Attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.

</details>


### [7] [LimAgents: Multi-Agent LLMs for Generating Research Limitations](https://arxiv.org/abs/2601.11578)
*Ibrahim Al Azher,Zhishuai Guo,Hamed Alhoori*

Main category: cs.CL

TL;DR: 提出LimAgents多智能体框架，结合多来源信息，系统生成更实质性的研究局限性，显著提升覆盖率和质量，优于传统零样本方法。


<details>
  <summary>Details</summary>
Motivation: 现有零样本LLM生成的局限性通常肤浅且重复作者已有陈述，缺乏深入分析，且作者披露的局限往往不完整，导致科学研究透明度和严谨性不足。

Method: 提出了多智能体LLM框架LimAgents，集成OpenReview评论、作者陈述、引用和被引用文献，利用不同角色的智能体分工合作，生成系统性的显性和隐性局限性。引入点对点评价协议和LLM作为评判者，以更准确衡量覆盖率。

Result: LimAgents在RAG + GPT-4o mini配置下比零样本基线覆盖率提升15.51%，Llama 3 8B多智能体设置提升4.41%，有效捕获更全面、更深入的局限性。

Conclusion: LimAgents框架显著提升了局限性识别的全面性和深度，超过了传统零样本大型语言模型的性能。

Abstract: Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.

</details>


### [8] [LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction](https://arxiv.org/abs/2601.13352)
*Yuxing Lu,J. Ben Tamo,Weichen Zhao,Nan Sun,Yishan Zhong,Wenqi Shi,Jinzhuo Wang,May D. Wang*

Main category: cs.CL

TL;DR: LLM-as-RNN 利用自然语言记忆机制将冻结的大型语言模型转变为递归预测器，实现在线学习，明显提升序列预测性能并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有大型语言模型在生成过程中无法更新上下文记忆的问题，提升错误后续预测的准确性。

Method: 提出 LLM-as-RNN 框架，通过将模型隐藏状态表示为自然语言形式的系统提示摘要，并在每个步骤通过反馈驱动的文本重写更新该状态。

Result: 在医疗、气象和金融三个领域的基准测试中，LLM-as-RNN 对比零样本、完整历史及 MemPrompt 基线，平均提高了6.5%的预测准确率，同时生成了可解释的人类可读学习轨迹。

Conclusion: LLM-as-RNN 成功将冻结的大型语言模型转变为具有更新记忆机制的递归预测器，实现了无需参数更新的在线学习，显著提升了序列预测准确率。

Abstract: Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.

</details>


### [9] [Bielik 11B v3: Multilingual Large Language Model for European Languages](https://arxiv.org/abs/2601.11579)
*Krzysztof Ociepa,Łukasz Flis,Remigiusz Kinas,Krzysztof Wróbel,Adrian Gwoździej*

Main category: cs.CL

TL;DR: Bielik 11B v3是一个以波兰语为主优化的11B参数级语言模型，采用四阶段训练，性能超越许多更大规模模型，参数高效且支持多种硬件。


<details>
  <summary>Details</summary>
Motivation: 为了提升波兰语的自然语言处理能力，开发一个既在波兰语又在其他欧洲语言上表现优异且资源高效的语言模型。

Method: 该模型基于Mistral 7B v0.2架构，通过深度扩展参数至11B，采用四阶段训练流程：连续预训练、监督微调（SFT）、直接偏好优化（DPO）及强化学习。

Result: Bielik 11B v3表现出极高的参数效率和广泛的量化选项，实现了在多种硬件配置上的有效部署，提升了波兰语及欧洲语言的AI处理能力。

Conclusion: Bielik 11B v3在多项评测中表现卓越，超越了其他专门针对波兰语的语言模型以及许多参数量更大的模型，成为波兰语及欧洲其他语言处理的新标杆。

Abstract: We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.
  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.
  The model's parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.

</details>


### [10] [Speculative Decoding: Performance or Illusion?](https://arxiv.org/abs/2601.11580)
*Xiaoxuan Liu,Jiaxiang Yu,Jongseok Park,Ion Stoica,Alvin Cheung*

Main category: cs.CL

TL;DR: 本研究首次在实际生产环境中系统评测推测解码多种变体，发现其加速性能受限于目标模型验证，实际效果远低于理论极限，提出未来提升方向。


<details>
  <summary>Details</summary>
Motivation: 尽管推测解码(SD)加速大规模语言模型推理成为热门技术，但其在真实场景中的实际效果尚不明确，尤其之前评估基于研究原型且批量大小不现实，促使本研究对SD效能进行系统验证。

Method: 在一个生产级的推理引擎(vLLM)上，系统地评估了多种推测解码(SD)变体，包括$n$-gram、EAGLE/EAGLE-3、Draft-Model和多令牌预测，覆盖多样化工作负载、模型规模和批量大小。通过理论界定SD加速的上限，对比理论与实际表现，分析关键影响因素。

Result: 研究首次在广泛部署的生产级推理引擎上验证SD的多种变体，揭示验证步骤占主导，输出接受长度差异大，实际加速远低于理论上限，从而指出改进空间和未来研究方向。

Conclusion: 实验证明目标模型的验证过程在推理中占主导地位，而接受长度在不同的位置、请求和数据集间变化显著。实际性能与理论上限存在较大差距，表明当前SD技术仍有较大优化空间。

Abstract: Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token Prediction) across diverse workloads, model scales, and batch sizes. We analyze key factors governing SD performance, and quantify a theoretical upper bound on SD speedup. Our results show that verification by the target model dominates the execution, while acceptance length varies markedly across output token positions, requests, and datasets. Comparing measured performance with theoretical bounds reveals substantial gaps between observed and theoretical upper bounds, and we leverage this observation to highlight new research opportunities that our study opens up in improving SD.

</details>


### [11] [Enhancing the QA Model through a Multi-domain Debiasing Framework](https://arxiv.org/abs/2601.11581)
*Yuefeng Wang,ChangJae Lee*

Main category: cs.CL

TL;DR: 本文针对QA模型偏见问题，提出多领域去偏框架，有效提升了模型在标准与对抗数据集上的表现，增强了自然语言理解系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前QA模型在处理复杂查询及对抗场景时存在偏见，影响其表现与鲁棒性，亟需有效的去偏策略提升模型的可靠性。

Method: 本文通过评估ELECTRA-small模型在SQuAD v1.1及对抗数据集AddSent和AddOneSent上的表现，识别了与词汇偏见、数值推理和实体识别相关的错误，进而采用知识蒸馏、去偏技术和领域扩展相结合的方法进行多领域去偏训练。

Result: 提出的多领域去偏框架在所有测试集中Exact Match和F1分数最高提升2.6个百分点，特别是在对抗数据集上表现出显著增益。

Conclusion: 针对自然语言理解系统中的偏见问题，本文提出了一种多领域去偏框架，有效提升了模型在标准和对抗数据集上的性能表现。

Abstract: Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.

</details>


### [12] [Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents](https://arxiv.org/abs/2601.11585)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 提出了一种基于信息论的上下文选择新方法ECS，有效提升了多轮对话中精准上下文筛选的性能，优于传统词汇相似度方法。


<details>
  <summary>Details</summary>
Motivation: 上下文工程需要区分真正有助于回答问题的信息和误导性干扰信息，而传统词汇相似度方法无法准确反映实际的语用效用。  

Method: 引入基于信息论的ECS框架，通过测量模型答案分布向正确答案转变的程度来评估上下文的实用性，摆脱了传统基于词汇重叠的相似性方法。

Result: 在LongMemEval和LoCoMo基准测试中，ECS在细粒度的轮次选择任务中使用Llama-3.1-8B模型实现了F1=0.265，相较传统的TF-IDF方法提升了71.83%。

Conclusion: 本论文提出的Entropic Context Shaping (ECS)方法有效区分了有助于回答问题的上下文信息和干扰信息，显著提升了多轮对话中的上下文选择性能。

Abstract: Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.

</details>


### [13] [Towards AGI A Pragmatic Approach Towards Self Evolving Agent](https://arxiv.org/abs/2601.11658)
*Indrajit Kar,Sammy Zonunpuia,Zonunfeli Ralte*

Main category: cs.CL

TL;DR: 本研究提出了一种集多种学习机制的层级自我进化多智能体框架，实现了LLM智能体的自主能力扩展和持续进化，在复杂任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）驱动的智能体在部署后能力固定，缺乏自我扩展、新工具生成和推理进化的能力。

Method: 提出了一个层级自我进化多智能体框架，整合基础LLM、操作SLM智能体、代码生成LLM及教师LLM，实现持续适应。工作流程包括尝试任务、工具合成、以及通过课程学习、基于奖励的学习或遗传算法进行进化。

Result: 在包含层级任务、工具使用轨迹和难度分级的TaskCraft数据集上评估，课程学习能快速恢复和泛化，基于奖励的学习在高难度任务表现优异，遗传算法提供高度行为多样性。进化后的智能体在各项指标均优于原始智能体，展现出鲁棒、自主、自我提升的进化能力。

Conclusion: 该框架有效实现了智能体的自主进化和持续自我提升，弥补了传统LLM智能体能力静态的不足，推动了多智能体系统的自适应发展。

Abstract: Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.

</details>


### [14] [RAC: Retrieval-Augmented Clarification for Faithful Conversational Search](https://arxiv.org/abs/2601.11722)
*Ahmed Rayane Kebir,Vincent Guigue,Lynda Said Lhadj,Laure Soulier*

Main category: cs.CL

TL;DR: 提出RAC框架，通过检索增强和对比优化，生成更加基于语料的可信澄清问题，显著提升对话检索系统表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏将澄清问题与底层语料库紧密结合，导致产生无法从文档中回答的问题。

Method: 设计了RAC框架，优化索引策略，微调大语言模型，利用对比偏好优化促进基于证据的问题生成。

Result: RAC在四个基准测试中显著优于基线方法，在多项新颖指标上验证了生成问题的语料库锚定性和可信度提升。

Conclusion: RAC方法有效提升了对话检索系统中澄清问题的语料库可信度和准确性。

Abstract: Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented Clarification), a framework for generating corpus-faithful clarification questions. After comparing several indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the generation of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over ungrounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge assessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demonstrate that our approach consistently enhances faithfulness.

</details>


### [15] [Bridging Human Interpretation and Machine Representation: A Landscape of Qualitative Data Analysis in the LLM Era](https://arxiv.org/abs/2601.11739)
*Xinyu Pi,Qisen Yang,Chuong Nguyen,Hua Shen*

Main category: cs.CL

TL;DR: 本文建立了一个4×4框架分析LLM在定性研究中的应用，发现现有系统多停留在低层次描述与静态建模，未来应增强解释性推断和动态建模能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM系统在支持定性研究时，输出质量差异大，缺乏对不同理解层次和建模方式的明确区分，需要一个明确框架帮助理解和提升系统的解释力和模型能力。

Method: 提出一个4×4的框架，结合四个意义层次（描述性、分类性、解释性、理论性）与四个建模层次（静态结构、阶段/时间线、因果路径、反馈动态），并将该框架应用于已有LLM自动化系统的分析。

Result: 通过该4×4框架分析发现现有LLM系统普遍偏重低层意义和低承诺建模，提出未来应发展能够明确、可选择、可管理解释及建模层次的LLM系统。

Conclusion: 现有基于大型语言模型（LLM）的系统多停留在低层次的描述和结构建模，缺乏对解释性和理论推断以及动态建模的可靠尝试。

Abstract: LLMs are increasingly used to support qualitative research, yet existing systems produce outputs that vary widely--from trace-faithful summaries to theory-mediated explanations and system models. To make these differences explicit, we introduce a 4$\times$4 landscape crossing four levels of meaning-making (descriptive, categorical, interpretive, theoretical) with four levels of modeling (static structure, stages/timelines, causal pathways, feedback dynamics). Applying the landscape to prior LLM-based automation highlights a strong skew toward low-level meaning and low-commitment representations, with few reliable attempts at interpretive/theoretical inference or dynamical modeling. Based on the revealed gap, we outline an agenda for applying and building LLM-systems that make their interpretive and modeling commitments explicit, selectable, and governable.

</details>


### [16] [LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text](https://arxiv.org/abs/2601.11746)
*George Mihaila,Suleyman Olcay Polat,Poli Nemkova,Himanshu Sharma,Namratha V. Urs,Mark V. Albert*

Main category: cs.CL

TL;DR: 提出LIME-LLM，利用假设驱动的受控扰动替代随机屏蔽，显著提升NLP黑盒模型局部解释的忠实度，优于传统及生成式方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机符号屏蔽的局部解释方法在NLP中生成了语义无效且分布外的扰动，降低了解释模型的忠实度；而最新生成式方法采用无约束的意译，导致混淆变量难以分离特征贡献，因此需要一种更准确且控制性更强的扰动方法。

Method: LIME-LLM提出了“一掩码-一样本”的严格协议，结合中性填充和边界填充策略，利用大语言模型生成流畅且在数据分布内的扰动样本，从而构建严谨隔离特征效应的邻域。

Result: 在CoLA、SST-2和HateXplain三个基准数据集上，LIME-LLM相较于LIME、SHAP、Integrated Gradients及生成式LLiMe均实现了本地解释忠实度的显著提升，通过人工注释的理由作为金标准验证效果。

Conclusion: LIME-LLM通过假设驱动的控制性扰动替代随机噪声，实现了语义有效且严谨的邻域生成，显著提升了本地解释模型的忠实度，优于传统和生成式的解释方法，成为黑盒NLP可解释性的新的基准。

Abstract: Local explanation methods such as LIME (Ribeiro et al., 2016) remain fundamental to trustworthy AI, yet their application to NLP is limited by a reliance on random token masking. These heuristic perturbations frequently generate semantically invalid, out-of-distribution inputs that weaken the fidelity of local surrogate models. While recent generative approaches such as LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large Language Models for neighborhood generation, they rely on unconstrained paraphrasing that introduces confounding variables, making it difficult to isolate specific feature contributions. We introduce LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations. By enforcing a strict "Single Mask-Single Sample" protocol and employing distinct neutral infill and boundary infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. We evaluate our method against established baselines (LIME, SHAP, Integrated Gradients) and the generative LLiMe baseline across three diverse benchmarks: CoLA, SST-2, and HateXplain using human-annotated rationales as ground truth. Empirical results demonstrate that LIME-LLM establishes a new benchmark for black-box NLP explainability, achieving significant improvements in local explanation fidelity compared to both traditional perturbation-based methods and recent generative alternatives.

</details>


### [17] [Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation](https://arxiv.org/abs/2601.11758)
*Arnab Das Utsa*

Main category: cs.CL

TL;DR: 本研究提出一种透明、语言解释性的社交媒体焦虑检测方法，利用Reddit数据和临床验证，展示了模型的准确性、稳健性和通用性，推动了可解释心理健康筛查的发展。


<details>
  <summary>Details</summary>
Motivation: 焦虑影响众多个体，现有大规模筛查手段有限，社交媒体语言为可扩展检测提供机会，但现有模型缺乏解释性、关键词稳健性验证及用户级数据完整性保障。

Method: 通过对Reddit大量帖子数据进行处理，使用语言学解释性强的特征构建逻辑回归分类器，进行特征消融、关键词遮蔽及密度差异分析等全面评估，并结合来自临床访谈的焦虑诊断数据进行跨领域验证。

Result: 模型在去除情感词或关键词遮蔽后依然保持高准确率，利用最少的发帖历史即可实现早期检测，且跨领域分析显示与临床数据高度一致。

Conclusion: 本研究表明，基于透明且语言可解释的特征模型，能够实现可靠、通用且对关键词稳健的焦虑检测。

Abstract: Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, we trained a logistic regression classifier on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.

</details>


### [18] [Industry-Aligned Granular Topic Modeling](https://arxiv.org/abs/2601.11762)
*Sae Young Moon,Myeongjun Erik Jang,Haoyan Luo,Chunyang Xiao,Antonios Georgiadis,Fran Silavong*

Main category: cs.CL

TL;DR: 本文提出了基于大语言模型的细粒度主题建模框架TIDE，性能优越并具备实用工业应用功能，具备较大应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法在细粒度主题提取能力不足，而细粒度主题对商业应用能提供更深入的洞察，因此提出新框架以解决这一问题。

Method: 基于大语言模型的细粒度主题建模方法，并结合文档摘要、主题层级关系构建和主题提炼等辅助功能。

Result: 通过在多个公开和真实业务数据集上的大量实验，证明TIDE的主题建模性能优越且辅助组件对工业应用场景有帮助。

Conclusion: 本文提出的TIDE框架利用大语言模型实现了细粒度主题建模，显著优于现有方法，并提供了多种辅助功能，适用于工业业务场景。

Abstract: Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.

</details>


### [19] [Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models](https://arxiv.org/abs/2601.11776)
*Kaituo Zhang,Zhimeng Jiang,Na Zou*

Main category: cs.CL

TL;DR: 本文利用大型语言模型自带的自我纠错能力，设计了无需外部干预的自动有害内容消除框架，效果优异，提升了文本生成的安全性与责任感。


<details>
  <summary>Details</summary>
Motivation: 当前的有害内容消除技术依赖外部模块和人工标注，难以扩展且一致性差，而大型语言模型（LLMs）具有自我纠正和自我奖励的内在能力未被充分利用。

Method: 提出了一个完全自我反思的有害内容消除框架，利用LLMs的内在能力进行有害内容检测和修正，设计了有害信号检测器和系统化的干预流程，迭代生成消毒数据集以微调模型。

Result: 实验表明该方法在DetoxLLM和ParaDetox基准数据集上，消毒效果优于最新技术且保持语义一致性，无需人工干预和外部组件。

Conclusion: 本文揭示了LLMs内在的自我消毒能力，提供了一种一致且有效的自动消毒方法，推动了自我调节语言模型的发展，实现更具责任感和伦理指导的文本生成。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fully self-reflective detoxification framework that harnesses the inherent capacities of LLMs to detect, correct toxic content, and refine LLMs without external modules and data annotation. Specifically, we propose a Toxic Signal Detector --an internal self-identification mechanism, coupled with a systematic intervention process to transform toxic text into its non-toxic counterpart. This iterative procedure yields a contrastive detoxification dataset used to fine-tune the model, enhancing its ability for safe and coherent text generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox show that our method achieves better detoxification performance than state-of-the-art methods while preserving semantic fidelity. By obviating the need for human intervention or external components, this paper reveals the intrinsic self-detoxification ability of LLMs, offering a consistent and effective approach for mitigating harmful content generation. Ultimately, our findings underscore the potential for truly self-regulated language models, paving the way for more responsible and ethically guided text generation systems.

</details>


### [20] [Translation as a Scalable Proxy for Multilingual Evaluation](https://arxiv.org/abs/2601.11778)
*Sheriff Issaka,Erick Rosas Gonzalez,Lieqi Liu,Evans Kofi Agyei,Lucas Bandarkar,Nanyun Peng,David Ifeoluwa Adelani,Francisco Guzmán,Saadia Gabriel*

Main category: cs.CL

TL;DR: 研究表明大型语言模型的翻译质量可作为快速、低成本的多语言能力评估代理指标，解决多语言评估中存在的规模及资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前多语言评估存在语言覆盖不足和基准构建成本高等问题，探索是否可用翻译质量作为多语言能力的经济有效评估方法。

Method: 系统评估了14个不同规模模型在9个多样基准和7个翻译指标上的表现，比较翻译质量与多语言任务成功率的相关性。

Result: 翻译绩效与多语言任务成功高度相关（Pearson相关系数最高达0.91），表明翻译代表的能力与多语言理解能力重叠。

Conclusion: 翻译质量是大型语言模型多语言能力的良好指标，能够有效预测其在下游任务中的表现。

Abstract: The rapid proliferation of LLMs has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000 languages in an empirical void. Traditional benchmark construction faces scaling challenges such as cost, scarcity of domain experts, and data contamination. We evaluate the validity of a simpler alternative: can translation quality alone indicate a model's broader multilingual capabilities? Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse benchmarks and 7 translation metrics, we find that translation performance is a good indicator of downstream task success (e.g., Phi-4, median Pearson r: MetricX = 0.89, xCOMET = 0.91, SSA-COMET = 0.87). These results suggest that the representational abilities supporting faithful translation overlap with those required for multilingual understanding. Translation quality, thus emerges as a strong, inexpensive first-pass proxy of multilingual performance, enabling a translation-first screening with targeted follow-up for specific tasks.

</details>


### [21] [Beyond Tokens: Concept-Level Training Objectives for LLMs](https://arxiv.org/abs/2601.11791)
*Laya Iyer,Pranav Somani,Alice Guo,Dan Jurafsky,Chen Shani*

Main category: cs.CL

TL;DR: 本文提出用概念级监督替代传统的单词级预测训练，提升大语言模型的语义理解能力和泛化效果。


<details>
  <summary>Details</summary>
Motivation: 传统的单词预测目标忽视了词汇之间的语义等价性，导致模型对表层形式依赖过重，限制了模型的语义表达能力。

Method: 提出概念级预测方法，将表述同一概念的不同词汇归为同一类别，并将其整合到训练中，提升模型对语义的理解。

Result: 概念级监督训练的模型在困惑度、领域转移稳健性及多项自然语言处理任务中表现优于传统基于单词预测的模型。

Conclusion: 用概念级监督替代传统的基于单词的训练目标，可以使大语言模型在语义理解和泛化能力上表现更优，减少对表层形式的依赖。

Abstract: The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., ``mom'' vs. ``mother''). As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., ``mom,'' ``mommy,'' ``mother'' $\rightarrow$ \textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests \textit{concept-level supervision} as an improved training signal that better aligns LLMs with human semantic abstractions.

</details>


### [22] [TWeddit : A Dataset of Triggering Stories Predominantly Shared by Women on Reddit](https://arxiv.org/abs/2601.11819)
*Shirlene Rose Bandela,Sanjeev Parthasarathy,Vaibhav Garg*

Main category: cs.CL

TL;DR: 该论文发布了涵盖女性相关触发性经历的Reddit数据集TWeddit，填补了触发警告数据标注的空缺，对理解相关话题及道德基础有重要意义。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的流产、性暴力等经历分享可能包含令人不适的内容，而Reddit上手动触发警告使用不足，缺乏相关标注数据集，影响研究和用户体验。

Method: 通过收集并注释Reddit上长篇详细叙述的故事，构建数据集TWeddit，同时进行语言学分析以展示数据集中故事表达的独特主题和道德基础。

Result: 创建了TWeddit数据集，包含带触发警告的故事，并发现这些故事表达了不同的话题和道德观念，数据集对于未来广泛研究具有价值。

Conclusion: 该论文提出了一个经过精心策划的Reddit数据集TWeddit，涵盖了与女性主要面临的问题相关的可能引发不适体验的内容。

Abstract: Warning: This paper may contain examples and topics that may be disturbing to some readers, especially survivors of miscarriage and sexual violence. People affected by abortion, miscarriage, or sexual violence often share their experiences on social media to express emotions and seek support. On public platforms like Reddit, where users can post long, detailed narratives (up to 40,000 characters), readers may be exposed to distressing content. Although Reddit allows manual trigger warnings, many users omit them due to limited awareness or uncertainty about which categories apply. There is scarcity of datasets on Reddit stories labeled for triggering experiences. We propose a curated Reddit dataset, TWeddit, covering triggering experiences related to issues majorly faced by women. Our linguistic analyses show that annotated stories in TWeddit express distinct topics and moral foundations, making the dataset useful for a wide range of future research.

</details>


### [23] [The Third VoicePrivacy Challenge: Preserving Emotional Expressiveness and Linguistic Content in Voice Anonymization](https://arxiv.org/abs/2601.11846)
*Natalia Tomashenko,Xiaoxiao Miao,Pierre Champion,Sarina Meyer,Michele Panariello,Xin Wang,Nicholas Evans,Emmanuel Vincent,Junichi Yamagishi,Massimiliano Todisco*

Main category: cs.CL

TL;DR: 2024年VoicePrivacy挑战赛推进了保护语音隐私的匿名技术，提出系统框架和评估指标，展示多种匿名系统及创新方案，指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 推动语音匿名技术的发展，通过竞赛形式激励创新，提高语音数据在保护说话人身份隐私的同时保留内容和情感状态的能力。

Method: 采用了系统性的挑战框架，设计了语音匿名任务，使用了多样的数据集，并引入了攻击模型和客观评估指标来衡量隐私保护和实用性。

Result: 介绍了六个基准匿名系统以及参赛者开发的创新方法，并总结了关键的洞察和观察结果，为未来的VoicePrivacy挑战赛设计和语音匿名研究指明了方向。

Conclusion: 我们总结了第三届2024年VoicePrivacy挑战赛的成果和分析，强调了语音匿名技术在保护说话人身份隐私中的有效性，同时保持了语音的内容和情感状态。

Abstract: We present results and analyses from the third VoicePrivacy Challenge held in 2024, which focuses on advancing voice anonymization technologies. The task was to develop a voice anonymization system for speech data that conceals a speaker's voice identity while preserving linguistic content and emotional state. We provide a systematic overview of the challenge framework, including detailed descriptions of the anonymization task and datasets used for both system development and evaluation. We outline the attack model and objective evaluation metrics for assessing privacy protection (concealing speaker voice identity) and utility (content and emotional state preservation). We describe six baseline anonymization systems and summarize the innovative approaches developed by challenge participants. Finally, we provide key insights and observations to guide the design of future VoicePrivacy challenges and identify promising directions for voice anonymization research.

</details>


### [24] [CTPD: Cross Tokenizer Preference Distillation](https://arxiv.org/abs/2601.11865)
*Truong Nguyen,Phi Van Dat,Ngan Nguyen,Linh Ngo Van,Trung Le,Thanh Hong Nguyen*

Main category: cs.CL

TL;DR: 提出一种跨不同分词器的偏好蒸馏框架CTPD，通过字符级对齐和重要性采样等技术，实现语言模型与人类偏好的高效对齐，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前知识蒸馏在语言模型与人类偏好对齐中的应用较少，尤其是在跨不同分词器的现实场景中存在难题。

Method: 提出了跨分词器偏好蒸馏（CTPD）框架，包括对齐跨度投影、跨分词器的重要性采样(TIS-DPO)及教师锚定参考机制。

Result: 多基准实验验证了CTPD的有效性，性能显著优于现有方法。

Conclusion: CTPD为解决异构分词器间偏好蒸馏提供了实用且通用的方案，有助于更加高效地对齐语言模型与人类偏好。

Abstract: While knowledge distillation has seen widespread use in pre-training and instruction tuning, its application to aligning language models with human preferences remains underexplored, particularly in the more realistic cross-tokenizer setting. The incompatibility of tokenization schemes between teacher and student models has largely prevented fine-grained, white-box distillation of preference information. To address this gap, we propose Cross-Tokenizer Preference Distillation (CTPD), the first unified framework for transferring human-aligned behavior between models with heterogeneous tokenizers. CTPD introduces three key innovations: (1) Aligned Span Projection, which maps teacher and student tokens to shared character-level spans for precise supervision transfer; (2) a cross-tokenizer adaptation of Token-level Importance Sampling (TIS-DPO) for improved credit assignment; and (3) a Teacher-Anchored Reference, allowing the student to directly leverage the teacher's preferences in a DPO-style objective. Our theoretical analysis grounds CTPD in importance sampling, and experiments across multiple benchmarks confirm its effectiveness, with significant performance gains over existing methods. These results establish CTPD as a practical and general solution for preference distillation across diverse tokenization schemes, opening the door to more accessible and efficient alignment of language models.

</details>


### [25] [Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving](https://arxiv.org/abs/2601.11866)
*Kie Shidara,Preethi Prem,Jonathan Kim,Anna Podlasek,Feng Liu,Ahmed Alaa,Danilo Bernardo*

Main category: cs.CL

TL;DR: 研究表明，先进的推理型大语言模型在医学问题推理上展现出接近人类的灵活性和准确性，能有效避免因固定思维模式带来的推理错误。


<details>
  <summary>Details</summary>
Motivation: 探究推理型大语言模型在临床医学推理中的认知灵活性，特别是它们对固有思维陷阱的抵抗能力。

Method: 评估了OpenAI、Grok、Gemini、Claude和DeepSeek等模型在医学抽象和推理语料库(mARC)上的表现，该语料库设计利用Einstellung效应诱导模型过度依赖已有模式。

Result: 表现最强的5个模型在医生最常错答的问题上，正确率达到55%-70%，且置信度较高，显示这些模型对Einstellung效应的敏感度低于人类。

Conclusion: 强大的推理型大语言模型在医学推理方面表现出较高的灵活性，能够避免因固有思维模式导致的错误，更接近人类水平。

Abstract: Large Language Models (LLMs) have achieved high accuracy on medical question-answer (QA) benchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked whether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We assessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the medicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which utilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts where they become suboptimal. We found that strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC. On questions most commonly missed by physicians, the top 5 performing models answered 55% to 70% correctly with high confidence, indicating that these models may be less susceptible than humans to Einstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility in medical reasoning, achieving performance on par with humans on mARC.

</details>


### [26] [GloCTM: Cross-Lingual Topic Modeling via a Global Context Space](https://arxiv.org/abs/2601.11872)
*Nguyen Tien Phat,Ngo Vu Minh,Linh Van Ngo,Nguyen Thi Ngoc Diep,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 提出一种基于统一语义空间的跨语言主题模型GloCTM，通过扩展输入和多层对齐机制提升主题连贯性和跨语言语义对齐，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题模型在语言特定空间中学习主题，并依赖对齐机制，这些方法未能深度捕捉跨语言语义，且忽视了多语言预训练表示中的语义信号，限制了细粒度对齐能力。

Method: 提出GloCTM框架，通过统一语义空间实现跨语言主题对齐。输入扩展为跨语言词汇邻域，结合局部和全局编码器，并通过内部正则化对齐潜在表示。输出层通过全局主题-词分布同步跨语言主题语义，并引入中心核对齐（CKA）损失以对齐潜在主题空间与多语言上下文嵌入。

Result: 多基准实验显示，GloCTM在主题连贯性和跨语言对齐方面显著优于强基线模型。

Conclusion: GloCTM有效整合多语言语义信息，提升了跨语言主题模型的语义一致性和对齐质量，推动了多语言理解任务的发展。

Abstract: Cross-lingual topic modeling seeks to uncover coherent and semantically aligned topics across languages - a task central to multilingual understanding. Yet most existing models learn topics in disjoint, language-specific spaces and rely on alignment mechanisms (e.g., bilingual dictionaries) that often fail to capture deep cross-lingual semantics, resulting in loosely connected topic spaces. Moreover, these approaches often overlook the rich semantic signals embedded in multilingual pretrained representations, further limiting their ability to capture fine-grained alignment. We introduce GloCTM (Global Context Space for Cross-Lingual Topic Model), a novel framework that enforces cross-lingual topic alignment through a unified semantic space spanning the entire model pipeline. GloCTM constructs enriched input representations by expanding bag-of-words with cross-lingual lexical neighborhoods, and infers topic proportions using both local and global encoders, with their latent representations aligned through internal regularization. At the output level, the global topic-word distribution, defined over the combined vocabulary, structurally synchronizes topic meanings across languages. To further ground topics in deep semantic space, GloCTM incorporates a Centered Kernel Alignment (CKA) loss that aligns the latent topic space with multilingual contextual embeddings. Experiments across multiple benchmarks demonstrate that GloCTM significantly improves topic coherence and cross-lingual alignment, outperforming strong baselines.

</details>


### [27] [Faithfulness vs. Safety: Evaluating LLM Behavior Under Counterfactual Medical Evidence](https://arxiv.org/abs/2601.11886)
*Kaijie Mo,Siddhartha Venkatayogi,Chantal Shaib,Ramez Kouzy,Wei Xu,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 本文通过一个反事实医疗问答数据集，发现当前大型语言模型在面对不实甚至危险医疗证据时，依然盲目信任并给出过于自信的回答，暴露出信实性与安全性之间缺乏界限的问题。


<details>
  <summary>Details</summary>
Motivation: 探究在医疗等高风险领域，当模型所接收的上下文信息与其先验知识或安全协议不一致时，模型如何推理和表现。

Method: 构建了MedCounterFact这一反事实医疗问答数据集，通过系统替换真实医疗干预手段，生成四种类型的反事实刺激，进而评估多种前沿大型语言模型的表现。

Result: 实验结果显示，模型在面对反事实医疗证据时，会盲目接受不合理甚至危险的信息，未能有效区分可信信息与对抗性信息，表明尚未实现信实性与安全性的有效界限。

Conclusion: 现有大型语言模型在面对反事实或对抗性医疗证据时，往往盲目接受这些信息，缺乏对危险或不合理内容的警惕，表现出过度自信且无保留的回答。

Abstract: In high-stakes domains like medicine, it may be generally desirable for models to faithfully adhere to the context provided. But what happens if the context does not align with model priors or safety protocols? In this paper, we investigate how LLMs behave and reason when presented with counterfactual or even adversarial medical evidence. We first construct MedCounterFact, a counterfactual medical QA dataset that requires the models to answer clinical comparison questions (i.e., judge the efficacy of certain treatments, with evidence consisting of randomized controlled trials provided as context). In MedCounterFact, real-world medical interventions within the questions and evidence are systematically replaced with four types of counterfactual stimuli, ranging from unknown words to toxic substances. Our evaluation across multiple frontier LLMs on MedCounterFact reveals that in the presence of counterfactual evidence, existing models overwhelmingly accept such "evidence" at face value even when it is dangerous or implausible, and provide confident and uncaveated answers. While it may be prudent to draw a boundary between faithfulness and safety, our findings reveal that there exists no such boundary yet.

</details>


### [28] [PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning](https://arxiv.org/abs/2601.11908)
*Byeongjin Kim,Gyuwan Kim,Seo Yeon Park*

Main category: cs.CL

TL;DR: 针对大语言模型长上下文推理中计划生成不可靠的问题，提出PPA-Plan主动预防错误假设和逻辑陷阱，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长上下文推理中因依赖表层线索导致计划生成不可靠，进而影响执行效果，且错误计划难以及时识别和修改，限制了反应式优化的有效性。

Method: 提出PPA-Plan策略，在计划生成前预防可能的失败，识别潜在的逻辑陷阱和错误假设，将其作为负面约束，并在生成计划时显式避免这些约束。

Result: 在长上下文问答基准测试中，PPA-Plan生成的计划执行效果稳定优于现有的计划-执行方法和直接提示策略。

Conclusion: PPA-Plan通过主动识别并避免逻辑陷阱和错误假设，有效提升了长上下文推理中计划生成的可靠性，显著优于现有的计划执行框架和直接提示方法。

Abstract: Large language models (LLMs) struggle with reasoning over long contexts where relevant information is sparsely distributed. Although plan-and-execute frameworks mitigate this by decomposing tasks into planning and execution, their effectiveness is often limited by unreliable plan generation due to dependence on surface-level cues. Consequently, plans may be based on incorrect assumptions, and once a plan is formed, identifying what went wrong and revising it reliably becomes difficult, limiting the effectiveness of reactive refinement. To address this limitation, we propose PPA-Plan, a proactive planning strategy for long-context reasoning that focuses on preventing such failures before plan generation. PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and conditions plan generation on explicitly avoiding these constraints. Experiments on long-context QA benchmarks show that executing plans generated by PPA-Plan consistently outperforms existing plan-and-execute methods and direct prompting.

</details>


### [29] [LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding](https://arxiv.org/abs/2601.11913)
*Yichen Jiang,Peng Ye,Jiakang Yuan,Chongjun Tu,Lei Bai,Tao Chen*

Main category: cs.CL

TL;DR: 本文提出的LSTM-MAS多智能体系统借鉴LSTM结构，有效处理长文上下文，显著提升多项长文本理解任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有单一大语言模型方法受限于计算成本或上下文长度，多智能体框架虽可缓解，但依然存在错误累积和幻觉传播问题，亟需一种机制以有效处理长上下文并保持信息准确性。

Method: 设计了一个模仿LSTM结构的多智能体链式架构，包括负责分段理解的工作智能体、减少冗余的过滤智能体、检测错误的评判智能体及全局调控信息传播和保留的管理智能体，实现选择性长期依赖建模和信息控制传递。

Result: 在NarrativeQA、Qasper、HotpotQA和MuSiQue四个数据集上，LSTM-MAS相比目前最佳的多智能体方法CoA，性能提升分别达到40.93%、43.70%、121.57%和33.12%。

Conclusion: 本文提出的LSTM-MAS多智能体系统通过模拟LSTM的层次信息流和门控记忆机制，有效解决了大语言模型处理长文本上下文时的错误积累和幻觉传播问题，显著提升了多项长文本理解任务的性能。

Abstract: Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.

</details>


### [30] [Enhancing LLM-Based Data Annotation with Error Decomposition](https://arxiv.org/abs/2601.11920)
*Zhen Xu,Vedant Khatri,Yijun Dai,Xiner Liu,Siyan Li,Xuanming Zhang,Renzhe Yu*

Main category: cs.CL

TL;DR: 本文提出一种结合人工参与的诊断性评估方法，细化分析大语言模型在主观标注任务中的错误类型，提升标注质量的理解与评价，验证其在教育标注任务中的有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在客观标注任务中表现接近人类，但在涉及心理构念等主观标注任务中表现不稳定，且常用单一对齐指标无法区分不同类型的错误，导致对模型标注质量评价不足。

Method: 提出诊断分类法，将LLM标注错误按照来源（模型特异 vs. 任务内在）和类型（边界歧义 vs. 概念误识别）进行分类；设计轻量级人工标注测试以估计任务内在歧义；开发计算方法分解观测到的LLM错误。

Result: 验证了诊断评估范式在四个教育标注任务中的有效性，揭示了过高对齐指标在特定任务中不现实的原因，以及单一指标对标注质量反映不充分的问题。该范式可作为评价LLM标注适用性和技术优化的低成本诊断工具。

Conclusion: 本研究提出了一种诊断性评估范式，通过引入人工参与过程，将任务内在的歧义与模型驱动的错误区分开来，精细化评估大语言模型（LLM）在主观标注任务中的标注质量。实验证明该范式在教育领域四个标注任务上具有概念有效性和实用价值。

Abstract: Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.

</details>


### [31] [Mapping the maturation of TCM as an adjuvant to radiotherapy](https://arxiv.org/abs/2601.11923)
*P. Bilha Githinji,Aikaterini Melliou,Xi Yuan,Dayan Zhang,Lian Zhang,Zhenglin Chen,Jiansong Ji,Chengying Lv,Jinhao Xu,Peiwu Qin,Dongmei Yu*

Main category: cs.CL

TL;DR: 该文通过大规模文献分析揭示了中医辅助放疗研究领域的五大主题和周期性演化，指出该领域研究趋于成熟并存在积极报告偏倚，暗示未来可能出现新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨中医融入肿瘤学辅助放疗的研究进展与轨迹，评估该领域的发展态势和未来潜力。

Method: 通过分析2000至2025年间69745篇相关文献，运用主题建模方法识别领域内五大主题轴并揭示了领域的演化周期。

Result: 发现中医辅助放疗的研究围绕癌症类型、支持性护理、临床终点、机制和方法学五大主题，表现为以患者为中心、系统导向的跨主题整合，且领域演化呈现扩张收缩的循环模式。

Conclusion: 中医作为放疗辅助治疗的研究已经进入成熟阶段，显示出研究议题的深化和可能的研究饱和，同时存在系统性积极报告偏倚。

Abstract: The integration of complementary medicine into oncology represents a paradigm shift that has seen to increasing adoption of Traditional Chinese Medicine (TCM) as an adjuvant to radiotherapy. About twenty-five years since the formal institutionalization of integrated oncology, it is opportune to synthesize the trajectory of evidence for TCM as an adjuvant to radiotherapy. Here we conduct a large-scale analysis of 69,745 publications (2000 - 2025), emerging a cyclical evolution defined by coordinated expansion and contraction in publication output, international collaboration, and funding commitments that mirrors a define-ideate-test pattern. Using a theme modeling workflow designed to determine a stable thematic structure of the field, we identify five dominant thematic axes - cancer types, supportive care, clinical endpoints, mechanisms, and methodology - that signal a focus on patient well-being, scientific rigor and mechanistic exploration. Cross-theme integration of TCM is patient-centered and systems-oriented. Together with the emergent cycles of evolution, the thematic structure demonstrates progressive specialization and potential defragmentation of the field or saturation of existing research agenda. The analysis points to a field that has matured its current research agenda and is likely at the cusp of something new. Additionally, the field exhibits positive reporting of findings that is homogeneous across publication types, thematic areas, and the cycles of evolution suggesting a system-wide positive reporting bias agnostic to structural drivers.

</details>


### [32] [Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes](https://arxiv.org/abs/2601.11932)
*Abdullah Al Monsur,Nitesh Vamshi Bommisetty,Gene Louis Kim*

Main category: cs.CL

TL;DR: 本文指出传统事件检测模型因单向结构及评价指标偏差存在局限，提出利用句子上下文和LoRA微调提升了模型在长尾事件类别上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前事件检测研究存在两个主要不足：解码器模型的单向限制和过度依赖Micro-F1分数评价模型性能，忽视了长尾事件类别的表现。

Method: 本研究通过引入句子上下文信息增强传统的解码器模型，并在微调过程中采用低秩适配（LoRA）技术来提升模型性能。

Result: 实验结果表明，句子上下文增强模型在Macro-F1指标上显著优于传统解码器模型，LoRA微调进一步提升了模型对长尾事件类别的识别能力。

Conclusion: 本研究发现，采用句子上下文增强的模型在事件检测任务中表现优于传统的仅解码器模型，并且微调时使用低秩适配（LoRA）技术显著提升了模型对长尾事件类别的Macro-F1分数。

Abstract: The current state of event detection research has two notable re-occurring limitations that we investigate in this study. First, the unidirectional nature of decoder-only LLMs presents a fundamental architectural bottleneck for natural language understanding tasks that depend on rich, bidirectional context. Second, we confront the conventional reliance on Micro-F1 scores in event detection literature, which systematically inflates performance by favoring majority classes. Instead, we focus on Macro-F1 as a more representative measure of a model's ability across the long-tail of event types. Our experiments demonstrate that models enhanced with sentence context achieve superior performance over canonical decoder-only baselines. Using Low-Rank Adaptation (LoRA) during finetuning provides a substantial boost in Macro-F1 scores in particular, especially for the decoder-only models, showing that LoRA can be an effective tool to enhance LLMs' performance on long-tailed event classes.

</details>


### [33] [Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence](https://arxiv.org/abs/2601.11956)
*Yuyin Lu,Ziran Liang,Yanghui Rao,Wenqi Fan,Fu Lee Wang,Qing Li*

Main category: cs.CL

TL;DR: 该论文提出DoublyCal框架，通过双重校准技术有效提升KG增强LLM的推理准确性及置信度校准，解决了推理中的不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前KG增强的LLM推理方法难以量化检索证据和推理过程中的认知不确定性，导致推理结果缺乏置信度的可靠性。

Method: 提出了基于双重校准原理的DoublyCal框架，利用轻量级代理模型生成知识图谱证据及其校准置信度，随后引导黑盒LLM进行推理，实现更准确且置信度可追溯的预测。

Result: 在知识密集型基准测试中，DoublyCal在提升黑盒LLM的准确率和置信度校准方面表现出显著优势，且代价低廉（低token消耗）。

Conclusion: DoublyCal显著提升了基于知识图谱的LLM推理的准确性和置信度校准，有效解决了推理过程中的不确定性问题。

Abstract: Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.

</details>


### [34] [PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning](https://arxiv.org/abs/2601.11957)
*Bingxuan Li,Jeonghwan Kim,Cheng Qian,Xiusi Chen,Eitan Anzenberg,Niran Kundapur,Heng Ji*

Main category: cs.CL

TL;DR: 该工作提出了CalConflictBench日程冲突解决基准，发现现有大型语言模型表现不佳，进而提出PEARL强化学习框架，大幅提升模型处理日程冲突的能力。


<details>
  <summary>Details</summary>
Motivation: 繁忙专业人士在日程冲突时需要频繁决策参加、调整或拒绝会议，人工调度耗时且难以规模化，探究是否可以信任大型语言模型来管理时间。

Method: 提出PEARL强化学习框架，结合外部记忆模块和圆次奖励设计，逐步推断与适应用户偏好，从而提高日程冲突解决的准确性。

Result: 在CalConflictBench基准测试上，PEARL相较最强基线模型错误率降低了55%，错误减少率达0.76，表现大幅提升。

Conclusion: 当前大型语言模型在日程冲突解决任务中表现欠佳，存在较高错误率。通过引入强化学习框架PEARL，结合外部记忆模块和轮次奖励优化，能够显著提升模型推断和适应用户偏好的能力，显著降低错误率。

Abstract: Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.

</details>


### [35] [$\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models](https://arxiv.org/abs/2601.11969)
*Zecheng Tang,Baibei Ji,Ruoxi Sun,Haitian Wang,WangJie You,Zhang Yijun,Wenpeng Zhu,Ji Qi,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出首个系统性评测奖励模型评估长期记忆管理能力的基准MemoryRewardBench，涵盖多种任务和超长上下文，揭示当前模型的表现及不足。


<details>
  <summary>Details</summary>
Motivation: 有效的长期记忆管理对大型语言模型处理长上下文至关重要，而需要使用奖励模型来自动且可靠地评估记忆质量。

Method: 本文提出了MemoryRewardBench基准，包含10种不同内存管理模式和8K到128K令牌长度的长期上下文理解及生成任务，评估了13个先进奖励模型的记忆管理表现。

Result: 评测显示开源模型与专有模型的性能差距正在缩小，新一代模型不论参数大小均优于前代模型，同时揭示了现有奖励模型在不同设置下评估记忆管理的能力与限制。

Conclusion: 当前的奖励模型在评估大型语言模型的长期记忆管理方面表现出一定能力，但仍存在基本的限制，特别是在不同内存管理模式和超长上下文下。

Abstract: Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.

</details>


### [36] [Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning](https://arxiv.org/abs/2601.12019)
*Chaowei Zhang,Xiansheng Luo,Zewei Zhang,Yi Zhu,Jipeng Qiang,Longwei Wang*

Main category: cs.CL

TL;DR: 本文通过利用大语言模型的奉承性偏差生成对立推理对，设计了新的点击诱饵检测框架，实现了比现有方法更优的检测效果。


<details>
  <summary>Details</summary>
Motivation: 面对点击诱饵标题普遍存在问题，且大语言模型受到奉承性偏差影响，提出利用这种偏差生成对立观点推理以提升检测效果。

Method: 设计了SORG框架用于生成支持和反对的推理对，并基于三个BERT编码器构建ORCD模型结合对比学习和软标签进行点击诱饵检测。

Result: 在三个基准数据集上的实验表明，该方法在检测准确性和稳健性方面均超过了现有的LLM提示、微调模型及最先进基线方法。

Conclusion: 本研究提出的SORG框架和ORCD模型显著提升了点击诱饵检测的性能，验证了利用对立观点推理生成的有效性。

Abstract: The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users' beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.

</details>


### [37] [Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection](https://arxiv.org/abs/2601.12033)
*Muhammad Alif Al Hakim,Alfan Farizki Wicaksono,Fajri Koto*

Main category: cs.CL

TL;DR: 本研究系统分析了量化对大语言模型公平性和安全性的影响，发现量化带来负面影响，提出关键权重保护技术以减轻风险，保持模型的可信度和效率。


<details>
  <summary>Details</summary>
Motivation: 虽然量化在降低大语言模型计算成本方面广泛应用，但其对公平性和安全性的影响，尤其是在动态量化和多语言环境中，尚未得到充分研究。

Method: 系统性地比较了静态量化和动态量化方法在多语言、多任务（公平性和安全性）基准测试上的表现，并提出了关键权重保护技术来保护重要权重，避免量化带来的性能下降。

Result: 量化损害了模型的公平性和安全性，动态量化比静态量化更稳定，不同语言的公平性表现存在差异，非英语语言的安全性下降更严重。关键权重保护技术可以有效缓解这些问题，无需昂贵的再训练或对齐。

Conclusion: 量化普遍降低了大语言模型的公平性和安全性，动态量化在稳定性方面优于静态量化，不同语言的公平性受损情况不同，非英语环境下的安全性下降更为突出。通过引入关键权重保护技术，可以有效缓解这些负面影响，保持模型的可信性和效率。

Abstract: Quantization is widely adopted to reduce the computational cost of large language models (LLMs); however, its implications for fairness and safety, particularly in dynamic quantization and multilingual contexts, remain underexplored. In this work, we conduct a systematic study of how static and dynamic quantization methods impact fairness and safety across benchmarks measuring intrinsic and extrinsic bias and safety alignment. For fairness, we evaluate English, French, Dutch, Spanish, and Turkish; for safety, we focus on English, Korean, and Arabic. Our findings reveal that quantization consistently degrades fairness and safety, with dynamic methods demonstrating greater stability than static ones. Moreover, fairness degradation varies across languages, while safety deterioration is especially pronounced in non-English settings. To address these risks, we introduce Critical Weight Protection, a novel technique that identifies and preserves fairness- and safety-critical weights during quantization. This approach effectively mitigates bias and safety deterioration without costly retraining or alignment, maintaining trustworthiness while retaining efficiency.

</details>


### [38] [Don't Start Over: A Cost-Effective Framework for Migrating Personalized Prompts Between LLMs](https://arxiv.org/abs/2601.12034)
*Ziyi Zhao,Chongming Gao,Yang Zhang,Haoyan Liu,Weinan Gan,Huifeng Guo,Yong Liu,Fuli Feng*

Main category: cs.CL

TL;DR: PUMA是一种轻量级适配器框架，可高效迁移个性化提示，降低计算成本，实现模型升级时的个性化持续性。


<details>
  <summary>Details</summary>
Motivation: 解决个性化软提示在基础模型升级后失效，需要代价高昂的全面重新训练的问题。

Method: 采用参数高效的适配器架构弥合语义差距，结合基于用户组的选择策略降低训练成本。

Result: 在三个大规模数据集上，PUMA方法在保持性能的同时，将计算成本降低了最多98%，且适用于多种模型架构和复杂迁移场景。

Conclusion: 提出的PUMA框架能够高效地迁移个性化软提示，显著降低重新训练成本，并保持或超越从头训练的性能。

Abstract: Personalization in Large Language Models (LLMs) often relies on user-specific soft prompts. However, these prompts become obsolete when the foundation model is upgraded, necessitating costly, full-scale retraining. To overcome this limitation, we propose the Prompt-level User Migration Adapter (PUMA), a lightweight framework to efficiently migrate personalized prompts across incompatible models. PUMA utilizes a parameter-efficient adapter to bridge the semantic gap, combined with a group-based user selection strategy to significantly reduce training costs. Experiments on three large-scale datasets show our method matches or even surpasses the performance of retraining from scratch, reducing computational cost by up to 98%. The framework demonstrates strong generalization across diverse model architectures and robustness in advanced scenarios like chained and aggregated migrations, offering a practical path for the sustainable evolution of personalized AI by decoupling user assets from the underlying models.

</details>


### [39] [Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation](https://arxiv.org/abs/2601.12061)
*Jinsook Lee,Kirk Vanacore,Zhuqian Zhou,Jeanine Grutter,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 该文提出基于注释代码注入的大型语言模型对话分段方法，改进了段内一致性，揭示了分段质量在不同指标间的权衡，强调应根据下游应用优化对话分段。


<details>
  <summary>Details</summary>
Motivation: 现有的Dialogue Act注释将意图局限于单句或单轮中，导致分段边界判断存在不一致，降低了注释的可靠性。

Method: 提出了注释代码注入分段方法，通过下游注释标准来决定边界，并使用大型语言模型（LLM）进行分段，比对传统和检索增强的基线方法。

Result: 发现基于Dialogue Act意识的分段在内部一致性上优于仅基于文本的基线；LLM更擅长生成构造一致的段落，基于连贯性的基线方法则更好地检测对话流的全局变化。没有单一分段器在两个数据集上均占优，分段器在内部一致性、边界区分和人机分布协议之间存在权衡。

Conclusion: 对话分段应视为影响后续任务的重要设计选择，应根据具体下游任务目标进行优化，而非仅追求单一性能指标。

Abstract: Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.

</details>


### [40] [Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset](https://arxiv.org/abs/2601.12068)
*Rowzatul Zannat,Abdullah Al Shafi,Abdul Muntakim*

Main category: cs.CL

TL;DR: 本研究构建了孟加拉语症状疾病数据集，利用多模型集成实现高准确率疾病预测，助力孟加拉语人群健康信息获取。


<details>
  <summary>Details</summary>
Motivation: 针对非英语人群尤其是孟加拉语使用者缺乏可靠健康信息及疾病预测资源的问题，提升其获取医疗服务的公平性。

Method: 构建孟加拉语症状疾病数据集，评估多种机器学习模型对症状进行疾病预测，通过软投票和硬投票集成顶尖模型提升预测准确率。

Result: 数据集公开，模型在孟加拉语症状输入的疾病预测上达到98%准确率，集成方法表现出优异的鲁棒性和泛化能力。

Conclusion: 本研究开发了一个包含758个独特症状-疾病关系、涵盖85种疾病的孟加拉语症状疾病数据集，并通过多模型集成方法实现了98%的疾病预测准确率，构建了孟加拉语疾病预测的基础资源。

Abstract: Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.

</details>


### [41] [To Copy or Not to Copy: Copying Is Easier to Induce Than Recall](https://arxiv.org/abs/2601.12075)
*Mehrdad Farahani,Franziska Penzkofer,Richard Johansson*

Main category: cs.CL

TL;DR: 本文提出一种“仲裁向量”机制，控制语言模型在复制上下文与调用内存知识间的选择，证明复制比调用更易触发，揭示了二者行为机制上的不对称性。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在结合参数知识和上下文信息时如何调节复制和调用知识的行为机制，特别是在面对无关或虚假上下文时的反应差异。

Method: 通过从模型激活中提取“仲裁向量”，并在不同层和位置注入该向量以控制模型行为的方向，在两个模型架构和两个开放领域问答基准上进行实验验证。

Result: 实验显示模型行为可通过“仲裁向量”成功调节，实现复制与调用的切换，且复制的诱导更容易且不依赖特定位置，而调用的恢复更脆弱且需要更具体的干预。

Conclusion: 通过引入和操控“仲裁向量”，研究揭示了语言模型在检索增强环境下在复制上下文和调用参数知识间的机制差异，表明诱导模型复制比恢复调用更容易且更具灵活性。

Abstract: Language models used in retrieval-augmented settings must arbitrate between parametric knowledge stored in their weights and contextual information in the prompt. This work presents a mechanistic study of that choice by extracting an \emph{arbitration vector} from model activations on a curated dataset designed to disentangle (i) irrelevant contexts that elicit parametric recall and (ii) relevant but false contexts that elicit copying. The vector is computed as the residual-stream centroid difference between these regimes across 27 relations, and is injected as an additive intervention at selected layers and token spans to steer behavior in two directions: Copy$\rightarrow$Recall (suppressing context use) and Recall$\rightarrow$Copy (inducing the model to copy any token from the context). Experiments on two architectures (decoder-only and encoder/decoder) and two open-domain QA benchmarks show consistent behavior shifts under moderate scaling while monitoring accuracy and fluency. Mechanistic analyses of attention routing, MLP contributions, and layer-wise probability trajectories reveal an asymmetry: inducing copying is an easy ``reactivation'' process that can be triggered at different locations in the input, while restoring recall is a ``suppression'' process that is more fragile and strongly tied to object-token interventions.

</details>


### [42] [Optimizing User Profiles via Contextual Bandits for Retrieval-Augmented LLM Personalization](https://arxiv.org/abs/2601.12078)
*Linfeng Du,Ye Yuan,Zichen Zhao,Fuyuan Lyu,Emiliano Penaloza,Xiuying Chen,Zipeng Sun,Jikun Kang,Laurent Charlin,Xue Liu,Haolun Wu*

Main category: cs.CL

TL;DR: 提出PURPLE，通过优化用户画像选择策略提升大语言模型个性化响应效果，实验验证优于传统方法，兼顾效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过语义相关性选择用户历史记录来增强大语言模型的响应，但语义相关性并不总能有效提升生成质量，可能因冗余或冲突信息反而带来负面影响。

Method: 提出PURPLE，一种基于上下文赌博机的框架，通过Plackett-Luce排列模型将用户画像构建视为集合生成过程，捕捉记录间复杂依赖关系，利用参考响应的似然度作为密集反馈来优化检索与生成质量的对齐。

Result: 在九个个性化任务上的大量实验表明，PURPLE在效果和效率上均显著优于现有启发式方法和基于检索的增强方法，提供了一个原理明确且可扩展的用户画像优化方案。

Conclusion: PURPLE成功改善了用户历史记录选择策略，使大语言模型的个性化响应更加高效且质量更优，突破了单纯依赖语义相关性的限制。

Abstract: Large Language Models (LLMs) excel at general-purpose tasks, yet adapting their responses to individual users remains challenging. Retrieval augmentation provides a lightweight alternative to fine-tuning by conditioning LLMs on user history records, and existing approaches typically select these records based on semantic relevance. We argue that relevance serves as an unreliable proxy for utility: a record may be semantically similar to a query yet fail to improve generation quality or even degrade it due to redundancy or conflicting information. To bridge this gap, we propose PURPLE, a contextual bandit framework that oPtimizes UseR Profiles for Llm pErsonalization. In contrast to a greedy selection of the most relevant records, PURPLE treats profile construction as a set generation process and utilizes a Plackett-Luce ranking model to capture complex inter-record dependencies. By training with dense feedback provided by the likelihood of the reference response, our method aligns retrieval directly with generation quality. Extensive experiments on nine personalization tasks demonstrate that PURPLE consistently outperforms strong heuristic and retrieval-augmented baselines in both effectiveness and efficiency, establishing a principled and scalable solution for optimizing user profiles.

</details>


### [43] [Large language models struggle with ethnographic text annotation](https://arxiv.org/abs/2601.12099)
*Leonardo S. Goodall,Dor Shilton,Daniel A. Mullins,Harvey Whitehouse*

Main category: cs.CL

TL;DR: 当前大型语言模型在民族志文本注释任务中的表现远不及人类，尚不能取代人类专家。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在自动文本注释中的潜力，希望通过自动化处理加速跨文化研究。

Method: 评估了7个最先进的大型语言模型在121个仪式特征和567段民族志文本中的注释能力。

Result: 模型性能明显不足，低于可靠自动注释的要求；处理较长文本、需要顺序区分的特征及模糊构造尤其困难；模型性能受限于人类编码者一致性水平。

Conclusion: 大型语言模型目前无法替代人类专家进行民族志文本的结构化注释。

Abstract: Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.

</details>


### [44] [Powerful Training-Free Membership Inference Against Autoregressive Language Models](https://arxiv.org/abs/2601.12104)
*David Ilić,David Stanojević,Kostadin Cvejoski*

Main category: cs.CL

TL;DR: EZ-MIA通过分析模型错误预测位置的概率特征，有效提升了会员推断攻击的检测率，揭示了微调语言模型更高的隐私风险，对隐私审计和模型部署有重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有会员推断攻击在实际隐私审计所需的低假阳性阈值下检测率有限，尤其难以检测微调语言模型中记忆的敏感信息。通过发现模型在错误位置的记忆表现，可以提升攻击的有效性。

Method: 提出了EZ-MIA方法，通过引入Error Zone(EZ)分数，利用模型在错误预测位置相较于预训练模型的概率偏移方向不平衡性，无需训练额外模型，仅需两次前向传播即可进行有效会员推断。

Result: 在多个数据集和模型上实验证明，EZ-MIA在1%假阳性率下相较先前方法提高3到8倍的真阳性率，AUC达到0.98，且无需训练参考模型。结果表明微调语言模型的隐私风险被大大低估。

Conclusion: 微调语言模型存在显著的隐私风险，尤其是在错误预测的token位置表现出记忆效应时。EZ-MIA方法显著提升了会员推断攻击的检测效果，显示出微调模型的隐私风险远超此前认知。

Abstract: Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive information from their training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at the low false-positive thresholds required for practical privacy auditing. We present EZ-MIA, a membership inference attack that exploits a key observation: memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. We introduce the Error Zone (EZ) score, which measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This principled statistic requires only two forward passes per query and no model training of any kind. On WikiText with GPT-2, EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under identical conditions (66.3% versus 17.5% true positive rate at 1% false positive rate), with near-perfect discrimination (AUC 0.98). At the stringent 0.1% FPR threshold critical for real-world auditing, we achieve 8x higher detection than prior work (14.0% versus 1.8%), requiring no reference model training. These gains extend to larger architectures: on AG News with Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR). These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for both privacy auditing and deployment decisions. Code is available at https://github.com/JetBrains-Research/ez-mia.

</details>


### [45] [Bengali Text Classification: An Evaluation of Large Language Model Approaches](https://arxiv.org/abs/2601.12132)
*Md Mahmudul Hoque,Md Mehedi Hassain,Md Hojaifa Tanvir,Rahul Nandy*

Main category: cs.CL

TL;DR: 研究比较了三种大型语言模型在孟加拉语新闻文本分类上的表现，发现Qwen 2.5表现最佳，展示了大型语言模型在资源匮乏的孟加拉语NLP中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模标注数据和预训练模型，孟加拉语文本分类面临挑战，研究探索大型语言模型在该任务中的应用效果。

Method: 采用三种经过指令微调的大型语言模型（LLaMA 3.1, LLaMA 3.2, Qwen 2.5）在同一框架下对孟加拉语新闻文章进行分类评估。

Result: Qwen 2.5模型在分类准确率上优于LLaMA 3.1和3.2，分别为72%、53%和56%，在“体育”类别表现尤为突出。

Conclusion: 大型语言模型在孟加拉语文本分类中表现有效，尤其是Qwen 2.5模型达到了最高72%的准确率。

Abstract: Bengali text classification is a Significant task in natural language processing (NLP), where text is categorized into predefined labels. Unlike English, Bengali faces challenges due to the lack of extensive annotated datasets and pre-trained language models. This study explores the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. The dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for this task under the same classification framework. Among the evaluated models, Qwen 2.5 achieved the highest classification accuracy of 72%, showing particular strength in the "Sports" category. In comparison, LLaMA 3.1 and LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings highlight the effectiveness of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches to improve classification performance.

</details>


### [46] [Analyzing Cancer Patients' Experiences with Embedding-based Topic Modeling and LLMs](https://arxiv.org/abs/2601.12154)
*Teodor-Călin Ionescu,Lifeng Han,Jan Heijdra Suasnabar,Anne Stiggelbout,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文使用BERTopic和GPT-4，从癌症患者访谈文本中抽取主话题，结合BioClinicalBERT嵌入模型提升话题质量，揭示了患者关心的癌症护理协调及决策主题，表明此技术可助力强化患者声音，促进以患者为中心的医疗实践。


<details>
  <summary>Details</summary>
Motivation: 旨在通过分析患者讲述数据，提取能反映患者真实体验和诉求的主题，以支持更以患者为中心的医疗实践，增强患者在医疗服务中的声音，并帮助临床医师更高效地理解患者需求。

Method: 对13个癌症患者的转录访谈文本进行预处理、分块和聚类，比较BERTopic和Top2Vec两种神经话题模型的关键词提取性能。随后，使用GPT-4对话题进行标注，并通过人工评价进行质量评定。基于评价结果，选择BERTopic结合三种临床领域嵌入模型中的最佳模型(BioClinicalBERT)对全部访谈文本进行全局话题分析。

Result: BERTopic整体表现优于Top2Vec，结合BioClinicalBERT的领域特定嵌入模型能提高话题的精准度和一致性。全局分析发现“癌症护理管理中的协调与沟通”和“癌症治疗过程中的患者决策”是贯穿所有访谈的主要话题。尽管数据翻译和未涉及临床专家评估存在限制，结果显示该方法具备实际应用潜力。

Conclusion: 基于神经话题建模（尤其是BERTopic）和大型语言模型（GPT-4），本文展示了从癌症患者讲述数据中提取有意义主题的有效方法，且利用领域特定的嵌入模型（BioClinicalBERT）提升了主题的精准性和可解释性，证明该方法可为临床医生提供有价值的患者反馈，促进以患者为中心的医疗实践。

Abstract: This study investigates the use of neural topic modeling and LLMs to uncover meaningful themes from patient storytelling data, to offer insights that could contribute to more patient-oriented healthcare practices. We analyze a collection of transcribed interviews with cancer patients (132,722 words in 13 interviews). We first evaluate BERTopic and Top2Vec for individual interview summarization by using similar preprocessing, chunking, and clustering configurations to ensure a fair comparison on Keyword Extraction. LLMs (GPT4) are then used for the next step topic labeling. Their outputs for a single interview (I0) are rated through a small-scale human evaluation, focusing on {coherence}, {clarity}, and {relevance}. Based on the preliminary results and evaluation, BERTopic shows stronger performance and is selected for further experimentation using three {clinically oriented embedding} models. We then analyzed the full interview collection with the best model setting. Results show that domain-specific embeddings improved topic \textit{precision} and \textit{interpretability}, with BioClinicalBERT producing the most consistent results across transcripts. The global analysis of the full dataset of 13 interviews, using the BioClinicalBERT embedding model, reveals the most dominant topics throughout all 13 interviews, namely ``Coordination and Communication in Cancer Care Management" and ``Patient Decision-Making in Cancer Treatment Journey''. Although the interviews are machine translations from Dutch to English, and clinical professionals are not involved in this evaluation, the findings suggest that neural topic modeling, particularly BERTopic, can help provide useful feedback to clinicians from patient interviews. This pipeline could support more efficient document navigation and strengthen the role of patients' voices in healthcare workflows.

</details>


### [47] [Tolerance Principle and Small Language Model Learning](https://arxiv.org/abs/2601.12179)
*Adam E. Friedman,Stevan Harnad,Rushen Shi*

Main category: cs.CL

TL;DR: 研究发现，基于变换器的BabyBERTa模型在少量数据训练下无法像幼儿那样依据容忍原则学习抽象语法规则。


<details>
  <summary>Details</summary>
Motivation: 探索在仅有少量训练数据时，基于变换器的语言模型是否能像幼儿一样根据容忍原则学习和推广抽象语法规则。

Method: 使用优化小数据集的变换器模型BabyBERTa，在人工设计的语法数据上训练，控制训练集大小、句子类型数量以及规则遵循和例外比例。

Result: 发现BabyBERTa无法像14个月大的儿童那样依据容忍原则有效学习规则，表现出不同的学习动态。

Conclusion: BabyBERTa的学习动态与容忍原则不符，即其在规则推广中的表现不符合该理论的预测。

Abstract: Modern language models like GPT-3, BERT, and LLaMA require massive training data, yet with sufficient training they reliably learn to distinguish grammatical from ungrammatical sentences. Children aged as young as 14 months already have the capacity to learn abstract grammar rules from very few exemplars, even in the presence of non-rule-following exceptions. Yang's (2016) Tolerance Principle defines a precise threshold for how many exceptions a rule can tolerate and still be learnable. The present study explored the minimal amount and quality of training data necessary for rules to be generalized by a transformer-based language model to test the predictions of the Tolerance Principle. We trained BabyBERTa (Huebner et al. 2021), a transformer model optimized for small datasets, on artificial grammars. The training sets varied in size, number of unique sentence types, and proportion of rule-following versus exception exemplars. We found that, unlike human infants, BabyBERTa's learning dynamics do not align with the Tolerance Principle.

</details>


### [48] [CTC-DID: CTC-Based Arabic dialect identification for streaming applications](https://arxiv.org/abs/2601.12199)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: 本文提出基于CTC损失的方言识别方法，在低资源阿拉伯方言任务中优于现有模型，表现稳定且适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 解决低资源阿拉伯方言识别任务，提升方言识别的准确性和实时性。

Method: 提出将方言识别视为有限词汇的自动语音识别问题，利用CTC损失函数进行训练；采用语言无关启发式方法或预训练ASR模型估计转录中方言标签的重复。

Result: 基于SSL的CTC-DID模型在有限数据集上表现优于微调的Whisper和ECAPA-TDNN模型，在Casablanca零-shot测试中表现更佳，对短语音鲁棒且适合实时流式应用。

Conclusion: CTC-DID方法有效提升了低资源方言识别的性能和鲁棒性，支持低时延在线应用。

Abstract: This paper proposes a Dialect Identification (DID) approach inspired by the Connectionist Temporal Classification (CTC) loss function as used in Automatic Speech Recognition (ASR). CTC-DID frames the dialect identification task as a limited-vocabulary ASR system, where dialect tags are treated as a sequence of labels for a given utterance. For training, the repetition of dialect tags in transcriptions is estimated either using a proposed Language-Agnostic Heuristic (LAH) approach or a pre-trained ASR model. The method is evaluated on the low-resource Arabic Dialect Identification (ADI) task, with experimental results demonstrating that an SSL-based CTC-DID model, trained on a limited dataset, outperforms both fine-tuned Whisper and ECAPA-TDNN models. Notably, CTC-DID also surpasses these models in zero-shot evaluation on the Casablanca dataset. The proposed approach is found to be more robust to shorter utterances and is shown to be easily adaptable for streaming, real-time applications, with minimal performance degradation.

</details>


### [49] [CoReflect: Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement](https://arxiv.org/abs/2601.12208)
*Yunzhe Li,Richie Yueqi Feng,Tianxin Wei,Chin-Chia Hsu*

Main category: cs.CL

TL;DR: CoReflect通过对话模拟与反思分析的迭代共进化，实现了多轮对话系统评测的自动化和动态优化，有效适应了对话模型的快速进化。


<details>
  <summary>Details</summary>
Motivation: 传统多轮对话系统评测依赖静态和手工定义的标准，难以覆盖多样化行为，且无法适应模型的快速发展。

Method: 通过对话规划器生成结构化模板引导用户模拟，随后利用反思分析器自动识别对话行为并优化评测标准，形成对话规划与评测标准的共进化循环。

Result: CoReflect成功构建了一个减少人工干预、可持续自我优化的评测框架，提升了测试用例的复杂度和评测标准的诊断精度。

Conclusion: CoReflect实现了对多轮对话系统更动态和精细的评估，显著提升了评测体系的适应性和诊断能力。

Abstract: Evaluating conversational systems in multi-turn settings remains a fundamental challenge. Conventional pipelines typically rely on manually defined rubrics and fixed conversational context$-$a static approach that limits coverage and fails to capture the diverse, emergent behaviors of dialogue models. To address this, we introduce CoReflect (Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement), which unifies dialogue simulation and evaluation into an adaptive, iterative process. CoReflect employs a conversation planner that generates structured templates to guide a user simulator through diverse, goal-directed dialogues. Subsequently, a reflective analyzer processes these dialogues to identify systematic behavioral patterns and automatically refine the evaluation rubrics. Crucially, the insights from the conversation analysis are fed back into the planner to update conversation templates for subsequent iterations. This co-evolution loop ensures that the complexity of test cases and the diagnostic precision of rubrics improve in tandem. By minimizing human intervention, CoReflect provides a scalable and self-refining methodology that allows evaluation protocols to adapt alongside the rapidly advancing capabilities of dialogue models.

</details>


### [50] [Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models](https://arxiv.org/abs/2601.12247)
*Miao Li,Hanyang Jiang,Sikai Chen,Hengyu Fu,Yuhang Cai,Baihe Huang,Tinghan Ye,Xuanzhou Chen,Pascal Van Hentenryck*

Main category: cs.CL

TL;DR: 本文提出的PVF方法通过规划与验证策略，显著提升了扩散语言模型的解码效率，减少了计算开销且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型的解码策略未能充分利用全局双向上下文来指导生成路径，效率和效果有待提升。

Method: 提出了Plan-Verify-Fill (PVF)方法，通过量化验证进行规划，主动构建层次化的骨架结构，优先考虑关键语义锚点，并采用验证协议确定何时停止进一步推理。

Result: 在LLaDA-8B-Instruct和Dream-7B-Instruct数据集上的实验表明，PVF相比基于置信度的并行解码，能够减少高达65%的函数调用次数，同时保持生成文本的准确性。

Conclusion: PVF方法有效提高了扩散语言模型的解码效率，通过结构性规划和验证策略实现了更优的计算资源利用，提升了文本生成的性能。

Abstract: Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.

</details>


### [51] [Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers](https://arxiv.org/abs/2601.12263)
*Yixuan Du,Chenxiao Yu,Haoyan Xu,Ziyi Wang,Yue Zhao,Xiyang Hu*

Main category: cs.CL

TL;DR: 本文发现视觉语言模型在多模态产品搜索中存在可被利用的安全隐患，提出了一种联合攻击图像和文本的新方法MGEO，实验证明该方法显著提升了攻击效果，威胁了搜索排名的公正性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在检索和推荐中表现优越，但其在对抗操控和鲁棒性方面研究不足，尤其是在涉及竞价排名的多模态攻击问题上尚未被深入探究。

Method: 提出了一种名为多模态生成引擎优化（MGEO）的对抗框架，利用交替的梯度优化策略，联合优化图像和文本两种模态的扰动，充分利用VLM中的跨模态深度耦合性实现攻击。

Result: 在使用真实数据集和最先进模型的实验中，MGEO所设计的攻击大幅度优于单一文本或单一图像的攻击基线，且不会触发传统的内容过滤器。

Conclusion: 论文揭示了基于视觉语言模型的多模态产品搜索存在严重的安全漏洞，即多模态排序攻击。攻击通过联合优化图像微扰和文本后缀，能显著提升目标产品排名，破坏搜索结果的公正性。

Abstract: Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.

</details>


### [52] [Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models](https://arxiv.org/abs/2601.12269)
*Xucong Hu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 本文通过改进采样方法，从自回归语言模型中无需再训练即可提取出良好的Theory of Mind能力，显著提升了模型的推理表现。


<details>
  <summary>Details</summary>
Motivation: 当前自回归语言模型主要优化局部连贯性，未能有效维护潜在状态表示，导致其在Theory of Mind任务中表现较差。

Method: 利用基于Markov链蒙特卡洛（MCMC）的power-sampling方法，从序列级概率分布中采样，并引入退火过程以提升性能。

Result: 发现通过power-sampling及退火方法，可以显著提升语言模型在Theory of Mind任务中的表现，且无需模型权重更新。

Conclusion: 通过基于采样的方法优化，语言模型能够在不重新训练的情况下表现出强大的Theory of Mind能力。

Abstract: Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.

</details>


### [53] [Conversational Context Classification: A Representation Engineering Approach](https://arxiv.org/abs/2601.12286)
*Jonathan Pan*

Main category: cs.CL

TL;DR: 本文提出基于表示工程和单类支持向量机的方法，成功识别大语言模型内部隐状态中特定上下文子空间，实现了对模型脱离上下文回复的检测，为更好理解和监控LLM提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易生成脱离上下文的回复，传统异常检测方法难以直接应用于语义上下文中，因此需要新的方法准确检测模型偏离正常对话规范的情况。

Method: 利用表示工程技术构建上下文相关的表示，再利用OCSVM训练并划定LLM隐藏状态潜在空间中该上下文的边界，进而实现异常检测。

Result: 在Llama和Qwen两个开源大语言模型上进行实验，结果显示所提方法能较准确识别出与特定上下文相关的子空间，有效区分对话是否符合上下文。

Conclusion: 本文实验表明，通过使用表示工程（RepE）和单类支持向量机（OCSVM）可以有效识别大语言模型内部状态中的特定上下文子空间，有助于检测模型产生的偏离预期的话题、事实错误或幻觉。

Abstract: The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.

</details>


### [54] [Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies](https://arxiv.org/abs/2601.12369)
*Ming Zhang,Jiabao Zhuang,Wenqing Jing,Ziyu Kong,Jingyi Deng,Yujiong Shen,Kexin Tan,Yuhang Zhao,Ning Luo,Renzhe Zheng,Jiahui Lin,Mingqi Wu,Long Ma,Yi Zou,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的评测基准TaxoBench，用于诊断自动研究综述工具在关键论文检索和知识结构组织方面的能力，结果显示现有工具与专家水平尚有显著差距。


<details>
  <summary>Details</summary>
Motivation: 目前的基准主要评估语句流畅性或引用准确性，缺乏对自动综述核心能力——关键论文检索和知识结构组织的评测。因此，推动深度研究代理的发展需要一个更全面、专业的诊断工具。

Method: 提出了TaxoBench，这是基于72篇高被引计算机科学综述人工提取的分类树构建的诊断基准，包含3815个精确分类的引用。基准支持两种评测模式：深度研究模式（端到端检索与组织）和自下而上模式（仅组织能力测试，提供专家选用的论文）。对比评估了7个领先的深度研究代理和12个先进大型语言模型。

Result: 最佳的深度研究代理仅能召回20.9%的专家选取论文；即使输入完美，最佳模型的组织能力指标（ARI）也只有0.31，远低于专家水平。

Conclusion: 当前的深度研究代理在自动生成专家级综述方面表现有限，尤其在检索关键论文和组织知识结构上存在显著瓶颈。

Abstract: Deep Research Agents are increasingly used for automated survey generation. However, whether they can write surveys like human experts remains unclear. Existing benchmarks focus on fluency or citation accuracy, but none evaluates the core capabilities: retrieving essential papers and organizing them into coherent knowledge structures. We introduce TaxoBench, a diagnostic benchmark derived from 72 highly-cited computer science surveys. We manually extract expert-authored taxonomy trees containing 3,815 precisely categorized citations as ground truth. Our benchmark supports two evaluation modes: Deep Research mode tests end-to-end retrieval and organization given only a topic, while Bottom-Up mode isolates structuring capability by providing the exact papers human experts used. We evaluate 7 leading Deep Research agents and 12 frontier LLMs. Results reveal a dual bottleneck: the best agent recalls only 20.9% of expert-selected papers, and even with perfect input, the best model achieves only 0.31 ARI in organization. Current deep research agents remain far from expert-level survey writing. Our benchmark is publicly available at https://github.com/KongLongGeFDU/TaxoBench.

</details>


### [55] [A Scalable Entity-Based Framework for Auditing Bias in LLMs](https://arxiv.org/abs/2601.12374)
*Akram Elbouanani,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 该论文提出了一个基于命名实体的偏见审计框架，通过大规模合成数据分析揭示了大型语言模型在政治立场、地域和行业偏见上的系统性问题，强调部署前需严格审计。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型偏见评估方法在生态效度和统计控制之间存在权衡，人工提示不反映现实，或自然任务缺乏规模和严谨性。

Method: 引入使用命名实体作为探针的可扩展偏见审计框架，利用合成数据模拟自然文本中的偏见模式，进行大规模分析。

Result: 完成迄今为止最大的偏见审计，涵盖19亿数据点，多实体类型、任务、语言、模型和提示策略，发现系统性偏见如对右翼政治家处罚、支持左翼政治家，偏好西方及富裕国家，声援西方公司，惩罚防御和制药行业公司，指令微调减少偏见，模型规模增大偏见加剧，中文和俄文提示未减弱西方偏好。

Conclusion: 大语言模型在应用于高风险场景前需接受严格偏见审计。

Abstract: Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.

</details>


### [56] [LR-DWM: Efficient Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.12376)
*Ofek Raban,Ethan Fetaya,Gal Chechik*

Main category: cs.CL

TL;DR: 本文提出LR-DWM水印方法，低开销高效为扩散语言模型嵌入水印，解决现有方法不适用问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法主要针对自回归模型，不适用于通过非顺序迭代去噪生成文本的扩散语言模型，导致需要对DLM进行大量改动或产生高开销。

Method: 提出了Left-Right Diffusion Watermarking (LR-DWM)方案，通过结合左右邻居的生成信息，在扩散语言模型中嵌入水印信号。

Result: LR-DWM在标准评估下实现了接近无水印模型的性能，并实现了高检测能力，且只需极少的运行时和内存资源。

Conclusion: LR-DWM方案能够有效地为扩散语言模型添加水印，实现高检测率，同时保持极低的计算和内存开销。

Abstract: Watermarking (WM) is a critical mechanism for detecting and attributing AI-generated content. Current WM methods for Large Language Models (LLMs) are predominantly tailored for autoregressive (AR) models: They rely on tokens being generated sequentially, and embed stable signals within the generated sequence based on the previously sampled text. Diffusion Language Models (DLMs) generate text via non-sequential iterative denoising, which requires significant modification to use WM methods designed for AR models. Recent work proposed to watermark DLMs by inverting the process when needed, but suffers significant computational or memory overhead. We introduce Left-Right Diffusion Watermarking (LR-DWM), a scheme that biases the generated token based on both left and right neighbors, when they are available. LR-DWM incurs minimal runtime and memory overhead, remaining close to the non-watermarked baseline DLM while enabling reliable statistical detection under standard evaluation settings. Our results demonstrate that DLMs can be watermarked efficiently, achieving high detectability with negligible computational and memory overhead.

</details>


### [57] [NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages](https://arxiv.org/abs/2601.12389)
*Lakshya Tomar,Vinayak Abrol,Puneet Agarwal*

Main category: cs.CL

TL;DR: 本文提出了结合差分变换器和专家混合机制的NADIR非自回归模型，实现了多语种音译任务中速度与准确性的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 自回归模型高准确度但推理速度慢，非自回归模型速度快但存在幻觉和长度控制差，探索两者之间的权衡。

Method: 引入了差分变换器和专家混合机制的非自回归模型NADIR，解决了序列到序列任务中的局部依赖问题。

Result: NADIR比最先进的自回归基线快13倍，字符错误率为15.78%，接近自回归的14.44%，显著优于标准非自回归模型，并显著减少多种错误。

Conclusion: NADIR模型在多语种音译任务中有效平衡了速度和准确性，显著提升推理速度，同时保持竞争性的字符错误率，减少多种错误类型。

Abstract: In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.

</details>


### [58] [Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification](https://arxiv.org/abs/2601.12419)
*Mahammad Namazov,Tomáš Koref,Ivan Habernal*

Main category: cs.CL

TL;DR: 该研究比较了多种模型无关的法律文本解释技术，发现模型解释与法律专家意见差异明显，表明解释方法尚需改进以满足法律领域的需求。


<details>
  <summary>Details</summary>
Motivation: 法律领域的应用要求大语言模型具有可解释性以确保信任和透明度，但目前尚不清楚哪种解释技术最适合法律结果预测。

Method: 提出了一个模型无关的解释技术比较分析框架，使用两种理由提取方法，通过归一化充分性和完备性指标评估忠实度，并通过法律专家评价理由的合理性进行对比分析。

Result: 实验显示现有解释方法的预测理由与法律专家意见差异大，尽管定量指标和分类效果良好，同时验证了LLM作为法律判决工具的可行性有限。

Conclusion: 模型的“理由”与法律专家的理由有显著差异，尽管量化分析和分类表现良好，表明现有解释技术在法律领域的适用性有限。

Abstract: Interpretability is critical for applications of large language models in the legal domain which requires trust and transparency. While some studies develop task-specific approaches, other use the classification model's parameters to explain the decisions. However, which technique explains the legal outcome prediction best remains an open question. To address this challenge, we propose a comparative analysis framework for model-agnostic interpretability techniques. Among these, we employ two rationale extraction methods, which justify outcomes with human-interpretable and concise text fragments (i.e., rationales) from the given input text. We conduct comparison by evaluating faithfulness-via normalized sufficiency and comprehensiveness metrics along with plausibility-by asking legal experts to evaluate extracted rationales. We further assess the feasibility of LLM-as-a-Judge using legal expert evaluation results. We show that the model's "reasons" for predicting a violation differ substantially from those of legal experts, despite highly promising quantitative analysis results and reasonable downstream classification performance. The source code of our experiments is publicly available at https://github.com/trusthlt/IntEval.

</details>


### [59] [System-Mediated Attention Imbalances Make Vision-Language Models Say Yes](https://arxiv.org/abs/2601.12430)
*Tsan Tsai Chan,Varsha Suresh,Anisha Saha,Michael Hahn,Vera Demberg*

Main category: cs.CL

TL;DR: 该论文指出视觉语言模型中的幻觉与系统模块注意力分配有关，提出调整系统注意力可有效减少模型泛滥的yes偏差，优于传统图像集中方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于强调图像注意力增加，忽略了系统模块和文本输入的作用，导致模型幻觉问题未能得到根本解决。

Method: 提出并验证了系统介导的注意力分配框架，通过因果重分配系统注意力至图像和文本输入，抑制了模型的yes偏差。

Result: 通过重新分配注意力权重，显著减少了视觉语言模型中广泛存在的yes偏差幻觉表现，表现优于现有图像中心的方法。

Conclusion: 系统注意力的不平衡是视觉语言模型幻觉现象的关键因素，调整系统注意力分配可以有效减少模型的yes偏差幻觉。

Abstract: Vision-language model (VLM) hallucination is commonly linked to imbalanced allocation of attention across input modalities: system, image and text. However, existing mitigation strategies tend towards an image-centric interpretation of these imbalances, often prioritising increased image attention while giving less consideration to the roles of the other modalities. In this study, we evaluate a more holistic, system-mediated account, which attributes these imbalances to functionally redundant system weights that reduce attention to image and textual inputs. We show that this framework offers a useful empirical perspective on the yes-bias, a common form of hallucination in which VLMs indiscriminately respond 'yes'. Causally redistributing attention from the system modality to image and textual inputs substantially suppresses this bias, often outperforming existing approaches. We further present evidence suggesting that system-mediated attention imbalances contribute to the yes-bias by encouraging a default reliance on coarse input representations, which are effective for some tasks but ill-suited to others. Taken together, these findings firmly establish system attention as a key factor in VLM hallucination and highlight its potential as a lever for mitigation.

</details>


### [60] [Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping](https://arxiv.org/abs/2601.12465)
*Miao Peng,Weizhou Shen,Nuo Chen,Chenliang Li,Ming Yan,Jia Li*

Main category: cs.CL

TL;DR: 本文针对长上下文推理中的“almost-there”问题，设计了新的数据合成与信用分配方法，提升了大语言模型的推理性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习方法（RLVR）在长上下文推理中表现下降，原因在于缺乏高密度推理样本和训练中对部分正确但最终错误轨迹的惩罚过度，导致有效学习信号丢失。

Method: 通过引入知识图谱驱动的高难度多跳长上下文问答合成框架DeepReasonQA，结合细粒度信用分配方法LongPAS，分别从题目构造和强化学习奖励信号两个方面提升模型推理能力。LongPAS在推理步骤的有效性和相关性维度上进行评价，实现更加精准的奖励引导。

Result: 在三个长上下文推理基准测试中，提出的方法显著优于RLVR基线，表现接近最先进的大语言模型，同时使用的参数量更少，实现了推理能力和训练稳定性的双重提升。

Conclusion: 本文提出的DeepReasonQA和LongPAS方法显著提升了大语言模型在长上下文推理任务中的表现，解决了以前方法中“almost-there”现象导致的性能瓶颈。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the "almost-there" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from "almost-there" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.

</details>


### [61] [Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty](https://arxiv.org/abs/2601.12471)
*Sravanthi Machcha,Sushrita Yerra,Sahil Gupta,Aishwarya Sahoo,Sharmin Sultana,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: 提出MedAbstain基准，系统评测医疗问答中模型放弃能力，发现显式放弃机制显著提升安全性，强调放弃能力对现实可信大模型应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在评估中主要关注准确率，但在实际和安全关键场景下，模型在不确定时选择放弃回答的能力同样重要。

Method: 提出MedAbstain，一个统一的多项选择医疗问答中放弃机制的基准和评估方案，结合了共形预测、对抗性问题扰动和显式放弃选项。

Result: 评测显示即使是高准确率的先进模型，也常在不确定时未能放弃回答。显式放弃选项明显提升了模型的不确定性反映和安全放弃能力，超越输入扰动效果；而增加模型规模和高级提示作用有限。

Conclusion: 放弃机制在可信赖的大语言模型部署中发挥核心作用，应作为提高高风险应用安全性的重要方向。

Abstract: Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.

</details>


### [62] [Capability-Aware Early-Stage Research Idea Evaluation](https://arxiv.org/abs/2601.12473)
*Renlong Jie,Chen Chu,Zhen Wang*

Main category: cs.CL

TL;DR: 本文提出基于作者信息和研究想法的能力感知三路Transformer模型，实现早期科研成果预测，优化资源配置，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完整稿件或同行评审，难以在研究初期预测结果。本文旨在实现早期研究成果的预测以优化资源分配。

Method: 通过整合作者信息、推断的能力表示与研究想法，设计三路Transformer架构和灵活的融合机制；引入两阶段架构学习能力表示。

Result: 实验结果表明，所提方法相比基线模型，准确率显著提升，且能力表示预测进一步提高最终模型的效果。

Conclusion: 本文提出的基于能力感知的三路Transformer框架能够仅通过作者信息和研究想法，有效预测论文接受与评分，显著优于单路模型。

Abstract: Predicting the outcomes of research ideas at their conceptual stage (i.e. before significant resources are committed) holds great potential for optimizing scientific resource allocation and research planning. While existing methods rely heavily on finished manuscripts or peer reviews, we propose a novel capability-aware framework that predicts paper acceptance and ratings using only author information and research ideas, without requiring full text or experimental results. Our approach integrates author information, (inferred) capability presentation, and research ideas through a three-way transformer architecture with flexible fusion mechanisms. We also introduce a two-stage architecture for learning the capability representation given the author information and idea. Experiments show that our method significantly outperform the single-way models by finetuning bert-base and bert-large, and the capability predicting significantly increase the predictive accuracy of the final model. The proposed method can be applied in both early-stage research outcome prediction and scientific resource allocation.

</details>


### [63] [DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity](https://arxiv.org/abs/2601.12505)
*Ashish Raj Shekhar,Shiven Agarwal,Priyanuj Bordoloi,Yash Shah,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: DoPE通过文档层面嵌入诱饵机制，有效防御和检测多模态大语言模型在自动考试中的作弊行为，促进学术诚信保护。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型能直接处理考试文档，对传统考试和学术诚信构成威胁，急需有效的模型无关防护和检测手段。

Method: 通过在PDF/HTML考试文档中嵌入语义诱饵（semantic decoys），利用渲染与解析差异构建防御框架。引入FewSoRT-Q生成诱饵，FewSoRT-D实现文档水印封装，无需依赖传统分类器。

Result: 在包括1826份考试的Integrity-Bench基准测试中，DoPE在黑盒MLLM上实现91.4%的检测率（误报率8.7%），阻止或干扰了96.3%的答题尝试。

Conclusion: 本文提出的DoPE框架有效提升了针对多模态大语言模型的考试防护能力，显著降低了自动答题的成功率，同时能准确检测依赖盲目AI的行为。

Abstract: Multimodal Large Language Models (MLLMs) can directly consume exam documents, threatening conventional assessments and academic integrity. We present DoPE (Decoy-Oriented Perturbation Encapsulation), a document-layer defense framework that embeds semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies in MLLM pipelines. By instrumenting exams at authoring time, DoPE provides model-agnostic prevention (stop or confound automated solving) and detection (flag blind AI reliance) without relying on conventional one-shot classifiers. We formalize prevention and detection tasks, and introduce FewSoRT-Q, an LLM-guided pipeline that generates question-level semantic decoys and FewSoRT-D to encapsulate them into watermarked documents. We evaluate on Integrity-Bench, a novel benchmark of 1826 exams (PDF+HTML) derived from public QA datasets and OpenCourseWare. Against black-box MLLMs from OpenAI and Anthropic, DoPE yields strong empirical gains: a 91.4% detection rate at an 8.7% false-positive rate using an LLM-as-Judge verifier, and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. We release Integrity-Bench, our toolkit, and evaluation code to enable reproducible study of document-layer defenses for academic integrity.

</details>


### [64] [Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning](https://arxiv.org/abs/2601.12535)
*Ahmed Attia,Alham Fikri*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的往返翻译微调方法，有效提升低资源语言机器翻译性能，增强了翻译的流畅度和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言社区平行数据有限，现有提升低资源语言机器翻译的方法仍未充分探索，因此探索自我训练策略以提升低资源机器翻译性能。

Method: 利用No Language Left Behind模型家族，通过将英文翻译成目标低资源语言，再翻译回英文，结合chrF++和BLEU指标作为奖励函数进行强化学习微调。

Result: 在NLLB-MD数据集上，600M和1.3B参数的NLLB模型在Central Aymara、Friulian、Wolof和Russian语言上均取得稳定的性能提升。

Conclusion: 通过基于自监督强化学习的往返自我训练方法，低资源语言机器翻译模型在多种语言上的翻译质量得到了显著提升，表现为流畅度和语义一致性的增强。

Abstract: Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.

</details>


### [65] [Benchmarking Concept-Spilling Across Languages in LLMs](https://arxiv.org/abs/2601.12549)
*Ilia Badanin,Daniil Dzenhaliou,Imanol Schlag*

Main category: cs.CL

TL;DR: 本文提出了一个创新的多义词生成框架，用以评测多语大语言模型的语义鲁棒性，发现模型在跨语言语义干扰问题上差异显著，构建了多语言语义评价的基准与验证流程。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在生成非英语语言内容时，常因对其他语言的偏见导致语义干扰，亟需一种定量比较语义鲁棒性的方法。

Method: 提出一种比较框架，利用多义词生成任务，系统测量模型在多语言环境下的语义表现，通过生成序列中语义保持时间点评价模型强弱。

Result: 通过对9种语言、100个英语高多义词的生成测试，发现各模型及语言间语义鲁棒性差异明显，建立了无须因果归因的模型排名体系。

Conclusion: 不同多语大语言模型在处理多义词时表现出语义鲁棒性显著差异，且语言间语义干扰（语言溢出）现象普遍存在。

Abstract: Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.

</details>


### [66] [Evaluating Contextually Mediated Factual Recall in Multilingual Large Language Models](https://arxiv.org/abs/2601.12555)
*Yihong Liu,Bingyu Xiong,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本研究探讨多语言大规模语言模型在间接上下文中回忆事实的能力，发现上下文调解普遍降低回忆性能，较大模型更鲁棒，真实姓名影响不明显，揭示了现实语言理解与孤立事实回忆间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有事实回忆评估通常只考察明确提及实体和直接请求事实的情况，而现实语言中事实常通过上下文间接访问，研究这种上下文调解下的事实回忆能力及其多语言表现具有实际意义。

Method: 构建控制性提示，保持事实不变，同时通过上下文句子引入指代调解。比较使用合成姓名与真实姓名，评估多模型家族在五种语言中的表现。

Result: 上下文调解普遍降低了模型的事实回忆能力，不同关系间效果差异较大。较大模型在上下文调解下表现更稳健。真实姓名和姓名来源对性能影响混杂且无系统性。

Conclusion: 多语言大规模语言模型在通过上下文间接访问事实知识时表现出事实回忆能力下降，尤其是在不同关系中表现差异明显。较大的模型对这种上下文调解较为鲁棒，表现差距较小，而关于真实姓名及其来源的影响则没有系统性规律。

Abstract: Large language models (LLMs) can recall a wide range of factual knowledge across languages. However, existing factual recall evaluations primarily assess fact retrieval in isolation, where the queried entity is explicitly named and the fact is requested directly. In natural language use, facts are often accessed through context, where the relevant entity is introduced only indirectly. In this work, we study contextually mediated factual recall, asking whether LLMs can reliably retrieve factual knowledge when the target entity is embedded in a naturalistic context rather than queried explicitly, across languages. We construct controlled prompts that preserve the underlying fact while introducing referential mediation through contextual sentences. To disentangle contextual effects from name-specific associations, we further compare performance using synthetic names and real names across languages. Evaluating multiple model families in five languages, we find that contextual mediation consistently degrades factual recall, with substantial variation across relations. Larger models are more robust to contextual mediation, exhibiting a reduced performance gap relative to direct queries, while the effect of real names and name origin is mixed and unsystematic. These findings highlight a gap between isolated factual recall and context-dependent language understanding in multilingual LLMs.

</details>


### [67] [A Cloud-based Multi-Agentic Workflow for Science](https://arxiv.org/abs/2601.12607)
*Anurag Acharya,Timothy Vega,Rizwan A. Ashraf,Anshu Sharma,Derek Parker,Robert Rallo*

Main category: cs.CL

TL;DR: 本文提出了一个可扩展的多代理科学助理框架，能够高效完成复杂科学任务，验证其准确性和经济性，具备广泛推广潜力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽广泛应用但无法完成复杂模拟或决策任务，多代理系统通过调用外部工具弥补了这一缺陷，但工作流设计复杂，实施困难。

Method: 设计了一个领域无关、模型独立的云端运行代理框架，由一个主管代理协调多个具备不同能力的代理，能够执行从文献综述到复杂模拟的多种任务。

Result: 系统在合成任务中任务路由正确率达90%，任务完成率达97.5%；在真实任务中完成率达91%，准确率达到或优于前沿模型，运营成本合理。

Conclusion: 本文提出的基于多代理协作的科学助理框架能够高效路由任务并成功完成任务，准确率达到或超过当前先进模型，验证了该框架在科学领域的应用可行性。

Abstract: As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.

</details>


### [68] [Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems](https://arxiv.org/abs/2601.12618)
*Elham Tajik,Conrad Borchers,Bahar Shahrokhian,Sebastian Simon,Ali Keramati,Sonika Pal,Sreecharan Sankaranarayanan*

Main category: cs.CL

TL;DR: 本文提出利用多智能体大语言模型生成的推理轨迹作为新型过程数据，结合余弦相似度检测智能体分歧，从而提升定性编码分析的严谨性和效率，促进教育学习分析方法的发展。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成式AI的自动化与人机协同学习分析方法缺乏系统的方法学标准，亟需开发新型富含解释价值的过程数据形式以提升定性编码的分析效果。

Method: 利用大语言模型（LLM）多智能体系统生成的推理轨迹，通过余弦相似度量化和解读智能体间分歧，结合定量相似度指标与定性分析指导代码一致性审查。

Result: 通过分析近万次智能体对人类辅导对话编码，发现LLM的语义推理相似度能有效区分共识与分歧，并与人类编码一致性相关；定性分析揭示编码的教学子功能细节及代码本修订机会。

Conclusion: 推理轨迹中的分歧是一类有价值的分析信号，可提升教育研究中的方法严谨性和解释深度。

Abstract: Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents' semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.

</details>


### [69] [BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models](https://arxiv.org/abs/2601.12632)
*Kriti Bhattarai,Vipina K. Keloth,Donald Wright,Andrew Loza,Yang Ren,Hua Xu*

Main category: cs.CL

TL;DR: 本文提出了一种基于最新生物医学文档的新问答基准BioPulse-QA，评估多款大语言模型在临床和药物文本上的表现，发现GPT-o1优于其他模型，临床试验内容最具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学领域大语言模型评测基准数据集静态且过时，无法充分反映该领域知识的动态性和复杂性，并存在数据泄露和偏见问题。

Method: 引入BioPulse-QA基准数据集，包含2280对专家验证的问答对，基于最新发布的生物医学文档，涵盖提取式和生成式问答，并测试四个大语言模型。

Result: GPT-o1在药物标签问答中表现最佳，F1分数达到0.92，临床试验数据类别最具挑战性，提取式问答F1分数低至0.36。

Conclusion: BioPulse-QA提供了一个可扩展且临床相关的框架，用于更准确评估生物医学领域的大语言模型性能，检测模型在不同语言变体和偏见上的表现差异。

Abstract: Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.
  Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.
  Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.
  Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.

</details>


### [70] [Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift](https://arxiv.org/abs/2601.12639)
*Daniel Vennemeyer,Punya Syon Pandey,Phan Anh Duong,Michael Umeokoli,Samuel Ratnam*

Main category: cs.CL

TL;DR: 本文系统比较了六种微调目标在相同条件下对大模型安全性和能力的影响，发现微调目标对安全性影响随训练规模显著变化，是提升对抗鲁棒性和保持人格稳定的重要因素。


<details>
  <summary>Details</summary>
Motivation: 当前研究中对微调不同目标如何影响大模型的安全性，尤其是对齐性和对抗鲁棒性的影响分析有限。

Method: 设计六种微调目标（监督微调、直接偏好优化、条件微调、接种提示、赔率比偏好优化、KL正则化微调），在相同数据、领域、架构和优化条件下进行对比实验。

Result: 在封闭推理和开放生成任务中，不同微调目标在安全与能力的权衡上表现出系统性且依赖规模的差异：小规模训练时各目标鲁棒性相近但能力不同，大规模训练时监督或偏好目标虽提升能力但带来对抗脆弱性和人格漂移；限制信号的目标（尤其是赔率比优化和KL正则化）有效减轻了这些问题。

Conclusion: 微调目标在小规模训练时对安全影响有限，但随着训练规模增加，成为决定模型对抗鲁棒性和人格稳定性的关键因素。

Abstract: Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.

</details>


### [71] [Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?](https://arxiv.org/abs/2601.12648)
*Nafiz Imtiaz Khan,Kylie Cleland,Vladimir Filkov,Roger Eric Goldman*

Main category: cs.CL

TL;DR: 利用大语言模型自动提取放射学病例日志信息，能有效减轻学员文书负担并提升记录一致性，验证了AI辅助医学教育文档的可行性。


<details>
  <summary>Details</summary>
Motivation: 放射学培训中病例日志需要手动填写，既耗时又易产生不一致，故探索使用LLMs自动化病例日志文档的可行性。

Method: 采用多种本地及商业大语言模型，在指令式和链式思维提示下，从414份介入放射学报告中提取结构化程序性信息，评估模型的灵敏度、特异性、F1分数以及推理延迟和令牌效率。

Result: 模型取得了接近0.87的最佳F1分数，表现优异，同时在速度和成本方面存在不同权衡，显示出自动化病例日志记录的潜力。

Conclusion: 本研究表明大语言模型（LLMs）能够有效自动化放射学培训中的病例日志记录，显著减轻学员的文书负担，提高记录一致性。

Abstract: Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.

</details>


### [72] [Augmenting Question Answering with A Hybrid RAG Approach](https://arxiv.org/abs/2601.12658)
*Tianyi Yang,Nashrah Haque,Vaishnave Jonnalagadda,Yuya Jeremy Ong,Zhehui Chen,Yanzhao Wu,Lei Yu,Divyesh Jadav,Wenqi Wei*

Main category: cs.CL

TL;DR: 本文提出SSRAG，通过结构化语义搜索提升问答系统回答的准确性和丰富性，在多个数据集和模型中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在检索上下文相关信息上存在不足，导致回答不完整或次优。

Method: 提出SSRAG混合架构，结合向量和图检索技术及上下文统一，通过查询增强和代理路由优化检索流程。

Result: 在TruthfulQA、SQuAD和WikiQA三个数据集及五个大型语言模型上，SSRAG均显著优于标准RAG实现。

Conclusion: SSRAG通过查询增强、代理路由和结构化检索机制改进检索过程，提升了问答系统的答案准确性和信息丰富度。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.

</details>


### [73] [UbuntuGuard: A Culturally-Grounded Policy Benchmark for Equitable AI Safety in African Languages](https://arxiv.org/abs/2601.12696)
*Tassallah Abdullahi,Macton Mgonzo,Mardiyyah Oduwole,Paul Okewunmi,Abraham Owodunni,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CL

TL;DR: 本文介绍UbuntuGuard，首个非洲语言安全基准，基于专家设计的对抗查询，评估现有守护模型在非洲语言和文化中的表现，发现现有方法不足，强调开发多语言文化安全基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有守护模型主要针对高资源的西方语言设计，不适合低资源的非洲语言，存在跨语言安全失败及文化不匹配问题，亟需结合当地文化和风险情境的灵活安全政策和基准。

Method: 构建了UbuntuGuard，这是首个基于政策的非洲安全基准，利用155名领域专家撰写的对抗性查询制定具体安全政策和参考回答，基于这些实现对守护模型的政策对齐评估。评估了13个模型，包含通用大型语言模型和三种变体的守护模型（静态、动态、多语言）。

Result: 通过专家设计的上下文特定政策和查询，评估显示现有模型在多语言环境和非洲文化背景下表现不足，表明需要开发灵活、多语言、多文化的安全基准以提升低资源语言的模型安全性。

Conclusion: 现有的以英语为中心的安全基准高估了多语言环境下的实际安全性，跨语言迁移能力有限，动态模型虽有所提升但仍无法完全适应非洲语言的具体文化语境，迫切需要多语言、文化依托的安全基准来支持低资源语言的监管模型发展。

Abstract: Current guardian models are predominantly Western-centric and optimized for high-resource languages, leaving low-resource African languages vulnerable to evolving harms, cross-lingual safety failures, and cultural misalignment. Moreover, most guardian models rely on rigid, predefined safety categories that fail to generalize across diverse linguistic and sociocultural contexts. Robust safety, therefore, requires flexible, runtime-enforceable policies and benchmarks that reflect local norms, harm scenarios, and cultural expectations. We introduce UbuntuGuard, the first African policy-based safety benchmark built from adversarial queries authored by 155 domain experts across sensitive fields, including healthcare. From these expert-crafted queries, we derive context-specific safety policies and reference responses that capture culturally grounded risk signals, enabling policy-aligned evaluation of guardian models. We evaluate 13 models, comprising six general-purpose LLMs and seven guardian models across three distinct variants: static, dynamic, and multilingual. Our findings reveal that existing English-centric benchmarks overestimate real-world multilingual safety, cross-lingual transfer provides partial but insufficient coverage, and dynamic models, while better equipped to leverage policies at inference time, still struggle to fully localize African-language contexts. These findings highlight the urgent need for multilingual, culturally grounded safety benchmarks to enable the development of reliable and equitable guardian models for low-resource languages. Our code can be found online.\footnote{Code repository available at https://github.com/hemhemoh/UbuntuGuard.

</details>


### [74] [A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization](https://arxiv.org/abs/2601.12698)
*Qiuyi Qu,Yicheng Sui,Yufei Sun,Rui Chen,Xiaofei Zhang,Yuzhi Zhang,Haofeng Wang,Ge Lan,Ning Zhang*

Main category: cs.CL

TL;DR: 本文提出一种基于模板重写加搜索自动调优的代理驱动GPU核函数优化方法，显著提升性能稳定性和加速效果，最高超3倍，优化过程更具可控性和系统性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型代理直接代码重写方法存在参数隐式难控和依赖人工干预的问题，导致性能提升不稳定，难实现接近硬件极限的性能优化。

Method: 设计了一个代理驱动的迭代循环框架，将核函数语义重构为显式可调参数的模板，并通过基于搜索的自动调优对模板参数进行优化。该框架包括模板化、测试、分析和规划步骤，并利用性能分析反馈在硬件资源限制下进行受约束参数搜索。

Result: 在实际来自SGLang的CUDA核函数测试中，所提方法实现了最高超过3倍的加速，优化过程更具可解释性和系统性，显著减少迭代优化的随机性。

Conclusion: 提出的基于模板的重写层结合代理驱动的迭代循环及搜索自动调优，实现了更加稳定和高质量的GPU核函数性能优化，最大加速超过3倍。

Abstract: GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.

</details>


### [75] [A Shared Geometry of Difficulty in Multilingual Language Models](https://arxiv.org/abs/2601.12731)
*Stefano Civelli,Pietro Bernardelle,Nicolò Brunello,Gianluca Demartini*

Main category: cs.CL

TL;DR: 本文研究大型语言模型中问题难度的多语言内部表征，发现存在浅层语言无关和深层语言特定的两阶段难度编码机制。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在多语言环境下如何编码问题难度及其内部表征的几何结构。

Method: 通过使用Easy2Hard基准的AMC子集，翻译成21种语言，并训练线性探针分别在模型的浅层和深层表示上进行问题难度预测。

Result: 浅层表示的探针在跨语言难度预测中表现更好，深层表示在单一语言表现更优，揭示了问题难度表征的两阶段过程及其语言依赖性差异。

Conclusion: 大型语言模型在处理问题难度时表现出两个不同阶段的内部表征：浅层表示具有较好的跨语言泛化能力，而深层表示在同语言内表现更优但跨语言泛化较差，表明模型先形成语言无关的难度表征，后转化为语言特定的表征。

Abstract: Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.

</details>


### [76] [Towards Robust Process Reward Modeling via Noise-aware Learning](https://arxiv.org/abs/2601.12748)
*Bin Xie,Bingbing Xu,Xueyun Tian,Yilin Chen,Huawei Shen*

Main category: cs.CL

TL;DR: 本文针对过程奖励模型中由蒙特卡洛估计引发的标签噪声问题，提出反思感知标签校正与噪声感知迭代训练的两阶段框架，大幅提升步骤正确性的判别效果。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型依赖昂贵的过程级监督，蒙特卡洛估计虽普遍使用但产生的奖励依赖于策略模型，导致标签噪声问题，影响模型性能。

Method: 引入反思感知的标签校正机制利用大语言模型检测反思与自我修正行为以减少过高奖励，结合噪声感知迭代训练框架使模型基于自身置信度逐步优化噪声标签。

Result: 方法实现了在步骤正确性判别上的大幅提升，平均F1分数较带噪声监督训练的PRM提升了27个百分点。

Conclusion: 提出了一个两阶段框架，有效缓解了过程奖励模型中由于蒙特卡洛估计产生的标签噪声问题，显著提升了步骤正确率判别能力。

Abstract: Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \underline{\textbf{N}}oise-\underline{\textbf{A}}ware \underline{\textbf{I}}terative \underline{\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\% absolute gain in average F1 over PRMs trained with noisy supervision.

</details>


### [77] [VISPA: Pluralistic Alignment via Automatic Value Selection and Activation](https://arxiv.org/abs/2601.12758)
*Shenyan Zheng,Jiayou Zhong,Anudeex Shetty,Heng Ji,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: VISPA是一种无需额外训练，通过内部激活机制实现语言模型多元价值对齐的框架，提升了模型输出对多样价值观的反映能力，适用于高风险领域。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现价值多元化对齐时存在价值控制不足和表示有限的问题，难以满足高风险领域对多元视角的需求。

Method: 通过训练免费的方法，采用动态选择与内部模型激活引导，实现对模型输出价值表达的控制。

Result: 在多个模型和评估环境（包括医疗领域）中，VISPA表现出优异的多元化价值对齐能力，并能适应不同的启动方式及价值标准。

Conclusion: 本文提出的VISPA框架实现了对大型语言模型内在价值表达的直接控制，成功实现了多元化价值取向的对齐，适应不同模型和价值观。

Abstract: As large language models are increasingly used in high-stakes domains, it is essential that their outputs reflect not average} human preference, rather range of varying perspectives. Achieving such pluralism, however, remains challenging. Existing approaches consider limited values or rely on prompt-level interventions, lacking value control and representation. To address this, we introduce VISPA, a training-free pluralistic alignment framework, that enables direct control over value expression by dynamic selection and internal model activation steering. Across extensive empirical studies spanning multiple models and evaluation settings, we show VISPA is performant across all pluralistic alignment modes in healthcare and beyond. Further analysis reveals VISPA is adaptable with different steering initiations, model, and/or values. These results suggest that pluralistic alignment can be achieved through internal activation mechanisms, offering a scalable path toward language models that serves all.

</details>


### [78] [Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory](https://arxiv.org/abs/2601.12771)
*Keito Inoshita*

Main category: cs.CL

TL;DR: 本文提出基于大语言模型联想记忆的多智能体框架LAMA，通过召回相同名字的知名人物间接推理，显著提升了国籍预测准确率，超越传统推理方法。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型的直接推理方法在应用抽象语言规则时存在局限，而国籍预测任务需要结合语言、文化和历史背景知识，需要更有效地激发大语言模型的世界知识。

Method: 采用双智能体架构（人员智能体和媒体智能体）并行召回具有相同名字的知名人物，通过投票和条件完成的方式实现间接推理，从而预测国籍。

Result: 在99国家的国籍预测任务中，LAMA框架取得了0.817的准确率，表现显著优于传统方法，并展现了对低频国籍的鲁棒性和多智能体体系的协同效应。

Conclusion: 本文提出的LAMA框架通过利用大语言模型作为联想记忆，显著提升了国籍预测任务的准确性，超越了传统的直接推理方法和神经模型。

Abstract: Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.

</details>


### [79] [Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?](https://arxiv.org/abs/2601.12812)
*Sushant Kumar Ray,Gautam Siddharth Kashyap,Sahil Tripathi,Nipun Joshi,Vijay Govindarajan,Rafiq Ali,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出无需微调的推理时对齐策略MEDASSESS-X，有效提升了临床问答大模型的准确性和安全性，挑战了专业化微调的传统假设。


<details>
  <summary>Details</summary>
Motivation: 目前专业医学大模型在临床问答中虽广泛使用，却存在覆盖面窄、微调成本高和适应性差等局限，且存在专业化谬误。

Method: 提出MEDASSESS-X框架，在推理时利用轻量级引导向量调整模型激活，增强医学推理的一致性，无需微调模型权重。

Result: MEDASSESS-X在多种大模型上均提升性能，准确率提升最高6%，事实一致性提升7%，安全错误率降低50%。

Conclusion: 基于领域特定微调的假设在临床问答系统中并非必需，通过推理时的对齐策略同样可以实现性能提升，消除专业化谬误。

Abstract: Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.

</details>


### [80] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: 本文提出了任务分解和多模态数据结合的JurisMMA框架，并构建大规模司法多模态数据集JurisMM，有效提升法律判决预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统法律判决预测方法在处理多重指控、多样化证据以及适应性方面存在显著挑战，亟需一种有效的框架和丰富的数据资源来提升性能。

Method: 提出了JurisMMA框架，采用任务分解、流程标准化和阶段化组织方法，同时构建了包含文本及多模态视频-文本数据的JurisMM大规模司法记录数据集。

Result: 在JurisMM数据集及LawBench基准测试中的实验结果验证了JurisMMA框架的有效性，证明其对法律判决预测及其他法律应用具有推广价值。

Conclusion: JurisMMA框架在法律判决预测任务中表现出色，不仅提升了多指控和多模态数据的处理能力，还适用于更广泛的法律应用场景。

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


### [81] [Rapport du Projet de Recherche TRAIMA](https://arxiv.org/abs/2601.12844)
*Julie Rançon,Jean-François Cerisier,Emilie Remond,Aurélien Nguyen,Andrew Peterson,Ladjel Bellatreche*

Main category: cs.CL

TL;DR: TRAIMA探讨多模态课堂互动自动处理的可能性，结合机器学习和详细的转录方法，建立多模态教学互动自动分析的理论和方法框架。


<details>
  <summary>Details</summary>
Motivation: 传统教育和互动研究中，语言、言语外及非语言数据分析均依赖手工处理，耗时且难以规模化，亟需自动化方法支持多模态互动的高效分析。

Method: 该项目采用机器学习方法，对多模态课堂互动中的语音、言语外、非言语数据进行分类和标注，结合多摄像头视频、同步音频、眼动追踪等多模态数据，利用手工转录和注释以支撑自动工具开发。

Result: 项目明确了适用于机器学习的转录规范、注释类别和分析单元，提出解释性话语的三段式定义，展示了多模态互动转录的变异性和解释性，为未来自动处理工具的开发奠定基础。

Conclusion: TRAIMA项目的目标是建立一个严格的方法论框架，用于多模态教学互动的自动处理，而非开发完全自动化系统。

Abstract: The TRAIMA project (TRaitement Automatique des Interactions Multimodales en Apprentissage), conducted between March 2019 and June 2020, investigates the potential of automatic processing of multimodal interactions in educational settings. The project addresses a central methodological challenge in educational and interactional research: the analysis of verbal, paraverbal, and non-verbal data is currently carried out manually, making it extremely time-consuming and difficult to scale. TRAIMA explores how machine learning approaches could contribute to the categorisation and classification of such interactions. The project focuses specifically on explanatory and collaborative sequences occurring in classroom interactions, particularly in French as a Foreign Language (FLE) and French as a First Language (FLM) contexts. These sequences are analysed as inherently multimodal phenomena, combining spoken language with prosody, gestures, posture, gaze, and spatial positioning. A key theoretical contribution of the project is the precise linguistic and interactional definition of explanatory discourse as a tripartite sequence (opening, explanatory core, closure), drawing on discourse analysis and interactional linguistics. A substantial part of the research is devoted to the methodological foundations of transcription, which constitute a critical bottleneck for any form of automation. The report provides a detailed state of the art of existing transcription conventions (ICOR, Mondada, GARS, VALIBEL, Ferr{é}), highlighting their respective strengths and limitations when applied to multimodal classroom data. Through comparative analyses of manually transcribed sequences, the project demonstrates the inevitable variability and interpretative dimension of transcription practices, depending on theoretical positioning and analytical goals. Empirical work is based on several corpora, notably the INTER-EXPLIC corpus (approximately 30 hours of classroom interaction) and the EXPLIC-LEXIC corpus, which serve both as testing grounds for manual annotation and as reference datasets for future automation. Particular attention is paid to teacher gestures (kin{é}sic and proxemic resources), prosodic features, and their functional role in meaning construction and learner comprehension. The project also highlights the strategic role of the Techn{é}LAB platform, which provides advanced multimodal data capture (multi-camera video, synchronized audio, eye-tracking, digital interaction traces) and constitutes both a research infrastructure and a test environment for the development of automated tools. In conclusion, TRAIMA does not aim to deliver a fully operational automated system, but rather to establish a rigorous methodological framework for the automatic processing of multimodal pedagogical interactions. The project identifies transcription conventions, annotation categories, and analytical units that are compatible with machine learning approaches, while emphasizing the need for theoretical explicitness and researcher reflexivity. TRAIMA thus lays the groundwork for future interdisciplinary research at the intersection of didactics, discourse analysis, multimodality, and artificial intelligence in education.

</details>


### [82] [Race, Ethnicity and Their Implication on Bias in Large Language Models](https://arxiv.org/abs/2601.12868)
*Shiyue Hu,Ruizhe Li,Yanjun Gao*

Main category: cs.CL

TL;DR: 本研究通过分析大型语言模型内部机制揭示种族和族裔信息的分布及其对偏见的影响，指出单一干预难以根治偏见，强调对模型行为的系统性调控。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型输出层面的偏差，缺乏对大型语言模型内部机制如何导致种族和族裔差异的深入理解。

Method: 结合探测、神经元级归因和有针对性的干预，使用两个公开数据集和三个开源模型进行机制性分析，研究模型内部如何表示和操作种族及族裔信息。

Result: 发现人口统计信息分布在多个神经元中且模型间差异显著，干预相关神经元可减少偏见但效果有限，表明需要更系统的偏见缓解策略。

Conclusion: 大型语言模型内部的种族和族裔信息分布广泛且因模型而异，偏见的存在不仅源于表示层面，还涉及模型行为，单纯抑制特定神经元无法完全消除偏见。

Abstract: Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.

</details>


### [83] [From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.12904)
*Jiahao Wang,Weiyu Xie,Mingxing Zhang,Boxing Zhang,Jianwei Dong,Yuening Zhu,Chen Lin,Jinqi Tang,Yaochen Han,Zhiyuan Ai,Xianglin Chen,Yongwei Wu,Congfeng Jiang*

Main category: cs.CL

TL;DR: FusionRAG通过嵌入跨块信息和选择性重计算KV缓存，在保持高生成质量的同时大幅提升了检索增强生成的效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法因提示长度增长导致计算成本和响应时间增加，且简单重用KV缓存缺乏跨块上下文信息，导致生成质量下降。如何在重用KV缓存的同时保持生成质量是主要挑战。

Method: 提出FusionRAG推理框架，通过在离线预处理阶段将相关文本块信息嵌入每个块内，在线重新处理中仅对关注的关键Token重新计算KV缓存，从而提升生成质量和效率。

Result: FusionRAG在相同重计算比例下，生成质量明显优于现有方法。仅重计算小于15%Token，生成F1分数提升达70%，TTFT比全注意力机制减少2.66倍至9.39倍。

Conclusion: FusionRAG框架在提高生成质量的同时显著降低了计算成本和响应时间，实现了效果与效率的最佳平衡。

Abstract: Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.

</details>


### [84] [Gated Differentiable Working Memory for Long-Context Language Modeling](https://arxiv.org/abs/2601.12906)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Yuyao Ge,Baolong Bi,Jiayu Yao,Jun Wan,Ziling Yin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 针对长上下文挑战，本文提出Gdwm框架通过选择性巩固记忆，实现高效且性能优异的推理适应。


<details>
  <summary>Details</summary>
Motivation: 长上下文环境下Transformer模型面临注意力分散、关键信息丢失以及适应新模式困难的问题，现有统一写入策略效率低且梯度方差大。

Method: 提出Gdwm（门控可微工作记忆）框架，利用信息论的上下文效用度量动态调节梯度更新步数，有针对性地巩固重要上下文信息。

Result: 在ZeroSCROLLS和LongBench v2数据集上，Gdwm在使用4倍更少梯度步数的情况下实现了等同或更优的性能，提升了适应效率。

Conclusion: 本文提出的Gdwm框架通过引入写入控制器，有效选择上下文中应保留于工作记忆的部分，提升了长文本上下文下模型的推理效率和性能。

Abstract: Long contexts challenge transformers: attention scores dilute across thousands of tokens, critical information is often lost in the middle, and models struggle to adapt to novel patterns at inference time. Recent work on test-time adaptation addresses this by maintaining a form of working memory -- transient parameters updated on the current context -- but existing approaches rely on uniform write policies that waste computation on low-utility regions and suffer from high gradient variance across semantically heterogeneous contexts. In this work, we reframe test-time adaptation as a budget-constrained memory consolidation problem, focusing on which parts of the context should be consolidated into working memory under limited computation. We propose Gdwm (Gated Differentiable Working Memory), a framework that introduces a write controller to gate the consolidation process. The controller estimates Contextual Utility, an information-theoretic measure of long-range contextual dependence, and allocates gradient steps accordingly while maintaining global coverage. Experiments on ZeroSCROLLS and LongBench v2 demonstrate that Gdwm achieves comparable or superior performance with 4$\times$ fewer gradient steps than uniform baselines, establishing a new efficiency-performance Pareto frontier for test-time adaptation.

</details>


### [85] [SciCoQA: Quality Assurance for Scientific Paper--Code Alignment](https://arxiv.org/abs/2601.12910)
*Tim Baumgärtner,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出SciCoQA数据集用于检测论文与代码不一致，评测显示现有大型语言模型在此任务中表现有限。


<details>
  <summary>Details</summary>
Motivation: 确保科学论文与其代码库之间的一致实现，以避免不准确性。

Method: 构建包含真实和合成数据的SciCoQA数据集，分析论文与代码的不匹配类型，并利用合成数据生成方法扩展数据集。

Result: 收集了611个论文-代码不一致实例，涵盖多个科学领域，评测21个大型语言模型，发现检测任务难度大，最佳模型仅能识别45.7%的真实不一致。

Conclusion: SciCoQA数据集揭示了当前模型在识别论文与代码不一致性方面的局限性，强调了改进模型能力的必要性。

Abstract: We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\% of real-world paper-code discrepancies.

</details>


### [86] [Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs](https://arxiv.org/abs/2601.12921)
*Adimulya Kartiyasa,Bao Gia Cao,Boyang Li*

Main category: cs.CL

TL;DR: 本文构建了基于印度尼西亚社会科学期刊的文化知识数据集IndoSoSci，结合检索增强生成方法，有效提升了大型语言模型在印度尼西亚文化理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型对印度尼西亚文化的理解仍有不足，本研究希望通过本地社会科学期刊的文化知识来提升模型表现。

Method: 收集151份开放获取的印度尼西亚社会科学期刊文本，构建IndoSoSci数据集，提取印度尼西亚文化相关事实，采用检索增强生成（RAG）方法，利用语言模型生成的假设文档作为查询进行检索。

Result: 提出的方法在IndoCulture基准测试中表现优异，超越多个强基线。同时结合IndoSoSci和印度尼西亚维基百科数据，创新性地刷新了该基准的准确率。

Conclusion: 通过利用本地社会科学期刊文本和检索增强生成技术，可以显著提升大型语言模型对印度尼西亚文化的理解和表现。

Abstract: Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.

</details>


### [87] [A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits](https://arxiv.org/abs/2601.12945)
*Miao Xie,Siguang Chen,Chunli Lv*

Main category: cs.CL

TL;DR: 本文首次系统综述了大型语言模型与多臂老虎机算法的双向应用，揭示两者结合带来的优势与挑战，并提供了详细的文献索引资源。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs与MAB算法结合的潜力，解决LLMs面临的关键挑战，同时利用LLMs提升MAB算法的核心组成和性能。

Method: 通过调研和分析现有文献，归纳了LLMs与MAB算法在预训练、增强生成、个性化及决策制定中的应用与改进方法，并建立相关文献索引库。

Result: 总结了LLM增强的MAB系统和MAB增强的LLM系统的设计和性能表现，指出关键挑战和未来研究方向。

Conclusion: 本文系统回顾了大型语言模型（LLMs）与多臂老虎机算法（MAB）之间的双向交互及其在各自领域中的相互促进作用。

Abstract: Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.

</details>


### [88] [Trustworthy Data-driven Chronological Age Estimation from Panoramic Dental Images](https://arxiv.org/abs/2601.12960)
*Ainhoa Vivel-Couso,Nicolás Vila-Blanco,María J. Carreira,Alberto Bugarín-Diz,Inmaculada Tomás,Jose M. Alonso-Moral*

Main category: cs.CL

TL;DR: 本文设计了一个结合透明与不透明方法的牙齿年龄估计系统，通过自然语言生成提供临床友好解释，专家评分高，提升了模型透明性和可信度。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医疗中的应用虽实现个性化，但模型不透明导致信任问题。为提升透明度和可信性，设计便于临床医生理解的解释系统。

Method: 采用结合不透明与透明技术的自然语言生成模块，基于规则设计文本解释，专家通过问卷验证解释质量，并进行可信度自我评估。

Result: 生成的解释被牙科专家评为4.77/5分，可信度自评也达4.40/5分，表明系统在解释质量和信任度方面表现优异。

Conclusion: 本文提出的结合不透明与透明方法的牙齿年龄估计系统通过自然语言生成模块提供可信的解释，显著提升了模型的透明性和临床可用性。

Abstract: Integrating deep learning into healthcare enables personalized care but raises trust issues due to model opacity. To improve transparency, we propose a system for dental age estimation from panoramic images that combines an opaque and a transparent method within a natural language generation (NLG) module. This module produces clinician-friendly textual explanations about the age estimations, designed with dental experts through a rule-based approach. Following the best practices in the field, the quality of the generated explanations was manually validated by dental experts using a questionnaire. The results showed a strong performance, since the experts rated 4.77+/-0.12 (out of 5) on average across the five dimensions considered. We also performed a trustworthy self-assessment procedure following the ALTAI checklist, in which it scored 4.40+/-0.27 (out of 5) across seven dimensions of the AI Trustworthiness Assessment List.

</details>


### [89] [Pardon? Evaluating Conversational Repair in Large Audio-Language Models](https://arxiv.org/abs/2601.12973)
*Shuanghong Huang,Jinlei Xu,Youchao Zhou,Yanghao Zhou,Xuan Zhao,Chong Feng,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 本文提出了修复感知的评估方法及EAR评分，揭示现有大型音频语言模型虽能答题准确，但对语义不可答的输入缺乏识别和修复能力，促进模型在实际对话中的可靠性提升。


<details>
  <summary>Details</summary>
Motivation: 现有评估假设语音输入始终具备语义可答性，但实际交互中关键信息缺失导致输入不可答，需设计能反映模型在此类情况表现的评估指标。

Method: 本文提出了一种修复感知的评估设置，通过语义-声学掩码协议构造可答与不可答的配对评估条件，并引入了EAR评分，联合评估模型在回答问题和修复无答案输入时的表现。

Result: 通过在两个语音问答基准上测试多种大型音频语言模型，发现在可答条件下模型准确率较高，但大多数模型无法识别语义不可答的输入并进行适当的交互修复，暴露出现有评估的不足。

Conclusion: 现有大型音频语言模型在语音问答任务中表现良好，但主要评估集中于答案准确性，忽视了输入语音在语义上可能不可回答的情况，导致模型在实际交互中缺乏识别和修正无答案输入的能力。

Abstract: Large Audio-Language Models (LALMs) have demonstrated strong performance in spoken question answering (QA), with existing evaluations primarily focusing on answer accuracy and robustness to acoustic perturbations. However, such evaluations implicitly assume that spoken inputs remain semantically answerable, an assumption that often fails in real-world interaction when essential information is missing. In this work, we introduce a repair-aware evaluation setting that explicitly distinguishes between answerable and unanswerable audio inputs. We define answerability as a property of the input itself and construct paired evaluation conditions using a semantic-acoustic masking protocol. Based on this setting, we propose the Evaluability Awareness and Repair (EAR) score, a non-compensatory metric that jointly evaluates task competence under answerable conditions and repair behavior under unanswerable conditions. Experiments on two spoken QA benchmarks across diverse LALMs reveal a consistent gap between answer accuracy and conversational reliability: while many models perform well when inputs are answerable, most fail to recognize semantic unanswerability and initiate appropriate conversational repair. These findings expose a limitation of prevailing accuracy-centric evaluation practices and motivate reliability assessments that treat unanswerable inputs as cues for repair and continued interaction.

</details>


### [90] [Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios](https://arxiv.org/abs/2601.12974)
*Hongyang Ma,Tiantian Gu,Huaiyuan Sun,Huilin Zhu,Yongxin Wang,Jie Li,Wubin Sun,Zeliang Lian,Yinghong Zhou,Yi Gao,Shirui Wang,Zhihui Tang*

Main category: cs.CL

TL;DR: 本文提出SCMPE基准全面评估牙科大型语言模型，发现模型在动态互动中的信息收集和状态追踪能力不足，外部知识辅助有限，指出未来需加强领域适应训练以实现更安全自主的临床应用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从被动知识检索者向自主临床代理转变，评估标准需要从静态准确性转向动态行为可靠性，特别是在牙科领域，这对患者参与决策至关重要。

Method: 提出了标准化临床管理与性能评估（SCMPE）基准，涵盖知识导向的静态任务和基于工作流程的多轮模拟患者互动，以全面评估模型性能。

Result: 模型在静态任务中表现优异，但在动态临床对话中表现显著下降，主要瓶颈在于主动信息收集和动态状态跟踪。一般模型常出现高效但低安全的风险。检索增强生成（RAG）虽然减少了静态任务中的幻觉，但在动态工作流程中的效果不稳定，有时反而退化。

Conclusion: 牙科领域的大型语言模型在动态临床应用中仍存在能力边界，外部知识补充无法完全解决推理缺口，需结合领域自适应预训练以提升安全性和自主临床实践能力。

Abstract: The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping "Guideline Adherence" versus "Decision Quality" reveals a prevalent "High Efficacy, Low Safety" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.

</details>


### [91] [The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check](https://arxiv.org/abs/2601.12979)
*Qingyu Lu,Liang Ding,Kanjian Zhang,Jinxia Zhang,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文评估了基于扩散机制的大语言模型在智能代理中的表现，发现其虽提升效率但频繁失败，提出需结合因果精确推理以实现实用智能代理。


<details>
  <summary>Details</summary>
Motivation: 探讨基于扩散的语言模型作为替代自回归模型在智能代理交互中的潜力及其效率提升是否能转化为有效的智能行为。

Method: 本文通过在两种不同智能代理范式（具体现身代理和工具调用代理）上综合评估dLLMs性能，设计并引入了DiffuAgent多代理评估框架，将dLLMs作为认知核心进行实验分析。

Result: 实验结果表明，dLLMs在非因果角色（如记忆总结和工具选择）中表现有效，但在因果推理和符号精度方面存在明显缺陷，限制了其作为智能代理骨干的可行性。

Conclusion: 当前基于扩散的语言模型(dLLMs)虽然在效率上有所提升，但在作为智能代理骨干时表现不佳，无法有效应对长程规划和精确格式需求，频繁失败。

Abstract: The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a "bitter lesson": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.

</details>


### [92] [ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation](https://arxiv.org/abs/2601.12983)
*Jesus-German Ortiz-Barajas,Jonathan Tonglet,Vivek Gupta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了ChartAttack框架，揭示多模态大语言模型生成图表时可被滥用制造误导，显著降低问答性能并影响人类判断，强调需加强系统的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型自动生成图表的广泛应用，存在被滥用生成误导性图表的风险，迫切需要评估和防范此类风险。

Method: 提出了ChartAttack框架，通过在图表设计中注入误导元素来大规模生成误导性图表，并构建了带有误导标签的图表问答数据集AttackViz，用于评估误导效果。

Result: 实验表明，ChartAttack在同域和跨域场景下均显著降低多模态大语言模型的问答准确率，分别下降19.6点和14.9点；人类实验中参与者的准确率也下降了20.2点。

Conclusion: ChartAttack框架揭示了多模态大语言模型在生成图表时可能被滥用以产生误导性图表，显著降低了图表问答系统的准确率，提醒了此类系统设计中必须重视稳健性和安全性问题。

Abstract: Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.

</details>


### [93] [Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.12995)
*Runxuan Liu,Xianhao Ou,Xinyan Ma,Jiyuan Wang,Jiafeng Liang,Jiaqi Li,Tao He,Zheng Chu,Rongchuan Mu,Zekun Wang,Baoxin Wang,Dayong Wu,Ming Liu,Shijin Wang,Guoping Hu,Bing Qin*

Main category: cs.CL

TL;DR: 论文提出了基于图结构的结构化推理范式和过程感知强化学习优化算法，解决了长链推理训练中的效率和泛化难题，在数学和代码生成任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的长链推理主要基于纯文本生成，语义评价计算复杂且低效，且强化学习优化面临监督粗糙、奖励欺骗等难题，影响训练效果和泛化能力。因此需要构建结构化、符号化的推理框架及相应优化方法。

Method: 通过引入基于图结构和步骤认知标签的结构化符号推理（GRP），替代传统的文本语义评价，并设计了分层裁剪优势估计策略的优化算法（PASC-GRPO），实现了过程感知的验证和奖励优化。

Result: 实验结果表明，所提方法在数学推理和代码生成任务中均有显著性能提升，证明了结构化推理与过程感知优化策略的有效性。数据、模型和代码将随后公开。

Conclusion: 本文提出的图推理范式（GRP）结合分层裁剪策略优化方法（PASC-GRPO）有效解决了现有基于强化学习的长链推理方法中存在的粗粒度监督、奖励欺骗、高训练成本和泛化性差等问题，显著提升了大模型在数学推理和代码生成任务中的性能。

Abstract: Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.

</details>


### [94] [Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context](https://arxiv.org/abs/2601.13018)
*Ghislain Dorian Tchuente Mondjo*

Main category: cs.CL

TL;DR: 为解决仇恨言论检测中解释不稳定问题，提出了一种基于双向注意力和BiRNN的多任务学习模型，显著提升了分类性能和可解释性，减少了偏见。


<details>
  <summary>Details</summary>
Motivation: 现有基于HateXplain的算法中，注意力值波动较大，导致解释不一致、预测不稳定和学习困难，限制了仇恨言论检测模型的可靠性和可解释性。

Method: 采用双向注意力和双向循环神经网络（BiRNN）结合多任务学习，同时进行分类和解释，在模型训练过程中考虑输入数据的序列特征，提高解释的稳定性和准确性。

Result: 实验结果表明，所提模型在HateXplain数据集上实现了检测性能、解释效果的明显提升，同时降低了无意偏见的发生。

Conclusion: 本文提出了BiAtt-BiRNN-HateXplain模型，有效解决了HateXplain基算法中注意力变异性大的问题，提升了仇恨言论检测的性能和可解释性，减少了无意的社区偏见。

Abstract: Technological advances in the Internet and online social networks have brought many benefits to humanity. At the same time, this growth has led to an increase in hate speech, the main global threat. To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities. The experimental results on HateXplain data show a clear improvement in detection performance, explainability and a reduction in unintentional bias.

</details>


### [95] [Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses](https://arxiv.org/abs/2601.13024)
*Chongyuan Dai,Yaling Shen,Jinpeng Hu,Zihan Gao,Jia Li,Yishun Jiang,Yaxiong Wang,Liu Liu,Zongyuan Ge*

Main category: cs.CL

TL;DR: 提出CEDAR基准以评估大型语言模型的跨文化情感理解能力，结果显示现有模型难以准确捕捉文化驱动的情感差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注地理事实和社会习俗等陈述性知识，无法充分反映文化多样性带来的主观情感诠释差异。

Method: 提出了CEDAR多模态基准，运用LLM生成初步标签筛选出跨文化情感差异显著的实例，再通过严格人工评估获得高质量标注。

Result: 构建了涵盖7种语言、14个细分类别情感、共10962个样本的CEDAR基准，测试了17个多语种模型，发现语言一致性与文化对齐存在脱节现象。

Conclusion: 当前大型语言模型在文化对齐方面仍面临重大挑战，特别是在带有文化背景色彩的情感理解上表现不足。

Abstract: Culture serves as a fundamental determinant of human affective processing and profoundly shapes how individuals perceive and interpret emotional stimuli. Despite this intrinsic link extant evaluations regarding cultural alignment within Large Language Models primarily prioritize declarative knowledge such as geographical facts or established societal customs. These benchmarks remain insufficient to capture the subjective interpretative variance inherent to diverse sociocultural lenses. To address this limitation, we introduce CEDAR, a multimodal benchmark constructed entirely from scenarios capturing Culturally \underline{\textsc{E}}licited \underline{\textsc{D}}istinct \underline{\textsc{A}}ffective \underline{\textsc{R}}esponses. To construct CEDAR, we implement a novel pipeline that leverages LLM-generated provisional labels to isolate instances yielding cross-cultural emotional distinctions, and subsequently derives reliable ground-truth annotations through rigorous human evaluation. The resulting benchmark comprises 10,962 instances across seven languages and 14 fine-grained emotion categories, with each language including 400 multimodal and 1,166 text-only samples. Comprehensive evaluations of 17 representative multilingual models reveal a dissociation between language consistency and cultural alignment, demonstrating that culturally grounded affective understanding remains a significant challenge for current models.

</details>


### [96] [SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification](https://arxiv.org/abs/2601.13035)
*Xu Xiaodan,Hu Xiaolin*

Main category: cs.CL

TL;DR: SASA通过分离注意力和语义感知对比学习改善知识图谱三元组分类，效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了知识图谱不同组件间的有效语义交互，且单一二分类训练目标导致语义表示学习不足。

Method: 提出了分离注意力机制对三元组进行解耦编码，并通过语义感知分层对比学习作为辅助训练目标，增强模型的判别能力和语义表示。

Result: 在FB15k-237数据集提高5.9%准确率，在YAGO3-10数据集提升3.4%，显著优于现有最先进方法。

Conclusion: SASA框架通过分离注意力机制和语义感知对比学习显著提升了知识图谱三元组分类的准确性，优于当前最先进方法。

Abstract: Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\% on FB15k-237 and +3.4\% on YAGO3-10.

</details>


### [97] [Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition](https://arxiv.org/abs/2601.13044)
*Warit Sirichotedumrong,Adisai Na-Thalang,Potsawee Manakul,Pittawat Taveekitworachai,Sittipong Sripaisarnmongkol,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 提出低延迟的Typhoon ASR Real-time泰语语音识别模型，通过文本规范化和课程学习实现高效准确的流式识别，并发布了标准化评测基准，填补了泰语流式ASR的空白。


<details>
  <summary>Details</summary>
Motivation: 现有泰语语音识别主要依赖高延迟的离线大型模型，缺少高效低延迟的流式解决方案。为解决这一实际需求，提出适用于流式应用的低延迟泰语ASR模型。

Method: 采用115M参数的FastConformer-Transducer模型，结合严格的文本规范化处理泰语特有的文本歧义问题，并通过两阶段课程学习方法适应东北方言，保持中心泰语性能不受影响。

Result: 模型在计算成本上相比Whisper Large-v3减少了45倍，同时在准确率上保持可比性能，规范化处理有效减少文本歧义，课程学习提升了方言适应能力。

Conclusion: Typhoon ASR Real-time模型通过严格的文本规范化和课程学习策略，在保持较低计算成本和低延迟的同时，实现了与大型模型Whisper Large-v3相当的准确性，满足泰语流式语音识别的应用需求。

Abstract: Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.

</details>


### [98] [Profiling German Text Simplification with Interpretable Model-Fingerprints](https://arxiv.org/abs/2601.13050)
*Lars Klöser,Mika Beele,Bodo Kraft*

Main category: cs.CL

TL;DR: 提出Simplification Profiler，生成模型简化的多维行为指纹，无需人工评分即可准确区分模型配置，助力构建更灵活高效的文本简化系统。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对大型语言模型文本简化行为的整体、高效、可复现诊断工具，且数据匮乏问题在多语言和多目标群体文本简化中尤为严重，需一种更相关的评估方式。

Method: 本文设计了Simplification Profiler诊断工具，通过聚合多个简化结果形成模型指纹，并通过线性分类器验证指纹对不同模型配置的区分能力，实现无需大量人工标注数据的元评估。

Result: 该方法在区分不同提示策略及细粒度提示工程调整时，F1-score最高达到71.9%，比基线提升超过48个百分点，证明其敏感且有效。

Conclusion: 本论文提出的Simplification Profiler能够生成多维度、可解释的文本简化特征指纹，准确区分模型的不同简化行为，提升了文本简化系统的诊断和适应能力。

Abstract: While Large Language Models (LLMs) produce highly nuanced text simplifications, developers currently lack tools for a holistic, efficient, and reproducible diagnosis of their behavior. This paper introduces the Simplification Profiler, a diagnostic toolkit that generates a multidimensional, interpretable fingerprint of simplified texts. Multiple aggregated simplifications of a model result in a model's fingerprint. This novel evaluation paradigm is particularly vital for languages, where the data scarcity problem is magnified when creating flexible models for diverse target groups rather than a single, fixed simplification style. We propose that measuring a model's unique behavioral signature is more relevant in this context as an alternative to correlating metrics with human preferences. We operationalize this with a practical meta-evaluation of our fingerprints' descriptive power, which bypasses the need for large, human-rated datasets. This test measures if a simple linear classifier can reliably identify various model configurations by their created simplifications, confirming that our metrics are sensitive to a model's specific characteristics. The Profiler can distinguish high-level behavioral variations between prompting strategies and fine-grained changes from prompt engineering, including few-shot examples. Our complete feature set achieves classification F1-scores up to 71.9 %, improving upon simple baselines by over 48 percentage points. The Simplification Profiler thus offers developers a granular, actionable analysis to build more effective and truly adaptive text simplification systems.

</details>


### [99] [Alexandria: A Multi-Domain Dialectal Arabic Machine Translation Dataset for Culturally Inclusive and Linguistically Diverse LLMs](https://arxiv.org/abs/2601.13099)
*Abdellah El Mekki,Samar M. Magdy,Houdaifa Atou,Ruwa AbuHweidi,Baraah Qawasmeh,Omer Nacar,Thikra Al-hibiri,Razan Saadie,Hamzah Alsayadi,Nadia Ghezaiel Hammouda,Alshima Alkhazimi,Aya Hamod,Al-Yas Al-Ghafri,Wesam El-Sayed,Asila Al sharji,Mohamad Ballout,Anas Belfathi,Karim Ghaddar,Serry Sibaee,Alaa Aoun,Areej Asiri,Lina Abureesh,Ahlam Bashiti,Majdal Yousef,Abdulaziz Hafiz,Yehdih Mohamed,Emira Hamedtou,Brakehe Brahim,Rahaf Alhamouri,Youssef Nafea,Aya El Aatar,Walid Al-Dhabyani,Emhemed Hamed,Sara Shatnawi,Fakhraddin Alwajih,Khalid Elkhidir,Ashwag Alasmari,Abdurrahman Gerrio,Omar Alshahri,AbdelRahim A. Elmadany,Ismail Berrada,Amir Azad Adli Alkathiri,Fadi A Zaraket,Mustafa Jarrar,Yahya Mohamed El Hadj,Hassan Alhuzali,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出了覆盖广泛方言和领域、带有细粒度标注的阿拉伯语方言翻译数据集Alexandria，推动方言机器翻译和语言模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译系统对阿拉伯语方言的支持不足，限制了其在广大方言使用者中的应用，亟需细粒度、多样化的方言数据支持。

Method: 构建并发布一个涵盖13个阿拉伯国家、11个领域、带有城市来源元数据及性别配置标注的大规模人工翻译对话数据集Alexandria，并通过自动和人工评测验证该数据集的有效性。

Result: Alexandria包含107K样本，覆盖多方言多领域对话场景，标注详尽，能作为训练数据和严格基准，评测结果显示当前阿拉伯语方言MT和LLM存在较大挑战。

Conclusion: Alexandria数据集为多种阿拉伯方言机器翻译和大型语言模型的训练与评测提供了重要资源，显著促进了方言MT系统的改进。

Abstract: Arabic is a highly diglossic language where most daily communication occurs in regional dialects rather than Modern Standard Arabic. Despite this, machine translation (MT) systems often generalize poorly to dialectal input, limiting their utility for millions of speakers. We introduce \textbf{Alexandria}, a large-scale, community-driven, human-translated dataset designed to bridge this gap. Alexandria covers 13 Arab countries and 11 high-impact domains, including health, education, and agriculture. Unlike previous resources, Alexandria provides unprecedented granularity by associating contributions with city-of-origin metadata, capturing authentic local varieties beyond coarse regional labels. The dataset consists of multi-turn conversational scenarios annotated with speaker-addressee gender configurations, enabling the study of gender-conditioned variation in dialectal use. Comprising 107K total samples, Alexandria serves as both a training resource and a rigorous benchmark for evaluating MT and Large Language Models (LLMs). Our automatic and human evaluation of Arabic-aware LLMs benchmarks current capabilities in translating across diverse Arabic dialects and sub-dialects, while exposing significant persistent challenges.

</details>


### [100] [Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification](https://arxiv.org/abs/2601.13105)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: 本文提出将LoRA微调与RAG框架结合，用于自动识别英语双及物结构，大幅提升了模型性能，且微调使模型判断更趋向语义理解。


<details>
  <summary>Details</summary>
Motivation: 提高大型语言模型对复杂语言结构——英语双及物结构的自动识别能力，弥补纯理论模型和未微调模型的不足。

Method: 本文通过在英国国家语料库的标注数据上对Qwen3-8B模型进行LoRA微调，并结合检索增强生成（RAG）框架，完成二分类任务。

Result: LoRA微调后的Qwen3-8B模型在二分类任务中性能明显优于原生Qwen3-MAX模型及仅依赖理论的RAG系统。

Conclusion: LoRA微调结合RAG框架能够显著提升大型语言模型对英语双及物结构的自动识别性能。

Abstract: This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.

</details>


### [101] [CORE-T: COherent REtrieval of Tables for Text-to-SQL](https://arxiv.org/abs/2601.13111)
*Hassan Soliman,Vivek Gupta,Dan Roth,Iryna Gurevych*

Main category: cs.CL

TL;DR: 针对多表文本到SQL检索难题，提出无训练CORE-T框架，显著提升检索准确率和执行效果，且推理成本低。


<details>
  <summary>Details</summary>
Motivation: 现实文本到SQL的工作流程通常需要连接多张表，因此准确检索相关表集成为性能瓶颈。

Method: 提出CORE-T框架，通过LLM生成的用途元数据丰富表信息，预计算轻量级表兼容性缓存，结合密集检索和单次LLM调用选择可连接子集。

Result: CORE-T在Bird、Spider和MMQA数据集上提高表选择F1值最高22.7点，减少42%表检索数，提升多表执行准确率上限6.9点，并减少4-5倍的令牌使用。

Conclusion: CORE-T是一种无训练、可扩展的多表检索框架，通过结合元数据和缓存显著提升了文本到SQL在多表环境下的检索准确性及执行性能。

Abstract: Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on extra assumptions and/or incur high inference overhead. We propose CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, DR returns top-K candidates; a single LLM call selects a coherent, joinable subset, and a simple additive adjustment step restores strongly compatible tables. Across Bird, Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while retrieving up to 42% fewer tables, improving multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens than LLM-intensive baselines.

</details>


### [102] [Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning](https://arxiv.org/abs/2601.13115)
*Fengran Mo,Yifan Gao,Sha Li,Hansi Zeng,Xin Liu,Zhaoxuan Tan,Xian Li,Jianshu Chen,Dakuo Wang,Meng Jiang*

Main category: cs.CL

TL;DR: 本文提出一种基于强化学习的多轮对话搜索与推理交织的会话代理，有效提升了多轮人机交互中动态理解和响应用户意图的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖静态的重写、检索和生成流水线，分别优化各步骤，忽视了多轮对话中用户意图随时间演变且需动态协调检索与生成过程的需求。

Method: 本文采用通过强化学习训练的会话代理，实现搜索和推理的交替进行，结合定制化奖励函数，使代理能够动态协调检索与生成过程，优化多轮对话中的响应质量。

Result: 在四个广泛使用的会话基准测试中，本文方法优于多种强基线模型，证明了其在多轮任务导向型对话中应对复杂用户意图变化的有效性。

Conclusion: 本文提出的交互式会话代理通过多轮中的搜索与推理交织，实现了适应性强和探索性强的行为，显著提升了多轮对话中用户意图演变的理解和响应能力。

Abstract: Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.

</details>


### [103] [Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains](https://arxiv.org/abs/2601.13137)
*Yuan Gao,Zhigang Liu,Xinyu Yao,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 本文针对大语言模型在敏感领域的偏见和价值不一致问题，提出一种对抗对齐框架，通过多阶段训练提升模型价值一致性，实验结果显示该方法显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在敏感领域（如种族、社会和政治）存在偏见及价值不一致问题，提出改进方法以提升模型的价值一致性和响应质量。

Method: 通过持续预训练、指令微调和对抗训练相结合的方法，使用攻击者生成争议性查询，行为者生成价值一致的回应，评论者筛选确保回应质量。

Result: 在中英文双语评估数据集上，训练得到的VC-LLM模型表现优于现有主流模型，验证了方法的有效性。

Conclusion: 本文提出的对抗对齐框架能够有效提升大语言模型在敏感领域的价值一致性，显著优于现有主流模型。

Abstract: With the wide application of large language models (LLMs), the problems of bias and value inconsistency in sensitive domains have gradually emerged, especially in terms of race, society and politics. In this paper, we propose an adversarial alignment framework, which enhances the value consistency of the model in sensitive domains through continued pre-training, instruction fine-tuning and adversarial training. In adversarial training, we use the Attacker to generate controversial queries, the Actor to generate responses with value consistency, and the Critic to filter and ensure response quality. Furthermore, we train a Value-Consistent Large Language Model, VC-LLM, for sensitive domains, and construct a bilingual evaluation dataset in Chinese and English. The experimental results show that VC-LLM performs better than the existing mainstream models in both Chinese and English tests, verifying the effectiveness of the method. Warning: This paper contains examples of LLMs that are offensive or harmful in nature.

</details>


### [104] [Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2601.13155)
*Zimeng Wu,Donghao Wang,Chaozhe Jin,Jiaxin Chen,Yunhong Wang*

Main category: cs.CL

TL;DR: 该论文提出SPTS，一种无训练的长上下文大模型高效推理框架，通过创新Token跳过策略显著提升推理速度，同时保证准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于token的剪枝和跳过方法加速潜力有限，存在过时的代理信号和冗余干扰，导致速度与准确性的权衡不佳，亟需一种更加高效且准确的跳过机制。

Method: 提出了一种无训练的长上下文推理加速框架SPTS，包括部分注意力探测（PAP）选择性跳过多头注意力中的关键token，低秩变换探测（LTP）预测前馈网络中的token变换，以及多阶段延迟剪枝（MSDP）策略逐层优化token剪枝。

Result: 实验显示，SPTS在前置和端到端生成阶段分别实现了最高2.46倍和2.29倍加速，同时保持了模型的高性能，优于现有方法。

Conclusion: 本文提出的SPTS方法在长上下文推理中实现了显著的加速效果，同时保持了高性能，达到了推理速度与精度的良好平衡。

Abstract: Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\times$ and 2.29$\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.

</details>


### [105] [Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages](https://arxiv.org/abs/2601.13178)
*Joseph Gatto,Parker Seegmiller,Timothy Burdick,Philip Resnik,Roshnik Rahat,Sarah DeLozier,Sarah M. Preum*

Main category: cs.CL

TL;DR: 本文构建了首个大规模公开医学分诊数据集PMR-Bench，通过创新的方法训练模型实现基于患者消息的医疗紧急度排序，显著提升医患信息处理效率。


<details>
  <summary>Details</summary>
Motivation: 医疗分诊任务需要有效地分配医疗资源，尤其是在门诊非实时异步患者消息处理中缺乏公开大规模数据集和高效方法。

Method: 将患者消息分诊任务构建为配对推断问题，通过头对头筛选优先级，设计了自动标注策略引导大模型训练；训练两类模型UrgentSFT（基于下一个词预测）和UrgentReward（基于Bradley-Terry模型）完成优先级排序。

Result: PMR-Bench包含1569条患者消息和2000多高质量测试配对，UrgentSFT-8B及UrgentReward-8B比通用8B模型在任务指标上分别提升15和16分，表现显著提升。

Conclusion: 本文成功构建了第一个大规模公开的用于医学分诊任务的数据集PMR-Bench，有效促进了基于患者消息的医疗优先级评估研究。训练的模型UrgentSFT和UrgentReward在该任务上表现优异，分别适合不同资源环境。

Abstract: Medical triage is the task of allocating medical resources and prioritizing patients based on medical need. This paper introduces the first large-scale public dataset for studying medical triage in the context of asynchronous outpatient portal messages. Our novel task formulation views patient message triage as a pairwise inference problem, where we train LLMs to choose `"which message is more medically urgent" in a head-to-head tournament-style re-sort of a physician's inbox. Our novel benchmark PMR-Bench contains 1569 unique messages and 2,000+ high-quality test pairs for pairwise medical urgency assessment alongside a scalable training data generation pipeline. PMR-Bench includes samples that contain both unstructured patient-written messages alongside real electronic health record (EHR) data, emulating a real-world medical triage scenario.
  We develop a novel automated data annotation strategy to provide LLMs with in-domain guidance on this task. The resulting data is used to train two model classes, UrgentReward and UrgentSFT, leveraging Bradley-Terry and next token prediction objective, respectively to perform pairwise urgency classification. We find that UrgentSFT achieves top performance on PMR-Bench, with UrgentReward showing distinct advantages in low-resource settings. For example, UrgentSFT-8B and UrgentReward-8B provide a 15- and 16-point boost, respectively, on inbox sorting metrics over off-the-shelf 8B models. Paper resources can be found at https://tinyurl.com/Patient-Message-Triage

</details>


### [106] [OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand](https://arxiv.org/abs/2601.13183)
*Sergio Servantez,Sarah B. Lawsky,Rajiv Jain,Daniel W. Linna,Kristian Hammond*

Main category: cs.CL

TL;DR: OpenExempt框架提出了一种基于法律符号表示的动态生成推理任务方法，构建了大规模法律推理诊断基准，能够细致评估语言模型的法律推理能力，公开发布以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 传统的静态问答对测试方法无法准确反映复杂法律推理中的模型表现，且构建成本高，难以针对具体失败模式进行分析。

Method: 通过专家设计的符号化表示，动态生成符合美国破产法条款的自然语言推理任务及其可计算解答，构建了包含9765个样本的多评估套件的法律推理基准。

Result: 在13个多样化语言模型上测试发现，只有在较长推理链路和存在混淆陈述时，模型性能才会出现明显下降，表明该基准能有效揭示模型推理弱点。

Conclusion: 现有的法律推理基准测试难以对模型进行细致诊断，OpenExempt框架和基准通过动态生成法律推理任务，提供了更加细粒度和多样化的测试方式，能够揭示模型在复杂推理路径和障碍性陈述下的性能瓶颈。

Abstract: Reasoning benchmarks have played a crucial role in the progress of language models. Yet rigorous evaluation remains a significant challenge as static question-answer pairs provide only a snapshot of performance, compressing complex behavior into a single accuracy metric. This limitation is especially true in complex, rule-bound domains such as law, where existing benchmarks are costly to build and ill suited for isolating specific failure modes. To address this, we introduce OpenExempt, a framework and benchmark for diagnostic evaluation of legal reasoning. The OpenExempt Framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate a large space of natural language reasoning tasks and their machine-computable solutions on demand. This gives users fine-grained control over task complexity and scope, allowing individual reasoning skills to be probed in isolation. Using this system, we construct the OpenExempt Benchmark, a diagnostic benchmark for legal reasoning with 9,765 samples across nine evaluation suites designed to carefully probe model capabilities. Experiments on 13 diverse language models reveal sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements. We release the framework and benchmark publicly to support research aimed at understanding and improving the next generation of reasoning systems.

</details>


### [107] [Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision](https://arxiv.org/abs/2601.13217)
*Bingsen Chen,Boyan Li,Ping Nie,Yuyu Zhang,Xi Ye,Chen Zhao*

Main category: cs.CL

TL;DR: 本研究提出Mr Dre评估套件，首次系统衡量多轮用户反馈驱动的报告修订能力，发现现有深度研究代理在多轮编辑中表现不足，无法稳定保持内容质量和引用准确性。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理（DRA）在生成报告方面仅作为一次性写作任务，忽视了人类研究者通过自我反思或同行反馈进行多轮迭代修订的过程。是否能通过用户反馈可靠地修订报告尚未被探讨。

Method: 引入Mr Dre评估套件，建立多轮报告修订的新评估维度，包含统一的长篇报告评估协议（涵盖全面性、事实性和展示质量）以及人工验证的反馈模拟多轮修订流程。

Result: 五个不同DRA的分析表明，代理能够响应大部分用户反馈，但也会在16-27%的已覆盖内容和引用质量上产生回退。多轮修订中，即使表现最好的代理仍存在显著问题，如扰乱反馈范围外的内容和无法保持早期编辑。

Conclusion: 当前DRA在多轮反馈修订任务中存在显著局限，普通的推理时优化方法（如提示工程或专门的子代理）难以有效解决内容回退和编辑保持问题，表明该领域尚有较大提升空间。

Abstract: Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback's scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.

</details>


### [108] [Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation](https://arxiv.org/abs/2601.13228)
*Tianqi Du,Lizhe Fang,Weijie Yang,Chenheng Zhang,Zeming Wei,Yifei Wang,Yisen Wang*

Main category: cs.CL

TL;DR: A3模型融合自回归和扩散模型优势，实现灵活高效的任意顺序语言生成，提升生成质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散语言模型虽然灵活支持任意顺序和双向生成，但单步依赖限制了建模深度，导致生成质量和稳定性不如自回归模型。

Method: 通过两流注意力结构和渐进适应策略，将预训练自回归模型转向任意顺序预测，实现A3模型的多组预测过程。

Result: 在问答、常识推理和故事填充任务上，A3模型优于基于扩散的模型，同时保持了解码的灵活性。

Conclusion: 本文提出的A3模型实现了任意顺序任意子集的自回归建模，兼具自回归模型的多层依赖优势和扩散模型的灵活性，提升了样本质量和生成稳定性。

Abstract: Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.

</details>


### [109] [Aligning Agentic World Models via Knowledgeable Experience Learning](https://arxiv.org/abs/2601.13247)
*Baochang Ren,Yunzhi Yao,Rui Sun,Shuofei Qiao,Ningyu Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: WorldMind利用环境反馈构建符号化世界知识库，提升大语言模型对物理规则的理解和任务执行能力，表现出优越的跨环境迁移效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然语义知识丰富，但缺乏物理世界的程序性基础，导致产生逻辑合理但物理不可执行的计划，传统通过训练或微调的对齐策略成本高且适应性差。

Method: 通过融合环境反馈，WorldMind整合过程经验以通过预测误差保证物理可行性，并利用目标经验引导任务最优轨迹，构建符号世界知识库。

Result: 在EB-ALFRED和EB-Habitat数据集上的实验表明，WorldMind在跨模型和跨环境的迁移能力方面均优于基线方法，表现出较高的任务执行性能。

Conclusion: WorldMind框架通过自主构建符号化的世界知识库，有效解决了大语言模型在物理世界规则遵守上的不足，显著提升了物理可执行性和任务完成度。

Abstract: Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.

</details>


### [110] [Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph](https://arxiv.org/abs/2601.13251)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: 该文提出了一个大规模语义聚类系统，通过新标注数据、三分类判别器和创新聚类算法，有效解决了神经嵌入无法区分同义词和反义词的问题，提升了语义搜索与生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 神经嵌入模型难以区分同义词与反义词，传统相似度阈值无法有效阻止反义词被错误聚合，因此需要一种更有效的语义聚类方法。

Method: 构建了一个包含843,000对概念的标注数据集，提出了一个三分类语义关系判别器，且设计了新颖的软硬结合聚类算法，通过拓扑感知的扩展-剪枝流程保证聚类的语义一致性和防止语义漂移。

Result: 处理了1500万个词项，评估了5.2亿个潜在关系，生成了290万个高精度语义聚类，语义关系判别器达到90%宏观F1分数。该资源促进了形态丰富及低资源语言的语义搜索和增强生成。

Conclusion: 该系统成功解决了神经嵌入模型难以区分同义词和反义词的问题，实现了高精度的语义聚类。

Abstract: Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.

</details>


### [111] [A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus](https://arxiv.org/abs/2601.13253)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: 本文提出一种结合词向量聚类、自动分类和词典融合的混合方法，成功构建出土耳其语大规模语义关系数据集，显著提升数据规模和质量，验证了模型性能，解决了低资源语言NLP数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 土耳其语等低资源语言缺乏大规模语义关系数据，限制了自然语言处理的效果。

Method: 结合FastText词向量及层次聚类识别语义簇，使用Gemini 2.5-Flash自动分类语义关系，并融合人工整理词典数据。

Result: 构建了包含843,000个土耳其语独特语义对（同义词、反义词、共上位词）的数据集，规模比现有资源提升10倍，成本仅65美元，模型在检索和分类任务中均达90%性能。

Conclusion: 该方法实现了低资源语言大规模语义关系数据构建的有效、低成本方案，显著缓解了数据匮乏问题，且具备推广到其他低资源语言的潜力。

Abstract: We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.

</details>


### [112] [Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models](https://arxiv.org/abs/2601.13260)
*Sawsan Alqahtani,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 本文强调分词应作为核心设计决策，提出结合上下文的分词与模型协同设计框架，提升模型公平性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有主流的子词分词方法如BPE虽具可扩展性，但存在与语言结构不对齐、加剧偏见及资源浪费等问题，分词仍缺乏理论指导和一致设计。

Method: 提出一种上下文感知的框架，将分词器与模型协同设计，并结合语言学、领域和部署需求进行指导。

Result: 通过标准化评估和透明报告，使分词选择更具责任感和可比性，推动语言技术的改进。

Conclusion: 将分词视为核心设计问题而非简单的预处理步骤，可以提升语言模型的公平性、效率和适应性。

Abstract: Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.

</details>


### [113] [Unlearning in LLMs: Methods, Evaluation, and Open Challenges](https://arxiv.org/abs/2601.13264)
*Tyler Lizzo,Larry Heck*

Main category: cs.CL

TL;DR: 本文系统综述了大型语言模型中的机器遗忘方法及评估体系，探讨其面临的挑战，旨在推动可靠且负责的遗忘技术的发展。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，数据隐私和模型安全问题日益突出，机器遗忘技术被提出以选择性移除模型中敏感或不需要的信息，避免完全重新训练。

Method: 通过系统梳理现有机器遗忘方法，根据关注点分为数据中心、参数中心、架构中心、混合及其他策略，并总结相关评估体系，包括基准测试、指标和数据集。

Result: 本文总结了现有机器遗忘技术的分类和性能评估体系，揭示了效率、形式保障、多语言和多模态遗忘以及对抗性重学等关键难题，为未来研究指明方向。

Conclusion: 本文综述了大型语言模型（LLMs）中机器遗忘技术的发展现状，强调了其在隐私保护、版权、安全和偏见等方面的重要性，并指出该领域的若干关键挑战。

Abstract: Large language models (LLMs) have achieved remarkable success across natural language processing tasks, yet their widespread deployment raises pressing concerns around privacy, copyright, security, and bias. Machine unlearning has emerged as a promising paradigm for selectively removing knowledge or data from trained models without full retraining. In this survey, we provide a structured overview of unlearning methods for LLMs, categorizing existing approaches into data-centric, parameter-centric, architecture-centric, hybrid, and other strategies. We also review the evaluation ecosystem, including benchmarks, metrics, and datasets designed to measure forgetting effectiveness, knowledge retention, and robustness. Finally, we outline key challenges and open problems, such as scalable efficiency, formal guarantees, cross-language and multimodal unlearning, and robustness against adversarial relearning. By synthesizing current progress and highlighting open directions, this paper aims to serve as a roadmap for developing reliable and responsible unlearning techniques in large language models.

</details>


### [114] [A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification](https://arxiv.org/abs/2601.13288)
*Gonzalo Ariel Meyoyan,Luciano Del Corro*

Main category: cs.CL

TL;DR: 通过在大语言模型隐藏层训练轻量级探针，实现安全和分类任务，减少延迟和显存消耗，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统大模型生产系统中，安全及分类步骤通常使用独立模型，导致延迟、显存和操作复杂度增加。本文通过重用生成模型计算，减少开销。

Method: 设计了两阶段聚合器，分别对每层的token进行汇总和跨层聚合，形成单一表示进行分类。使用了直接池化、得分注意力门控和多头自注意力探针等多种实现形式。

Result: 在安全和情感分析基准测试中，探针方法优于基于logit重用的方法，且在性能上与更大规模专用模型竞争，同时保持低延迟和显存需求。

Conclusion: 本文提出的利用轻量级探针在生成模型的隐藏状态上进行分类的方法，可以有效提升分类性能，同时保持接近生成延迟，避免了使用独立安全模型带来的额外显存和延迟负担。

Abstract: Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.

</details>


### [115] [OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference](https://arxiv.org/abs/2601.13300)
*Yow-Fu Liou,Yu-Chien Tang,Yu-Hsiang Liu,An-Zi Yen*

Main category: cs.CL

TL;DR: 本文设计了OI-Bench基准，通过注入误导性选项系统检测大型语言模型在多选题界面的鲁棒性，揭示其脆弱点并探讨缓解策略。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型的能力、局限及其在多选题界面中对社交暗示、指令等信号的敏感性，推动模型鲁棒性评价发展。

Method: 提出选项注入方法，在多选题中加入一个含误导性指令的选项，通过构建涵盖多种指令类型的3,000题基准库OI-Bench进行系统评估。

Result: 在12个大型语言模型上测试，发现模型对指令干扰的攻击成功率较高，行为反应多样，且通过推理时提示和后训练调整等策略可缓解部分脆弱性。

Conclusion: 大型语言模型在选择题界面中容易受到误导性指令的影响，表现出显著的脆弱性且各模型鲁棒性差异较大。

Abstract: Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.

</details>


### [116] [Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse](https://arxiv.org/abs/2601.13317)
*Samantha Sudhoff,Pranav Perumal,Zhaoqing Wu,Tunazzina Islam*

Main category: cs.CL

TL;DR: 本文对Meta付费广告与Bluesky公共帖子中的气候话语进行了比较分析，提出新方法识别和比较主题，揭示平台激励如何影响气候传播。


<details>
  <summary>Details</summary>
Motivation: 不同平台的气候传播环境具有不同的激励结构，现有研究常单独分析，限制了区分机构信息和公众表达的能力。

Method: 提出了一种可解释的端到端主题发现与分配框架，通过语义聚类和大语言模型生成主题标签，并通过多种评价标准验证主题质量。

Result: 发现付费广告与公共帖子在气候话语中的主题表现和动态变化存在显著差异，并展示了该框架支持异构传播环境下比较叙事分析的潜力。

Conclusion: 付费广告和公共社交平台上的气候话语在主题结构、立场一致性和时间响应性方面存在系统性差异，平台激励机制显著影响气候传播内容。

Abstract: Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.

</details>


### [117] [Arab Voices: Mapping Standard and Dialectal Arabic Speech Technology](https://arxiv.org/abs/2601.13319)
*Peter Sullivan,AbdelRahim Elmadany,Alcides Alcoba Inciarte,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 该论文分析了方言阿拉伯语语音数据的异质性，提出了统一的Arab Voices框架，整合多数据集和多方言，规范元数据和评估方法，促进标准化和可重复的方言阿拉伯语语音识别研究。


<details>
  <summary>Details</summary>
Motivation: 由于方言阿拉伯语语音数据在多方面的不一致，导致跨数据集比较和模型评估复杂，需要一种标准化的框架来减少碎片化，促进可重复的评估。

Method: 通过对多种广泛使用的方言阿拉伯语语料库的训练数据进行计算分析，包括语言学“方言特征”和音频质量的客观代理指标，揭示数据集之间的异质性。

Result: 提出了Arab Voices，一个涵盖31个数据集、14种方言、具有统一元数据和评估工具的标准化方言阿拉伯语语音识别框架，并基于此框架对多种先进的ASR系统进行了基准测试，建立了强有力的现代方言阿拉伯语语音识别基线。

Conclusion: 该论文指出现有方言阿拉伯语数据在域覆盖、方言标注和录音条件方面存在较大差异，强调了统一描述和标准化评估的必要性。

Abstract: Dialectal Arabic (DA) speech data vary widely in domain coverage, dialect labeling practices, and recording conditions, complicating cross-dataset comparison and model evaluation. To characterize this landscape, we conduct a computational analysis of linguistic ``dialectness'' alongside objective proxies of audio quality on the training splits of widely used DA corpora. We find substantial heterogeneity both in acoustic conditions and in the strength and consistency of dialectal signals across datasets, underscoring the need for standardized characterization beyond coarse labels. To reduce fragmentation and support reproducible evaluation, we introduce Arab Voices, a standardized framework for DA ASR. Arab Voices provides unified access to 31 datasets spanning 14 dialects, with harmonized metadata and evaluation utilities. We further benchmark a range of recent ASR systems, establishing strong baselines for modern DA ASR.

</details>


### [118] [Reducing Tokenization Premiums for Low-Resource Languages](https://arxiv.org/abs/2601.13328)
*Geoffrey Churchill,Steven Skiena*

Main category: cs.CL

TL;DR: 针对低资源语言分词成本高的问题，论文提出通过后期词汇合并降低成本的方法，在多种语言实验中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 低资源语言相比英语需要更多分词令牌来编码相同语句，导致API调用和能耗成本增加，影响模型上下文窗口大小，亟需降低分词成本的方法。

Method: 分析了十个流行LM的分词器设计与语言分词成本，通过向预训练模型词汇表中后期添加合并的多分词字符实现分词成本的降低，并在12种低资源语言中验证了该方法。

Result: 该方法在12种低资源语言中应用，显示压缩后的输入与原始输入在Llama 3.2 1B模型中产生的最后隐状态相似，说明压缩分词有效且不损害模型内表征。

Conclusion: 该论文分析了现代语言模型（LM）中低资源语言相较于英语存在的较高分词成本问题，提出了一种通过后期添加词汇表项合并多分词字符为单一分词的机制，有效降低了分词成本。

Abstract: Relative to English, low-resource languages suffer from substantial tokenization premiums in modern LMs, meaning that it generally requires several times as many tokens to encode a sentence in a low-resource language than to encode the analogous sentence in English. This tokenization premium results in increased API and energy costs and reduced effective context windows for these languages. In this paper we analyze the tokenizers of ten popular LMs to better understand their designs and per-language tokenization premiums. We also propose a mechanism to reduce tokenization premiums in pre-trained models, by post-hoc additions to the token vocabulary that coalesce multi-token characters into single tokens. We apply this methodology to 12 low-resource languages, demonstrating that the original and compressed inputs often have similar last hidden states when run through the Llama 3.2 1B model.

</details>


### [119] [RegCheck: A tool for automating comparisons between study registrations and papers](https://arxiv.org/abs/2601.13330)
*Jamie Cummins,Beth Clarke,Ian Hussey,Malte Elson*

Main category: cs.CL

TL;DR: RegCheck是一款辅助研究注册与论文比对的AI工具，保持人工判断，生成共享报告，助力多学科科学透明和严谨。


<details>
  <summary>Details</summary>
Motivation: 研究表明，尽管研究注册有助于科学的透明性和严谨性，但许多研究注册未被有效检查，原因是人工核对工作量大且耗时。

Method: 提出了RegCheck，一种基于大型语言模型（LLM）辅助的模块化工具，用于帮助各领域研究者、审核人员和编辑比对研究注册和对应论文。工具保留人工判断，允许用户自定义对比内容，并展示相关文本辅助人工判断。

Result: RegCheck生成可分享的报告，带有唯一ID，方便用户间共享和验证。该工具适用于多学科、多种注册和发表格式。

Conclusion: RegCheck通过结合人工智能与人工判断，提升了研究注册和发表论文比对的效率和可扩展性，推动了可重复性科学的发展。

Abstract: Across the social and medical sciences, researchers recognize that specifying planned research activities (i.e., 'registration') prior to the commencement of research has benefits for both the transparency and rigour of science. Despite this, evidence suggests that study registrations frequently go unexamined, minimizing their effectiveness. In a way this is no surprise: manually checking registrations against papers is labour- and time-intensive, requiring careful reading across formats and expertise across domains. The advent of AI unlocks new possibilities in facilitating this activity. We present RegCheck, a modular LLM-assisted tool designed to help researchers, reviewers, and editors from across scientific disciplines compare study registrations with their corresponding papers. Importantly, RegCheck keeps human expertise and judgement in the loop by (i) ensuring that users are the ones who determine which features should be compared, and (ii) presenting the most relevant text associated with each feature to the user, facilitating (rather than replacing) human discrepancy judgements. RegCheck also generates shareable reports with unique RegCheck IDs, enabling them to be easily shared and verified by other users. RegCheck is designed to be adaptable across scientific domains, as well as registration and publication formats. In this paper we provide an overview of the motivation, workflow, and design principles of RegCheck, and we discuss its potential as an extensible infrastructure for reproducible science with an example use case.

</details>


### [120] [AfroScope: A Framework for Studying the Linguistic Landscape of Africa](https://arxiv.org/abs/2601.13346)
*Sang Yun Kwon,AbdelRahim Elmadany,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: AfroScope构建了涵盖713种非洲语言的大规模数据集和模型，采用层次分类和专门嵌入技术显著提升了非洲语言识别的准确性和范围，支持数字文本中的非洲语言测量。


<details>
  <summary>Details</summary>
Motivation: 现有非洲语言识别工作支持的语言数量有限且难以区分相近语言变体，亟需一个覆盖更广、精度更高的统一框架。

Method: 提出层次分类方法并利用专门的Mirror-Serengeti嵌入模型，针对29种高度混淆或地理邻近语言进行识别，此外构建包含713种非洲语言的数据集和一套强大的LID模型。

Result: 推出的层次分类方法在高度混淆语言子集上的宏F1提高了4.55，相较基础模型表现更佳；并分析了跨语言迁移和领域影响，为构建鲁棒系统提供指导。

Conclusion: AfroScope框架显著提升了非洲语言识别的覆盖范围和区分细粒度语言变体的能力，成为非洲语言大规模数字文本测量的重要技术基础。

Abstract: Language Identification (LID) is the task of determining the language of a given text and is a fundamental preprocessing step that affects the reliability of downstream NLP applications. While recent work has expanded LID coverage for African languages, existing approaches remain limited in (i) the number of supported languages and (ii) their ability to make fine-grained distinctions among closely related varieties. We introduce AfroScope, a unified framework for African LID that includes AfroScope-Data, a dataset covering 713 African languages, and AfroScope-Models, a suite of strong LID models with broad language coverage. To better distinguish highly confusable languages, we propose a hierarchical classification approach that leverages Mirror-Serengeti, a specialized embedding model targeting 29 closely related or geographically proximate languages. This approach improves macro F1 by 4.55 on this confusable subset compared to our best base model. Finally, we analyze cross linguistic transfer and domain effects, offering guidance for building robust African LID systems. We position African LID as an enabling technology for large scale measurement of Africas linguistic landscape in digital text and release AfroScope-Data and AfroScope-Models publicly.

</details>


### [121] [Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection](https://arxiv.org/abs/2601.13359)
*Asen Dotsinski,Panagiotis Eustratiadis*

Main category: cs.CL

TL;DR: 本文提出了简单易用的sockpuppetting攻击方法，通过在模型输出前插入特定序列，实现高效越狱，挑战了开源大模型的安全防护。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型能力提升，防范恶意提示和理解攻击途径变得重要，而现有越狱方法计算资源需求大且需专业知识。

Method: 在模型输出开始插入接受序列（如“Sure, here is how to...”），让模型完成响应，实现对开源大模型的越狱攻击。

Result: sockpuppetting在Qwen3-8B上比GCG提高了80%的攻击成功率，在Llama-3.1-8B上的混合方法提升64%攻击成功率，证明了其有效性和低成本。

Conclusion: sockpuppetting作为一种简单有效的攻击方法，在无需优化和专业知识的情况下，显著提升了攻击成功率，表明开放权重大模型需要加强对输出前缀注入攻击的防御。

Abstract: As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce "sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., "Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.

</details>


### [122] [Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.13368)
*Zhenjiang Mao,Anirudhh Venkat*

Main category: cs.CL

TL;DR: 本文提出利用步骤间注意力和隐藏置信度机制改进大型语言模型推理答案的不确定性评估，在两个基准数据集上表现优异，增强了模型答题的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前在大型语言模型中应用推理模块时，缺乏准确评估答案不确定性的方法，容易导致整体置信度过高，从而引发误导或严重幻觉。

Method: 提出了一种结合步骤间注意力机制分析步骤之间语义关联的新方法，并引入隐藏置信度机制以保留历史置信度信息，结合逐步置信度给出更准确的整体置信度估计。

Result: 在GAOKAO数学基准和CLadder因果推理数据集上，所提方法优于现有最先进方法，在负对数似然和期望校准误差等指标上表现更优，平衡了预测质量与校准性能。

Conclusion: 该方法有效提升了大型语言模型中推理答案的不确定性评估精度，有助于减少误导和幻觉现象，提高模型的可靠性。

Abstract: As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.

</details>


### [123] [Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning](https://arxiv.org/abs/2601.13387)
*Zhenjiang Mao,Anirudhh Venkat,Artem Bisliouk,Akshat Kothiyal,Sindhura Kumbakonam Subramanian,Saithej Singhu,Ivan Ruchkin*

Main category: cs.CL

TL;DR: 本文利用信号时序逻辑描述大语言模型逐步置信信号，挖掘动态置信模式，开发了一种更准确的置信度估计方法，提升了多步推理任务置信度的校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的置信度估计方法多将整个推理过程压缩为一个标量分值，忽视推理过程中置信度的动态变化，导致对回答长度、冗长度敏感，难以准确判断置信错误的推理。

Method: 采用信号时序逻辑(STL)描述逐步置信信号，通过判别性STL规则挖掘区分正确和错误回答的置信模式，并利用参数超网络调整STL模型参数，实现更加精确的置信度估计。

Result: 所提出的STL基置信度估计方法在多个多步推理任务中表现出比基线模型更优的校准效果，且发现STL模式具有任务间的泛化能力，参数对个别问题表现出敏感性。

Conclusion: 本文提出的基于信号时序逻辑（STL）的逐步置信度估计方法提高了大规模语言模型多步骤推理的置信度校准能力，相较于传统方法更能区分正确推理与错误陈述的置信差异。

Abstract: Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.

</details>


### [124] [Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction](https://arxiv.org/abs/2601.13388)
*Sasha Ronaghi,Prerit Choudhary,David H Rehkopf,Bryant Lin*

Main category: cs.CL

TL;DR: 研究利用大型语言模型从非结构化患者访谈中提取社会健康决定因素，结构化数据与传统生物指标结合用于糖尿病风险预测，达到较好效果，LLMs还能直接预测糖尿病控制水平，提升临床风险评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前电子健康记录及风险预测模型中缺少个体层面的社会健康决定因素数据，而现有结构化筛查工具不能充分捕捉患者复杂的生活经历和个体差异。利用LLMs处理非结构化访谈文本，有望填补这一空白，提高糖尿病管理和风险预测的精确度。

Method: 本研究收集了65名65岁及以上2型糖尿病患者的非结构化生活访谈文本，利用LLMs结合检索增强生成技术，提取定性总结与结构化的SDOH评分，并将这些评分与传统生物标志物一起用于多种机器学习模型（岭回归、套索回归、随机森林和XGBoost）进行风险预测。此外，评估LLMs直接从访谈文本预测糖尿病控制水平的能力。

Result: LLMs从访谈文本中提取的结构化SDOH评分与传统生物标志物结合使用，提升了风险预测模型的表现。同时，LLMs在无糖化血红蛋白（A1C）数据的情况下，能够以60%的准确率预测患者的糖尿病控制水平，展现了语言模型在临床决策支持中的潜力。

Conclusion: 大型语言模型（LLMs）能够有效地将非结构化的社会健康决定因素（SDOH）数据转化为结构化信息，并通过此方式提升糖尿病控制风险预测的准确性，补充传统实验指标。

Abstract: Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.

</details>


### [125] [Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks](https://arxiv.org/abs/2601.13392)
*Shlok Shelat,Jay Raval,Souvik Roy,Manas Gaur*

Main category: cs.CL

TL;DR: 研究发现大型语言模型虽能生成格式正确的有限自动机，但在复杂符号推理任务中存在语义理解缺陷，限制了其推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在形式语言任务中表现优异的背后原因，判断其是否具备真正的符号推理能力，还是仅依赖模式匹配。

Method: 提出一个DFA构造基准测试，包含知识问答、公开来源的构造任务及两类未见过的问题，并通过三阶段提示协议及多种提示策略（直接提示、链式思维、树式思维）对应模型表现进行评估。

Result: 模型在事实知识和已见任务上准确率较高，但在未见任务上准确率大幅下降，主要错误包括语言约束误解、对Kleene星操作符处理不当及全局一致性缺失。多种提示策略未能根本解决这些错误。

Conclusion: 大型语言模型在处理正式语言任务时表现良好，但在确定性有限自动机（DFA）构造的严肃推理任务上存在显著缺陷，尤其是在未见过的问题中表现急剧下降，暴露出模型在符号推理和语义理解上的不足。

Abstract: Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.

</details>


### [126] [Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models](https://arxiv.org/abs/2601.13433)
*Priyanka Mary Mammen,Emil Joswin,Shankar Venkitachalam*

Main category: cs.CL

TL;DR: 语言模型在推理中会因权威来源的错误背书而更易出错，但该权威偏见可被识别和纠正，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在推理任务中是否存在基于背书者专业水平的系统性偏见。

Method: 在数学、法律和医学推理的4个数据集上，使用代表四种专业水平的角色设置对11个模型进行评估。

Result: 发现模型对背书者专业水平越高的错误信息越易受影响，表现为不仅准确率下降，还对错误答案的置信度提高。

Conclusion: 语言模型存在权威偏见，该偏见内在编码在模型中，但可以通过调整引导模型避免该偏见，从而提高其在专家误导情况下的表现。

Abstract: Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.

</details>


### [127] [MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization](https://arxiv.org/abs/2601.13437)
*Adriana-Valentina Costache,Daria-Nicoleta Dragomir,Silviu-Florin Gheorghe,Eduard Poesina,Paul Irofti,Radu Tudor Ionescu*

Main category: cs.CL

TL;DR: 本文提出了首个多语言文本开放集学习与发现基准MOSLD，包含12种语言的960K样本，并设计多阶段框架进行新类别发现和学习，评估多模型获得基准结果，推动文本领域的开放集研究。


<details>
  <summary>Details</summary>
Motivation: 针对文本领域中开放集学习与发现任务的缺乏研究和资源，本文旨在填补该空白，提升模型对未知类别的识别与学习能力。

Method: 本文通过重新整理现有数据集和采集新闻领域新样本，构建了覆盖12种语言的960K样本的多语言基准数据集，并设计了一种多阶段框架，用于持续发现和学习新类别。

Result: 作者评估了多个语言模型，包括自己提出的模型，取得了一组可供未来研究参考的基准结果，并公开了数据集以促进该领域的发展。

Conclusion: 本文首次提出了多语言开放集学习与发现（MOSLD）基准，为文本主题分类任务中的新类别发现提供了标准化评价平台，并通过实验验证了所提框架的有效性。

Abstract: Open-set learning and discovery (OSLD) is a challenging machine learning task in which samples from new (unknown) classes can appear at test time. It can be seen as a generalization of zero-shot learning, where the new classes are not known a priori, hence involving the active discovery of new classes. While zero-shot learning has been extensively studied in text classification, especially with the emergence of pre-trained language models, open-set learning and discovery is a comparatively new setup for the text domain. To this end, we introduce the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. To construct the benchmark, we (i) rearrange existing datasets and (ii) collect new data samples from the news domain. Moreover, we propose a novel framework for the OSLD task, which integrates multiple stages to continuously discover and learn new classes. We evaluate several language models, including our own, to obtain results that can be used as reference for future work. We release our benchmark at https://github.com/Adriana19Valentina/MOSLD-Bench.

</details>


### [128] [PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving](https://arxiv.org/abs/2601.13453)
*Aditya Thole,Anmol Agrawal,Arnav Ramamoorthy,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文提出PSA，自动生成物理问题视频解释，虽展示潜力但存在视觉生成和评估瓶颈，强调未来多模态教育系统需强化视觉理解和验证机制。


<details>
  <summary>Details</summary>
Motivation: 文本基的解答不足以充分解释数值物理问题，视觉推理有助于提升概念理解，然而大型语言模型在生成高质量长视觉解释方面的能力尚未被充分研究。

Method: 提出了PhysicsSolutionAgent(PSA)，利用Manim动画自动生成物理问题解释视频，并设计了包括15个定量指标和视觉语言模型反馈的评估流程以迭代提升视频质量。

Result: 在32个数值和理论物理问题的视频测试中，PSA使用GPT-5-mini实现了100%视频完成率，平均自动评分3.8/5，但人类检测发现视觉布局和视觉内容解释存在错误，揭示了多模态推理和评估的关键挑战。

Conclusion: PSA能够自动生成长达六分钟的物理问题解释视频，展示了在多模态教育系统中视觉解释的潜力，但仍存在视觉布局不一致和视觉内容解释错误等问题，显示出生成高质量视觉解释的挑战。

Abstract: Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8/5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems

</details>


### [129] [Anonpsy: A Graph-Based Framework for Structure-Preserving De-identification of Psychiatric Narratives](https://arxiv.org/abs/2601.13503)
*Kyung Ho Lim,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: Anonpsy通过语义图引导的大型语言模型生成，实现精神病叙述结构化脱敏，有效保护患者隐私且保留临床信息。


<details>
  <summary>Details</summary>
Motivation: 精神病案例中的身份信息隐含在结构化临床语义中，传统纯文本脱敏难以选择性保留关键语义，需更细粒度控制的脱敏方法。

Method: Anonpsy先将叙述转化为编码临床实体、时间点和关系的语义图，进行图约束扰动以修改身份相关内容，并利用图条件的大型语言模型生成脱敏文本。

Result: 在90个临床病例上，Anonpsy在专家、语义和GPT-5评估下保持诊断一致性，显著降低了语义相似性和身份识别风险，优于大型语言模型单纯重写方法。

Conclusion: 匿名精神病叙述中的身份信息通过显式标识符和独特的生活事件体现，传统文本层面脱敏方法控制有限。Anonpsy通过图引导的语义重写有效降低可识别性，同时保持诊断信息的准确性。

Abstract: Psychiatric narratives encode patient identity not only through explicit identifiers but also through idiosyncratic life events embedded in their clinical structure. Existing de-identification approaches, including PHI masking and LLM-based synthetic rewriting, operate at the text level and offer limited control over which semantic elements are preserved or altered. We introduce Anonpsy, a de-identification framework that reformulates the task as graph-guided semantic rewriting. Anonpsy (1) converts each narrative into a semantic graph encoding clinical entities, temporal anchors, and typed relations; (2) applies graph-constrained perturbations that modify identifying context while preserving clinically essential structure; and (3) regenerates text via graph-conditioned LLM generation. Evaluated on 90 clinician-authored psychiatric case narratives, Anonpsy preserves diagnostic fidelity while achieving consistently low re-identification risk under expert, semantic, and GPT-5-based evaluations. Compared with a strong LLM-only rewriting baseline, Anonpsy yields substantially lower semantic similarity and identifiability. These results demonstrate that explicit structural representations combined with constrained generation provide an effective approach to de-identification for psychiatric narratives.

</details>


### [130] [When Wording Steers the Evaluation: Framing Bias in LLM judges](https://arxiv.org/abs/2601.13537)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Minwoo Lee,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文系统研究了提示语框架对大型语言模型评估判断的影响，发现显著的框架偏见，强调了制定考虑框架效应的评估协议的必要性。


<details>
  <summary>Details</summary>
Motivation: 虽然已知大型语言模型的回答会因提示语措辞而变化，但其对评估任务中判断稳定性和公正性的影响尚未充分研究，因而借鉴心理学中的框架效应进行探讨。

Method: 设计对称的正负述语提示，系统研究了提示框架对模型判断的影响，涵盖四个高风险评估任务，并对14个大型语言模型进行评估。

Result: 发现不同模型对提示框架表现出不同的敏感度，模型家族间在接受或拒绝的倾向上存在明显差异，表明框架偏见是现有LLM评估系统的结构性特征。

Conclusion: 当前大型语言模型在基于提示语的评估中存在显著的框架偏见，影响模型的判断稳定性和客观性。

Abstract: Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.

</details>


### [131] [HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations](https://arxiv.org/abs/2601.13547)
*Yujia Hu,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文提出HateXScore指标套件，评估仇恨言论检测模型解释的推理质量，提升内容审查的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前仇恨言论检测评估主要依赖准确率和F1等指标，缺乏对模型判定理由的评估，导致无法有效理解为何文本被认定为仇恨言论。

Method: 设计并引入HateXScore，该指标包括结论明确性、引用片段的忠实性和因果依据、受保护群体识别（可配置政策）以及逻辑一致性四个部分；在六个多样的仇恨言论数据集上进行了评估。

Result: HateXScore能够诊断出模型解释中的缺陷和标注不一致，表现出与人工评估高度一致，证实其作为解释质量评价工具的有效性和实用性。

Conclusion: HateXScore作为一种四成分指标套件，有效评估了模型解释中推理质量，揭示了传统指标无法捕捉的解释失败和标注不一致问题，且与人工评估高度一致，验证了其用于可信透明内容审查的实用性。

Abstract: Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.
  \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}

</details>


### [132] [Comparing Without Saying: A Dataset and Benchmark for Implicit Comparative Opinion Mining from Same-User Reviews](https://arxiv.org/abs/2601.13575)
*Thanh-Lam T. Nguyen,Ngoc-Quang Le,Quoc-Trung Phu,Thi-Phuong Le,Ngoc-Huyen Pham,Phuong-Nguyen Nguyen,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: 提出SUDO隐式比较观点挖掘数据集，包含同用户多评论标注，评测传统与语言模型基线，结果显示任务难度较大，数据集是有价值的研究基准。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注显式比较表达，但现实评论中显式比较较少，隐式比较未被充分研究。通过分析同一用户的多条评论实现偏好推断。

Method: 提出SUDO数据集，包含4150对注释评审对，采用双层结构捕捉层面提及和整体偏好。基于传统机器学习与语言模型两种基线架构进行任务基准测试。

Result: 语言模型基线优于传统方法，但整体表现中等，表明隐式比较观点挖掘任务具有较大挑战。

Conclusion: SUDO数据集为隐式比较观点挖掘提供了有价值的基准，尽管语言模型在该任务中表现优于传统方法，但整体性能仍有限，显示出任务的挑战性。

Abstract: Existing studies on comparative opinion mining have mainly focused on explicit comparative expressions, which are uncommon in real-world reviews. This leaves implicit comparisons - here users express preferences across separate reviews - largely underexplored. We introduce SUDO, a novel dataset for implicit comparative opinion mining from same-user reviews, allowing reliable inference of user preferences even without explicit comparative cues. SUDO comprises 4,150 annotated review pairs (15,191 sentences) with a bi-level structure capturing aspect-level mentions and review-level preferences. We benchmark this task using two baseline architectures: traditional machine learning- and language model-based baselines. Experimental results show that while the latter outperforms the former, overall performance remains moderate, revealing the inherent difficulty of the task and establishing SUDO as a challenging and valuable benchmark for future research.

</details>


### [133] [TREX: Tokenizer Regression for Optimal Data Mixture](https://arxiv.org/abs/2601.13588)
*Inho Won,Hangyeol Yoo,Minkyung Cho,Jungyeul Park,Hoyun Song,KyungTae Lim*

Main category: cs.CL

TL;DR: 本文提出TREX，一种基于回归的多语言分词器数据混合比例预测方法，显著提高分词器压缩效率，避免了传统方法的高成本搜索。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型的分词器构建需要精确控制语言特定的数据混合比例，以提升训练和推理效率。现有方法依赖启发式或高成本的大规模搜索，效率低下。

Method: 提出了基于回归的预测框架TREX，通过在随机数据混合上训练小型代理分词器，收集压缩性能数据，学习预测不同数据混合下的压缩效果，从而高效搜索最优数据混合比例。

Result: 使用TREX预测的最优数据混合训练的分词器，在压缩效率上较LLaMA3和均匀分布混合方案提升最多12%，表现出良好的扩展性和鲁棒性。

Conclusion: TREX框架有效解决了多语言分词器设计中的准确性与成本权衡问题，显著提升了分词器的压缩性能和训练效率，具备实际应用价值。

Abstract: Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizer's compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. TREX trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReX's predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both inand out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.

</details>


### [134] [Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions](https://arxiv.org/abs/2601.13590)
*Fan Huang,Haewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 研究揭示大语言模型对说服极易动摇信念，元认知提示非但无助防护反而加剧脆弱，且现有对抗微调方法效果因模型差异显著，需进一步探索更可靠的防御策略。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在问答任务中易受说服采纳反事实信念的问题，评估其脆弱性及提升鲁棒性的可能方法。

Method: 基于传者-信息-渠道-接收者(SMCR)框架，系统评估五种主流大语言模型在三个领域中面对不同说服策略的反应及多轮交互中的信念稳定性，进一步测试元认知提示及对抗微调的防御效果。

Result: 小模型表现出超过80%的信念变更集中在首轮说服，元认知提示导致信念更快被侵蚀，防御微调使GPT-4o-mini几乎完全鲁棒，Mistral 7B显著提升，但Llama模型依然高度易受影响。

Conclusion: 现有的大型语言模型在说服抵抗力方面存在显著差异，较小模型极易受影响，元认知提示反而加剧脆弱性，且对抗微调效果有限，尤其对于Llama模型。

Abstract: Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.

</details>


### [135] [CauScientist: Teaching LLMs to Respect Data for Causal Discovery](https://arxiv.org/abs/2601.13614)
*Bo Peng,Sirui Chen,Lei Xu,Chaochao Lu*

Main category: cs.CL

TL;DR: 提出CauScientist框架，结合大语言模型和统计验证，显著提升因果发现性能，尤其在复杂图结构中效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法存在统计不可区分性、建模假设限制，或忽视统计证据、引入未经验证先验的问题，亟需融合LLM和统计验证的解决方案。

Method: 该方法结合LLM作为假设生成者和概率统计作为验证者，通过混合初始化、迭代结构优化及错误记忆指导搜索空间，实现协同因果发现。

Result: CauScientist在F1分数上提升了53.8%，召回率从35.0%提升至100.0%，在37节点图上相较Qwen3-32B结构汉明距离减少44.0%。

Conclusion: CauScientist显著提升了因果发现的准确性和可靠性，在复杂图结构下表现优异，克服了纯数据驱动和纯LLM方法的不足。

Abstract: Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating "data scientists" with probabilistic statistics as rigorous "verifiers". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.

</details>


### [136] [Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models](https://arxiv.org/abs/2601.13630)
*Zhaopeng Zhang,Pengcheng Sun,Lan Zhang,Chen Tang,Jiewei Lai,Yunhao Wang,Hui Jin*

Main category: cs.CL

TL;DR: 文章提出一种基于激活空间锚点的不需训练的权限控制方法AAAC，有效防止大语言模型知识库问答中的权限泄露和攻击。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识库问答中可能超越用户权限范围回答，导致敏感信息泄露，亟需细粒度权限控制机制。

Method: 利用中间激活表现的几何规律，构建锚点库对应不同权限类别，推理时通过多锚点引导机制将查询激活重定向至授权区域，无需微调。

Result: AAAC在三种大语言模型上实验验证，权限违规率降低最高达86.5%，基于提示的攻击成功率降低90.7%，且推理开销小幅提升。

Conclusion: 本文提出的Activation-space Anchored Access Control (AAAC)框架有效解决了大语言模型在知识库访问中的权限控制问题，显著降低了权限违规和攻击成功率，提高了响应的可用性。

Abstract: Large language models (LLMs) are increasingly deployed over knowledge bases for efficient knowledge retrieval and question answering. However, LLMs can inadvertently answer beyond a user's permission scope, leaking sensitive content, thus making it difficult to deploy knowledge-base QA under fine-grained access control requirements. In this work, we identify a geometric regularity in intermediate activations: for the same query, representations induced by different permission scopes cluster distinctly and are readily separable. Building on this separability, we propose Activation-space Anchored Access Control (AAAC), a training-free framework for multi-class permission control. AAAC constructs an anchor bank, with one permission anchor per class, from a small offline sample set and requires no fine-tuning. At inference time, a multi-anchor steering mechanism redirects each query's activations toward the anchor-defined authorized region associated with the current user, thereby suppressing over-privileged generations by design. Finally, extensive experiments across three LLM families demonstrate that AAAC reduces permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while improving response usability with minor inference overhead compared to baselines.

</details>


### [137] [Towards Token-Level Text Anomaly Detection](https://arxiv.org/abs/2601.13644)
*Yang Cao,Bicheng Yu,Sikun Yang,Ming Liu,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文首次提出词元级文本异常检测，设计统一框架并构建带词元级标注的数据集，实现了比传统文档级方法更精确的异常定位。


<details>
  <summary>Details</summary>
Motivation: 现有文本异常检测方法仅限于文档级，无法定位文本中具体异常的部分，缺乏细粒度异常检测能力。

Method: 提出了一种能够跨文档和词元级别进行异常检测的统一框架，并且构建了包含词元级标注的三个人工标注数据集用于训练和评估。

Result: 基于新提出的框架，在垃圾邮件、评论和语法错误三个不同领域的标注数据集上取得优于其他六个基线方法的检测效果。

Conclusion: 本文提出的统一检测框架在多层级文本异常检测中表现优越，能够实现细粒度的异常定位，优于现有的六个基线方法。

Abstract: Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on https://github.com/charles-cao/TokenCore.

</details>


### [138] [Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge](https://arxiv.org/abs/2601.13649)
*Xiaolin Zhou,Zheng Luo,Yicheng Gao,Qixuan Chen,Xiyang Hu,Yue Zhao,Ruishan Liu*

Main category: cs.CL

TL;DR: 本文系统研究了LLM作为评判者时的语言偏见，发现存在跨语言和同语言性能差异及对英语的偏好，且语言偏见不能完全由低困惑度偏差解释。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现LLM作为评判者时存在与人类偏好不一致的偏见，尤其是语言偏见，本文旨在系统揭示和理解这种语言偏见的具体表现及成因。

Method: 论文通过分析LLM在成对文本比较任务中的表现，分别考察了同语种比较中的性能差异以及跨语种比较时的语言偏好，并进一步探讨了语言偏见是否可由低困惑度偏差解释。

Result: 研究显示欧洲语言在同语种评判中性能优于非洲语言，且在文化相关主题更明显；跨语言比较时，大多数模型偏好英语答案，其偏好主要受答案语言影响；低困惑度偏差与语言偏见相关但不能完全解释。

Conclusion: 本论文发现LLM作为评判者在判断文本质量时存在显著的语言偏见，表现为同一语言比较中的性能差异及跨语言比较中对主流语言（尤其是英语）的偏好。

Abstract: Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.

</details>


### [139] [Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation](https://arxiv.org/abs/2601.13658)
*Arthur Amalvy,Hen-Hsen Huang*

Main category: cs.CL

TL;DR: 本论文针对时间知识图谱提取数据集稀缺及评测污染问题，构建了一个基于预测未来事实的无污染合成数据集，提供了更真实的评估环境。


<details>
  <summary>Details</summary>
Motivation: 现有时间知识图谱提取数据集稀缺且存在训练评估数据污染，导致大型语言模型在评测中表现被高估，亟需无污染的鲁棒基准数据集。

Method: 采用两步法：第一步是时间知识图谱预测生成未来合理四元组，并进行模式筛选；第二步用大型语言模型生成与四元组语义对齐的文本描述，并在此基础上构建评测数据集。

Result: 通过对比基于已知事实数据集的评测，发现LLM在新合成未来事实数据集上性能明显下降，验证了新数据集更能真实反映模型能力。并公开发布了含4200条未来四元组与文本的数据集及生成方法。

Conclusion: 提出了一种基于未来未见事实的合成评估数据集，消除数据污染，实现了时间知识图谱提取的鲁棒、公正评测。

Abstract: The automatic extraction of information is important for populating large web knowledge bases such as Wikidata. The temporal version of that task, temporal knowledge graph extraction (TKGE), involves extracting temporally grounded facts from text, represented as semantic quadruples (subject, relation, object, timestamp). Many recent systems take advantage of large language models (LLMs), which are becoming a new cornerstone of the web due to their performance on many tasks across the natural language processing (NLP) field. Despite the importance of TKGE, existing datasets for training and evaluation remain scarce, and contamination of evaluation data is an unaddressed issue, potentially inflating LLMs' perceived performance due to overlaps between training and evaluation sets. To mitigate these challenges, we propose a novel synthetic evaluation dataset constructed from predicted future, previously unseen temporal facts, thereby eliminating contamination and enabling robust and unbiased benchmarking. Our dataset creation involves a two-step approach: (1) Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema; (2) LLMs perform quadruple-to-text generation, creating semantically aligned textual descriptions. We benchmark Extract, Define and Canonicalize (EDC), a state-of-the-art LLM-based extraction framework, demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts. We publicly release our dataset consisting of 4.2K future quadruples and corresponding textual descriptions, along with the generation methodology, enabling continuous creation of unlimited future temporal datasets to serve as long-term, contamination-free benchmarks for TKGE.

</details>


### [140] [Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.13659)
*Chunlei Meng,Ziyang Zhou,Lucas He,Xiaojing Du,Chun Ouyang,Zhongxue Gan*

Main category: cs.CL

TL;DR: TSDA方法通过时空解耦和因子一致性对齐改进多模态情感分析，性能优越且设计合理。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法依赖时空混合建模，忽略了时空异质性，导致信息不对称和性能受限。

Method: 提出TSDA方法，先将每种模态的信号解耦为时间动态和空间结构，通过时间编码器和空间编码器分别处理，再利用一致性跨模态对齐仅对齐时间特征与时间特征，空间特征与空间特征，配合因子监督和去相关正则化减少信息泄漏，最后通过门控模块重新结合特征进行任务处理。

Result: TSDA在大量实验中表现优于基线方法，消融分析验证了设计的必要性和可解释性。

Conclusion: 通过显式的时空解耦和因子一致性对齐，TSDA有效解决了多模态情感分析中的时空异质性问题，提升了性能和模型可解释性。

Abstract: Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.

</details>


### [141] [CommunityBench: Benchmarking Community-Level Alignment across Diverse Groups and Tasks](https://arxiv.org/abs/2601.13669)
*Jiayu Lin,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本研究提出社区级对齐作为折中策略，通过CommunityBench评测模型对社区偏好的适应能力，发现当前模型表现不足，但社区级对齐为实现个性化且多元的模型对齐开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 现有对齐策略要么假设单一价值观导致边缘化少数群体，要么为个体定制模型成本过高，鉴于社会以社区为单位组织，探索社区级对齐作为折中方案。

Method: 提出了CommunityBench这一首个大规模社区级对齐评测平台，基于共同身份和共同纽带理论设计四个任务，系统评估基础语言模型在社区偏好模拟上的表现。

Result: 实验结果显示现有大型语言模型对社区特定偏好的建模能力有限，同时验证了社区级对齐在推动个体模型定制方面的潜力。

Conclusion: 当前大型语言模型在处理社区特定偏好方面能力有限，社区级别的对齐提供了个体化和通用性之间的有效桥梁，有助于实现规模化且多元化的模型对齐。

Abstract: Large language models (LLMs) alignment ensures model behaviors reflect human value. Existing alignment strategies primarily follow two paths: one assumes a universal value set for a unified goal (i.e., one-size-fits-all), while the other treats every individual as unique to customize models (i.e., individual-level). However, assuming a monolithic value space marginalizes minority norms, while tailoring individual models is prohibitively expensive. Recognizing that human society is organized into social clusters with high intra-group value alignment, we propose community-level alignment as a "middle ground". Practically, we introduce CommunityBench, the first large-scale benchmark for community-level alignment evaluation, featuring four tasks grounded in Common Identity and Common Bond theory. With CommunityBench, we conduct a comprehensive evaluation of various foundation models on CommunityBench, revealing that current LLMs exhibit limited capacity to model community-specific preferences. Furthermore, we investigate the potential of community-level alignment in facilitating individual modeling, providing a promising direction for scalable and pluralistic alignment.

</details>


### [142] [HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference](https://arxiv.org/abs/2601.13684)
*Zhiyuan Shi,Qibo Qiu,Feng Xue,Zhonglin Jiang,Li Yu,Jian Jiang,Xiaofei He,Wenxiao Wang*

Main category: cs.CL

TL;DR: 为解决长上下文任务中KV缓存的内存和效率瓶颈，本文提出了动态的HeteroCache压缩框架，通过注意力头的异质性和冗余性分层管理，实现显著的性能提升和推理加速。


<details>
  <summary>Details</summary>
Motivation: KV缓存的线性内存增长成为长上下文任务中大规模语言模型推理的瓶颈，现有静态压缩方法不能动态捕捉注意力漂移导致的重要信息丢失。

Method: 提出HeteroCache，一个无需训练的动态压缩框架，通过基于稳定性和冗余性分类注意力头，采用细粒度权重分配策略和分层存储机制，实现动态压缩和异步按需上下文检索。

Result: HeteroCache在多个长上下文基准测试中达到了最先进的性能，并在224K上下文长度的任务中实现了最高3倍的解码加速。

Conclusion: HeteroCache有效解决了长上下文推理中KV缓存的内存瓶颈和效率问题，动态捕获注意力变化，显著提升了推理速度和性能。

Abstract: The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\times$ compared to the original model in the 224K context. Our code will be open-source.

</details>


### [143] [Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning](https://arxiv.org/abs/2601.13690)
*Yue Guo,Fanfu Wang,Jianwei Lv,Xincheng Shi,Yuchen Li,Youya Wang,Yunsheng Zeng,Yujing Liu,Yunhao Qiao,Gen Li,Junfeng Wang,Bo Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种结合临床诊断推理数据结构和双阶段训练的语言模型Dr. Assistant，显著提升了临床诊断推理和询问能力，优于多种开源模型，表现接近闭源模型。


<details>
  <summary>Details</summary>
Motivation: 传统临床决策支持系统存在维护成本高和泛化能力弱的问题，且大型语言模型虽具丰富知识与沟通能力，但其诊断推理和询问技能受限，亟需提升。

Method: 设计了临床诊断推理数据结构（CDRD）并构建了相应的数据流水线；开发了Dr. Assistant模型，采用先监督微调（SFT）再基于定制奖励函数的强化学习（RL）两阶段训练策略。

Result: Dr. Assistant在新构建的诊断推理与询问评测基准上超越了开源模型，并在性能上接近闭源模型，体现出其在临床诊断支持中的有效性和竞争力。

Conclusion: Dr. Assistant模型通过引入临床诊断推理数据结构和两阶段训练方法，有效提升了诊断推理和询问能力，在临床诊断引导任务上表现优于开源模型并与闭源模型竞争。

Abstract: Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.

</details>


### [144] [OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens](https://arxiv.org/abs/2601.13695)
*Sifan Li,Hongkai Chen,Yujun Cai,Liyang Chen,Qingwen Ye,Yiwei Wang*

Main category: cs.CL

TL;DR: OptiSQL通过将表格图像转换成紧凑的光学标记，成功实现了从视觉表格和自然语言问题生成可执行SQL，极大减少输入规模且保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的可执行SQL生成依赖于文本到SQL的设置，假设表格作为线性化的文本模式和内容提供，这在许多实际场景中不适用，因为表格通常以视觉形式出现。

Method: 提出OptiSQL，一个基于视觉的框架，通过OCR视觉编码器将表格图像压缩成紧凑的光学标记，结合预训练解码器生成SQL，编码器保持冻结以验证表示能力。

Result: 在视觉化的Spider 2.0-Snow数据集上，OptiSQL在减少一数量级表格输入标记的同时，保持了强大的执行准确率，还能在视觉扰动下保持结构信息的鲁棒性。

Conclusion: 使用紧凑的光学表示可以有效替代传统文本形式的表格输入，实现了高效且准确的可执行SQL生成，适应视觉表格信息处理的实际需求。

Abstract: Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.

</details>


### [145] [Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning](https://arxiv.org/abs/2601.13697)
*Zhihang Yuan,Chengyu Yue,Long Huang,Litu Ou,Lei Shi*

Main category: cs.CL

TL;DR: 提出了一种考虑不确定性的梯度信噪比数据筛选方法GRADFILTERING，有效提升指令调优效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有指令调优数据集庞大且冗余，传统数据选择方法忽视不确定性，导致成本高且效率低下。

Method: 提出了GRADFILTERING，一个基于小型GPT-2代理和LoRA集成的梯度信噪比(G-SNR)数据选择框架，结合不确定性监测进行样本筛选。

Result: GRADFILTERING选择的数据子集不仅在人类和模型评估中表现优异，还能在相同计算预算下更快收敛。

Conclusion: GRADFILTERING方法在数据选择中表现优异，能在大多数评估中超越随机选择和现有强基线，且提高了训练效率。

Abstract: Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.

</details>


### [146] [GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark](https://arxiv.org/abs/2601.13711)
*Lotta Kiefer,Christoph Leiter,Sotaro Takeshita,Elena Schmidt,Steffen Eger*

Main category: cs.CL

TL;DR: 该论文提出了一个德语作者验证的综合基准GerAV，包含丰富的数据源和领域划分，系统评测了多种模型，证明了一个微调的大型语言模型在德语作者验证上的优越性能，并揭示了模型专门化与泛化之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有作者验证研究多集中于英语，缺少大规模且系统的其他语言基准，尤其是德语作者验证缺乏相关资源和综合评测。

Method: 收集了来自Twitter和Reddit的超过60万标注文本对，构建了GerAV基准，包括不同来源和领域的子集，并在此基础上系统评估了多种基线模型和先进模型。

Result: 在提供的训练集上，微调的大型语言模型表现最佳，F1分数超越了最新基线和零样本GPT-5，模型在特定数据类型上表现出色但泛化能力有限，结合不同训练源可提升泛化表现。

Conclusion: GerAV基准为德语作者验证任务提供了一个大规模、多样化且具挑战性的评价平台，推动了德语及跨领域作者验证研究的发展。

Abstract: Authorship verification (AV) is the task of determining whether two texts were written by the same author and has been studied extensively, predominantly for English data. In contrast, large-scale benchmarks and systematic evaluations for other languages remain scarce. We address this gap by introducing GerAV, a comprehensive benchmark for German AV comprising over 600k labeled text pairs. GerAV is built from Twitter and Reddit data, with the Reddit part further divided into in-domain and cross-domain message-based subsets, as well as a profile-based subset. This design enables controlled analysis of the effects of data source, topical domain, and text length. Using the provided training splits, we conduct a systematic evaluation of strong baselines and state-of-the-art models and find that our best approach, a fine-tuned large language model, outperforms recent baselines by up to 0.09 absolute F1 score and surpasses GPT-5 in a zero-shot setting by 0.08. We further observe a trade-off between specialization and generalization: models trained on specific data types perform best under matching conditions but generalize less well across data regimes, a limitation that can be mitigated by combining training sources. Overall, GerAV provides a challenging and versatile benchmark for advancing research on German and cross-domain AV.

</details>


### [147] [Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff](https://arxiv.org/abs/2601.13717)
*Zehan Li,Yuxuan Wang,Ali El Lahib,Ying-Jieh Xia,Xinyu Pi*

Main category: cs.CL

TL;DR: 研究首次系统检验了模拟无知方法在评估大语言模型预测能力中的可靠性，发现该方法存在显著偏差，建议不采用基于模拟无知的回顾性预测评估。


<details>
  <summary>Details</summary>
Motivation: 在评估大语言模型的预测能力时，面临着前瞻性评估时间长和基于已解决事件的回顾性预测数据迅速减少的矛盾，希望验证用模拟无知方法克服这一问题的有效性。

Method: 通过对477个竞赛级问题和9个模型进行系统测试，比较了SI和TI的表现差异，分析了截断指令、链式思维推理以及推理优化模型对SI忠实度的影响。

Result: 发现SI与TI之间存在52%的性能差距，链式思维推理未能有效抑制模型的先验知识，且推理优化模型尽管生成高质量推理痕迹，却表现出更差的SI忠实度。

Conclusion: 该研究发现模拟无知（SI）方法无法准确模拟真实无知（TI），提示基于SI的回顾性预测评估存在系统性误差，导致无法可靠评估大语言模型的预测能力。

Abstract: Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably "rewind" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.

</details>


### [148] [OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents](https://arxiv.org/abs/2601.13722)
*Yulin Hu,Zimo Long,Jiahe Guo,Xingyu Sui,Xing Fu,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: 本文揭示了记忆增强对话系统中常见的过度个性化问题，提出了OP-Bench数据集进行系统评估，并设计Self-ReCheck机制有效缓解该问题，实现更合适的个性化对话。


<details>
  <summary>Details</summary>
Motivation: 当前个性化对话系统虽然在利用用户长期记忆提升对话质量，但主要考察是否能回忆和应用用户信息，忽视了个性化使用的适当性，导致过度个性化现象，影响用户体验，亟需对此进行系统化的研究和解决方案设计。

Method: 构建了包含1700个实例的OP-Bench数据集，定义了三种过度个性化类型（Irrelevance, Repetition, Sycophancy），评估了多种大型语言模型及记忆增强方法，并提出Self-ReCheck模型无关的记忆过滤机制减少过度个性化。

Result: 通过OP-Bench评测发现，随着记忆增强引入，过度个性化普遍存在，模型往往不必要地检索并过度关注用户记忆。使用Self-ReCheck机制后，显著减少了过度个性化反应，同时保持个性化性能。

Conclusion: 现有的个性化对话系统存在过度个性化问题，即在对话中过度使用用户个人信息，导致回复显得勉强、不合适甚至冒犯。本文通过提出OP-Bench数据集和分析模型行为揭示了这一问题的普遍性，并提出了Self-ReCheck机制有效缓解该问题，提升了个性化对话的适当性。

Abstract: Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.

</details>


### [149] [On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation](https://arxiv.org/abs/2601.13729)
*Weichuan Wang,Mingyang Liu,Linqi Song,Chen Ma*

Main category: cs.CL

TL;DR: 本文首次系统研究了非确定性机器翻译，发现其提升了翻译质量但带来了评估挑战，提出ExpectoSample策略以改进评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 探讨机器翻译中非确定性属性及其对系统性能评估的影响。

Method: 系统评估现代非确定性机器翻译系统，使用词汇和语义指标在不同采样规模下进行测试，并提出ExpectoSample策略。

Result: 发现温度约束下的非确定性机器翻译提高了候选质量，但传统的确定性评估框架对其评估不一致，并揭示了“Buckets效应”。

Conclusion: 非确定性机器翻译作为独特现象，能有效缓解多模态问题，但需设计新的评估策略以保证评测结果的一致性和可靠性。

Abstract: In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.

</details>


### [150] [Towards robust long-context understanding of large language model via active recap learning](https://arxiv.org/abs/2601.13734)
*Chenyu Hui*

Main category: cs.CL

TL;DR: 本文提出主动回顾学习，通过设计针对性序列和生成回顾摘要，提高大型语言模型对长上下文的理解能力，实验表现出显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在理解长上下文时存在挑战，亟需提升其长上下文理解能力。

Method: 提出主动回顾学习（ARL）框架，通过在继续预训练中构建针对性序列和推理时的回顾性摘要，使模型能够重新审视并总结早期内容，实现段落间的递归记忆机制。

Result: 在RULER数据集上提升26.8%，在LongBench上提升9.44%，显著增强了模型处理长上下文的能力。

Conclusion: ARL是一种简单有效的基于继续预训练的方法，有助于提升大型语言模型的长上下文理解能力，推动了可扩展记忆增强技术的发展。

Abstract: In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM

</details>


### [151] [Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues](https://arxiv.org/abs/2601.13742)
*Arjun Chandra,Kevin Miller,Venkatesh Ravichandran,Constantinos Papayiannis,Venkatesh Saligrama*

Main category: cs.CL

TL;DR: 本文提出TRACE框架，使LLM基于文本推理音频线索，实现高效且人类一致的语音到语音评价，优于现有模型且成本更低。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音到语音（S2S）评价方法依赖于昂贵且不可透明的音频语言模型（ALMs），而大型语言模型（LLM）虽然具有强大的推理能力，但仅限于文本内容，难以应用于S2S评价。

Method: 提出了TRACE框架，通过构建廉价音频信号的文本蓝图，使LLM能够基于音频线索进行推理评价。引入人类链式思维注释协议（HCoT），将评价拆分为内容、音质和副语言三个明确维度，利用这些数据驱动LLM进行分维度判断，再通过确定性策略融合为整体评分。

Result: TRACE在与人类评分者的一致性方面优于ALMs和仅基于转录文本的LLM评分，同时成本显著降低。

Conclusion: TRACE框架有效提升了S2S评价的准确性和经济性，未来可实现大规模且符合人类判断偏好的S2S评价。将公开HCoT注释和TRACE框架，促进该领域发展。

Abstract: Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.

</details>


### [152] [Pro-AI Bias in Large Language Models](https://arxiv.org/abs/2601.13749)
*Benaya Trabelsi,Jonathan Shaki,Sarit Kraus*

Main category: cs.CL

TL;DR: 研究发现大语言模型在建议和估值中存在系统性的亲AI偏见，可能影响理性决策。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在决策支持中的潜在偏见，特别是是否存在对人工智能自身的偏好。

Method: 通过三项互补实验：1）分析模型在不同咨询问题中推荐AI相关选项的频率；2）比较模型对AI相关职位与非AI职位的薪资估计；3）探究开放权重模型内部表示中‘人工智能’与学术领域提示的相似度。

Result: 发现大语言模型显著偏向推荐AI选项，专有模型几乎是确定性推荐；对AI职位薪资的估计普遍偏高，专有模型更甚；‘人工智能’在模型内部表示中维持高相似度，无关情感倾向。

Conclusion: 大语言模型存在系统性的偏向，倾向于支持人工智能相关的选项和评价，这可能会影响重大决策中的选择和认知。

Abstract: Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.

</details>


### [153] [Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis](https://arxiv.org/abs/2601.13802)
*Yushen Chen,Junzhe Liu,Yujie Tu,Zhikang Niu,Yuzhe Liang,Kai Yu,Chunyu Qiang,Chen Zhang,Xie Chen*

Main category: cs.CL

TL;DR: 针对阿拉伯语多方言语音合成缺乏统一模型和标准资源的问题，本文提出Habibi模型，通过课程学习有效整合多方言语料，开创了高质量生成和系统评测的新局面。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言语音合成领域存在统一建模缺失、语言复杂性和缺少标准化数据及评测体系的问题，阻碍了研究进展。

Method: 利用现有开源自动语音识别语料库，结合语言学指导的课程学习，构建专门化且统一的阿拉伯语多方言文本到语音模型，实现无文本元音标注的高质量生成。

Result: Habibi模型在生成质量上超过领先商业产品，支持广泛阿拉伯语方言，并具备良好扩展性，同时开源资源和评测标准推动领域发展。

Conclusion: 本论文提出的Habibi模型显著提升了多方言阿拉伯语语音合成的质量，优于商业领先服务，并首次建立了多方言阿拉伯语语音合成的系统基准测试。

Abstract: A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .

</details>


### [154] [Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning](https://arxiv.org/abs/2601.13806)
*Dezhao Song,Guglielmo Bonifazi,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: 本文提出基于法律知识图谱的后训练方法，结合SFT和DPO，显著提升了大型语言模型在法律推理任务上的表现，超过了多个基线及更大型模型，展示了知识图谱辅助推理的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型后训练依赖于大规模文本和人类反馈，未能捕捉领域知识结构，导致复杂推理尤其是法律领域的困难。

Method: 基于法律领域的IRAC框架构建了知识图谱（KG），用包含12K法律案例的KG生成训练数据，结合监督微调（SFT）与直接偏好优化（DPO）方法，对三种不同规模与架构的SOTA大型语言模型进行后训练。

Result: 后训练模型在多项法律基准测试（14个任务中的4/5）中表现优于基线模型，70B DPO模型在6个推理任务中4个取得最佳成绩，超越了包括141B规模SOTA法律LLM的表现，证明了KG辅助方法提升法律推理能力的有效性。

Conclusion: 通过构建基于IRAC框架的法律知识图谱并结合先进训练方法，显著提升了大型语言模型在法律领域复杂推理任务中的能力，该方法具备推广至其他高风险专业领域的潜力。

Abstract: LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.

</details>


### [155] [The Role of Prosodic and Lexical Cues in Turn-Taking with Self-Supervised Speech Representations](https://arxiv.org/abs/2601.13835)
*Sam OConnor Russell,Delphine Charuau,Naomi Harte*

Main category: cs.CL

TL;DR: 研究表明人机对话中的转话轮模型既能利用韵律也能利用词汇线索，且任一线索即可单独支撑准确预测，提示未来模型可仅用韵律信息提升隐私和性能。


<details>
  <summary>Details</summary>
Motivation: 流畅的轮流对话是人机交互中的关键挑战，当前基于自监督语音表示的模型是否依赖韵律或词汇线索尚不明确。

Method: 提出一种基于vocoder的方法，精确控制语音中的韵律和词汇线索，对语音活动预测模型进行探究。

Result: 在韵律匹配但内容不可懂的噪声上，模型预测准确率与清晰语音相近，且在破坏其中一种线索时模型自适应利用另一种线索，无需额外训练，这表明韵律和词汇线索在S3Rs中编码独立。

Conclusion: 基于自监督语音表示的转话轮模型既依赖韵律线索也依赖词汇线索，但任一线索均可单独有效，这为未来只需韵律信息的隐私友好模型提供了可能。

Abstract: Fluid turn-taking remains a key challenge in human-robot interaction. Self-supervised speech representations (S3Rs) have driven many advances, but it remains unclear whether S3R-based turn-taking models rely on prosodic cues, lexical cues or both. We introduce a vocoder-based approach to control prosody and lexical cues in speech more cleanly than prior work. This allows us to probe the voice-activity projection model, an S3R-based turn-taking model. We find that prediction on prosody-matched, unintelligible noise is similar to accuracy on clean speech. This reveals both prosodic and lexical cues support turn-taking, but either can be used in isolation. Hence, future models may only require prosody, providing privacy and potential performance benefits. When either prosodic or lexical information is disrupted, the model exploits the other without further training, indicating they are encoded in S3Rs with limited interdependence. Results are consistent in CPC-based and wav2vec2.0 S3Rs. We discuss our findings and highlight a number of directions for future work. All code is available to support future research.

</details>


### [156] [FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs](https://arxiv.org/abs/2601.13836)
*Qian Chen,Jinlan Fu,Changsong Li,See-Kiong Ng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出了FutureOmni基准和OFF训练策略，提升多模态模型对视听信息的未来事件预测能力，弥补了该领域的评测空白。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型虽然在全模态感知方面表现强劲，但缺乏针对未来事件预测的评估，现有基准多集中于回顾性理解，因此亟需设计专门用于视听未来预测的评测基准和改进策略。

Method: 设计FutureOmni基准，包含视频和多选问答，采用大语言模型辅助人类循环标注；提出Omni-Modal Future Forecasting (OFF)训练策略，并构建7K样本的指令微调数据集进行训

Result: FutureOmni包含919个视频和1034个多选问答，对13个全模态及7个视频模型评测发现，最高准确率仅64.8%，表明预测难度大；采用OFF训练后，模型在FutureOmni及其他视听基准测试中表现提升显著。

Conclusion: 当前多模态大语言模型在基于视听线索进行未来事件预测方面表现不足，尤其在语音密集场景中表现较弱。通过构建FutureOmni基准和提出OFF训练策略，模型的未来预测能力得到了显著提升。

Abstract: Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).

</details>


### [157] [Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education](https://arxiv.org/abs/2601.13876)
*Unggi Lee,Jahyun Jeong,Sunyoung Shin,Haeun Park,Jeongsu Moon,Youngchang Song,Jaechang Shim,JaeHwan Lee,Yunju Noh,Seungwon Choi,Ahhyun Kim,TaeHyeon Kim,Kyungtae Joo,Taeyeong Kim,Gyeonggeon Lee*

Main category: cs.CL

TL;DR: 提出了一种轻量级可解释的视觉-语言-动作模型框架，用于安全且一致的科学教学演示，兼顾任务表现与教学质量。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型计算复杂且牺牲语言生成，难以满足需要可解释教学系统的教育环境需求。

Method: 通过文本修复、大语言模型蒸馏、安全训练和教学评估四个部分，实现对轻量级视觉-语言-动作模型的教学对齐。

Result: 该框架在物理、化学、生物和地球科学等五个科学演示中表现出与基线模型相当的任务执行力，同时生成了适合教育场景的解释文本。

Conclusion: Pedagogical VLA Framework在保证执行力的同时，提升了科学演示中的教学解释质量，适合资源受限的教育环境。

Abstract: Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.

</details>


### [158] [OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models](https://arxiv.org/abs/2601.13882)
*Unggi Lee,Sookbun Lee,Heungsoo Choi,Jinseo Lee,Haeun Park,Younghoon Jeon,Sungmin Cho,Minju Kang,Junbo Koh,Jiyeong Bae,Minwoo Nam,Juyeon Eun,Yeonji Jung,Yeil Jeong*

Main category: cs.CL

TL;DR: 提出OpenLearnLM基准，基于教育理论多维度评估大型语言模型在真实教育场景中的知识、技能和态度表现，发现各模型能力互补，强调多轴测评必要性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的测评多针对单一技能，缺乏教育科学理论支撑，难以全面反映模型在教育场景中的真实能力。

Method: 构建OpenLearnLM基准测试框架，基于教育评估理论设计知识、技能和态度三大维度，涵盖124K+测评项目，结合了Bloom认知分类法和真实评测项目，且引入了行为一致性检测方法。

Result: 通过评测七个先进模型，发现模型在不同维度表现差异明显，如Claude-Opus-4.5在技能上优异但知识较弱，Grok-4.1-fast知识领先但存在一致性问题。

Conclusion: 没有单一的大型语言模型在知识、技能和态度三个维度全面领先，显示了多维评估框架的重要性。

Abstract: Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.

</details>


### [159] [Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores](https://arxiv.org/abs/2601.13885)
*Esma Balkır,Alice Pernthaller,Marco Basaldella,José Hernández-Orallo,Nigel Collier*

Main category: cs.CL

TL;DR: 本文扩展了IRT自适应测试以适应生成任务的连续评分，通过异方差正态分布建模和不确定性感知排序，实现了高效且准确的LLM模型评价。


<details>
  <summary>Details</summary>
Motivation: 传统CAT主要针对选择题的正确/错误评分，现代大型语言模型评价趋向于生成任务，其输出得分是连续值，现有方法难以高效准确评估。

Method: 通过将传统IRT模型中的伯努利分布替换为异方差正态分布，实现了对连续有界得分（如ROUGE、BLEU等）的自适应测试，并结合不确定性感知排序器和自适应停止准则。

Result: 在五个不同的基准测试上，方法仅使用2%的测试条目，排名相关性提升0.12τ，且在置信预测中达到95%的准确率，显著提高测试效率和效果。

Conclusion: 本文提出的方法在生成任务中实现了基于IRT的连续得分自适应测试，有效提升了模型排名的准确性和测试效率。

Abstract: Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.

</details>


### [160] [AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization](https://arxiv.org/abs/2601.13918)
*Yusheng Liao,Chuan Xuan,Yutong Cai,Lina Yang,Zhe Chen,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种结合回顾性总结和经验演进的新框架RetroSum，显著提升了大型语言模型在复杂医疗电子健康记录导航中的决策能力和推理连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在医疗领域电子健康记录自主导航任务中，受到对整理输入和简化检索任务的依赖，存在信息丢失和推理连贯性不足的问题，难以满足现实临床环境下复杂决策的需求。

Method: 提出了RetroSum框架，该框架结合了回顾性总结机制和动态经验演进策略，通过重新评估互动历史和利用记忆库中的积累经验，增强模型的长期推理能力和领域适应性。

Result: 实验结果显示，RetroSum在性能上相比竞争基线提升最高达29.16%，且总交互错误降低最高达92.3%，显著增强了诊断和治疗规划等复杂推理任务的执行效果。

Conclusion: RetroSum框架通过回顾性总结机制和经验演进策略，有效解决了长上下文信息丢失和逻辑连贯性断裂问题，从而显著提升了在复杂医疗电子健康记录自主导航任务中的表现。

Abstract: Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.

</details>


### [161] [HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs](https://arxiv.org/abs/2601.13919)
*Yuezhe Yang,Hao Wang,Yige Peng,Jinman Kim,Lei Bi*

Main category: cs.CL

TL;DR: 提出HyperWalker框架，通过动态超图和测试时训练结合多模态医疗数据及电子健康记录，提升了自动临床诊断的准确率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统医疗视觉语言模型多在样本孤立的推断范式下工作，忽视了纵向电子健康记录和结构相关病例，导致推理仅基于图像信息，限制了诊断的准确性。

Method: 构建动态超图iBrochure表示结构异构的EHR数据及多模态临床信息的高阶关联，利用强化学习代理Walker在超图中导航以寻找最佳诊断路径，结合多跳正交检索的linger机制选取临床互补邻近病例实现全面推理。

Result: 在MIMIC医学报告生成和EHRXQA医疗视觉问答数据集上的实验表明，HyperWalker实现了最先进的性能，验证了其方法的有效性。

Conclusion: HyperWalker通过动态超图和测试时训练重塑了临床推理模型，有效整合长期电子健康记录和多模态临床信息，显著提升了医学报告生成和医疗视觉问答的性能。

Abstract: Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \textbf{HyperWalker}, a \textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker

</details>


### [162] [Automatic Prompt Optimization for Dataset-Level Feature Discovery](https://arxiv.org/abs/2601.13922)
*Adrian Cosma,Oleg Szehr,David Kletz,Alessandro Antonucci,Olivier Pelletier*

Main category: cs.CL

TL;DR: 本文通过多智能体提示优化，自动从非结构文本中发现全局共享的可解释特征，提升了文本分类的特征提取效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本特征提取多依赖手工构造的提示或固定特征模式，缺乏自动发现适用于整个数据集的全局特征的方法。

Method: 通过多智能体语言模型联合提出特征定义、提取特征值、并基于数据集级性能和解释性反馈评估特征质量，迭代优化提示语以发现共享特征集合。

Result: 提出的方法突破了以往依赖样本级监督的提示优化方式，实现了基于数据集反馈的提示迭代和全局特征发现。

Conclusion: 本文提出了一种多智能体提示优化框架，实现了从非结构文本中自动发现全局可解释且具有区分性的特征，优化下游监督学习任务。

Abstract: Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.

</details>


### [163] ["The Whole Is Greater Than the Sum of Its Parts": A Compatibility-Aware Multi-Teacher CoT Distillation Framework](https://arxiv.org/abs/2601.13992)
*Jin Cui,Jiaqi Guo,Jiepeng Zhou,Ruixuan Yang,Jiayi Lu,Jiajun Xu,Jiangcheng Song,Boran Zhao,Pengju Ren*

Main category: cs.CL

TL;DR: 本文提出COMPACT，通过动态加权机制自适应融合多教师指导，有效提升学生模型的推理能力并减少灾难性遗忘，实现了多项基准上的最好性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT蒸馏方法通常依赖单一教师模型，限制了学生模型的发展潜力，同时不同教师模型存在能力偏差，且学生模型可能会出现灾难性遗忘。使用多教师监督虽然有吸引力，但如何有效融合多教师的指导仍具挑战，存在模型不兼容和逻辑真正内化难题。

Method: 提出COMPACT框架，通过动态加权教师梯度，实现多教师监督的自适应融合。该方法评估学生与教师的兼容性，依据多维指标包括基于图的共识（过滤误导性推理路径）、基于互信息的适应性（检测真正理解时刻），以及基于损失的难度（防止负迁移）。

Result: 实验及潜空间分析显示COMPACT能有效整合多种推理能力，避免损害模型原有知识结构，在多个基准测试中达到最先进表现，并缓解了灾难性遗忘。

Conclusion: COMPACT框架成功实现了多教师推理监督的自适应融合，提升了学生模型的推理能力和稳定性，突破了单一教师限制，促进了CoT蒸馏技术的发展。

Abstract: Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect "epiphany moments" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.

</details>


### [164] [From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning](https://arxiv.org/abs/2601.13995)
*Zihan Niu,Wenping Hu,Junmin Chen,Xiyue Wang,Tong Xu,Ruiming Tang*

Main category: cs.CL

TL;DR: 提出基于细粒度知识树的TAGS框架，通过层级结构实现精确数据采样，显著提升LLM指令微调效果，节省数据量并增强性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于平面嵌入或粗粒度标签，忽视了知识的细粒度层级依赖，导致数据评估与采样不能精准匹配目标知识，影响模型微调效果。

Method: 利用LLM标注器提取细粒度知识概念，构建自底向上的层级知识树，基于该树设计树感知数据质量与多样性度量，结合KL散度实现目标域对齐，制定树级信息增益最大化的可控采样策略。

Result: TAGS在多项实验中优于现有最先进方法，使用5%数据即可超过全数据集训练模型5.84%的性能，且对齐采样策略带来额外4.24%的提升。

Conclusion: TAGS方法通过知识树和树感知的采样策略，有效提升了大规模数据集的指令微调效果，实现了少量数据下的显著性能提升，优于现有最先进方法。

Abstract: Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \textbf{+5.84\%} using only \textbf{5\%} of the data, while our aligned sampling strategy further boosts average performance by \textbf{+4.24\%}.

</details>


### [165] [Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models](https://arxiv.org/abs/2601.14004)
*Hengyuan Zhang,Zhihao Zhang,Mingyang Wang,Zunhai Su,Yiwei Wang,Qianli Wang,Shuzhou Yuan,Ercong Nie,Xufeng Duan,Qibo Xue,Zeping Yu,Chenming Shang,Xiao Liang,Jing Xiong,Hui Shen,Chaofan Tao,Zhengwu Liu,Senjie Jin,Zhiheng Xi,Dongdong Zhang,Sophia Ananiadou,Tao Gui,Ruobing Xie,Hayden Kwok-Hay So,Hinrich Schütze,Xuanjing Huang,Qi Zhang,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出了一个实用的机械解释性体系，将定位诊断与干预结合，促进大语言模型的优化和应用。


<details>
  <summary>Details</summary>
Motivation: 现有机械解释性(MI)研究主要停留在观察性科学阶段，缺乏可执行的干预体系。

Method: 提出基于“定位、引导和提升”流程的系统调研框架，分类局部诊断与干预方法，并基于可解释对象建立严格的干预协议。

Result: 该框架有效提升了大语言模型（LLM）的对齐性、能力和效率，实现了MI的可操作性方法。

Conclusion: 通过构建系统干预框架，MI从观察性研究转变为可执行的模型优化工具，推动了LLM的实际改进。

Abstract: Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: "Locate, Steer, and Improve." We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.

</details>


### [166] [BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models](https://arxiv.org/abs/2601.14007)
*Junyu Zhang,Yipeng Kang,Jiong Guo,Jiayu Zhan,Junqi Wang*

Main category: cs.CL

TL;DR: 本文通过抽象-根基框架和价值检测，对大型语言模型的抽象理解能力进行了系统分析，证明其内部价值表示结构稳定且可影响具体行为，有助于构建更可控的AI系统。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否真正理解抽象概念，还是仅仅将其作为统计模式进行操作。

Method: 提出抽象-根基框架，将概念理解分解为三种能力：抽象概念解释（A-A）、抽象与具体事件的联系（A-C）、抽象原则对具体决策的应用（C-C）。通过探针检测和引导技术，在六个开源大型语言模型和十个价值维度上进行测试。

Result: 发现探针能够检测到从抽象价值描述到具体事件叙述和决策推理中的价值转移，表明存在跨层次转移。引导实验显示对价值表示的干预会影响具体判断和决策，但不改变抽象解释，表明抽象价值具有稳定性。

Conclusion: 大型语言模型保持了结构化的价值表示，能够连接抽象与行动，为构建具有透明度、可泛化的价值驱动自主AI系统提供了机制基础。

Abstract: Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.

</details>


### [167] [RM-Distiller: Exploiting Generative LLM for Reward Model Distillation](https://arxiv.org/abs/2601.14032)
*Hongli Zhou,Hui Huang,Wei Liu,Chenglong Wang,Xingyuan Bu,Lvyuan Han,Fuhai Song,Muyun Yang,Wenhao Jiang,Hailong Cao,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文提出 RM-Distiller，充分利用教师大语言模型的多种能力，显著提升了奖励模型蒸馏和对齐效果，是首个系统研究生成式大语言模型奖励模型蒸馏的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将教师模型视为简单的二元注释者，未充分挖掘教师模型丰富的知识和能力，导致奖励模型蒸馏效果有限。

Method: 提出 RM-Distiller 框架，系统地利用教师模型的三种能力：精炼能力生成细粒度对比信号，评分能力通过边际优化捕捉偏好强度，生成能力利用生成分布正则化奖励模型。

Result: 在奖励模型基准测试和基于强化学习的对齐任务中，RM-Distiller 明显优于传统蒸馏方法，展示出多能力利用的重要性。

Conclusion: RM-Distiller 框架有效利用教师大型语言模型的多重能力，大幅提升了奖励模型的蒸馏效果和对齐性能。

Abstract: Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher's generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.

</details>


### [168] [Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants](https://arxiv.org/abs/2601.14041)
*Yunhe Wang,Kai Han,Huiling Zhen,Yuchuan Tian,Hanting Chen,Yongbing Huang,Yufei Cui,Yingte Shu,Shan Gao,Ismail Elezi,Roy Vaughan Miles,Songcen Xu,Feng Wen,Chao Xu,Sinan Zeng,Dacheng Tao*

Main category: cs.CL

TL;DR: 传统的自回归大语言模型受限于顺序生成的因果瓶颈，扩散语言模型通过双向去噪提供了突破口。本文分析了实现DLMs潜力的挑战，提出了全面的战略路线图，促进未来AI在推理、自我纠正和多模态智能上的飞跃。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归大语言模型存在因果瓶颈，限制了其全局结构预见和迭代完善能力，而扩散语言模型为文本生成提供了一种整体且双向的去噪过程，这一潜力尚未被充分发掘。

Method: 提出了一个围绕基础架构、算法优化、认知推理和统一多模态智能四大支柱的战略路线图，强调建立一个多尺度分词、积极重掩码和潜在思考的扩散原生生态系统。

Result: 通过识别十项根本性挑战，提出了向扩散原生框架转型的必要性和具体策略，以支持下一个世代人工智能的发展，包括复杂结构推理和多模态融合。

Conclusion: 扩散语言模型（DLMs）由于其全局性和双向的文本生成方式，有望突破自回归（AR）模型的因果瓶颈，实现更复杂的结构推理和动态自我优化。

Abstract: The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.

</details>


### [169] [PRiSM: Benchmarking Phone Realization in Speech Models](https://arxiv.org/abs/2601.14046)
*Shikhar Bharadwaj,Chin-Jou Li,Yoonjae Kim,Kwanghee Choi,Eunjung Yeo,Ryan Soh-Eun Shim,Hanyu Zhou,Brendon Boldt,Karen Rosero Jacome,Kalvin Chang,Darsh Agrawal,Keer Xu,Chao-Han Huck Yang,Jian Zhu,Shinji Watanabe,David R. Mortensen*

Main category: cs.CL

TL;DR: 本文提出PRiSM，首个开放源码的音素识别基准，系统评估模型的音素感知能力，发现多语言训练和编码器-CTC模型表现优异，推动多语种音频建模发展。


<details>
  <summary>Details</summary>
Motivation: 现有音素识别系统评估仅限于表层转录准确率，无法全面检测音素识别系统的盲点。

Method: 提出了PRiSM基准，结合内在和外在评估方法，标准化转录评估并在临床、教育和多语种场景中进行下游任务测试。

Result: PRiSM揭示了训练语言多样性对性能的重要性，编码器-CTC结构带来稳定性，并提供了开源代码及数据集推动领域进展。

Conclusion: 多语言训练和编码器-CTC模型在音素识别中表现最佳，专门的音素识别模型仍优于大型音频语言模型。

Abstract: Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.

</details>


### [170] [Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering](https://arxiv.org/abs/2601.14050)
*Yuxin Chen,Zhengzhou Cai,Xiangtian Ji,Weixiang Zhao,An Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文系统分析了混合专家模型中多语言处理的路由和专家利用模式，发现模型层次结构对应不同语言处理角色，并提出路由引导策略显著提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 虽然MoE架构显示出强大的多语言能力，但其内部机制和跨语言性能差异尚未充分理解，亟需系统性分析以揭示模型的多语言处理特性并提升性能。

Method: 系统地分析了MoE模型的路由行为和专家专门化，研究其与语言、网络深度的关系；通过层次干预验证了各层的功能差异；基于分析结果提出了路由引导方法以优化中间层的专家使用。

Result: 发现MoE模型的多语言处理高度结构化，路由显现语言家族特征，层次对应不同处理功能；提出的路由引导方法在推理阶段引导中间层专家选择，提升多语言性能尤其是语言相关的语言对。

Conclusion: 多语言混合专家模型的路由行为与语言家族紧密相关，且不同层次的专家具有不同的语言处理角色。高资源语言主要依赖共享专家，而低资源语言则更多依赖语言专属专家。中间层作为语言无关的处理中心，早期和晚期层支持语言特定处理。

Abstract: Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.

</details>


### [171] [Kakugo: Distillation of Low-Resource Languages into Small Language Models](https://arxiv.org/abs/2601.14051)
*Peter Devine,Mardhiyah Sanni,Farid Adilazuarda,Julieta Gil Loizaga,Barry Haddow*

Main category: cs.CL

TL;DR: Kakugo利用大模型生成数据训练54种低资源语言小型语言模型，实现低成本、高性能的多语言支持。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏大量训练数据，限制了小型语言模型的发展和应用。

Method: 提出Kakugo流水线，利用大规模教师模型生成合成提示和翻译指令数据集，自动生成训练数据，从而训练54种低资源语言的小型语言模型。

Result: 在翻译、分类和问答等多种自然语言处理任务中，Kakugo训练的小型语言模型表现优于基础模型。

Conclusion: Kakugo是一种低成本（每种语言训练费用低于50美元）、高效且通用的方法，能够支持低资源语言社区开发专属的小型语言模型。

Abstract: We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.

</details>


### [172] [XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs](https://arxiv.org/abs/2601.14063)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Shaoxiong Ji,Hassan Alhuzali,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 该论文提出了一个跨文化推理基准XCR-Bench，解决了高质量跨文化语料缺乏的问题，发现大型语言模型在文化适应中存在明显不足和偏见。


<details>
  <summary>Details</summary>
Motivation: 缺乏高质量注释的CSI语料库及跨文化平行句对，限制了评估大型语言模型跨文化能力的发展。

Method: 构建包含4900个平行句子和1098个独特CSI的XCR-Bench跨文化推理基准，结合Newmark的CSI框架与Hall的三文化模型，设计并评估三类推理任务及相应指标。

Result: 完成了高质量的跨文化平行句库建立，涵盖多层次文化元素，并揭示了模型在社会规范和文化参考的CSI识别与适应不足及潜在区域族群偏见的事实。

Conclusion: 当前最先进的大型语言模型在识别和适应与社会礼仪和文化参考相关的特定文化项目（CSI）方面表现出持续的弱点，并且在文化适应过程中存在区域和族群宗教偏见。

Abstract: Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.

</details>


### [173] [Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks](https://arxiv.org/abs/2601.14105)
*Olesya Razuvayevskaya,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 该研究首次大规模比较了社区与专业辟谣中的劝说技巧，发现两者使用量相当但表达方式不同，群众评价能够有效惩戒不当修辞。


<details>
  <summary>Details</summary>
Motivation: 揭示社区辟谣与专业辟谣在使用劝说技巧上的差异，挑战先前认为社区辟谣更依赖主观劝说的假设。

Method: 利用Community Notes、EUvsDisinfo和Known Fakes数据库的大规模数据，量化比较社区及专业辟谣中的劝说技巧，并分析群众对劝说语言的评价。

Result: 证实社区辟谣并不比专业辟谣包含更多劝说技巧，发现两者在修辞方式上有制度和话题覆盖的不同，并表明群众评价机制在识别问题性修辞上有效。

Conclusion: 社区生产的辟谣内容在使用劝说技巧的数量上并不超过专业辟谣内容，但存在系统性的修辞差异。群众评价机制能够识别并惩罚不当的修辞手段。

Abstract: This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means

</details>


### [174] [Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns](https://arxiv.org/abs/2601.14112)
*George Mihaila*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的轻量级神经网络（ExpNet），能够自动从变压器注意力数据中学习最优解释映射，显著提高了模型解释的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于注意力的解释方法依赖手动定义的聚合策略和固定归因规则，模型无关的方法如LIME、SHAP计算开销大且把模型当黑盒处理，亟需一种自动、高效且透明的解释机制。

Method: 本文提出了一个轻量级神经网络架构——解释网络（ExpNet），通过学习从变压器的注意力模式到单词层重要性得分的显式映射来实现解释。

Result: ExpNet在跨任务设置中进行了评估，并与多种模型无关方法及基于注意力的技术进行了广泛比较，显示出更优的性能和解释效果。

Conclusion: ExpNet能够自动学习最佳的注意力特征组合，有效提升了解释性，并且在跨任务评价中表现优于现有主流的模型无关和基于注意力的解释方法。

Abstract: Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.

</details>


### [175] [NewsRECON: News article REtrieval for image CONtextualization](https://arxiv.org/abs/2601.14121)
*Jonathan Tonglet,Iryna Gurevych,Tinne Tuytelaars,Marie-Francine Moens*

Main category: cs.CL

TL;DR: 提出了一种无需反向图像搜索证据，通过新闻文章元数据推断新闻图片时间地点的方法 NewsRECON，显著提升了相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖反向图像搜索引擎，但这些工具常常无法返回有用结果，限制了其实用性，因此需要一种无需依赖 RIS 证据的新方法。

Method: NewsRECON 使用包含九万多篇文章的语料库，结合双编码器检索事件相关报道、两个跨编码器重新排序文章以匹配地点和事件一致性。

Result: 在 TARA 和 5Pils-OOC 数据集上的实验表明，NewsRECON 超越了之前的方法，并且可以与多模态大语言模型结合，达到新的最先进水平。

Conclusion: NewsRECON 方法能够在缺乏反向图像搜索证据时，有效地将新闻图片与相关报道文章关联，推断图片的拍摄时间和地点，优于现有方法。

Abstract: Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.

</details>


### [176] [A Systematic Analysis of Chunking Strategies for Reliable Question Answering](https://arxiv.org/abs/2601.14123)
*Sofia Bennani,Charles Moslonka*

Main category: cs.CL

TL;DR: 研究不同文档分块策略对工业级RAG系统性能和成本的影响，提出无重叠句子分块更优，且上下文长度需根据任务调整。


<details>
  <summary>Details</summary>
Motivation: 当前工业实践中文档分块多基于经验法则，缺乏系统评估。研究目的是明确分块方式对RAG系统性能和成本的具体影响，从而指导实际部署。

Method: 系统地在Natural Questions数据集上，使用SPLADE检索和Mistral-8B生成器，评估了不同分块方法（基于token、句子、语义、代码）、分块大小、重叠和上下文长度对性能的影响。

Result: 发现重叠分块无明显效益但提高索引成本，句子分块在5k token以内性能优且成本低，超过2.5k tokens上下文长度性能下降明显，且不同目标对应不同的上下文最优长度。

Conclusion: 文档分块策略显著影响RAG系统的可靠性和成本效益。建议采用句子分块以降低成本，避免重叠分块，并根据具体任务调整上下文长度。

Abstract: We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a "context cliff" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).

</details>


### [177] [Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic](https://arxiv.org/abs/2601.14124)
*Saad Mankarious,Aya Zirikly*

Main category: cs.CL

TL;DR: 本文提出一种无预训练的基于扩散模型的风格迁移方法，有效生成性别平衡的阿拉伯语心理健康合成文本，提高多样性和语义忠实度，缓解性别偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法依赖预训练大型语言模型，存在输出多样性不足和偏见传递问题，且精神健康领域数据稀缺且存在性别失衡。

Method: 利用扩散模型框架，将性别偏见缓解问题视为风格迁移问题，专门针对阿拉伯语心理健康语料库中的男性到女性文本风格转换，训练五个不同语义和语言风格的扩散模型。

Result: 通过定量指标显示生成文本在语义上高度忠实原文，同时在表层风格上存在显著变化；定性分析表明生成文本的性别转换在语言上合理。

Conclusion: 本文提出了一种基于扩散模型的风格迁移方法，用于生成无偏见的合成文本，从而缓解低资源精神健康领域的数据稀缺和性别偏见问题。

Abstract: Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.

</details>


### [178] [Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models](https://arxiv.org/abs/2601.14152)
*Hyunjong Ok,Jaeho Lee*

Main category: cs.CL

TL;DR: 本文研究了大语言模型对提示结构的敏感性，发现信息流受因果注意机制影响，解释了为什么上下文置于前比置于后效果更好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对提示结构非常敏感，但其背后的机制尚不清楚，本文旨在揭示这种提示敏感性的原因。

Method: 通过系统的架构分析，研究不同提示结构对大语言模型性能的影响，特别是分析因果注意力机制如何导致信息屏障。

Result: 发现CQO结构显著优于QOC，因果注意力机制中因果掩码限制了选项访问上下文信息，导致性能差异。

Conclusion: 在多项选择问答任务中，将上下文置于问题和选项之前（CQO）比将问题和选项置于上下文之前（QOC）的表现高出14个百分点以上，这是因为QOC中的因果注意力机制限制了选项对上下文信息的访问，造成信息瓶颈。

Abstract: Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.

</details>


### [179] [Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law](https://arxiv.org/abs/2601.14160)
*Ali Hamza Bashir,Muhammad Rehan Khalid,Kostadin Cvejoski,Jana Birr,Jule Berghaus,Armin Berger,Sandra Halscheidt,Christian Temath,Rafet Sifa,David Berghaus*

Main category: cs.CL

TL;DR: 本文通过创新合成数据生成方法改善大型语言模型德国法律问答能力，提供了一种无需人工标注即可获得高质量训练数据的有效方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域（如法律推理）因缺乏专家知识导致错误答案或幻觉，需要可靠且高效的数据资源进行适应训练。

Method: 提出一种基于权威德国法律条文系统生成多样且准确的问答对的合成数据方法，结合自动筛选和参数高效微调技术，提升模型的法律问答能力。

Result: 使用该合成数据微调后的模型在德国法律问答任务中显著优于未经适应的基础模型。

Conclusion: 通过合成数据生成方法对大型语言模型进行针对性微调，显著提升了其在德国法律问答任务中的表现，证明了高质量合成数据作为人工标注替代方案的可行性。

Abstract: Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.

</details>


### [180] [Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum](https://arxiv.org/abs/2601.14172)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 本文研究了新闻与政治文本中句子级人类价值检测，发现轻量级信号与小型集成最有效，分层门控收益有限。调优的有监督模型在有限GPU资源下表现优异，提出未来可通过更丰富的价值结构和上下文信息提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在缺乏上下文的新闻和政治文本中，句子级人类价值检测因道德线索稀疏且类别极度不平衡而导致的识别难题。

Method: 采用DeBERTa-base模型，结合轻量级特征（前句上下文、情感词典和主题特征），比较二元道德存在检测和多标签分类，测试多款指令调优大语言模型及其零/少样本和QLoRA设置，构建软投票集成。

Result: 二元道德存在任务表现较好（正类F1约0.74），直接多标签分类优于分层门控结构，软投票集成模型宏F1达0.332，超越单模型和先前基线。

Conclusion: 在新闻和政治宣言的句子级稀疏道德线索和类别不平衡背景下，轻量级信号和小型集成模型能显著提升人类价值检测性能，分层门控结构收益有限。经过调优的有监督编码器在有限计算资源下仍是强有力的基线。

Abstract: We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task ("does any value appear?") and show that it is learnable from single sentences (positive-class F1 $\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.

</details>


### [181] [HALT: Hallucination Assessment via Latent Testing](https://arxiv.org/abs/2601.14210)
*Rohan Bhatnagar,Youran Sun,Chi Andrew Zhang,Yixin Wen,Haizhao Yang*

Main category: cs.CL

TL;DR: 通过设计轻量级残差探针，快速读取大语言模型中间状态的不确定性，实现低延迟高准确度的幻觉风险估计，提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成答案时虽然内部编码了不确定性，但最终解码过程仍会产生流畅但可能不准确的回答，导致幻觉。需要一种快速可靠的方法直接从模型内部状态中读取不确定性信号，以降低幻觉风险。

Method: 设计了一个小型辅助网络（轻量级残差探针），通过读取问题词令牌的中间隐藏层状态中的知识不确定信号，快速估计幻觉风险。该探针计算开销极低，可与推理并行执行，实现近乎零延迟的风险评估。

Result: 该方法在四个问答基准测试和多个大语言模型家族中表现出优异的AUROC和AURAC，且在数据集变化下表现稳健，能够发现中间表示中的可解释结构，支持快速且可靠的幻觉风险评估。

Conclusion: 本文提出的轻量级残差探针能够从中间隐藏状态中直接读取大语言模型的幻觉风险，达到快速准确估计幻觉风险的目的。该方法在多种问答基准和多个大语言模型家族上表现优异，且具有良好的泛化能力和可解释性。

Abstract: Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.

</details>


### [182] [MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems](https://arxiv.org/abs/2601.14230)
*Yiyang Wang,Yiqiao Jin,Alex Cabral,Josiah Hester*

Main category: cs.CL

TL;DR: 针对多智能体系统角色崩塌问题，提出MASCOT双层优化框架，显著提升角色一致性和对话质量。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统面临角色崩塌和社交谄媚现象，导致行为单一和对话缺乏建设性。

Method: 提出MASCOT框架，采用双层优化策略：1）基于RLAIF的角色感知行为对齐，提升个体角色一致性；2）基于群体奖励的协作对话优化，促进多样和高效交流。

Result: 在心理支持和职场领域的广泛评估中，MASCOT在角色一致性提升了14.1，社交贡献提升了10.6，显著优于现有最先进方法。

Conclusion: MASCOT框架有效解决了多智能体系统中的角色崩塌和社交谄媚问题，推动了更具社会智能的多智能体系统发展。

Abstract: Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.

</details>


### [183] [APEX-Agents](https://arxiv.org/abs/2601.14242)
*Bertie Vidgen,Austin Mann,Abby Fennelly,John Wright Stanly,Lucas Rothman,Marco Burstein,Julien Benchek,David Ostrofsky,Anirudh Ravichandran,Debnil Sur,Neel Venugopal,Alannah Hsia,Isaac Robinson,Calix Huang,Olivia Varones,Daniyal Khan,Michael Haines,Zach Richards,Chirag Mahapatra,Brendan Foody,Osvald Nitski*

Main category: cs.CL

TL;DR: 提出了一种评估AI代理执行复杂工作任务能力的基准APEX-Agents，展示了不同代理的性能排名并开源了相关数据和基础设施。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在复杂、跨应用环境中执行真实工作任务的能力，以推动生产力提升。

Method: 构建了APEX-Agents基准，包括由投资银行分析师、管理顾问和公司律师设计的真实任务，测试代理如何使用文件和工具完成任务，采用Pass@1指标评估。

Result: Gemini 3 Flash（Thinking=High）取得最高24.0%的成绩，其他高表现代理包括GPT-5.2、Claude Opus 4.5和Gemini 3 Pro。

Conclusion: APEX-Agents基准测试验证了AI代理在执行长周期、跨应用任务中的能力，展示了部分代理在复杂工作环境中的优越表现。

Abstract: We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.

</details>


### [184] [Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment](https://arxiv.org/abs/2601.14249)
*Yuming Yang,Mingyoung Lai,Wanxu Zhao,Xiaoran Fan,Zhiheng Xi,Mingqi Wu,Chiyue Huang,Jun Zhao,Haijun Lv,Jian Tong,Yunhua Zhou,Yicheng Zou,Qipeng Guo,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出RSR指标，有效评估推理轨迹与学生模型的适配性，提升蒸馏效率和效果。


<details>
  <summary>Details</summary>
Motivation: 发现更强教师生成的推理轨迹不一定产生更好的学生模型，现有评估方法忽视了轨迹的潜在信息价值，导致适配性判断不足。

Method: 设计Rank-Surprisal Ratio (RSR)指标，即轨迹中每个token的平均排名与平均负对数似然的比率，综合评估轨迹的行为一致性与信息含量。

Result: RSR在五个学生模型及11个教师生成的推理轨迹中与训练后性能高度相关（平均Spearman相关系数0.86），优于现有指标，并证明了其在轨迹及教师选择中的实用性。

Conclusion: 提出的RSR指标有效衡量推理轨迹与学生模型的适配性，更好地提升了蒸馏后的学生模型表现。

Abstract: Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [185] [Rethinking the Value of Multi-Agent Workflow: A Strong Single Agent Baseline](https://arxiv.org/abs/2601.12307)
*Jiawei Xu,Arief Koesdwiady,Sisong Bei,Yan Han,Baixiang Huang,Dakuo Wang,Yutong Chen,Zheshen Wang,Peihao Wang,Pan Li,Ying Ding*

Main category: cs.MA

TL;DR: 本文证明单一大型语言模型通过多轮对话能有效模拟多智能体系统，提出OneFlow算法优化单体执行流程，提升效率且不损性能，但异质多智能体实现仍需突破。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统多采用同质化框架，所有智能体共享相同基础模型，难以评估单一智能体多轮对话是否能模拟多智能体工作流。

Method: 通过七个不同领域的基准测试比较单一智能体多轮对话与多智能体同质及异质工作流性能，并提出OneFlow算法自动优化单智能体执行流程。

Result: 单智能体多轮对话能达到同质多智能体工作流的性能并且更高效，同时可以匹配自动优化的异质工作流性能，OneFlow降低推理成本且不降低精度。

Conclusion: 单模型实现的多智能体工作流可作为强基线，且OneFlow提升了单智能体工作流效率，但因无法共享不同模型的缓存，真正异质多智能体系统仍具挑战和研究空间。

Abstract: Recent advances in LLM-based multi-agent systems (MAS) show that workflows composed of multiple LLM agents with distinct roles, tools, and communication patterns can outperform single-LLM baselines on complex tasks. However, most frameworks are homogeneous, where all agents share the same base LLM and differ only in prompts, tools, and positions in the workflow. This raises the question of whether such workflows can be simulated by a single agent through multi-turn conversations. We investigate this across seven benchmarks spanning coding, mathematics, general question answering, domain-specific reasoning, and real-world planning and tool use. Our results show that a single agent can reach the performance of homogeneous workflows with an efficiency advantage from KV cache reuse, and can even match the performance of an automatically optimized heterogeneous workflow. Building on this finding, we propose \textbf{OneFlow}, an algorithm that automatically tailors workflows for single-agent execution, reducing inference costs compared to existing automatic multi-agent design frameworks without trading off accuracy. These results position the single-LLM implementation of multi-agent workflows as a strong baseline for MAS research. We also note that single-LLM methods cannot capture heterogeneous workflows due to the lack of KV cache sharing across different LLMs, highlighting future opportunities in developing \textit{truly} heterogeneous multi-agent systems.

</details>


### [186] [Generative AI Agents for Controllable and Protected Content Creation](https://arxiv.org/abs/2601.12348)
*Haris Khan,Sadia Asif*

Main category: cs.MA

TL;DR: 本文提出一种结合多智能体协作与数字水印的框架，解决生成型AI中内容可控性和版权保护的挑战，推动负责任且安全的创意生成。


<details>
  <summary>Details</summary>
Motivation: 生成型人工智能虽然促进了创意流程的发展，但现有系统在可控性和内容保护方面存在关键挑战。

Method: 提出了一种新颖的多智能体框架，通过设定专门的智能体角色和集成水印机制，实现内容的可控合成和生成过程中的溯源保护。框架包括导演/规划者、生成者、审阅者、整合者和保护者智能体，并结合人类反馈，确保生成内容符合用户意图同时嵌入不可察觉的数字水印。

Result: 该框架实现了内容合成的可控性、语义对齐和保护鲁棒性的联合优化，提升了生成内容的质量和安全性。

Conclusion: 多智能体架构为负责任的生成型人工智能提供了解决方案，支持值得信赖的创意工作流程，内置所有权追踪和内容可追溯性。

Abstract: The proliferation of generative AI has transformed creative workflows, yet current systems face critical challenges in controllability and content protection. We propose a novel multi-agent framework that addresses both limitations through specialized agent roles and integrated watermarking mechanisms. Unlike existing multi-agent systems focused solely on generation quality, our approach uniquely combines controllable content synthesis with provenance protection during the generation process itself. The framework orchestrates Director/Planner, Generator, Reviewer, Integration, and Protection agents with human-in-the-loop feedback to ensure alignment with user intent while embedding imperceptible digital watermarks. We formalize the pipeline as a joint optimization objective unifying controllability, semantic alignment, and protection robustness. This work contributes to responsible generative AI by positioning multi-agent architectures as a solution for trustworthy creative workflows with built-in ownership tracking and content traceability.

</details>


### [187] [Semantic Fusion: Verifiable Alignment in Decentralized Multi-Agent Systems](https://arxiv.org/abs/2601.12580)
*Sofiya Zaichyk*

Main category: cs.MA

TL;DR: 提出Semantic Fusion框架，实现多智能体系统中去中心化且可验证的语义协调，支持动态更新和不完美通信，保证全局语义一致性和系统的安全活性属性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中缺乏中心化控制时如何在保持语义一致性、实现安全和活性属性验证的难题，支持异步通信和代理失效场景。

Method: 提出了基于本体验证和刷新机制的局部视图操作方法，利用双模拟定理证明局部执行与全局语义行为等价，设计了轻量级架构并通过大规模仿真验证其性能与鲁棒性。

Result: 实现了支持动态变化更新的代理模型，确保语义对齐的确定性和概率性保证，仿真展示了在有界通信和代理失效条件下系统的收敛性和鲁棒性。

Conclusion: Semantic Fusion为多智能体系统提供了一种去中心化的语义协调框架，能够在无中心控制和无显式消息传递的情况下保证全局语义的一致性和验证安全性。

Abstract: We present Semantic Fusion (SF), a formal framework for decentralized semantic coordination in multi-agent systems. SF allows agents to operate over scoped views of shared memory, propose structured updates, and maintain global coherence through local ontology-based validation and refresh without centralized control or explicit message passing. The central theoretical result is a bisimulation theorem showing that each agent's local execution is behaviorally equivalent to its projection of the global semantics, in both deterministic and probabilistic settings. This enables safety, liveness, and temporal properties to be verified locally and soundly lifted to the full system. SF supports agents whose update proposals vary across invocations, including those generated by learned or heuristic components, provided updates pass semantic validation before integration. We establish deterministic and probabilistic guarantees ensuring semantic alignment under asynchronous or degraded communication. To validate the model operationally, we implement a lightweight reference architecture that instantiates its core mechanisms. A 250-agent simulation evaluates these properties across over 11,000 validated updates, demonstrating convergence under probabilistic refresh, bounded communication, and resilience to agent failure. Together, these results show that Semantic Fusion can provide a formal and scalable basis for verifiable autonomy in decentralized systems.

</details>


### [188] [Communication Methods in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.12886)
*Christoph Wittner*

Main category: cs.MA

TL;DR: 本文综述了多智能体强化学习中的各种通信技术，分析了29篇相关文献，指出不同通信框架适用不同问题，强调了低开销方法的重要性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习面临部分可观测环境、非平稳性和动作空间指数增长等难题，通信方法被提出以促进智能体间的有效合作以解决这些问题。

Method: 通过对29篇关于多智能体强化学习中通信技术的文献进行深入分析，评估了显式、隐式、基于注意力、基于图和层次/角色通信的优缺点。

Result: 比较结果显示没有通用的最优通信框架，通信方法选择高度依赖具体问题，同时强调了低计算开销通信方法的重要性，并指出现有研究在系统级基准测试和通信鲁棒性方面存在不足。

Conclusion: 多智能体强化学习中的通信方法没有通用的最佳框架，其选择依赖于具体问题，并且低计算开销的通信方法对于多智能体环境的可扩展性至关重要。

Abstract: Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.

</details>


### [189] [OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts Graph Generative Models](https://arxiv.org/abs/2601.12996)
*Shiyuan Li,Yixin Liu,Yu Zheng,Mei Li,Quoc Viet Hung Nguyen,Shirui Pan*

Main category: cs.MA

TL;DR: 本文提出了一种通用的多智能体系统协作拓扑生成框架OFA-TAD，解决了传统方法泛化差和结构共享不足的问题，实现了高效且适应多任务的智能体协作拓扑设计。


<details>
  <summary>Details</summary>
Motivation: 当前基于图学习的方法针对每个任务训练专门模型，缺乏对未见领域的泛化能力和跨任务的结构共享。

Method: 设计了任务感知图状态编码器（TAGSE）和专家混合（MoE）架构，通过三阶段训练策略实现模型的通用化和适应性。

Result: 在六个不同基准测试上，OFA-TAD表现出更强的自适应性和泛化性能，生成的拓扑更适合多样化任务。

Conclusion: 提出的OFA-TAD框架通过单一通用模型生成适应不同任务的多智能体系统协作拓扑，显著优于传统的针对单一任务训练的模型。

Abstract: Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex problems, yet their performance is critically dependent on the design of their underlying collaboration topology. As MAS become increasingly deployed in web services (e.g., search engines), designing adaptive topologies for diverse cross-domain user queries becomes essential. Current graph learning-based design methodologies often adhere to a "one-for-one" paradigm, where a specialized model is trained for each specific task domain. This approach suffers from poor generalization to unseen domains and fails to leverage shared structural knowledge across different tasks. To address this, we propose OFA-TAD, a one-for-all framework that generates adaptive collaboration graphs for any task described in natural language through a single universal model. Our approach integrates a Task-Aware Graph State Encoder (TAGSE) that filters task-relevant node information via sparse gating, and a Mixture-of-Experts (MoE) architecture that dynamically selects specialized sub-networks to drive node and edge prediction. We employ a three-stage training strategy: unconditional pre-training on canonical topologies for structural priors, large-scale conditional pre-training on LLM-generated datasets for task-topology mappings, and supervised fine-tuning on empirically validated graphs. Experiments across six diverse benchmarks show that OFA-TAD significantly outperforms specialized one-for-one models, generating highly adaptive MAS topologies. Code: https://github.com/Shiy-Li/OFA-MAS.

</details>


### [190] [A simulation of urban incidents involving pedestrians and vehicles based on Weighted A*](https://arxiv.org/abs/2601.13452)
*Edgar Gonzalez Fernandez*

Main category: cs.MA

TL;DR: 本文提出一个基于多智能体和加权A*路径规划的城市行人与车辆仿真框架，研究环境因素和行为差异对交通安全与效率的影响。


<details>
  <summary>Details</summary>
Motivation: 模拟城市中行人和车辆的互动，以评估碰撞风险和出行效率。

Method: 基于多智能体系统，在二维网格城市环境中引入行人和车辆两种智能体，并采用加权A*算法进行路径规划，模拟不同的行为决策。

Result: 实验结果展示了障碍物密度、交通控制机制和行为偏差等因素对安全性和出行效率的影响。

Conclusion: 该仿真框架有效模拟了城市行人与车辆的互动，为评估城市交通安全和效率提供了工具。

Abstract: This document presents a comprehensive simulation framework designed to model urban incidents involving pedestrians and vehicles. Using a multiagent systems approach, two types of agents (pedestrians and vehicles) are introduced within a 2D grid based urban environment. The environment encodes streets, sidewalks, buildings, zebra crossings, and obstacles such as potholes and infrastructure elements. Each agent employs a weighted A* algorithm for pathfinding, allowing for variation in decision making behavior such as reckless movement or strict rule-following. The model aims to simulate interactions, assess risk of collisions, and evaluate efficiency under varying environmental and behavioral conditions. Experimental results explore how factors like obstacle density, presence of traffic control mechanisms, and behavioral deviations affect safety and travel efficiency.

</details>


### [191] [The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption](https://arxiv.org/abs/2601.13671)
*Apoorva Adimulam,Rajesh Gupta,Sumit Kumar*

Main category: cs.MA

TL;DR: 本文系统地整合规划、通信协议及治理机制，构建了可扩展且可治理的多智能体编排架构，增强了多智能体协作能力，推动AI系统的企业应用。


<details>
  <summary>Details</summary>
Motivation: 推动多智能体系统的发展，使自主智能体能够通过结构化协调和沟通协作，完成复杂的共享任务。

Method: 研究设计了两个关键通信协议——模型上下文协议和Agent2Agent协议，支持智能体之间的协同工作和工具访问，构建了一个可扩展、可审计且符合法规的通信基础。

Result: 提出的架构和协议实现了系统内部协调、透明和可治理，促进企业级AI生态系统的实施。

Conclusion: 本文提出了一个多智能体系统的统一架构框架，通过整合规划、策略执行、状态管理和质量控制，实现系统的有序编排。

Abstract: Orchestrated multi-agent systems represent the next stage in the evolution of artificial intelligence, where autonomous agents collaborate through structured coordination and communication to achieve complex, shared objectives. This paper consolidates and formalizes the technical composition of such systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. Another primary contribution of this work is the in-depth technical delineation of two complementary communication protocols - the Model Context Protocol, which standardizes how agents access external tools and contextual data, and the Agent2Agent protocol, which governs peer coordination, negotiation, and delegation. Together, these protocols establish an interoperable communication substrate that enables scalable, auditable, and policy-compliant reasoning across distributed agent collectives. Beyond protocol design, the paper details how orchestration logic, governance frameworks, and observability mechanisms collectively sustain system coherence, transparency, and accountability. By synthesizing these elements into a cohesive technical blueprint, this paper provides comprehensive treatments of orchestrated multi-agent systems - bridging conceptual architectures with implementation-ready design principles for enterprise-scale AI ecosystems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [192] [Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines](https://arxiv.org/abs/2601.11647)
*Aniket Abhishek Soni,Milan Parikh,Rashi Nimesh Kumar Dhenia,Jubin Abhishek Soni,Ayush Raj Jha,Sneja Mitinbhai Shah*

Main category: cs.SE

TL;DR: 本文提出利用强化学习动态优化CI/CD流水线测试策略，显著提升效率和缩短反馈时间，同时保持较低的缺陷漏检率，展示了智能DevOps流程的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代软件交付中CI/CD流水线的静态工作流随着系统规模扩大导致效率下降，亟需动态智能优化方案。

Method: 将CI/CD流水线建模为马尔可夫决策过程，训练强化学习代理在运行时动态决定测试执行策略（全部、部分或不执行测试）以最大化吞吐量和最小化测试开销。

Result: 强化学习优化后的流水线相比静态基线吞吐量提升了30%，测试执行时间减少了约25%，缺陷漏检率保持在5%以下，实现了加速反馈且风险可控。

Conclusion: 本文证明了基于强化学习的方法能够有效优化CI/CD流水线，使其在提升吞吐量的同时减少测试时间，并控制缺陷漏检率。

Abstract: Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software delivery, yet their static workflows often introduce inefficiencies as systems scale. This paper proposes a reinforcement learning (RL) based approach to dynamically optimize CI/CD pipeline workflows. The pipeline is modeled as a Markov Decision Process, and an RL agent is trained to make runtime decisions such as selecting full, partial, or no test execution in order to maximize throughput while minimizing testing overhead.
  A configurable CI/CD simulation environment is developed to evaluate the approach across build, test, and deploy stages. Experimental results show that the RL optimized pipeline achieves up to a 30 percent improvement in throughput and approximately a 25 percent reduction in test execution time compared to static baselines, while maintaining a defect miss rate below 5 percent. The agent learns to selectively skip or abbreviate tests for low risk commits, accelerating feedback cycles without significantly increasing failure risk.
  These results demonstrate the potential of reinforcement learning to enable adaptive and intelligent DevOps workflows, providing a practical pathway toward more efficient, resilient, and sustainable CI/CD automation.

</details>


### [193] [Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey](https://arxiv.org/abs/2601.11655)
*Caihua Li,Lianghong Guo,Yanlin Wang,Daya Guo,Wei Tao,Zhenyu Shan,Mingwei Liu,Jiachi Chen,Haoyu Song,Duyu Tang,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文系统总结了软件工程问题解决中人工智能的研究进展，涵盖数据构建、方法论、行为分析和应用，指出了当前挑战和未来方向，并提供了开源资源。


<details>
  <summary>Details</summary>
Motivation: 由于问题解决作为复杂的软件工程任务对大语言模型提出了严峻挑战，促进了自动编码代理的快速发展，亟需对该领域进行系统梳理和总结。

Method: 本论文通过系统综述，分析了数据采集与合成管线、无训练和训练驱动的方法（包括监督微调和强化学习）、数据质量及行为分析，并结合实际应用进行了总结。

Result: 论文全面总结了当前数据构建、算法方法及应用实践，搭建了开放资源库，清晰阐述了关键挑战和未来研究方向。

Conclusion: 当前人工智能在软件工程中的问题解决任务面临诸多挑战，但通过数据构建、训练方法和行为分析的系统研究，推动了自动编码代理的发展，未来需继续解决数据质量和模型泛化等关键问题。

Abstract: Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.

</details>


### [194] [The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment Notes](https://arxiv.org/abs/2601.11659)
*Aaron Adcock,Aayushi Srivastava,Abhimanyu Dubey,Abhinav Jauhri,Abhinav Pande,Abhinav Pandey,Abhinav Sharma,Abhishek Kadian,Abhishek Kumawat,Adam Kelsey,Adam Stelle,Adeel Cheema,Adela Kabiljo,Adina Katz,Adithya Gangidi,Aditya Tayade,Adolfo Victoria,Adrian Samatan Alastuey,Adrien Conrath,Afroz Mohiuddin,Ahmed Sharif,Ahnaf Siddiqui,Ahuva Goldstand,Aijung Li,Aidan Boyd,Aidin Kazemi Daliri,Aisha Iqbal,Ajay Menon,Ajit Mathews,Akhil Mathur,Akshat Agarwal,Alan Schelten,Alana Shine,Alejandro Castillejo Muñoz,Aleksei Guliaev,Alex Radovic,Alex Song,Alex Vaughan,Alexander Simeonov,Alexandre Rezende,Alexandre Rezende,Alexei Baevski,Alexey Roubaud,Allen Ma,Alvin Lee,Alyssa Pereira,Aman Ahmed,Aman Shankar,Amanda Kallet,Amar Budhiraja,Ameya Khandekar,Amine Benhalloum,Amir Gershman,Amit Nagpal,Amit Zohar,Amr Sharaf,Anant Desai,Anastasia Razdaibiedina,Anca Agape,Andranik Kurghinyan,Andre Perunicic,Andrea Madotto,Andrei Darabanov,Andrés Alvarado,Andrew Brown,Andrew Cohen,Andrew Fang,Andrew Freeman,Andrew Gallagher,Andrew Gu,Andrew Prasetyo Jo,Andrew Ryan,Andrew Steffen,Andrew Wei,Andrey Rusakov,Andrii Golovei,Andy Shang,Angela Fan,Angela Fan,Angela Flewellen,Animesh Pathak,Anirudh Goyal,Ankit Ramchandani,Ankur Pai,Ankur Singh,Ankush Garg,Anlu Xing,Anna Cai,Anna Grosul,Anna Prochowska,Anna Sun,Annie Dong,Annie Franco,Anqi Hu,Anshul Chawla,Anthony Hartshorn,Antonia Sheng,Antony Thomas,Anuj Goyal,Anusha De,Anvit Bodiwala,Anvit Bodiwala,Aobo Yang,Aparajita Saraf,Apurva Samudra,Aran Mun,Arash Rahnama,Archi Mitra,Archie Sravankumar,Archit Gupta,Aria Haghighi,Ariel Stolerman,Arkabandhu Chowdhury,Arnab Choudhury,Artem Korenev,Arthur Guo,Arthur Hinsvark,Arun Mallya,Arvind Neelakantan,Arya Talebzadeh,Ashish Shah,Ashmitha Jeevaraj Shetty,Ashwin Bharambe,Asif Islam,Aston Zhang,Austen Gregerson,Avi Lewis,Aya Ibrahim,Ayaz Minhas,Ayelet Dahan,Ayelet Regev Dabah,Bangsheng Tang,Bar Ulman,Bardiya Sadeghi,Bartosz Jedrzejewski,Barys Skarabahaty,Beibei Zhu,Beibin Li,Ben Bharier,Benjamin Leonhardi,Benjamin Muller,Bennett Plessala,Bernie Huang,Beth Loyd,Bhargavi Paranjape,Bhavik Sheth,Bill Bonner,Bill Holland,Bill Wang,Bingzhe Liu,Binh Tang,Bo Liu,Bo Wu,Boduo Li,Bokai Yu,Bor-Chun Chen,Boris Araya,Boris Vidolov,Botao Chen,Boya Peng,Boyu Ni,Bradley Davis,Bram Wasti,Brandon Adams,Brandon Taylor,Brandon Wu,Brant Swidler,Brian Chiang,Brian Clerkin,Brian Fuller,Brooks Cutter,Bruno Novais,Bryan Gmyrek,Bysshe Easton,Cait Campos,Canaan Case,Carl Chengyan Fu,Carly Burton,Caro Diaz,Catherine Cole,Ce Liu,Cedric Fougerat,Cen Peng,Cen Peng,Cen Zhao,Changhan Wang,Changkyu Kim,Chantal Shaib,Chao Zhou,Charlotte Caucheteux,Chau Nguyen,Chawin Sitawarin,Chaya Nayak,Chelsea Asher,Chen Fan,Chen Zhu,Cheng Cheng,Cheng Zhang,Chenguang Zhu,Chengxiong Ruan,Chengzhu Yu,Chenheli Hua,Chenxi Whitehouse,Cheryl Holloway,Ching-Hsiang Chu,Ching-Yao Chuang,Chinmay Karande,Chirag Nagpal,Chloé Bakalar,Chloe Bi,Chris Cai,Chris Marra,Chris McConnell,Chris Thi,Chris Tindal,Chris Waterson,Christian Deverall,Christian Fuegen,Christian Keller,Christine Cheng,Christine Jou,Christine Smith,Christine Wang,Christoph Feichtenhofer,Christophe Touret,Christopher Luc,Christy Sauper,Chuanhao Zhuge,Chun-Yi Sung,Chunqiang Tang,Chunyang Wu,Clara Siegel,Cody Heale,Cody Wilbourn,Colin White,Congying Xia,Corinne Wong,Cornel Rat,Cristian Canton Ferrer,Cyrille Habis,Cyrus Nikolaidis,D Lohachov,Da Ju,Dalton Flanagan,Damien Allonsius,Damon Civin,Dan Johnson,Daniel Bolya,Daniel Francisco,Daniel Fried,Daniel Hawthorne,Daniel Haziza,Daniel Ho,Daniel Kreymer,Daniel Li,Daniel Machlab,Daniel McKinnon,Daniel Obenshain,Daniel Rodriguez,Daniel Song,Daniel Tse,Danielle Pintz,Danny Livshits,Daryl James Rodrigo,Dat Huynh,Daulet Askarov,David Brandfonbrener,David Esiobu,David Kant,David Levin,David Renardy,David Soofian,David Stevens,David Xu,David Zhang,Deep Shah,Delia David,Demi Douglas,Denis Boyda,Desh Raj,Devamanyu Hazarika,Dheeraj Mekala,Dhruv Choudhary,Dhruv Mahajan,Di Jin,Didac Suris Coll-Vinent,Didem Foss,Diego Garcia-Olano,Diego Perino,Dieuwke Hupkes,DiJia Su,Dilip Madathil,Dinesh Govindasamy,Dinesh Yeduguru,Dmitry Vengertsev,Dong He,Dong Li,Dong Wang,Dongzhuo Li,Duc Le,Dunant Hin,Dustin Holland,Duy Nguyen,Duy Nguyen,Ed Dowling,Eden Litt,Egor Lakomkin,Ehab AlBadawy,Ehsan K. Ardestani,Elad Eckstein,Elahe Dabir,Elaine Montgomery,Elina Lobanova,Elior Abramoviz,Eliot Hedeman,Elissa Li,Elizabeth Hilbert,Ellen Xiaoqing Tan,Elliot Yun,Elodie Stener,Emilian Stoimenov,Emilien Garreau,Emily Dinan,Emily Hahn,Emily Wood,Emma Li,Emmanuel Ademuwagun,Emrah Seker,Eric Alamillo,Eric Gan,Eric Han,Eric Huang,Eric Michael Smith,Eric-Tuan Le,Ernie Chang,Eryk Helenowski,Eslam Elnikety,Esteban Arcaute,Ethan Myers,Eugene Nho,Eugene Poliukhovych,Evan Dunbar,Evgeniy Litvinenko,Evrim Altıntaş,Eyal Hochman,Eyal Shtrauch,Fabian Mastenbroek,Faiza Zeb,Faizan Ahmad,Farhad Farahbakhshian,Fei Kou,Fei Sun,Feiyu Chen,Felix Chung,Feng Tian,Feng Xu,Filip Radenovic,Filippos Kokkinos,Francesco Barbieri,Francesco Caggioni,Francisco Esparza,Francisco Guzmán,Frank Kanayet,Frank Seide,Frank Zhang,Fred Lewis,Freda Huang,Fulton Wang,Gabriel Synnaeve,Gabriela Jacques-Silva,Gabriella Schwarz,Gaganjit Ghardhora,Gal Elfer,Garrett Dickson,Gaurav Chaurasia,Gautam Sewani,Geet Shingi,Gefei Zuo,Geonhwa Jeong,George Puthanpurackal,Georgia Swee,Gerard Moreno-Torres Bertran,Gil Keren,Gina Ling,Gjergji Stasa,Gobinda Saha,Gor Safran,Gordy French,Goutham Rajendran,Govind Thattai,Grace Cineas,Graeme Nail,Greg Fletcher,Grégoire Mialon,Griffin Adams,Grigory Sizov,Guan Pang,Hady Elsahar,Hai Dang Tran,Hailey Nguyen,Haiping Wu,Hakan Inan,Hamid Eghbalzadeh,Han Fang,Han Zou,Hannah Doyle,Hannah Korevaar,Hannah Wang,Hannah Werbel,Hanwen Zha,Hany Morsy,Hao Ma,Haoci Zhang,Haonan Sun,Haozhu Wang,Hardik Shah,Haroun Habeeb,Harrison Rudolph,Harsh Gupta,Harsh Poddar,Harshil Parikh,Hejia Zhang,Heming Wang,Hengduo Li,Himanshu Sharma,Hoang Phi Nguyen,Hongbo Zhang,Honghao Qiu,Hongjiang Lv,Hongli Xu,Hongyuan Zhan,Hossein Hamooni,Howard Huang,Hu Xu,Hugo Laurençon,Hugo Touvron,Hung Dinh,Hunter Goldman,Hussein Mehanna,Huy Nguyen,Hweimi Tsuo,Ian Graves,Ian Yu,Ibrahim Damlaj,Idan Cohen,Igor Tufanov,Ilan Goldenstein,Ilias Leontiadis,Iliyan Zarov,Imad Ahmed,Innocent Djiofack,Iosif Spulber,Irina-Elena Veliche,Isabella Ramos,Ishan Misra,Itai Gal,Ivan Evtimov,Ivan Evtimov,Ivan Obraztsov,Jack Wu,Jacqueline Romero Vertino,Jaemo Koo,Jaewon Lee,Jake Jung,Jake Weissman,James Beldock,James Crnkovich,James Grinage,James Hongyi Zeng,James Kohli,James Tian,Jamie Cahill,Jan Geffert,Jan Seidel,Jan Seidel,Janey Tracey,Jang Hyun Cho,Janice Wei,Jarrod Kahn,Jasmyn Howell,Jason Long Vu,Jason Park,Jason Yan,Jason Yip,Jay Li,Jay Mahadeokar,Jaya Bharath R Goluguri,Jayasi Mehar,Jean-Baptiste Gaya,Jeet Shah,Jeff Hanson,Jeff Marcus,Jeff Walsh,Jeff Yang,Jelmer van der Linde,Jemma Fan,Jennifer Chan,Jenny Zhen,Jenya Lee,Jeremy Fu,Jeremy Reizenstein,Jeremy Teboul,Jesse He,Jessica Zhong,Ji Hou,Ji Yang,Jia Ding,Jiabo Hu,Jiacheng Zhu,Jiadong Guo,Jialiang Wang,Jialin Ouyang,Jianfeng Chi,Jianyu Huang,Jianyun Zhao,Jiaowen Yang,Jiatong Zhou,Jiawei Zhao,Jiawen Liu,Jie Wang,Jie You,Jiecao Yu,Jillian Schwiep,Jilong Wu,Jing Huang,Jing Li,Jing Yu Koh,Jing Zhang,Jingxiang Chen,Jingyi Yang,Jingyue Shen,Jinho Hwang,Jinxi Guo,Jiwan Khatiwada,Joanna Bitton,Joe Li,Joe Quanaim,Joel Beales,Johan Schuijt,John Chang,John Quan,Johnnie Chan,Jon Shepard,Jona Harris,Jonah Rubin,Jonathan Janzen,Jonathan Kaldor,Jorge Lopez Silva,Jose Leitao,Joseph Greer,Joseph Moon,Joseph Rocca,Joseph Tighe,Josh Fromm,Joshua Deng,Joshua Fernandes,Joshua Saxe,Joyce Zheng,Juan Pino,Julien Prigent,Jun Chen,Junjiao Tian,Junjie Qi,Junjie Wang,Junteng Jia,Kade Baker,Kai Londenberg,Kai Wang,Kainan Peng,Kaiyan Peng,Kaiyue Yang,Kalyan Vasudev Alwala,Kam Hou Yu,Kanika Narang,Karan Chadha,Karan Sikka,Karen Zhang,Karina Schuberts,Karishma Mandyam,Karthik Abinav Sankararaman,Karthik Padthe,Karthik Prasad,Karthik Sivakumar,Kartikeya Upasani,Kate Plawiak,Kate Saenko,Kateřina Žmolíková,Kathryn Stadler,Kathy Matosich,Katie Doulgass,Kaveh Hassani,Kay Ji,Ke Li,Kenneth Heafield,Kenny Yu,Keqian Li,Kevin Chih-Yao Ma,Kevin Hannan,Keyu Man,Kezhen Chen,Khalid El-Arini,Khrystyna Hutsulyak,Kieran Nash,Kiran Jagadeesh,Kody Bartelt,Konstantin Topaloglou-Mundy,Konstantinos Chatziioannou,Konstantinos Karanasos,Konstantinos Vougioukas,Kostas Tsiampouris,Kristen Hamill,Kristy Choi,Krithika Iyer,Kshitiz Malik,Kuenley Chiu,Kun Huang,Kunal Bhalla,Kunal Chawla,Kunpeng Li,Kushal Lakhotia,Kyle Monk,Lakshya Garg,Lalit Chourey,Lars Hamre,Laura Gustafson,Lauren Deason,Laurence Rouesnel,Laurens van der Maaten,Lavender A,Lawrence Chen,Lawrence Jang,Leandro Silva,Leda Sari,Lee Hetherington,Lei Zhang,Leiyu Zhao,Lele Chen,Leo Chenghui Li,Leon Yang,Leon Zhan,Levi Corallo,Liang Tan,Licheng Yu,Lijuan Liu,Lilach Mor,Lincoln Lin,Linfeng Li,Lisa Titus,Liz Jenkins,Lovish Madaan,Lu Fang,Lu Yuan,Lucas Nava,Lucas Pasqualin,Lucas Switzer,Lucia Fang,Lucy Sun,Luka Tadic,Lukas Blecher,Lukas Landzaat,Luxin Zhang,Madhavi Rao,Madian Khabsa,Mahalia Miller,Mahendra Kariya,Mahesh Pasupuleti,Mahi Luthra,Manaal Faruqui,Manav Avlani,Manchen Wang,Mannat Singh,Manohar Paluri,Manoj Chakkaravarthy,Manoj Nair,Maquelle Tiffany,Marcin Pawlowski,Marcus Wu,Maria Lomeli,Mario Consuegra,Marion Boiteux,Marios Andreas Galanis,Marshall Chen,Martin Gleize,Maryam Fazel-Zarandi,Matan Hasson,Mathew Oldham,Mathieu Rita,Matt Dordal,Matt Setzler,Matt Staats,Matt Staats,Matt Wilde,Matthew Clark,Matthew Grange,Matthew Lennie,Matthew Schmohl,Max Raphael,Maxim Naumov,Maxim Samoylov,Maxime Lecanu,Maya Pavlova,Md Taha Bin Jawaid,Meghan Keneally,Melanie Kambadur,Meng Zhang,Mengchen Liu,Mengdi Lin,Mengjiao Wang,Mervyn Abraham,Miao Liu,Michael Au-Yeung,Michael Feldergraf,Michael Man,Michael Matheny,Michael Suo,Michael Tontchev,Michel Meyer,Michelle Ma,Mihir Patel,Mihir Sanjay Kale,Mik Vyatskov,Mikayla Alexander,Mike Andersland,Mike Clark,Mike Lewis,Mike Li,Mike Macey,Mike Macey,Mike Seltzer,Mikel Jimenez Fernandez,Mikhail Antonov,Mikhail Plekhanov,Milan Zhou,Min Si,Ming Qiao,Mingbo Ma,Mingjun Zhang,Mingyi Liang,Miquel Jubert Hermoso,Mirac Suzgun,Mirjam Skarica,Mitesh Kumar Singh,Mohammad Kabbani,Mohammad Rastegari,Mona Sarantakos,Monica Sim,Monika Gangapuram,Mor Moshe,Morrie Doulaty,Morvarid Metanat,Moya Chen,Mrinal Kumar,Munish Bansal,Murali Ramarao,Na Li,Nadav Azaria,Nahiyan Malik,Naman Goyal,Nancy Vargas Balderas,Nanshu Wang,Naoyuki Kanda,Natalia Gimelshein,Natalia Neverova,Nathan Aclander,Natt Sithiviraporn,Navneet Madhu Kumar,Ned Newton,Neeraj Bahl,Negar Ghorbani,Neil Patel,Neta-lee Golan,Nicholas Longenbaugh,Nick Egebo,Nikhil Johri,Nikhil Mehta,Nikhil Naik,Niko Moritz,Nikolay Bashlykov,Nikolay Bogoychev,Nikolay Pavlovich Laptev,Niladri Chatterji,Nile Jones,Nimish Shah,Ning Dong,Ning Li,Ning Li,Ning Zhang,Nishant Yadav,Noam Paz,Norman Cheng,Norman Cheng,Olaoluwa Adesanya,Oleg Repin,Oleksandr Maksymets,Omkar Salpekar,Omri Harosh,Onkar Pednekar,Onur Çelebi,Oran Gafni,Oren Edinger,Osama Hanna,Owais Khan Mohammed,Ozlem Kalinli,Paden Tomasello,Pankaj Singh,Paola Quevedo,Parag Jain,Paria Rashidinejad,Parker Tooley,Parth Parekh,Parth Thakkar,Parvin Taheri,Pasan Hapuarachchi,Pascal Kesseli,Patrick Alrassy,Paulo de Rezende Pinatti,Pavan Balaji,Pawan Sisodiya,Pedro Jose Ferreira Moreira,Pedro Rittner,Pedro Valenzuela,Peize Sun,Peizhao Zhang,Peng-Jen Chen,Pengchao Wang,Pengchuan Zhang,Pengwei Li,Petar Vasic,Peter Carras,Peter Ney,Peter Weng,Petru Dumea,Phil Hayes,Philip Woods,Pierre Andrews,Pierre Ménard,Ping-Hao Wu,Pingchuan Liu,Piotr Dollar,Plamen Dzhelepov,Polina Zvyagina,Posten A,Prabhav Agrawal,Pradhapan Rajendran,Pradyot Prakash,Prajjwal Bhargava,Pramono,Pranay Shah,Pranshu Dave,Prash Jain,Pratik Dubal,Praveen Gollakota,Praveen Krishnan,Pritish Yuvraj,Projjal Ghosh,Punit Singh Koura,Puxin Xu,Qi Qi,Qi Zhou,Qian Guan,Qian Sun,Qiang Liu,Qing He,Qinqing Zheng,Qirui Yang,Qizhen Guo,Quanzeng You,Quentin Carbonneaux,Quentin Carbonneaux,Quentin Duval,Quintin Fettes,Rachad Alao,Rachel Batish,Rachel Guo,Rachel Rodriguez,Radhika Bhargava,Rafael Asuncion,Raghotham Murthy,Rahul Dutta,Rahul Jha,Rahul Kindi,Rahul Mitra,Raj Ganapathy,Raj Shah,Rajarshi Das,Rajat Shrivastava,Rajesh Nishtala,Ramakant Shankar,Raman Shukhau,Ramon Calderer,Rangaprabhu Parthasarathy,Ranjan Subramanian,Raphael Bensadoun,Rares Bostan,Rashnil Chaturvedi,Ravi Agrawal,Ray Gao,Raymond Li,Rebecca Kogen,Ricardo Juan Palma Duran,Ricardo Silveira Cabral,Richard Lee,Richard Yuanzhe Pang,Riddhish Bhalodia,Riham Mansour,Rishabh Singh,Rishi Godugu,Ritun Patney,Rob Boyle,Robbie Goldfarb,Robert Caldwell,Robert Kuo,Roberta Raileanu,Robin Battey,Robin Sharma,Rochit Sapra,Rocky Wang,Rodolfo Granata,Rodrigo De Castro,Rodrigo Paim,Rohan Maheshwari,Rohan Varma,Rohit Girdhar,Rohit Patel,Roshan Sumbaly,Roy Sheaffer,Ruan Silva,Ruben Rodriguez Buchillon,Rui Hou,Ruiming Xie,Ruslan Mavlyutov,Ruslan Semenov,Rustam Dinov,Ruxiao Bao,Ryan Fox,Ryan Kilpatrick,Ryan Kwan,Ryan Lim,Ryan Smith,Saaketh Narayan,Sabrina Qiao,Sachin Mehta,Sachin Siby,Sagar Jain,Saghar Hosseini,Sagie Gur-Ari,Sahana Chennabasappa,Sahin Geyik,Sai Jayesh Bondu,Sai Mounika Chowdhary Nekkalapudi,Saif Hasan,Saisuke Okabayashi,Saketh Rambhatla,Salil Sawhney,Sam Dunster,Sam Zhao,Saman Keon,Samaneh Azadi,Sameet Sapra,Samuel Dooley,Samyak Datta,Sandeep Parab,Sang Michael Xie,Sanjay Singh,Sanyuan Chen,Sara Behn,Sara Khodeir,Sarah Shirazyan,Sargun Dhillon,Sarunya Pumma,Sasha Sidorov,Saskia Adaime,Saurabh Khanna,Sayem Wani,Scott Brenton,Sean Bell,Sean Kelly,Sean Koger,Sean Nunley,Sean Perry,Sebastian Caicedo,Sebastian Dahlgren,Sebastian Ruder,Seiji Yamamoto,Selam Mehretu,Selvan Sunitha Ravi,Sen Lyu,Senthil Chellapan,Serafeim Mellos,Sergey Edunov,Sergey Royt,Shaina Cohen,Shangfu Peng,Shannon Adams,Shaoliang Nie,Sharadh Ramaswamy,Sharan Narang,Shashank Pisupati,Shashi Gandham,Shaun Lim,Shaun Lindsay,Sheena Artrip,Shelly Sheynin,Shen Yan,Sheng Feng,Sheng Shen,Shengbao Zheng,Shenghao Lin,Shengjie Bi,Shengxin Cindy Zha,Shengye Wan,Shengyi Qian,Shengyong Cai,Shengzhi Shao,Shervin Shahidi,Shikai Li,Shimon Bernholtz,Shiqi Wang,Shishir G. Patil,Shiv Verma,Shiva Shankar P,Shiyang Chen,Sho Yaida,Shoubhik Debnath,Shreyas Siravara,Shruti Bhosale,Shuang Ma,Shun Zhang,Shuo Tang,Shuqiang Zhang,Shuyan Zhou,Sicong Che,Sidd Srinivisan,Siddharth Bhattacharya,Siddharth Patki,Sijia Chen,Sili Chen,Simon Vandenhende,Simone Merello,Sinong Wang,Sivan Barzily,Sixian Yi,Siyu Lin,SK Bong,Sky Yin,Sneha Agarwal,Sneha Agarwal,Soerian Lieve,Soji Sajuyigbe,Song Jiang,Songlin Li,Sonia Kim,Sopan Khosla,Soumi Maiti,Spencer Whitman,Sravya Popuri,Sreen Tallam,Srinivas Vaidyanathan,Srinivas Vaidyanathan,Sten Sootla,Stephane Collot,Stephanie Ding,Stephen Chen,Steven Cai,Suchin Gururangan,Sudarshan Govindaprasad,Sue Young,Suganthi Dewakar,Sujan Kumar Gonugondla,Sujeet Bhandari,Suman Gumudavelli,Suman Gumudavelli,Sumit Gupta,Summer Deng,Sungmin Cho,Suresh Ganapathy,Surjyendu Dhal,Susan Fedynak,Susana Contrera,Suyoun Kim,Sylvestre Rebuffi,Takshak Chahande,Tamar Herman,Tan Li,Tao Xu,Tara Fowler,Tarek Sheasha,Tarun Anand,Tarun Kalluri,Tarun Singh,Tatiana Shavrina,Ted Li,Teja Rao,Tejas Patil,Teng Li,Thach Bui,Thai Quach,Thamer Alharbash,Thanh Vinh Vo,Thawan Kooburat,Thilo Koehler,Thomas Georgiou,Thomas Scialom,Tian Ye,Tianhe Li,Tianjun Zhang,Tianyu Li,Tijmen Blankevoort,Timon Willi,Timothy Chou,Timothy Leung,TJ Lee,Todor Mihaylov,Tom Heatwole,Tong Xiao,Tony Cao,Tony Lee,Trang Le,Tristan Rice,Tsz Kei Serena Chan,Tuan Tran,Tudor Tiplea,Tyler Baumgartner,Uday Savagaonkar,Ujjwal Karn,Ulises Martinez Araiza,Umar Farooq,Uriel Cohen,Usman Sharif,Utkarsh Murarka,Van Phung,Varun Joginpalli,Varun Saravagi,Vasu Sharma,Vasudha Viswamurthy,Vedanuj Goswami,Vedika Seth,Venkat Ramesh,Venkat Ramesh,Vibhor Gupta,Victoria Montanez,Vidhya Natarajan,Vidya Sarma,Vignesh Ramanathan,Viktor Kerkez,Vinay Rao,Vincent Gonguet,Vincent Mauge,Virginie Do,Vish Vogeti,Vishrav Chaudhary,Viswesh Sankaran,Vítor Albiero,Vivek Miglani,Vivek Pai,Vlad Cojanu,Vlad Shubin,Vlad Tiberiu Mihailescu,Vladan Petrovic,Vladimir Ivanov,Vladislav Vorotilov,Vrushali Bhutada,Wai I Ng,Wei Cheng,Wei Sun,Wei Tu,Wei Wei,Wei Zhou,Wei-Ning Hsu,Weiwei Chu,Weizhe Yuan,Wenchen Wang,Wenjun Zhao,Wenwen Jiang,Wenyin Fu,Wenzhe Jiang,Whitney Meers,Will Constable,Will Wang,William R. Wong,Xavier Martinet,Xi Victoria Lin,Xi Yan,Xi Yin,Xian Li,Xianfeng Rui,Xianjun Yang,Xiaocheng Tang,Xiaodong Wang,Xiaofang Wang,Xiaolan Wang,Xiaoliang Dai,Xiaoliang Peng,Xiaopeng Li,Xiaozhu Meng,Xibei Zhang,Xide Xia,Xin Jin,xinbo Gao,Xinfeng Xie,Xingyi Zhou,Xu Ma,Xuan Ju,Xuanyi Zhao,Xubo Liu,Xuchao Jia,Xuedong Zhang,Xuefei Cao,Xuewei Wang,Xuewei Wu,Xunnan Xu,Xutai Ma,Xuyang Wang,Yan Cui,Yang Chen,Yang Li,Yang Shu,Yang Xia,Yanjun Chen,Yanjun Zhou,Yash Mehta,Yash Patel,Yash Tekena,Yashesh Gaur,Yasmine Babaei,Yaxuan Zhou,Ye Hu,Ye Qi,Yejin Lee,Yeming Wen,Yen-Cheng Liu,Yexin Bruce Wu,Yi Pan,Yi Yang,Yi-Hui Lin,Yifan Wang,Yifan Wu,Yifan Yang,Yifei Huang,Yiftah Ben Aharon,Yilin Yang,Yiling You,Ying Xu,Ying Zhang,Yingquan Yuan,Yingru Liu,Yingyi Ma,Yining Yang,Yiting Lu,Yonatan Komornik,Yongjie Lin,Yoni Goyhman,Yossi Moran Mamo,Youngjin Nam,Yu Wang,Yu Lu,Yu Zhao,Yu-Ho Hsieh,Yu-Jung Lo,Yuandong Tian,Yuanhan Zhang,Yuanhao Xiong,Yuanshun Yao,Yuchen Hao,Yuchen Zhang,Yuchuan Li,Yue Cao,Yue Yu,Yue Zhao,Yuhan Guo,Yuhao Wang,Yuheng Huang,Yujie Lu,Yujun Shi,Yulun Wang,Yun He,Yun Wang,Yundi Qian,Yunfan Wang,Yunhao Tang,Yuning Mao,Yunlu Li,Yuqi Dai,Yuriy Hulovatyy,Yushi Hu,Yuxuan Sun,Zach Rait,Zach Wentz,Zacharie Delpierre Coudert,Zachary Collins,Zahra Hankir,Zecheng He,Zeeshan Ahmed,Zeeshan Ahmed,Zef RosnBrick,Zhan Shu,Zhanna Rohalska,Zhaoduo Wen,Zhe Liu,Zhe Liu,Zhen Qiao,Zhenggang Xu,Zhengwen Zhou,Zhengxing Chen,Zhenyu Tang,Zhichen Wu,Zhicheng Ouyang,Zhihong Lei,Zhipeng Hong,Zhiping Xiu,Zhiwei Zhao,Zhong Meng,Zhou Jin,Zhouhao Zeng,Zichang Liu,Zihang Meng,Zihuan Qiao,Zinnia Zheng,Zixi Qi,Ziyi Luo,Zoe Foulkes Birkhead,Zoey Sun,Zohar Achdut*

Main category: cs.SE

TL;DR: 本文整合了公开信息，系统总结了Metas Llama 4模型的架构、训练、性能及部署等技术细节，提供权威参考。


<details>
  <summary>Details</summary>
Motivation: 为研究人员和实践者提供Llama 4模型的精确、基于来源的技术参考。

Method: 整合并总结了公开报道的Metas Llama 4模型系列的技术细节，包括模型变体、架构特征、训练过程、性能基准及部署限制。

Result: 详细描述了Llama 4的多个版本（Scout、Maverick及Behemoth教师模型）、架构设计（MoE、多模态融合、长上下文支持）、训练方法（预训练、中期训练及后期微调）、性能表现及实际部署限制，并总结了相关许可和安全措施。

Conclusion: 本文为Llama 4模型提供了全面且权威的技术汇总，便于相关人员准确了解其架构和应用细节。

Abstract: This document consolidates publicly reported technical details about Metas Llama 4 model family. It summarizes (i) released variants (Scout and Maverick) and the broader herd context including the previewed Behemoth teacher model, (ii) architectural characteristics beyond a high-level MoE description covering routed/shared-expert structure, early-fusion multimodality, and long-context design elements reported for Scout (iRoPE and length generalization strategies), (iii) training disclosures spanning pre-training, mid-training for long-context extension, and post-training methodology (lightweight SFT, online RL, and lightweight DPO) as described in release materials, (iv) developer-reported benchmark results for both base and instruction-tuned checkpoints, and (v) practical deployment constraints observed across major serving environments, including provider-specific context limits and quantization packaging. The manuscript also summarizes licensing obligations relevant to redistribution and derivative naming, and reviews publicly described safeguards and evaluation practices. The goal is to provide a compact technical reference for researchers and practitioners who need precise, source-backed facts about Llama 4.

</details>


### [195] [From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy Informs the Design of Agentic AI Systems](https://arxiv.org/abs/2601.11672)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: 本文类比Unix系统‘一切皆文件’原则，探讨文件和代码中心的模型如何统一和简化自主智能体系统资源，提升系统性能与可管理性。


<details>
  <summary>Details</summary>
Motivation: 类比早期Unix系统中‘一切皆文件’的核心抽象，探讨在现代自主智能体领域中类似的统一抽象如何出现与应用。

Method: 通过追踪从Unix系统到DevOps、基础设施即代码，最终到自主软件代理的发展历程，分析文件式抽象和基于代码的规范如何统一多样化的资源。

Result: 展示了文件和代码为中心的交互模型在自主智能体系统中整合资源、提升系统可维护性和操作稳健性的潜力。

Conclusion: 采用文件和代码中心的交互模型有助于构建更易维护、可审计且运行健壮的自主智能体系统。

Abstract: A core abstraction in early Unix systems was the principle that 'everything is a file', enabling heterogeneous devices and kernel resources to be manipulated via uniform read/write interfaces. This paper explores how an analogous unification is emerging in contemporary agentic AI. We trace the evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous software agents, highlighting how file-like abstractions and code-based specifications collapse diverse resources into consistent, composable interfaces. The resulting perspective suggests that adopting file- and code-centric interaction models may enable agentic systems that are more maintainable, auditable, and operationally robust.

</details>


### [196] [Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems](https://arxiv.org/abs/2601.11687)
*Harmohit Singh*

Main category: cs.SE

TL;DR: 本文提出一种多代理系统，通过语义缓存、双阈值决策和意图驱动动态提示组装，实现高效且低成本的自然语言到Python代码转换，广泛应用于企业数据分析。


<details>
  <summary>Details</summary>
Motivation: 降低复杂查询转换的成本和延迟，提高自然语言到代码转换的准确率和效率，满足企业生产环境的需求。

Method: 基于三项创新技术：语义缓存系统（利用LLM检测等价性和结构化适配提示）、双阈值决策机制（区分精确检索与参考生成）、意图驱动的动态提示组装（通过表格感知筛选显著减少Token消耗）。

Result: 系统在企业库存管理中处理超过1万条查询，平均延迟8.2秒，语义准确率达94.3%，缓存命中率67%，有效提升了运行效率和成本控制。

Conclusion: 本文提出的多代理系统成功实现了自然语言查询到Python代码的高效转换，展示了高准确率和成本效益，适用于企业级结构化数据分析。

Abstract: We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.

</details>


### [197] [Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition](https://arxiv.org/abs/2601.12522)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 本文提出了基于多智能体的因果推理缺陷定位方法CogniGent，显著提升了缺陷定位性能，弥补了传统和LLM技术在因果和上下文处理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的缺陷定位方法往往孤立分析代码组件，忽视了组件间的联系，且现有大型语言模型缺乏因果推理和上下文管理能力，限制了定位效果。

Method: 提出了CogniGent，一种基于多智能体的缺陷定位技术，通过因果推理、调用图为基础的根因分析及上下文工程，模拟开发者的调试行为并进行假设测试。

Result: 在包含591条缺陷报告的数据集上，CogniGent在文档和方法层面的MAP提升了23.33%-38.57%，MRR提升了25.14%-53.74%，显著优于六种传统及LLM基线方法。统计测试确认其优越性。

Conclusion: CogniGent通过结合因果推理、调用图分析和上下文管理，有效突破了传统及LLM缺陷定位技术的限制，实现了更人性化且高效的缺陷定位，推动了自动化缺陷定位技术的发展。

Abstract: Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.

</details>


### [198] [SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering](https://arxiv.org/abs/2601.11688)
*Vedant Nipane,Pulkit Agrawal,Amit Singh*

Main category: cs.SE

TL;DR: 本文提出一种基于大语言模型的层次化数据手册与代码映射方法，显著提升嵌入式系统追溯精度和效率，支持大规模软件分析及多种应用。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统中，数据手册与代码实现之间的精确追溯仍是系统工程中的难题，尤其是低级软件，手动映射不可行。

Method: 提出一种层次化的数据手册到代码映射方法，利用大语言模型进行语义分析，通过多个抽象层次逐步缩小搜索空间，包括仓库结构推断、文件相关性估计和符号级对齐。

Result: 在多个开源嵌入式系统仓库上评估，文件映射准确率最高达73.3%，显著优于传统信息检索方法，同时减少了84%的LLM令牌消耗和约80%的运行时间。

Conclusion: 该方法有效支持大规模嵌入式软件的自动分析，并促进了系统感知的机器学习训练数据生成、标准合规验证及规格覆盖分析等下游应用。

Abstract: Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.

</details>


### [199] [Technical Lag as Latent Technical Debt: A Rapid Review](https://arxiv.org/abs/2601.11693)
*Shane K. Panter,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文综述了技术滞后的定义、检测、成因及影响，提出改进测量方法并展望未来研究方向，以更好管理软件技术债务。


<details>
  <summary>Details</summary>
Motivation: 软件系统未能跟上技术进步，导致技术滞后，从而引起软件质量下降，亟需系统综述与管理方法。

Method: 通过快速综述结合滚雪球方法，选取多个数据库中的同行评议研究进行综述分析。

Result: 技术滞后被动积累且常被忽视，影响软件依赖、API、平台及基础设施，管理策略包含自动依赖更新、持续集成和定期审计。

Conclusion: 技术滞后作为被动积累的隐性技术债务指标，现有的度量和检测方法需加强和扩展，以改善大型代码库的维护效率。

Abstract: Context: Technical lag accumulates when software systems fail to keep pace with technological advancements, leading to a deterioration in software quality. Objective: This paper aims to consolidate existing research on technical lag, clarify definitions, explore its detection and quantification methods, examine underlying causes and consequences, review current management practices, and lay out a vision as an indicator of passively accumulated technical debt. Method: We conducted a Rapid Review with snowballing to select the appropriate peer-reviewed studies. We leveraged the ACM Digital Library, IEEE Xplore, Scopus, and Springer as our primary source databases. Results: Technical lag accumulates passively, often unnoticed due to inadequate detection metrics and tools. It negatively impacts software quality through outdated dependencies, obsolete APIs, unsupported platforms, and aging infrastructure. Strategies to manage technical lag primarily involve automated dependency updates, continuous integration processes, and regular auditing. Conclusions: Enhancing and extending the current standardized metrics, detection methods, and empirical studies to use technical lag as an indication of accumulated latent debt can greatly improve the process of maintaining large codebases that are heavily dependent on external packages. We have identified the research gaps and outlined a future vision for researchers and practitioners to explore.

</details>


### [200] [The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing](https://arxiv.org/abs/2601.11783)
*Murtuza N. Shergadwala*

Main category: cs.SE

TL;DR: 本文针对生成式AI的审计，提出分类方法揭示大型语言模型评判的判决结果虽稳定但推理不稳定，建议将可确定逻辑代码化，复杂语义评判交给模型。


<details>
  <summary>Details</summary>
Motivation: 探索不同类型系统指令对基于大型语言模型评判可靠性的影响，提升可扩展且可复现的审计机制。

Method: 提出了Scoped Instruction Decomposition Framework，将测试应用指令分为客观和主观类型，评估四种评判体系在不同运行中的稳定性。

Result: 发现了"稳定陷阱"现象，即判决结果高度一致但推理轨迹不稳定，特别是客观指令中数值推理一致性低，主观指令推理稳定性随证据颗粒度波动明显。

Conclusion: 在生成式人工智能的企业治理中，高判决一致性并不代表解释合理性的稳定，判决的推理稳定性可能较低，尤其在处理定量分析和主观判断时。

Abstract: The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($>99\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\approx19\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\%$--$83\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($>90\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.

</details>


### [201] [Changes in Coding Behavior and Performance Since the Introduction of LLMs](https://arxiv.org/abs/2601.11835)
*Yufan Zhang,Jaromir Savelka,Seth Goldstein,Michael Conway*

Main category: cs.SE

TL;DR: 研究发现ChatGPT发布后，学生编程行为和成绩有显著变化，存在生产力和学习下降的风险，教育和用人单位需调整评价方法应对新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，学生使用这些工具提升编程和解决问题的效率，但也带来了教师难以准确评估学生真实学习成果和努力程度的挑战。

Method: 采用准纵向研究方法，分析研究生云计算课程中一个未更改作业的十个学期（五个学期前后ChatGPT发布）的学生源代码提交行为，比较不同时期学生编码行为和成绩变化。

Result: 发现学生提交的代码长度增加，连续提交之间编辑距离平均值上升但分数提升减少，且这些行为变化与整体表现存在显著统计相关性，支持学生过度依赖LLM导致学习质量下降的假设。

Conclusion: 学生编程行为自ChatGPT发布以来显著变化，表现为提交代码长度增加、编辑距离增大和成绩提升减小，暗示学生生产力和学习效果下降，可能因部分学生过度依赖大语言模型，学习成果受负面影响。

Abstract: The widespread availability of large language models (LLMs) has changed how students engage with coding and problem-solving. While these tools may increase student productivity, they also make it more difficult for instructors to assess students' learning and effort. In this quasi-longitudinal study, we analyze five years of student source code submissions in a graduate-level cloud computing course, focusing on an assignment that remained unchanged and examining students' behavior during the period spanning five semesters before the release of ChatGPT and five semesters after.
  Student coding behavior has changed significantly since Fall 2022. The length of their final submissions increased. Between consecutive submissions, average edit distances increased while average score improvement decreased, suggesting that both student productivity and learning have decreased after ChatGPT's release. Additionally, there are statistically significant correlations between these behavioral changes and their overall performance. Although we cannot definitively attribute them to LLM misuse, they are consistent with our hypothesis that some students are over-reliant on LLMs, which is negatively affecting their learning outcomes. Our findings raise an alarm around the first generation of graduates in the age of LLMs, calling upon both educators and employers to reflect on their evaluation methods for genuine expertise and productivity.

</details>


### [202] [Trace Validation of Unmodified Concurrent Systems with OmniLink](https://arxiv.org/abs/2601.11836)
*Finn Hackett,Evan Wrench,Peter Macko,A. Jesse Jiryu Davis,Yuanhao Wei,Ivan Beschastnikh*

Main category: cs.SE

TL;DR: OmniLink是一种基于TLA+的高效灵活并发系统验证方法，能处理更复杂行为，性能领先，且成功发现多个系统漏洞。


<details>
  <summary>Details</summary>
Motivation: 并发系统难以验证，现有工具通常需侵入式插桩或依赖不现实执行模型，现有TLA+基的验证方法受限，需提出更灵活高效的验证工具。

Method: OmniLink将系统事件视为带有时间区间和TLA+语义的黑盒，通过基于现成模型检测器的线性化检查方法，解决动作的逻辑全序问题，区别于传统的轨迹验证和线性化检查。

Result: OmniLink成功验证了多个复杂并发系统（如WiredTiger、BAT、ConcurrentQueue），改进了已有模型，发现了两个未知Bug，且性能优于当前同行方法。

Conclusion: OmniLink是一种创新的方法，能够高效且灵活地验证并发系统实现与TLA+高层规范之间的一致性，性能优于现有线性化检查工具，且能够发现已知及未知的系统漏洞。

Abstract: Concurrent systems are notoriously difficult to validate: subtle bugs may only manifest under rare thread interleavings, and existing tools often require intrusive instrumentation or unrealistic execution models. We present OmniLink, a new methodology for validating concurrent implementations against high-level specifications in TLA+. Unlike prior TLA+ based approaches which use a technique called trace validation, OmniLink treats system events as black boxes with a timebox in which they occurred and a meaning in TLA+, solving for a logical total order of actions. Unlike prior approaches based on linearizability checking, which already solves for total orders of actions with timeboxes, OmniLink uses a flexible specification language, and offers a different linearizability checking method based on off-the-shelf model checking. OmniLink offers different features compared existing linearizability checking tools, and we show that it outperforms the state of the art on large scale validation tasks.
  Our evaluation validates WiredTiger, a state-of-the-art industrial database storage layer, as well as Balanced Augmented Tree (BAT), a state-of-the art lock-free data structure from the research community, and ConcurrentQueue, a popular lock-free queue featuring aggressive performance optimizations. We use OmniLink to improve WiredTiger's existing TLA+ model, as well as develop new TLA+ models that closely match the behavior of the modeled systems, including non-linearizable behaviors. OmniLink is able to find known bugs injected into the systems under test, as well as help discover two previously unknown bugs (1 in BAT, 1 in ConcurrentQueue), which we have confirmed with the authors of those systems.

</details>


### [203] [Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces](https://arxiv.org/abs/2601.11868)
*Mike A. Merrill,Alexander G. Shaw,Nicholas Carlini,Boxuan Li,Harsh Raj,Ivan Bercovich,Lin Shi,Jeong Yeon Shin,Thomas Walshe,E. Kelly Buchanan,Junhong Shen,Guanghao Ye,Haowei Lin,Jason Poulos,Maoyu Wang,Marianna Nezhurina,Jenia Jitsev,Di Lu,Orfeas Menis Mastromichalakis,Zhiwei Xu,Zizhao Chen,Yue Liu,Robert Zhang,Leon Liangyu Chen,Anurag Kashyap,Jan-Lucas Uslu,Jeffrey Li,Jianbo Wu,Minghao Yan,Song Bian,Vedang Sharma,Ke Sun,Steven Dillmann,Akshay Anand,Andrew Lanpouthakoun,Bardia Koopah,Changran Hu,Etash Guha,Gabriel H. S. Dreiman,Jiacheng Zhu,Karl Krauth,Li Zhong,Niklas Muennighoff,Robert Amanfu,Shangyin Tan,Shreyas Pimpalgaonkar,Tushar Aggarwal,Xiangning Lin,Xin Lan,Xuandong Zhao,Yiqing Liang,Yuanli Wang,Zilong Wang,Changzhi Zhou,David Heineman,Hange Liu,Harsh Trivedi,John Yang,Junhong Lin,Manish Shetty,Michael Yang,Nabil Omi,Negin Raoof,Shanda Li,Terry Yue Zhuo,Wuwei Lin,Yiwei Dai,Yuxin Wang,Wenhao Chai,Shang Zhou,Dariush Wahdany,Ziyu She,Jiaming Hu,Zhikang Dong,Yuxuan Zhu,Sasha Cui,Ahson Saiyed,Arinbjörn Kolbeinsson,Jesse Hu,Christopher Michael Rytting,Ryan Marten,Yixin Wang,Alex Dimakis,Andy Konwinski,Ludwig Schmidt*

Main category: cs.SE

TL;DR: 该论文推出了Terminal-Bench 2.0，一个难度较大的真实工作流程计算任务集，测试发现现有模型表现有限，并提供了数据和工具支持未来改进。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么未能衡量真实世界任务，要么难度不足，无法有效评估先进模型的能力。为此，需要一个既真实又具有挑战性的测试平台。

Method: 设计并构建了Terminal-Bench 2.0，一个由89个计算机终端环境任务组成的难度较高的基准，任务基于真实工作流程编写，并配有人类解决方案及全面的验证测试。

Result: 前沿模型和智能体在该基准测试中的表现均未超过65%，通过错误分析揭示了改进的关键领域，并公开了数据集和评测框架以促进后续研究。

Conclusion: 当前前沿模型在Terminal-Bench 2.0基准测试中表现不佳，得分低于65%，表明其在完成复杂的长时任务方面仍有较大提升空间。

Abstract: AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .

</details>


### [204] [Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps](https://arxiv.org/abs/2601.11926)
*Ananya Halgatti,Shaunak Biswas,Hiya Bhatt,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 本文提出了Harmonica，一种基于MAPE-K循环的机器学习系统自适应控制框架，增强了MLOps流水线中系统的自适应性和可持续运行能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在不断变化的环境中运行时易受不确定性影响，传统MLOps在处理运行时不确定性和系统可持续性方面支持有限，因此需要一种机制来检测偏差并自适应调整系统行为。

Method: 基于HarmonE方法，构建Harmonica自适应示例，实现了高层适应策略与低层执行策略的分离，结合动态适应边界和持续监控机制，自动触发架构调整策略。

Result: 案例研究表明Harmonica能够有效提升系统的稳定性，减少人工干预，实现时间序列回归和计算机视觉任务中机器学习系统的持续可用性。

Conclusion: Harmonica为机器学习操作流水线提供了一种可持续运行的自适应控制机制，通过结构化的MAPE-K循环有效提升系统稳定性，减少人工干预，促进机器学习系统在不确定环境下的长期可用性。

Abstract: Machine learning enabled systems (MLS) often operate in settings where they regularly encounter uncertainties arising from changes in their surrounding environment. Without structured oversight, such changes can degrade model behavior, increase operational cost, and reduce the usefulness of deployed systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle of ML models, it provides limited support for addressing runtime uncertainties that influence the longer term sustainability of MLS. To support continued viability, these systems need a mechanism that detects when execution drifts outside acceptable bounds and adjusts system behavior in response. Despite the growing interest in sustainable and self-adaptive MLS, there has been limited work towards exemplars that allow researchers to study these challenges in MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar built on the HarmonE approach, designed to enable the sustainable operation of such pipelines. Harmonica introduces structured adaptive control through MAPE-K loop, separating high-level adaptation policy from low-level tactic execution. It continuously monitors sustainability metrics, evaluates them against dynamic adaptation boundaries, and automatically triggers architectural tactics when thresholds are violated. We demonstrate the tool through case studies in time series regression and computer vision, examining its ability to improve system stability and reduce manual intervention. The results show that Harmonica offers a practical and reusable foundation for enabling adaptive behavior in MLS that rely on MLOps pipelines for sustained operation.

</details>


### [205] [Enhancing Fuzz Testing Efficiency through Automated Fuzz Target Generation](https://arxiv.org/abs/2601.11972)
*Chi Thien Tran*

Main category: cs.SE

TL;DR: 本文通过静态分析自动生成模糊测试目标，解决了手工构建测试目标耗时问题，提高了大规模项目的模糊测试效率。


<details>
  <summary>Details</summary>
Motivation: 手工创建模糊测试目标在大型软件项目中非常耗时，且需要自动化技术以提升测试质量和效率。

Method: 使用静态代码分析构建函数调用，映射输入数据到函数参数，整合编译信息，并自动收集分析执行结果。

Result: 成功将该方法应用于C/C++库，效果良好，提升了模糊测试目标生成的自动化和覆盖率。

Conclusion: 本文提出了一种通过静态分析库源码来自动生成模糊测试目标的方法，从而提高模糊测试的覆盖率和效率。

Abstract: Fuzzing continues to be the most effective method for identifying security vulnerabilities in software. In the context of fuzz testing, the fuzzer supplies varied inputs to fuzz targets, which are designed to comprehensively exercise critical sections of the client code. Various studies have focused on optimizing and developing advanced fuzzers, such as AFL++, libFuzzer, Honggfuzz, syzkaller, ISP-Fuzzer, which have substantially enhanced vulnerability detection in widely used software and libraries. Nevertheless, achieving greater coverage necessitates improvements in both the quality and quantity of fuzz targets. In large-scale software projects and libraries -- characterized by numerous user defined functions and data types -- manual creation of fuzz targets is both labor-intensive and time-consuming. This challenge underscores the need for automated techniques not only to generate fuzz targets but also to streamline the execution and analysis of their results. In this paper, we introduce an approach to improving fuzz target generation through static analysis of library source code. The proposed method encompasses several key aspects: it analyzes source code structures to accurately construct function calls and generate fuzz targets; it maps fuzzer input data to the corresponding function parameters; it synthesizes compilation information for the fuzz targets; and it automatically collects and analyzes execution results. Our findings are demonstrated through the application of this approach to the generation of fuzz targets for C/C++ libraries.

</details>


### [206] [From LLMs to Agents in Programming: The Impact of Providing an LLM with a Compiler](https://arxiv.org/abs/2601.12146)
*Viktor Kjellberg,Miroslaw Staron,Farnaz Fotrousi*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型结合gcc编译器生成C语言代码的效果，发现编译器反馈大幅提升代码质量，一些小模型甚至优于大模型，强调工具辅助对代码生成的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的代码质量不稳定，存在无法编译的问题，需要探索其通过推理能力和工具辅助提升代码生成质量的可能性。

Method: 在RosettaCode数据集上，对699个C语言编程任务进行实验，使用不同规模的16个语言模型结合gcc编译器，评估模型在编译器反馈下迭代开发可运行程序的效果。

Result: 接入编译器后，编译成功率提升5.3至79.4个百分点，语法错误减少75%，未定义引用错误减少87%。部分小模型结合编译器的表现优于大型模型。

Conclusion: 大型语言模型结合软件开发工具（如编译器）能够显著提升代码生成质量，降低错误率，减少对大规模模型的依赖，提高软件工程效率。

Abstract: Large Language Models have demonstrated a remarkable capability in natural language and program generation and software development. However, the source code generated by the LLMs does not always meet quality requirements and may fail to compile. Therefore, many studies evolve into agents that can reason about the problem before generating the source code for the solution. The goal of this paper is to study the degree to which such agents benefit from access to software development tools, in our case, a \texttt{gcc} compiler. We conduct a computational experiment on the RosettaCode dataset, on 699 programming tasks in C. We evaluate how the integration with a compiler shifts the role of the language model from a passive generator to an active agent capable of iteratively developing runnable programs based on feedback from the compiler. We evaluated 16 language models with sizes ranging from small (135 million) to medium (3 billion) and large (70 billion). Our results show that access to a compiler improved the compilation success by 5.3 to 79.4 percentage units in compilation without affecting the semantics of the generated program. Syntax errors dropped by 75\%, and errors related to undefined references dropped by 87\% for the tasks where the agents outperformed the baselines. We also observed that in some cases, smaller models with a compiler outperform larger models with a compiler. We conclude that it is essential for LLMs to have access to software engineering tools to enhance their performance and reduce the need for large models in software engineering, such as reducing our energy footprint.

</details>


### [207] [Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages](https://arxiv.org/abs/2601.12148)
*Muhammad Umar Zeshan,Motunrayo Ibiyo,Claudio Di Sipio,Phuong T. Nguyen,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 本文提出了基于多智能体协作和大语言模型的LAMPS系统，有效提高了PyPI恶意包检测准确率，优于现有方法，验证了该方法在软件供应链安全中的优势。


<details>
  <summary>Details</summary>
Motivation: 开源代码库中恶意代码对软件供应链构成威胁，传统基于规则的方法缺乏对代码语义模式的捕捉，且大语言模型在可解释和模块化安全流程中应用有限。

Method: 设计了一个包含四个角色智能体（包检索、文件提取、分类、判决聚合）的多智能体系统LAMPS，利用CrewAI框架协调，结合微调CodeBERT模型进行分类和LLaMA-3模型进行上下文推理。

Result: 在两个数据集上评测，LAMPS在D1数据集实现97.7%准确率，超过了MPHunter；在D2数据集达到了99.5%的准确率和均衡准确率，显著优于RAG方法和单智能体基线，McNemar分析证实差异显著。

Conclusion: LAMPS系统通过多智能体协作和大语言模型，实现了对PyPI恶意代码的高效检测，显著优于现有先进方法，证明了分布式LLM推理在软件供应链安全中的可行性和优势。

Abstract: Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.

</details>


### [208] [Aletheia: What Makes RLVR For Code Verifiers Tick?](https://arxiv.org/abs/2601.12186)
*Vatsal Venkatkrishna,Indraneil Paul,Iryna Gurevych*

Main category: cs.SE

TL;DR: 本文探讨了基于RLVR的代码验证器训练机制，提出Aletheia测试平台评估其稳健性，发现不同规模验证器需重点关注不同训练策略以提升表现。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR验证器在生成模型输出的评估和重新排序中表现突出，但其在代码生成领域的应用较少，尤其是在执行反馈不可得的情况下，代码验证器作为辅助仍具价值。

Method: 构建Aletheia测试平台，基于执行反馈进行代码验证器的稳健性评估，分析RLVR训练配方的三个关键组成部分：中间思考轨迹、负样本学习和现场训练。

Result: 实验表明RLVR训练方法总体最优，执行验证呈现正向的训练和推理规模效应。不同规模验证器依赖不同训练策略，小规模以现场训练为关键，大规模则更依赖思考轨迹训练。

Conclusion: RLVR训练的多领域思考验证器在代码生成中的应用具有潜力，尤其是在执行反馈难以获得的场景下。尽管RLVR整体最优，但关键组成部分在不同规模的验证器中表现不同，小规模时以现场训练为主，大规模时则以思考式训练为关键。

Abstract: Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.

</details>


### [209] [Environment-Aware Code Generation: How far are We?](https://arxiv.org/abs/2601.12262)
*Tongtong Wu,Rongyi Chen,Wenjie Du,Suyu Ma,Guilin Qi,Zhenchang Xing,Shahram Khadivi,Ramesh Periyathambi,Gholamreza Haffari*

Main category: cs.SE

TL;DR: 该论文首次系统研究了环境感知代码生成，提出了多维度适配策略并构建了实际环境下评测基准，显著提升了大模型生成代码的环境适配能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估多聚焦小规模、孤立代码，忽视实际软件环境差异，导致难以判定LLM生成代码在特定环境下的可用性。

Method: 构建了VersiBCB基准测试，进行多包、执行验证和弃用感知的环境感知代码生成评估；针对数据、参数和缓存三个适配轴设计代表性策略进行实验。

Result: 通过VersiBCB和适配策略，发现现有LLM生成代码环境适应能力较弱，适配方法显著提高了代码的环境兼容性和可执行性。

Conclusion: 当前大规模语言模型在环境感知代码生成方面表现不足，但通过数据、参数和缓存三方面的适配策略可以显著提升代码的环境兼容性和可执行性。

Abstract: Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.

</details>


### [210] [Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs](https://arxiv.org/abs/2601.12273)
*Chihiro Yoshida,Yuta Ishimoto,Olivier Nourry,Masanari Kondo,Makoto Matsushita,Yasutaka Kamei,Yoshiki Higo*

Main category: cs.SE

TL;DR: 本文提出结合变异分析信息的提示设计，通过大语言模型提升量子程序自动修复的成功率和补丁解释质量，实现了94.4%的高修复成功率，推动了量子程序自动修复技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有量子程序自动修复技术成功率低且生成补丁的可理解性差，亟需提升修复性能及解释能力。

Method: 构建一个结合大语言模型生成代码修复及自然语言解释的自动修复框架，并设计包含不同上下文信息（静态信息、动态信息及变异分析结果）的四种提示配置进行对比实验。

Result: 利用变异分析结果作为动态上下文信息，使得自动修复成功率提高至94.4%，并在部分情况下提升了解释质量。

Conclusion: 基于变异分析信息的提示设计能够显著提升使用大语言模型进行量子程序自动修复的成功率和解释质量。

Abstract: In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.

</details>


### [211] [Hybrid Concolic Testing with Large Language Models for Guided Path Exploration](https://arxiv.org/abs/2601.12274)
*Mahdi Eslamimehr*

Main category: cs.SE

TL;DR: 本文提出了一种结合混合具体符号执行和大型语言模型的测试方法，有效解决了路径爆炸和约束求解成本高的问题，显著提升了测试覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 传统混合具体符号执行技术在大规模实际软件中的应用受限，主要因为路径爆炸和约束求解耗时高，亟需提高效率和可扩展性。

Method: 设计并实现了一个结合混合具体符号执行和大型语言模型的系统架构，通过利用LLMs的语义推理能力指导路径探索、优先选择有趣路径并辅助约束求解。

Result: 通过在合成和真实金融科技应用上的实验证明，该方法在分支覆盖率、路径覆盖率和覆盖时间方面均优于传统混合具体符号执行、随机测试及遗传算法方法。

Conclusion: 本文提出的结合混合具体符号执行与大型语言模型的新算法框架，有效克服了传统混合具体符号执行在路径爆炸和约束求解成本方面的限制，显著提升了测试效率和覆盖率。

Abstract: Concolic testing, a powerful hybrid software testing technique, has historically been plagued by fundamental limitations such as path explosion and the high cost of constraint solving, which hinder its practical application in large-scale, real-world software systems. This paper introduces a novel algorithmic framework that synergistically integrates concolic execution with Large Language Models (LLMs) to overcome these challenges. Our hybrid approach leverages the semantic reasoning capabilities of LLMs to guide path exploration, prioritize interesting execution paths, and assist in constraint solving. We formally define the system architecture and algorithms that constitute this new paradigm. Through a series of experiments on both synthetic and real-world Fintech applications, we demonstrate that our approach significantly outperforms traditional concolic testing, random testing, and genetic algorithm-based methods in terms of branch coverage, path coverage, and time-to-coverage. The results indicate that by combining the strengths of both concolic execution and LLMs, our method achieves a more efficient and effective exploration of the program state space, leading to improved bug detection capabilities.

</details>


### [212] [The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering](https://arxiv.org/abs/2601.12327)
*Lucas Gren,Felix Dobslaw*

Main category: cs.SE

TL;DR: 本文提出一种以专家为核心的验证框架，通过结构化流程保障生成式AI系统质量，提升企业中AI应用的信任和安全性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统虽能自动化多项任务，但在企业中应用受制于缺乏系统化质量保障机制及组织信任的缺口。

Method: 框架采用四阶段实施流程，包括规范制定、系统创建、验证和生产监控，确保生成式AI系统质量和专家监督。

Result: 该框架建立了一套严谨的专家驱动方法，促进了多样化生成式AI应用的质量保障和组织信任的提升。

Conclusion: 提出的专家验证框架成功解决了企业环境中生成式AI系统缺乏系统化质量保障的问题，通过结构化的规范、测试、验证和持续监控流程，实现了专家对系统行为的权威控制。

Abstract: Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.

</details>


### [213] [Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic Logic Recomposition](https://arxiv.org/abs/2601.12360)
*Xinabang He,Yuanwei Chen,Hao Wu,Jikang Zhang,Zicheng Wang,Ligeng Chen,Junjie Peng,Haiyang Wei,Yi Qian,Tiantai Zhang,Linzhang Wang,Bing Mao*

Main category: cs.SE

TL;DR: 该文提出FeatureFuzz，通过语义特征组合生成测试程序，显著提升了编译器模糊测试的效果，发现更多程序错误。


<details>
  <summary>Details</summary>
Motivation: 现有的编译器模糊测试方法多依赖语法变异或通用LLM微调，难以保持bug触发程序的语义，从而限制了生成程序的多样性和测试效果。

Method: FeatureFuzz定义了语义特征作为独立单元，结合自然语言描述和具体代码实例，采用三阶段方法提取特征、合成特征组并实例化为有效程序进行模糊测试。

Result: 在GCC和LLVM上，FeatureFuzz在24小时内发现了167个独特崩溃，是次优工具的2.78倍；72小时内发现106个bug，其中76个已被确认，验证了其高效测试能力。

Conclusion: FeatureFuzz通过显式复用历史bug的语义特征，实现了更有效的编译器模糊测试，显著提升了漏洞发现数量和质量。

Abstract: Compilers constitute the foundational root-of-trust in software supply chains; however, their immense complexity inevitably conceals critical defects. Recent research has attempted to leverage historical bugs to design new mutation operators or fine-tune models to increase program diversity for compiler fuzzing.We observe, however, that bugs manifest primarily based on the semantics of input programs rather than their syntax. Unfortunately, current approaches, whether relying on syntactic mutation or general Large Language Model (LLM) fine-tuning, struggle to preserve the specific semantics found in the logic of bug-triggering programs. Consequently, these critical semantic triggers are often lost, resulting in a limitation of the diversity of generated programs.
  To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer that combines features to generate programs. We define a feature as a decoupled primitive that encapsulates a natural language description of a bug-prone invariant, such as an out-of-bounds array access, alongside a concrete code witness of its realization. FeatureFuzz operates via a three-stage workflow: it first extracts features from historical bug reports, synthesizes coherent groups of features, and finally instantiates these groups into valid programs for compiler fuzzing.
  We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer. Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 106 bugs in GCC and LLVM, 76 of which have already been confirmed by compiler developers, validating the approach's ability to stress-test modern compilers effectively.

</details>


### [214] [Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software](https://arxiv.org/abs/2601.12448)
*Yang Liu,Yixing Luo,Xiaofeng Li,Xiaogang Dong,Bin Gu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文针对航天软件时间序列异常检测，构建首个基准ATSADBench，系统评估大语言模型性能，发现其在多变量异常检测中存在不足，少样本学习有效，而基于检索增强生成无显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有无人监督方法难以有效应用于复杂航天遥测数据的异常检测，且缺乏专门针对航天领域大语言模型检测性能的全面评估与领域知识注入方法。

Method: 构建了航天时间序列异常检测基准ATSADBench，包含9个任务、108,000数据点，基于两种范式（滑动窗口直接标注和基于预测误差检测），并提出用户导向的评估指标（报警准确性、延迟和连续性）。系统评估多种开源大语言模型，并尝试少样本学习与RAG改进策略。

Result: 提出ATSADBench基准及新的评估指标，发现LLMs在多变量任务中性能不足，少样本学习能缓解部分问题，而RAG未带来提升并加剧误报，LLMs可检测异常起始但存在误报风险。

Conclusion: 大语言模型（LLMs）在单变量时间序列异常检测中表现良好，但在多变量航天遥测数据异常检测中效果较差，尤其在报警准确性和连续性方面接近随机猜测。少样本学习能带来有限提升，而基于检索增强生成（RAG）未见显著改善，甚至可能加剧误报。

Abstract: Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.

</details>


### [215] [Automated Tool Support for Category-Partition Testing: Design Decisions, UI and Examples of Use](https://arxiv.org/abs/2601.12559)
*Yvan Labiche*

Main category: cs.SE

TL;DR: 本文提出一种基于图形界面的工具自动化实现Category-Partition测试方法，显著简化测试过程并提升效率，已通过案例验证。


<details>
  <summary>Details</summary>
Motivation: Category-Partition测试技术虽然有效，但手工划分输入域和组合测试用例过程复杂且易出错，因此需要自动化工具支持。

Method: 通过开发带有图形用户界面（GUI）的工具，允许用户定义参数、环境变量、类别及其选项和约束，然后自动生成测试框架和测试用例。

Result: 工具支持多种数据类型，能够根据不同选择标准自动构建测试框架并生成相应测试用例，已通过九个案例研究验证其实用性。

Conclusion: 本文成功实现了对Category-Partition方法中多个步骤的自动化，显著提升了功能测试的效率和准确性。

Abstract: Category-Partition is a functional testing technique that is based on the idea that the input domain of the system under test can be divided into sub-domains, with the assumption that inputs that belong to the same sub-domain trigger a similar behaviour and that therefore it is sufficient to select one input from each sub-domain. Category-Partition proceeds in several steps, from the identification of so-called categories and choices, possibly constrained, which are subsequently used to form test frames, i.e., combinations of choices, and eventually test cases. This paper reports on an ongoing attempt to automate as many of those steps as possible, with graphical-user interface tool support. Specifically, the user interface allows the user to specify parameters as well as so-called environment variables, further specify categories and choices with optional constraints. Choices are provided with precise specifications with operations specific to their types (e.g., Boolean, Integer, Real, String). Then, the tool automates the construction of test frames, which are combinations of choices, according to alternative selection criteria, and the identification of input values for parameters and environment variables for these test frames, thereby producing test cases. The paper illustrates the capabilities of the tool with the use of nine different case studies.

</details>


### [216] [OpenAI for OpenAPI: Automated generation of REST API specification via LLMs](https://arxiv.org/abs/2601.12735)
*Hao Chen,Yunchun Li,Chen Chen,Fengxu Lin,Wei Li*

Main category: cs.SE

TL;DR: 针对现有OAS生成局限，本文提出基于LLM的OOPS方法，实现跨语言框架的高质量自动化OAS生成，实验验证了其高准确率和实用性。


<details>
  <summary>Details</summary>
Motivation: 开发者在编写和维护OpenAPI规范（OAS）时面临挑战，现有静态分析方法受限于编程语言和框架。

Method: 提出基于大语言模型（LLM）的OOPS方法，通过构建API依赖图解决上下文限制，采用多阶段生成和自我优化缓解幻觉问题，实现技术无关的静态分析OAS生成。

Result: 在12个真实REST API上测试，涵盖5种编程语言和8种框架，OOPS在端点方法推断、请求参数和响应推断以及参数约束推断的F1-score分别达到98%、97%和92%，输入输出token量适中。

Conclusion: OOPS有效且准确地生成高质量的OAS，且具备技术无关性和较少人类干预，展示了LLM在自动生成API文档领域的应用潜力。

Abstract: REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.

</details>


### [217] [Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction](https://arxiv.org/abs/2601.12762)
*Xingjie Gao,Pengcheng Huang,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Chen Qian,Ge Yu,Yu Gu*

Main category: cs.SE

TL;DR: 本文提出ToolMaster，通过让大语言模型主动试错和学习工具使用，显著增强了其面对新工具的泛化和稳健性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）使用外部工具时，因过度依赖训练时静态的解决路径，难以适应新工具或变化的工具，导致稳健性不足。

Method: 提出ToolMaster框架，引入试错执行范式，先模仿教师生成的包含工具试验和自我纠正的轨迹，再通过强化学习联合优化试验与执行阶段，实现主动环境交互和工具使用学习。

Result: ToolMaster在面对未见过或不熟悉工具时，表现出明显优于现有方法的泛化能力和稳健性。

Conclusion: 通过试验与执行结合的主动学习策略，ToolMaster显著提升了大语言模型在多样化和动态工具使用环境中的适应能力和效果。

Abstract: Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.

</details>


### [218] [Docker Does Not Guarantee Reproducibility](https://arxiv.org/abs/2601.12811)
*Julien Malka,Stefano Zacchiroli,Théo Zimmermann*

Main category: cs.SE

TL;DR: 本文通过文献综述和大规模实证研究，评估了Docker对软件环境可重现性的实际保障及相关最佳实践的有效性。


<details>
  <summary>Details</summary>
Motivation: 明确Docker在实际应用中保障环境可重现性的程度及其局限，填补理论和实践之间的空白。

Method: 通过系统性文献综述结合对GitHub上5298个Docker文件的重建实证研究。

Result: 确认文献中最佳实践对提升Docker镜像可重现性有积极作用，但实际重建中仍存在不一致。

Conclusion: Docker在实践中对软件环境可重现性有一定保障，但仍存在局限性，文献中提出的最佳实践对提升可重现性有效。

Abstract: The reproducibility of software environments is a critical concern in modern software engineering, with ramifications ranging from the effectiveness of collaboration workflows to software supply chain security and scientific reproducibility. Containerization technologies like Docker address this problem by encapsulating software environments into shareable filesystem snapshots known as images. While Docker is frequently cited in the literature as a tool that enables reproducibility in theory, the extent of its guarantees and limitations in practice remains under-explored.
  In this work, we address this gap through two complementary approaches. First, we conduct a systematic literature review to examine how Docker is framed in scientific discourse on reproducibility and to identify documented best practices for writing Dockerfiles enabling reproducible image building. Then, we perform a large-scale empirical study of 5298 Docker builds collected from GitHub workflows. By rebuilding these images and comparing the results with their historical counterparts, we assess the real reproducibility of Docker images and evaluate the effectiveness of the best practices identified in the literature.

</details>


### [219] [Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles](https://arxiv.org/abs/2601.12845)
*João Pascoal Faria,Emanuel Trigo,Vinicius Honorato,Rui Abreu*

Main category: cs.SE

TL;DR: 本文使用大语言模型自动为Dafny程序生成形式验证注释，显著提高自动化程度并减轻人工负担，在多程序实验中表现优异，并集成于开发环境中获得用户认可。


<details>
  <summary>Details</summary>
Motivation: 尽管现代验证工具力图简化形式验证流程，但手动为传统程序添加形式规格和验证注释依然需要大量的人工努力和专业知识，本文旨在探索用大语言模型自动生成这些注释以降低门槛。

Method: 利用Claude Opus 4.5和GPT-5.2的多模型组合方法，结合验证器的反馈机制，在Dafny程序上迭代修正注释，自动生成形式验证所需的前置条件、后置条件、循环不变量等注释。

Result: 在110个Dafny程序的实验中，自动生成注释的正确率达到98.2%，通过最多8次的修正迭代完成。回归分析表明，辅助证明注释对模型挑战较大。测试用例中的断言用于自动验证生成的注释。并实现了Visual Studio Code插件，获得良好用户反馈。

Conclusion: 通过结合多模型方法和验证反馈，LLMs能够自动生成Dafny程序的形式验证注释，显著降低了手动注释的复杂度和所需专业知识。

Abstract: Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.

</details>


### [220] [Efficient Code Analysis via Graph-Guided Large Language Models](https://arxiv.org/abs/2601.12890)
*Hang Gao,Tao Peng,Baoquan Cui,Hong Huang,Fengge Wu,Junsuo Zhao,Jian Zhang*

Main category: cs.SE

TL;DR: 提出了一种结合代码图和图神经网络的恶意代码检测方法，通过引导大语言模型关注关键代码区域，实现更精确的检测和更低的标注成本，验证了其优越性与实用性。


<details>
  <summary>Details</summary>
Motivation: 恶意行为常隐藏于小且易被忽视的代码片段中，且存在跨文件依赖，导致即使是强大的大语言模型也难以可靠检测。

Method: 该方法将项目解析成代码图，利用大语言模型编码节点的语义和结构信息，再通过图神经网络进行初步检测，利用回溯机制定位重要代码区域，引导模型注意力进行深入分析。

Result: 实验表明该方法在多个公开及自建数据集上持续优于已有技术，能够有效减少无关信息干扰并降低标注成本。

Conclusion: 该论文提出的基于图的注意力获取方法有效提升了大语言模型在检测恶意代码行为中的表现，显著优于现有方法，具有实际应用潜力。

Abstract: Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.

</details>


### [221] [A Benchmark for Language Models in Real-World System Building](https://arxiv.org/abs/2601.12927)
*Weilin Jin,Chenyu Zhao,Zeshun Huang,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Shenglin Zhang,Yongqian Sun,Dan Pei,Yifan Wu,Tong Jia,Ying Li,Zhonghai Wu,Minghua Ma*

Main category: cs.SE

TL;DR: 该论文提出了一个多架构多语言的软件包构建修复基准，评估了六款先进语言模型，揭示了跨ISA修复的困难，为提升软件移植性奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前的软件包构建修复研究大多集中在单一指令集架构和同质编程语言，缺乏对跨指令集架构和多语言环境下修复能力的评估，因此需要一个新的基准来推动该领域的发展。

Method: 构建了一个包含268个真实软件包构建失败案例的多架构、多语言的基准测试，并在此测试上评估了六种最先进的大型语言模型。

Result: 评测结果显示跨指令集架构的软件包修复依然困难，现有模型的表现有限，显示了该任务的复杂性和未来改进的必要性。

Conclusion: 跨指令集架构的软件包构建修复仍然是一个挑战，现有的大型语言模型尚未能完全解决这一问题。

Abstract: During migration across instruction set architectures (ISAs), software package build repair is a critical task for ensuring the reliability of software deployment and the stability of modern operating systems. While Large Language Models (LLMs) have shown promise in tackling this challenge, prior work has primarily focused on single instruction set architecture (ISA) and homogeneous programming languages. To address this limitation, we introduce a new benchmark designed for software package build repair across diverse architectures and languages. Comprising 268 real-world software package build failures, the benchmark provides a standardized evaluation pipeline. We evaluate six state-of-the-art LLMs on the benchmark, and the results show that cross-ISA software package repair remains difficult and requires further advances. By systematically exposing this challenge, the benchmark establishes a foundation for advancing future methods aimed at improving software portability and bridging architectural gaps.

</details>


### [222] [Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models](https://arxiv.org/abs/2601.12951)
*Felix Mächtle,Jan-Niclas Serr,Nils Loose,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: 本文提出新诊断框架，发现大型语言模型代码理解与传统软件复杂度指标弱相关，存在模型特有的理解规律，呼吁细粒度诊断指标提升评测方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准只提供粗略性能总结，掩盖了大语言模型不同的能力与局限，需探究其代码理解能力是否符合传统人类软件复杂度指标或展现不同规律。

Method: 提出一种将代码理解转化为二元输入输出一致性任务的诊断框架，结合大规模数据集，分析模型表现与传统复杂度指标的相关性，并与影子模型预测效果对比。

Result: 人类定义的复杂度指标与大型语言模型代码理解表现的相关性较弱（AUROC 0.63），影子模型能捕捉更复杂且可预测的模式（AUROC 0.86），表明理解表现含有超越传统软件措施的模型特定规律。

Conclusion: 大型语言模型对代码理解的表现仅部分与传统人类软件复杂度指标相关，存在专属于模型的理解规律，传统指标难以充分预测其成功。

Abstract: Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.

</details>


### [223] [ArchAgent: Scalable Legacy Software Architecture Recovery with LLMs](https://arxiv.org/abs/2601.13007)
*Rusheng Pan,Bingcheng Mao,Tianyi Ma,Zhenhua Ling*

Main category: cs.SE

TL;DR: 提出ArchAgent框架，结合静态分析和大语言模型，实现跨代码库的多视图业务对齐架构恢复，显著提升遗留软件架构重构准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统架构恢复受架构漂移、缺失关系及LLM上下文限制影响，难以从大型遗留代码库中准确恢复架构。

Method: 采用静态分析、适应性代码分割、以及基于大语言模型的代码综合方法，结合跨代码库数据及上下文相关裁剪生成多视图业务对齐架构图。

Result: 在多个大型GitHub项目评测中表现优于现有基准，消融实验验证上下文依赖提升了架构生成准确度，实际案例证明能有效恢复关键业务逻辑。

Conclusion: ArchAgent框架有效解决了大规模遗留软件架构恢复中的关键问题，提升了架构重构的准确性和业务相关性。

Abstract: Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). We present ArchAgent, a scalable agent-based framework that combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases. ArchAgent introduces scalable diagram generation with contextual pruning and integrates cross-repository data to identify business-critical modules. Evaluations of typical large-scale GitHub projects show significant improvements over existing benchmarks. An ablation study confirms that dependency context improves the accuracy of generated architectures of production-level repositories, and a real-world case study demonstrates effective recovery of critical business logics from legacy projects. The dataset is available at https://github.com/panrusheng/arch-eval-benchmark.

</details>


### [224] [MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code Generation](https://arxiv.org/abs/2601.13015)
*Nowfel Mashnoor,Mohammad Akyash,Hadi Kamali,Kimia Azar*

Main category: cs.SE

TL;DR: MeltRTL利用多专家注意力和推理时干预机制，在不微调模型的情况下，大幅提升了RTL代码的合成和功能正确率，效率高，易部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的RTL代码生成难以同时保证语法和功能的正确性，尤其面对复杂数字设计时表现不佳。

Method: 提出了多专家注意力架构和推理时干预（ITI）机制，动态分配设计规范至专门专家网络，并使用非线性探测器在生成过程中纠正硬件特定错误，且仅对专家注意力头进行低开销干预。

Result: 在VerilogEval基准测试中，MeltRTL达到96%可综合率和60%功能正确率，优于原始模型的85.3%和45.3%，且推理时开销仅增加27%。

Conclusion: MeltRTL框架通过多专家注意力机制和推理时干预显著提升了基于大语言模型的RTL代码生成的准确性，实现了高合成率和功能正确率，并且不需模型微调即可部署。

Abstract: The automated generation of hardware register-transfer level (RTL) code with large language models (LLMs) shows promise, yet current solutions struggle to produce syntactically and functionally correct code for complex digital designs. This paper introduces MeltRTL, a novel framework that integrates multi-expert attention with inference-time intervention (ITI) to significantly improve LLM-based RTL code generation accuracy without retraining the base model. MeltRTL introduces three key innovations: (1) A multi-expert attention architecture that dynamically routes design specifications to specialized expert networks, enabling targeted reasoning across various hardware categories; (2) An inference-time intervention mechanism that employs non-linear probes to detect and correct hardware-specific inaccuracies during generation; and (3) An efficient intervention framework that selectively operates on expert-specific attention heads with minimal computational overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96% synthesizability and 60% functional correctness, compared to the base LLM's 85.3% and 45.3%, respectively. These improvements are obtained entirely at inference time, with only 27% computational overhead and no model fine-tuning, making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation studies further show the complementary benefits of multi-expert architecture and ITI, highlighting their synergistic effects when combined.

</details>


### [225] [RM -RF: Reward Model for Run-Free Unit Test Evaluation](https://arxiv.org/abs/2601.13097)
*Elena Bruches,Daniil Grebenkin,Mikhail Klementev,Vadim Alperovich,Roman Derunets,Dari Baturova,Georgy Mkrtchyan,Oleg Sedukhin,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: RM-RF提出了一种无需运行测试即可评估自动生成单元测试的模型，显著降低评估成本并保持较好准确率，适合大规模测试自动化。


<details>
  <summary>Details</summary>
Motivation: 传统自动生成测试的评估需要反复编译和执行测试用例，存在高延迟和高基础设施成本的问题，亟需一种无需运行即可快速评估的方法。

Method: 通过收集多语言（Java、Python、Go）文件和测试代码，构建基于执行结果标签的数据集，训练模型预测三种基于执行的信号，测试多种模型和调优方法，达到较高的F1分数。

Result: RM-RF在三项指标上平均F1达到0.69，显著降低了延迟和基础设施开销，同时保持了与传统方法相当的预测精度，适合大规模测试生成和基于强化学习的代码优化。

Conclusion: RM-RF是一种轻量级奖励模型，能够无需运行测试即可有效评估自动生成的单元测试，预测测试代码的编译运行状态、代码覆盖率提升和变异杀死率提升，表现出较好的预测准确性和较低的延迟及资源消耗。

Abstract: We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.

</details>


### [226] [Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization](https://arxiv.org/abs/2601.13118)
*Alessandro Midolo,Alessandro Giagnorio,Fiorella Zampetti,Rosalia Tufano,Gabriele Bavota,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 本研究通过自动化和测试驱动方法，归纳出10条针对代码生成提示词的优化指导原则，并通过开发者实证验证其有效性，为提升面向软件开发的语言模型提示词设计提供了实用参考。


<details>
  <summary>Details</summary>
Motivation: 尽管提示词工程已被证明能提升代码生成效果，但缺乏针对代码生成的具体提示词编写指导，导致开发者难以高效编写合适的提示词。

Method: 采用迭代、测试驱动的方法自动优化代码生成提示词，通过分析测试结果提炼出改进提示词的关键要素，最终总结成具体指导原则，并通过50名开发者的实证评估。

Result: 提炼出10条与输入输出、前后置条件、示例提供、细节丰富及歧义澄清相关的提示词优化指导，与实际使用行为进行比对，发现开发者在了解这些指导前后使用和感知存在差异。

Conclusion: 本研究提出了10条针对代码生成提示词优化的具体指导原则，验证了这些指导原则在实际开发中的有效性和实用性。

Abstract: Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.

</details>


### [227] [Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access](https://arxiv.org/abs/2601.13134)
*Heng Fang,Adam J. Stewart,Isaac Corley,Xiao Xiang Zhu,Hossein Azizpour*

Main category: cs.SE

TL;DR: 针对地理空间基础模型的高计算成本和嵌入产品碎片化问题，本文提出统一分类与接口标准，扩展工具实现了数据互操作性，提升应用便利性和重现性。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型的高计算成本限制了其广泛应用，嵌入式数据产品格式和分辨率不统一，导致工程瓶颈和模型比较困难。

Method: 我们提出一个包含数据、工具、价值三层的分类体系，并通过扩展TorchGeo，设计统一API实现对不同嵌入产品的标准化加载与查询。

Result: 实现了将嵌入向量视为空间数据集，解耦下游分析与模型工程，推动了地理空间嵌入数据的互操作性和重现性。

Conclusion: 本研究通过统一数据格式和接口，打破碎片化生态，提升了地理空间基础模型的应用效率和透明度，为地球观测工作流提供了优化方案。

Abstract: Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical "frozen" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.

</details>


### [228] [From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability](https://arxiv.org/abs/2601.13139)
*Alessandro Midolo,Emiliano Tramontana,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 本文通过实证研究评估了GPT-4o驱动的Python类重构，发现其在保证行为不变前提下提升了代码质量，但牺牲了部分可读性，揭示了大型语言模型在自动化重构中的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化重构工具已有广泛研究，但实用性仍有限；大型语言模型在代码重构中展现新机遇，但其对代码质量的影响尚未充分评估。

Method: 基于GPT-4o对ClassEval基准测试中的100个Python类进行多种类级重构，结合Fowler重构目录进行，使用单元测试验证行为正确性，采用Pylint、Flake8和SonarCloud评估代码质量，利用先进的可读性工具测量可读性。

Result: GPT-4o实现了行为保留的重构，显著降低代码异味、提升代码质量指标，但带来了代码可读性下降的副作用。

Conclusion: GPT-4o驱动的重构通常能够保持程序行为不变，同时减少代码异味并提升代码质量指标，但可读性有所下降。

Abstract: Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.

</details>


### [229] [KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?](https://arxiv.org/abs/2601.13240)
*Xue Jiang,Jiaru Qian,Xianjie Shi,Chenjie Li,Hao Zhu,Ziyu Wang,Jielun Zhang,Zheyu Zhao,Kechi Zhang,Jia Li,Wenpin Jiao,Zhi Jin,Ge Li,Yihong Dong*

Main category: cs.SE

TL;DR: KOCO-BENCH是一个针对领域专用代码开发的全新评测基准，包含丰富知识库和多层次任务，现有大型语言模型及其领域专用方法在该基准上表现有限，推动领域专用方法研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编程表现优异，但在领域专用软件开发中效果有限，且现有基准仅评估知识拥有情况，缺乏评估领域专用方法如何获取并应用新知识的工具。

Method: 构建了KOCO-BENCH基准，包含6个新兴领域，11个软件框架，25个项目，附带知识语料库和多粒度评测任务（代码生成和知识理解），通过必须获取和应用领域知识来完成评测任务。

Result: 在KOCO-BENCH评测中，当前最先进的领域专用方法提升有限，最佳模型Claude Code仅达到34.2%准确率，表明KOCO-BENCH具有挑战性。

Conclusion: 现有领域专用代码基准无法有效评估领域专用方法，KOCO-BENCH填补了这一空白，目前主流大型语言模型和领域专用方法在该基准上表现仍不理想，亟需更有效的领域专用技术。

Abstract: Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.

</details>


### [230] [SEER: Spectral Entropy Encoding of Roles for Context-Aware Attention-Based Design Pattern Detection](https://arxiv.org/abs/2601.13334)
*Tarik Houichime,Younes El Amrani*

Main category: cs.SE

TL;DR: SEER通过引入角色编码和时间加权机制改进了GoF设计模式检测，提升了准确率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 之前的方法缺乏对类中角色的明确区分，并且对调用边一视同仁，导致模型无法精准捕捉成员的具体职责和调用时长的重要性，限制了检测能力。

Method: SEER结合了两大创新模块：利用拉普拉斯谱对类交互图中的成员角色进行编码的谱熵角色编码器，以及针对方法类别分配时间权重的调用上下文机制，提升Transformer序列编码器中对角色和时间信息的敏感度。

Result: 在PyDesignNet数据集上，SEER将宏F1值提升至93.20%，准确率达93.98%，且假阳性率减少近20%，表现出更强的鲁棒性和解释性。

Conclusion: SEER通过引入谱熵角色编码器和时间加权调用上下文，有效提升了GoF设计模式检测的准确性和鲁棒性，相较于之前的方法表现出显著的性能改进和实用性增强。

Abstract: This paper presents SEER, an upgraded version of our prior method Context Is All You Need for detecting Gang of Four (GoF) design patterns from source code. The earlier approach modeled code as attention-ready sequences that blended lightweight structure with behavioral context; however, it lacked explicit role disambiguation within classes and treated call edges uniformly. SEER addresses these limitations with two principled additions: (i) a spectral-entropy role encoder that derives per-member role embeddings from the Laplacian spectrum of each class's interaction graph, and (ii) a time-weighted calling context that assigns empirically calibrated duration priors to method categories (e.g., constructors, getters/setters, static calls, virtual dispatch, cloning). Together, these components sharpen the model's notion of "who does what" and "how much it matters," while remaining portable across languages with minimal adaptation and fully compatible with Transformer-based sequence encoders. Importantly, SEER does not "force" a win by capacity or data; it nudges the classifier, steering attention toward role-consistent and temporally calibrated signals that matter most. We evaluate SEER on PyDesignNet (1,832 files, 35,000 sequences, 23 GoF patterns) and observe consistent gains over our previous system: macro-F1 increases from 92.47% to 93.20% and accuracy from 92.52% to 93.98%, with macro-precision 93.98% and macro-recall 92.52%. Beyond aggregate metrics, SEER reduces false positives by nearly 20%, a decisive improvement that strengthens its robustness and practical reliability. Moreover, SEER yields interpretable, symbol-level attributions aligned with canonical roles, exhibits robustness under small graph perturbations, and shows stable calibration.

</details>


### [231] [FlipFlop: A Static Analysis-based Energy Optimization Framework for GPU Kernels](https://arxiv.org/abs/2601.13345)
*Saurabhsingh Rajput,Alexander Brandt,Vadim Elisseev,Tushar Sharma*

Main category: cs.SE

TL;DR: FlipFlop利用静态代码分析预测GPU程序能耗，推荐最优线程配置，实现显著节能和性能提升，优化开发效率，推动绿色高效GPU计算。


<details>
  <summary>Details</summary>
Motivation: GPU程序消耗大量能源，但软件开发者缺乏硬件专业知识来优化能耗。

Method: 提出FlipFlop框架，利用静态代码分析PTX代码，预测能耗并推荐兼顾功耗和执行时间的帕累托最优线程块配置。

Result: FlipFlop在多种GPU和内核上验证，识别能效最优配置准确率达83%，减少93.4%优化搜索空间，多头注意力内核节能达79%，吞吐量提升106%。

Conclusion: FlipFlop通过静态分析结合实时监控，为开发者提供可解释的优化指导，实现高性能低能耗GPU软件开发，降低环境与计算成本。

Abstract: Artificial Intelligence (AI) applications, such as Large Language Models, are primarily driven and executed by Graphics Processing Units (GPUs). These GPU programs (kernels) consume substantial amounts of energy, yet software developers often lack the hardware expertise and ad hoc knowledge required to optimize for power efficiency. We propose FlipFlop, a framework using static code analysis to predict energy consumption and recommend Pareto-optimal thread block configurations considering both power consumption and execution time. Our framework requires no runtime execution and analyzes PTX code, a low-level instruction set for CUDA-enabled GPUs. It is validated across a diverse set of GPUs and kernels, including multi-head attention, convolution, and matrix multiplication. FlipFlop achieves 83% accuracy in identifying locally optimal energy-efficient configurations, while also minimizing developer effort by reducing the optimization search space by 93.4%. For multi-head attention kernels, it yields up to 79% energy savings and 106% throughput gains relative to NVIDIA's occupancy heuristic. By integrating static analysis with real-time monitoring and providing explainable optimization guidance, FlipFlop empowers developers to create sustainable, high-performance GPU software which minimizes environmental and computational costs.

</details>


### [232] [From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning](https://arxiv.org/abs/2601.13384)
*Jiajun Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Yuheng Jing,Zeyao Ma,Tianyi Bai,Zilei Wang,Qiang Liu,Liang Wang,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 本文提出SRI框架，通过动态编辑和上下文感知提升代码补全性能，兼顾安全性与推理效率，推动自动补全技术进步。


<details>
  <summary>Details</summary>
Motivation: 现有的代码补全方法存在无法纠正上下文错误和依赖不安全基础模型的问题，且Chat大模型虽然安全但性能和延迟存在缺陷。

Method: 提出了Search-and-Replace Infilling（SRI）框架，通过显式搜索阶段实现验证和编辑，结合Chat大模型的指令跟随能力，在单次推理中完成动态的上下文感知编辑。

Result: 利用合成的高质量数据集SRI-200K微调SRI-Coder系列，少量数据即可让Chat模型的补全性能超过基础模型，且保持较低推理延迟和良好的编码能力。

Conclusion: SRI框架有效解决了传统FIM方法的局限，提升了代码补全的性能和安全性，同时保证推理效率，推动了Qwen3-Coder系列的自动补全技术发展。

Abstract: The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.

</details>


### [233] [A Tool for Automatically Cataloguing and Selecting Pre-Trained Models and Datasets for Software Engineering](https://arxiv.org/abs/2601.13460)
*Alexandra González,Oscar Cerezo,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 本论文介绍了MLAssetSelection，一款面向软件工程的机器学习资产选择工具，简化了模型和数据集的挑选过程，提高了使用效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 机器学习资产数量迅速增长，现有大型资源库浏览耗时且不够针对软件工程任务，急需一种高效、精准的资产选择工具。

Method: 通过构建一个网页应用，实现了自动提取软件工程相关资产，提供多基准和指标的排行榜、基于需求的模型和数据集选择、实时自动更新以及用户定制功能。

Result: 成功开发了MLAssetSelection，实现了便捷的机器学习模型和数据集筛选及管理，支持实时更新和个性化服务。

Conclusion: MLAssetSelection提供了一个针对软件工程需求设计的机器学习资产选择工具，显著提升了模型和数据集的筛选效率和准确性。

Abstract: The rapid growth of machine learning assets has made it increasingly difficult for software engineers to identify models and datasets that match their specific needs. Browsing large registries, such as Hugging Face, is time-consuming, error-prone, and rarely tailored to Software Engineering (SE) tasks. We present MLAssetSelection, a web application that automatically extracts SE assets and supports four key functionalities: (i) a configurable leaderboard for ranking models across multiple benchmarks and metrics; (ii) requirements-based selection of models and datasets; (iii) real-time automated updates through scheduled jobs that keep asset information current; and (iv) user-centric features including login, personalized asset lists, and configurable alert notifications. A demonstration video is available at https://youtu.be/t6CJ6P9asV4.

</details>


### [234] [Governance Matters: Lessons from Restructuring the data.table OSS Project](https://arxiv.org/abs/2601.13466)
*Pedro Oliveira,Doris Amoakohene,Toby Hocking,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: 这篇论文通过对R包data.table的社区治理改革案例研究，展示了改进治理结构能大幅提升开源项目的贡献者招募、问题处理速度和社区氛围。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目面临运营风险，尤其是由于治理结构非正式或过于集中，影响项目的可扩展性和可持续性。

Method: 采用混合方法研究，包括贡献者调查（n=17）和项目代码库数据挖掘，评估社区治理结构改革的影响。

Result: 改革后新贡献者增长200%，拉取请求解决时间从700多天缩短到不到一周，贡献者保留率提升3倍，社区透明度和项目活力显著提升，但公平性和冲突解决仍有待改进。

Conclusion: 通过社区主导的治理改革，开源项目可显著提升贡献者参与度和运营效率，从而增强项目的可持续性和规模化能力。

Abstract: Open source software (OSS) forms the backbone of industrial data workflows and enterprise systems. However, many OSS projects face operational risks due to informal or centralized governance. This paper presents a practical case study of data.table, a high-performance R package widely adopted in production analytics pipelines, which underwent a community-led governance reform to address scalability and sustainability concerns. Before the reform, data.table faced a growing backlog of unresolved issues and open pull requests, unclear contributor pathways, and bottlenecks caused by reliance on a single core maintainer. In response, the community initiated a redesign of its governance structure. In this paper, we evaluated the impact of this transition through a mixed-methods approach, combining a contributor survey (n=17) with mining project repository data. Our results show that following the reform, the project experienced a 200% increase in new contributor recruitment, a drop in pull request resolution time from over 700 days to under a week, and a 3x increase in contributor retention. Community sentiment improved around transparency, onboarding, and project momentum, though concerns around fairness and conflict resolution remain. This case study provides practical guidance for maintainers, companies, and foundations seeking to enhance OSS governance.

</details>


### [235] [AI IDEs or Autonomous Agents? Measuring the Impact of Coding Agents on Software Development](https://arxiv.org/abs/2601.13597)
*Shyam Agarwal,Hao He,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 研究显示大型语言模型编码代理能短期提升开发速度，但软件质量问题长期存在，强调需谨慎使用并加强质量控制。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型编码代理在现实软件项目中对开发速度和软件质量的实际影响，尤其相对于传统IDE集成的AI助手。

Method: 采用分层差分中的匹配控制组方法，对开源代码库采用AIDev数据集，定义采用为首次代理生成的pull request，进行长期的因果影响分析。

Result: 发现代理首次引入时，开发速度显著提升，但若项目已有AI IDE使用，速度提升有限且短暂。质量风险普遍存在，静态分析警告和认知复杂度分别提升约18%和35%，表明复杂度债务持续增加。

Conclusion: AI编码代理带来初期开发速度提升，但伴随持续的软件质量风险，说明AI辅助效益递减，需结合质量保障、溯源追踪和选择性部署策略，以平衡开发加速与维护性。

Abstract: Large language model (LLM)-based coding agents increasingly act as autonomous contributors that generate and merge pull requests, yet their real-world effects on software projects are unclear, especially relative to widely adopted IDE-based AI assistants. We present a longitudinal causal study of agent adoption in open-source repositories using staggered difference-in-differences with matched controls. Using the AIDev dataset, we define adoption as the first agent-generated pull request and analyze monthly repository-level outcomes spanning development velocity (commits, lines added) and software quality (static-analysis warnings, cognitive complexity, duplication, and comment density). Results show large, front-loaded velocity gains only when agents are the first observable AI tool in a project; repositories with prior AI IDE usage experience minimal or short-lived throughput benefits. In contrast, quality risks are persistent across settings, with static-analysis warnings and cognitive complexity rising roughly 18% and 35%, indicating sustained agent-induced complexity debt even when velocity advantages fade. These heterogeneous effects suggest diminishing returns to AI assistance and highlight the need for quality safeguards, provenance tracking, and selective deployment of autonomous agents. Our findings establish an empirical basis for understanding how agentic and IDE-based tools interact, and motivate research on balancing acceleration with maintainability in AI-integrated development workflows.

</details>


### [236] [Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs](https://arxiv.org/abs/2601.13655)
*Guangba Yu,Zirui Wang,Yujie Huang,Renyi Zhong,Yuedong Zhong,Yilun Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 首次大规模分析开源LLM部署失败，揭示其可靠性瓶颈由模型转向部署系统，提出关键现象并提供改善方向。


<details>
  <summary>Details</summary>
Motivation: 开源LLM允许用户在本地基础设施上部署，但缺乏对用户管理的部署可靠性的深入研究。

Method: 对705个开源LLM（DeepSeek、Llama和Qwen）实际失败案例进行大规模实证分析。

Result: 发现三大现象：诊断分歧（运行时崩溃表征基础设施问题，功能错误指向分词器缺陷）、系统同质性（多模型共享生态系统的共性问题）、生命周期升级（部署难题从微调配置延伸到推理环境不兼容）。数据集公开，指导提升LLM部署可靠性。

Conclusion: 用户自主部署的开源大型语言模型的可靠性问题从模型算法缺陷转向部署系统的脆弱性。

Abstract: The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.
  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.

</details>


### [237] [CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation](https://arxiv.org/abs/2601.13682)
*Jianfeng Cai,Jinhua Zhu,Ruopei Sun,Kangwen Zhao,Dongyun Xue,Mingxiao Feng,Wengang Zhou,Houqiang Li*

Main category: cs.SE

TL;DR: 本文提出一种基于反馈驱动的迭代生成框架，用于构建高质量编程竞赛测试用例，显著提升验证准确率和模型性能，公开了数据集和代码以促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有大规模可验证数据不足，尤其是优质、具有鉴别能力的编程竞赛测试用例稀缺，而仅依赖LLM本身生成能力难以获得足够多样和精准的用例。

Method: 基于大型语言模型生成初始测试用例，通过执行这些用例在正确和错误解法上的结果反馈，迭代优化测试用例，使其在区分能力和可信度上得到提升。

Result: 生成了优化后的高质量测试用例数据集CodeContests-O，在海量解法评测中达到接近90%的准确率，优于原数据集4-9个百分点，且微调模型在真实测试集上表现提升近10%。

Conclusion: 我们提出的反馈驱动迭代框架显著提升了编程竞赛测试用例的质量和多样性，性能远超现有数据集，并有效提升了模型的执行效果。

Abstract: The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\%$ and True Negative Rate (TNR) of $90.89\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\%$ and $9.37\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.

</details>


### [238] [SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories](https://arxiv.org/abs/2601.13713)
*Aditya Bharat Soni,Rajat Ghosh,Vaishnavi Bhargava,Valerie Chen,Debojyoti Dutta*

Main category: cs.SE

TL;DR: 本文提出SWE-Tester流水线，利用大规模开源数据训练开源大型语言模型，实现自动生成问题复现测试，显著提升性能，促进测试驱动开发和自动化问题修复。


<details>
  <summary>Details</summary>
Motivation: 现有的自动生成问题复现测试的方法主要依赖闭源大型语言模型，缺乏对开源模型的研究。为探索开源模型在此任务上的表现，提出新的训练和评估方法。

Method: 通过从2600个开源GitHub仓库中收集41000个高质量训练样本，训练不同规模和类型的开源大型语言模型，构建SWE-Tester流水线，并在SWT-Bench Verified数据集上进行评估。

Result: 微调后的模型在SWT-Bench Verified数据集上取得了最高10%的成功率提升和21%的变更覆盖率提升。分析还显示增加推理计算量、训练数据和模型规模均带来稳定提升。

Conclusion: SWE-Tester框架有效提升了开源大型语言模型在生成问题复现测试方面的性能，验证了开源模型在该领域的潜力。

Abstract: Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- "test first, write code later", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.

</details>


### [239] [Counterexample Classification against Signal Temporal Logic Specifications](https://arxiv.org/abs/2601.13743)
*Zhenya Zhang,Parv Kapoor,Jie An,Eunsuk Kang*

Main category: cs.SE

TL;DR: 本文提出了一种基于PSTL的反例分类方法，通过参数搜索和类包含关系优化提升分类效率，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: STL反例可能由不同原因引起，关联不同的系统缺陷；对反例进行合理分类能帮助理解违反模式及其分布，从而更有效地定位和修正系统缺陷。

Method: 通过利用PSTL表达每个反例类别，寻找合适的参数值来分类反例；进一步通过推导类间包含关系，设计了类似二分搜索的策略减少查询的类别数，提升分类效率。

Result: 开发了一个原型工具，并在两个广泛研究的系统上进行了实验，验证了该方法在反例分类和效率优化上的有效性。

Conclusion: 本文提出了一种基于参数化信号时序逻辑(PSTL)的反例分类方法，能够有效识别和分类混合系统中违反STL规范的反例，提升了反例分析的精确度和效率。

Abstract: Signal Temporal Logic (STL) has been widely adopted as a specification language for specifying desirable behaviors of hybrid systems. By monitoring a given STL specification, we can detect the executions that violate it, which are often referred to as counterexamples. In practice, these counterexamples may arise from different causes and thus are relevant to different system defects. To effectively address this, we need a proper criterion for classifying these counterexamples, by which we can comprehend the possible violation patterns and the distributions of these counterexamples with respect to the patterns. In this paper, we propose a classification criterion by using parametric signal temporal logic (PSTL) to represent each class. Due to this formalism, identifying the classes of a counterexample requires finding proper parameter values of PSTL that enable a class to include the counterexample. To improve the efficiency of class identification, we further derive an inclusion relation between different classes, and then propose a binary search-like approach over it that significantly prunes the classes needed to query. We implement a prototype tool and experimentally evaluate its effectiveness on two widely-studied systems.

</details>


### [240] [On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source](https://arxiv.org/abs/2601.13754)
*Haoyu Gao,Peerachai Banyongrakkul,Hao Guan,Mansooreh Zahedi,Christoph Treude*

Main category: cs.SE

TL;DR: 本研究揭示AI协作拉取请求主要由无代码所有权的贡献者发起，缺乏指导且合并速度快反馈少，展现了与传统人类PR不同的开发者交互模式。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在软件工程任务中的应用日益增多，但对于开源软件中‘AI作为队友’的开发者交互模式研究不足，因此探讨AI辅助PR的贡献者特点和交互情况。

Method: 通过扩展AIDev数据集，加入贡献者代码所有权的细粒度信息，并与人类创建的PR进行比较，分析项目层面的指南及开发者对AI辅助PR的互动模式。

Result: 发现超过67.5%的AI协作PR来自无代码所有权的贡献者，且80%的此类PR在没有明确评审的情况下被合并。此外，大多数仓库缺乏针对AI编码代理的使用指南。

Conclusion: AI协助生成的拉取请求（PR）来自大多数没有先前代码所有权的贡献者，且通常在缺乏明确指南的仓库中快速合并且反馈较少，显示出与传统人类PR不同的交互模式。

Abstract: Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\% merged without any explicit review. Finally, we discuss implications for developers and researchers.

</details>


### [241] [A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems](https://arxiv.org/abs/2601.13772)
*Matteo Vaccargiu,Azmat Ullah,Pierluigi Gallo*

Main category: cs.SE

TL;DR: 本文设计并验证了一种结合IoT与区块链的光伏碳信用认证系统，解决了中小型设施认证难题，符合欧洲标准，提升碳信用认证的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有碳信用认证机制对中小型可再生能源设施支持有限，且缺乏符合实际法律法规要求的认证流程。

Method: 通过结合实时物联网数据收集、边缘级数据聚合以及基于许可链的智能合约安全链上存储，实现对100 kWp光伏系统的碳信用认证。

Result: 成功构建了一个结合IoT和区块链技术的碳信用认证系统，为光伏运营商提供清晰的认证路径和支持第三方验证的结构。

Conclusion: 本文提出的基于区块链的碳信用认证架构有效支持了中小规模光伏发电的碳减排认证，符合欧洲法规和自愿碳市场标准，保障了数据的真实性和可验证性。

Abstract: Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.

</details>


### [242] [Multi-Location Software Model Completion](https://arxiv.org/abs/2601.13894)
*Alisa Welter,Christof Tinnes,Sven Apel*

Main category: cs.SE

TL;DR: 本文提出NextFocus，通过神经网络实现多位置软件模型自动完成，解决了现有方法只能单点预测的局限，并在真实项目中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统AI辅助的软件模型自动完成只支持单位置预测，无法满足实际软件模型多位置协调修改需求。

Method: 提出了一种基于全局嵌入和注意力机制的神经网络NextFocus，利用历史软件模型演化数据进行训练，能够从一次已有修改出发预测后续相关多个模型元素的修改。

Result: 在真实项目的多位置模型修改任务中，NextFocus在Precision@k指标上平均达到0.98，明显优于三个基线方法。

Conclusion: NextFocus 能有效预测多个位置的模型修改，显著优于现有单点修改预测方法。

Abstract: In model-driven engineering and beyond, software models are key development artifacts. In practice, they often grow to substantial size and complexity, undergoing thousands of modifications over time due to evolution, refactoring, and maintenance. The rise of AI has sparked interest in how software modeling activities can be automated. Recently, LLM-based approaches for software model completion have been proposed, however, the state of the art supports only single-location model completion by predicting changes at a specific location. Going beyond, we aim to bridge the gap toward handling coordinated changes that span multiple locations across large, complex models. Specifically, we propose a novel global embedding-based next focus predictor, NextFocus, which is capable of multi-location model completion for the first time. The predictor consists of a neural network with an attention mechanism that is trained on historical software model evolution data. Starting from an existing change, it predicts further model elements to change, potentially spanning multiple parts of the model. We evaluate our approach on multi-location model changes that have actually been performed by developers in real-world projects. NextFocus achieves promising results for multi-location model completion, even when changes are heavily spread across the model. It achieves an average Precision@k score of 0.98 for $k \leq 10$, significantly outperforming the three baseline approaches.

</details>


### [243] [VulnResolver: A Hybrid Agent Framework for LLM-Based Automated Vulnerability Issue Resolution](https://arxiv.org/abs/2601.13933)
*Mingming Zhang,Xu Wang,Jian Zhang,Xiangxin Meng,Jiayi Zhang,Chunming Hu*

Main category: cs.SE

TL;DR: 本文提出的 VulnResolver 框架通过两个专门智能体结合工作流引导，实现了基于 LLM 的高效自动漏洞修复，在公开基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动漏洞修复方法依赖手动注解，获取困难且耗时，且未充分利用开发者问题报告中丰富的语义上下文信息。

Method: VulnResolver 采用两个专门的智能体：上下文预收集智能体（CPCAgent）用于自适应地收集依赖和上下文信息，安全属性分析智能体（SPAAgent）用于生成及验证被漏洞破坏的安全属性，结合工作流引导的修复方式。

Result: 在 SEC-bench 评测中，VulnResolver 在 SEC-bench Lite 上解决率达到75%，表现优于现有最强基线 OpenHands，在 SEC-bench Full 上也有显著提升。

Conclusion: VulnResolver 提供了一个基于大型语言模型（LLM）的混合智能体框架，能够显著提升自动化漏洞问题解决的性能，实现更准确的漏洞定位和修复。

Abstract: As software systems grow in complexity, security vulnerabilities have become increasingly prevalent, posing serious risks and economic costs. Although automated detection tools such as fuzzers have advanced considerably, effective resolution still often depends on human expertise. Existing automated vulnerability repair (AVR) methods rely heavily on manually provided annotations (e.g., fault locations or CWE labels), which are often difficult and time-consuming to obtain, while overlooking the rich, naturally embedded semantic context found in issue reports from developers.
  In this paper, we present VulnResolver, the first LLM-based hybrid agent framework for automated vulnerability issue resolution. VulnResolver unites the adaptability of autonomous agents with the stability of workflow-guided repair through two specialized agents. The Context Pre-Collection Agent (CPCAgent) adaptively explores the repository to gather dependency and contextual information, while the Safety Property Analysis Agent (SPAAgent) generates and validates the safety properties violated by vulnerabilities. Together, these agents produce structured analyses that enrich the original issue reports, enabling more accurate vulnerability localization and patch generation.
  Evaluations on the SEC-bench benchmark show that VulnResolver resolves 75% of issues on SEC-bench Lite, achieving the best resolution performance. On SEC-bench Full, VulnResolver also significantly outperforms the strongest baseline, the agent-based OpenHands, confirming its effectiveness. Overall, VulnResolver delivers an adaptive and security-aware framework that advances end-to-end automated vulnerability issue resolution through workflow stability and the specialized agents' capabilities in contextual reasoning and property-based analysis.

</details>


### [244] [RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository](https://arxiv.org/abs/2601.13943)
*Zhiyuan Peng,Xin Yin,Pu Zhao,Fangkai Yang,Lu Wang,Ran Jia,Xu Chen,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: 提出多语言微服务生成基准RepoGenesis，揭示当前代码生成模型的不足，推动微服务仓库端到端生成研究。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准多聚焦于函数级或代码修改，缺少真实微服务仓库的端到端生成评测，无法反映实际0到1开发流程。

Method: 提出全新的多语言微服务仓库生成基准RepoGenesis，包含多种语言和领域的真实仓库，采用“审核-反驳”质量保证流程，评测多种开源和商业生成系统的性能。

Result: RepoGenesis包含106个仓库，覆盖多语言多框架，评测发现当前最佳生成系统在准确性方面仍有不足，表明需要改进架构一致性和依赖管理。GenesisAgent-8B在该基准上微调后性能接近GPT-5 mini，证明基准数据质量。

Conclusion: RepoGenesis基准测试揭示了当前大语言模型在微服务仓库生成中的局限性，尽管API覆盖率和部署成功率较高，但整体代码质量和架构一致性仍需提升。

Abstract: Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a "review-rebuttal" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.

</details>


### [245] [Software Testing in the Quantum World](https://arxiv.org/abs/2601.13996)
*Rui Abreu,Shaukat Ali,Paolo Arcaini,Jose Campos,Michael Felderer,Claude Gravel,Fuyuki Ishikawa,Stefan Klikovits,Andriy Miranskyy,Mohammad Mousavi,Masaomi Yamaguchi,Lei Zhang,Jianjun Zhao,Anila Mjeda*

Main category: cs.SE

TL;DR: 传统经典模拟难以满足复杂量子软件测试需求，本文从软件工程视角探讨了在真实量子计算机上测试量子软件的关键挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件复杂性的增加，传统的量子计算机经典模拟变得不可行，这需要直接在真实量子计算机上进行质量保证的新方法。

Method: 分析大规模量子软件测试中的关键挑战，并从软件工程的角度提出应对策略。

Result: 提出了一套基于软件工程视角，针对大规模量子软件测试的解决方案。

Conclusion: 直接在真实量子计算机上进行测试是未来量子软件质量保证的必要方向，软件工程方法可有效解决测试中面临的挑战。

Abstract: Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.

</details>


### [246] [Analyzing the Availability of E-Mail Addresses for PyPI Libraries](https://arxiv.org/abs/2601.14034)
*Alexandros Tsakpinis,Alexander Pretschner*

Main category: cs.SE

TL;DR: 本文分析了Python开源库维护者的电子邮件联系信息，发现大部分维护者都可被联系，但仍需改进信息的准确性和验证机制。


<details>
  <summary>Details</summary>
Motivation: 维护者的可达性对于开源软件的支持、协调和安全报告至关重要，因此需要评估维护者联系信息的可用性。

Method: 通过对686,034个Python库及其GitHub仓库中电子邮件地址的有效性和覆盖情况进行实证分析。

Result: 81.6%的库至少包含一个有效的电子邮件地址，依赖链中直接和传递依赖的有效联系方式比例分别达到97.8%和97.7%，但存在大量无效条目。

Conclusion: 开源软件库的维护者大多数可通过有效的电子邮件地址被联系到，这保证了软件系统的长期可持续性。

Abstract: Open Source Software (OSS) libraries form the backbone of modern software systems, yet their long-term sustainability often depends on maintainers being reachable for support, coordination, and security reporting. In this paper, we empirically analyze the availability of contact information - specifically e-mail addresses - across 686,034 Python libraries on the Python Package Index (PyPI) and their associated GitHub repositories. We examine how and where maintainers provide this information, assess its validity, and explore coverage across individual libraries and their dependency chains. Our findings show that 81.6% of libraries include at least one valid e-mail address, with PyPI serving as the primary source (79.5%). When analyzing dependency chains, we observe that up to 97.8% of direct and 97.7% of transitive dependencies provide valid contact information. At the same time, we identify over 698,000 invalid entries, primarily due to missing fields. These results demonstrate strong maintainer reachability across the ecosystem, while highlighting opportunities for improvement - such as offering clearer guidance to maintainers during the packaging process and introducing opt-in validation mechanisms for existing e-mail addresses.

</details>


### [247] [Feature-Aware Test Generation for Deep Learning Models](https://arxiv.org/abs/2601.14081)
*Xingcheng Chen,Oliver Weissl,Andrea Stocco*

Main category: cs.SE

TL;DR: 本文提出了Detect，一种通过干扰潜在空间中可控语义特征生成视觉深度学习模型测试用例的方法，实现了对模型行为的精细语义洞察和鲁棒性缺陷的有效识别。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式AI的测试方法在揭示错误原因的语义解释和细粒度语义控制方面存在不足，迫切需要一种能够提供语义洞察且具有精细控制能力的测试生成方法。

Method: 设计了Detect特征感知的测试生成框架，通过控制性扰动潜在空间中的单个语义特征，并结合视觉-语言模型进行语义归因，区分任务相关和无关特征，用以生成高质量测试用例。

Result: 实验证明Detect在图像分类和检测任务中生成了高质量的测试用例，能揭示不同模型架构中的快捷方式行为和未被准确率指标捕捉的缺陷；在决策边界发现和鲁棒性故障识别方面优于现有方法。

Conclusion: Detect框架通过在潜在空间中扰动解耦的语义属性，实现对视觉深度学习模型输入的细粒度语义控制，能够揭示模型的行为变化和鲁棒性缺陷，体现了解释性和特征感知测试的重要价值。

Abstract: As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.

</details>


### [248] [Practitioner Views on Mobile App Accessibility: Practices and Challenges](https://arxiv.org/abs/2601.14131)
*Amila Indika,Rick Kazman,Anthony Peruma*

Main category: cs.SE

TL;DR: 本研究通过全球110名开发者调查，揭示了移动应用无障碍实践中的平台差异与经验影响，指出当前的挑战并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 移动应用普及，但缺乏关于开发者如何实际实施无障碍的跨平台和全球视角研究。

Method: 采用混合方法调查，收集来自43个国家110名移动应用开发者的数据，比较iOS与Android平台及开发者经验对无障碍实践的影响。

Result: 发现开发者主要实现文本相关无障碍功能，面临平台特定障碍，且无障碍实践在不同平台及经验层次间存在显著差异。

Conclusion: 开发者认识到无障碍的重要性，但主要依赖平台特定的指南，且通常在开发后期才进行合规性测试。无障碍实践因平台生态和开发者经验而异，并存在API限制和组织约束等挑战。

Abstract: As mobile applications (apps) become ubiquitous in everyday life, it is crucial for developers to prioritize accessibility for users with diverse abilities. While previous research has identified widespread accessibility issues and raised awareness of developer challenges, there remains a lack of cross-platform, globally representative insights into how practitioners approach accessibility in practice. This paper presents findings from a mixed-methods survey of 110 mobile app developers across 43 countries, examining how platform ecosystems (iOS vs. Android) and developer experience shape accessibility practices. Results show that while developers recognize the importance of accessibility, they often rely on platform-specific guidelines and typically perform compliance testing late in the development process. Developers primarily implement text-focused features while struggling with API limitations and organizational constraints. Through systematic cross-platform comparison, we identify novel platform-specific barriers and demonstrate how accessibility practices differ across developer experience levels. Our findings offer new insights into the challenges of achieving accessibility in practice and provide actionable steps for various stakeholders to promote more consistent and inclusive app development.

</details>


### [249] [Toward self-coding information systems](https://arxiv.org/abs/2601.14132)
*Rodrigo Falcão,Frank Elberzhager,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 本文提出自编码信息系统，能自适应生成和部署代码，助力智能代理快速迭代。


<details>
  <summary>Details</summary>
Motivation: 为了实现智能代理系统的自主适应与快速功能迭代，提升系统灵活性和响应速度。

Method: 提出自编码信息系统的形式化定义，结合评估适应性决策、代码生成、测试与自动部署的能力。

Result: 定义了自编码信息系统的概念，阐述其技术影响，并指出未来研究方向。

Conclusion: 自编码信息系统能够动态适应自身结构与行为，实现代码生成、测试及重新部署，显著缩短新功能的上市时间。

Abstract: In this extended abstract, we propose a novel research topic in the field of agentic AI, which we refer to as self-coding information systems. These systems will be able to dynamically adapt their structure or behavior by evaluating potential adaptation decisions, generate source code, test, and (re)deploy their source code autonomously, at runtime, reducing the time to market of new features. Here we motivate the topic, provide a formal definition of self-coding information systems, discuss some expected impacts of the new technology, and indicate potential research directions.

</details>


### [250] [An Empirical Study on Remote Code Execution in Machine Learning Model Hosting Ecosystems](https://arxiv.org/abs/2601.14163)
*Mohammed Latif Siddiq,Tanzim Hossain Romel,Natalie Sekerak,Beatrice Casey,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 本文首次大规模调研主流模型共享平台中执行远程代码的安全问题，揭示了平台设计的安全隐患和社区认知不足，提出提高安全性与易用性平衡的建议。


<details>
  <summary>Details</summary>
Motivation: 当前模型共享平台为便捷使用预训练模型引入了执行远程代码的灵活性，但这带来了安全隐患，尚缺乏大规模实证研究理解风险及用户看法。

Method: 通过大规模实证研究，使用静态分析工具（Bandit、CodeQL、Semgrep）、恶意代码检测（YARA）、文档和API分析，以及对开发者社区讨论的定性分析，系统评估模型加载中的安全风险和开发者认知。

Result: 发现模型加载时执行任意代码的现象普遍，安全检查不统一且多数平台默认不安全，开发者对远程代码执行的安全影响存在误解和担忧。

Conclusion: 模型共享平台存在执行不受信任代码的安全风险，当前安全措施不足且用户意识不足，需改进平台设计以提升安全性和平衡易用性。

Abstract: Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have become central to modern machine learning development, enabling developers to share, load, and fine-tune pre-trained models with minimal effort. However, the flexibility of these ecosystems introduces a critical security concern: the execution of untrusted code during model loading (i.e., via trust_remote_code or trust_repo). In this work, we conduct the first large-scale empirical study of custom model loading practices across five major model-sharing platforms to assess their prevalence, associated risks, and developer perceptions. We first quantify the frequency with which models require custom code to function and identify those that execute arbitrary Python files during loading. We then apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep, to detect security smells and potential vulnerabilities, categorizing our findings by CWE identifiers to provide a standardized risk taxonomy. We also use YARA to identify malicious patterns and payload signatures. In parallel, we systematically analyze the documentation, API design, and safety mechanisms of each platform to understand their mitigation strategies and enforcement levels. Finally, we conduct a qualitative analysis of over 600 developer discussions from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow, to capture community concerns and misconceptions regarding security and usability. Our findings reveal widespread reliance on unsafe defaults, uneven security enforcement across platforms, and persistent confusion among developers about the implications of executing remote code. We conclude with actionable recommendations for designing safer model-sharing infrastructures and striking a balance between usability and security in future AI ecosystems.

</details>
