<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 68]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeliberationBench: When Do More Voices Hurt? A Controlled Study of Multi-LLM Deliberation Protocols](https://arxiv.org/abs/2601.08835)
*Vaarunay Kaushal,Taranveer Singh*

Main category: cs.CL

TL;DR: 实验表明，复杂的多模型讨论系统不如简单选择最佳回应的方法高效且效果更好。


<details>
  <summary>Details</summary>
Motivation: 分析多语言大模型协作中复杂讨论协议是否能带来实际效果提升。

Method: 引入DELIBERATIONBENCH基准，比较三种讨论协议与基线方法在270个问题上的表现，进行了810次评估。

Result: 基线方法以82.5%胜率显著优于最优讨论协议的13.8%，且后者计算成本更高。

Conclusion: 多模型协作系统中复杂的讨论协议未能超越简单的单模型最佳回答选择方法，且计算代价更高。

Abstract: Multi-agent systems where Large Language Models (LLMs) deliberate to form consensus have gained significant attention, yet their practical value over simpler methods remains under-scrutinized. We introduce DELIBERATIONBENCH, a controlled benchmark evaluating three deliberation protocols against a strong baseline of selecting the best response from a pool of model outputs. Across 270 questions and three independent seeds (810 total evaluations), we find a striking negative result: the best-single baseline achieves an 82.5% +- 3.3% win rate, dramatically outperforming the best deliberation protocol(13.8% +- 2.6%). This 6.0x performance gap is statistically significant (p < 0.01) and comes at 1.5-2.5x higher computational cost. Our findings challenge assumptions that complexity enhances quality in multi-LLM systems.

</details>


### [2] [A Review: PTSD in Pre-Existing Medical Condition on Social Media](https://arxiv.org/abs/2601.08836)
*Zaber Al Hassan Ayon,Nur Hafieza Ismail,Nur Shazwani Kamarudin*

Main category: cs.CL

TL;DR: 本文综述了PTSD与慢性病患者在社交媒体上的表现，着重于通过自然语言处理和机器学习技术识别PTSD及其干预策略。


<details>
  <summary>Details</summary>
Motivation: 探讨PTSD与慢性病交叉影响及早期发现和干预的必要性，利用社交媒体表达揭示患者独特挑战。

Method: 系统回顾2008至2024年的文献，结合自然语言处理和机器学习技术分析社交媒体平台如X和Facebook上的相关内容。

Result: 机器学习模型诊断PTSD的准确率达74%-90%，社交支持社区有助于应对策略和干预的形成。

Conclusion: 社交媒体数据为理解和干预合并慢性病的PTSD患者提供了重要价值，强调需结合慢性病背景考虑PTSD研究与治疗。

Abstract: Post-Traumatic Stress Disorder (PTSD) is a multifaceted mental health condition, particularly challenging for individuals with pre-existing medical conditions. This review critically examines the intersection of PTSD and chronic illnesses as expressed on social media platforms. By systematically analyzing literature from 2008 to 2024, the study explores how PTSD manifests and is managed in individuals with chronic conditions such as cancer, heart disease, and autoimmune disorders, with a focus on online expressions on platforms like X (formally known as Twitter) and Facebook. Findings demonstrate that social media data offers valuable insights into the unique challenges faced by individuals with both PTSD and chronic illnesses. Specifically, natural language processing (NLP) and machine learning (ML) techniques can identify potential PTSD cases among these populations, achieving accuracy rates between 74% and 90%. Furthermore, the role of online support communities in shaping coping strategies and facilitating early interventions is highlighted. This review underscores the necessity of incorporating considerations of pre-existing medical conditions in PTSD research and treatment, emphasizing social media's potential as a monitoring and support tool for vulnerable groups. Future research directions and clinical implications are also discussed, with an emphasis on developing targeted interventions.

</details>


### [3] [From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda](https://arxiv.org/abs/2601.08837)
*Piercosma Bisconti,Marcello Galisai,Matteo Prandi,Federico Pierucci,Olga Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Marcantonio Brancale,Daniele Nardi*

Main category: cs.CL

TL;DR: 本文提出的Adversarial Tales攻击通过结构化叙事绕过大型语言模型防御，平均成功率高达71.3%，显示此类结构性绕过为普遍漏洞，需从机制层面理解与防御。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的安全机制易受通过文化编码结构重新表达有害请求的攻击影响，需研究新型攻击手段及其对模型安全性的影响，进而提出有效防御策略。

Method: 通过将有害内容以赛博朋克叙事形式嵌入并应用普罗普的故事形态学，促使模型从结构分解角度对文本进行功能分析，从而重构并输出有害内容。

Result: 本文提出了一种名为Adversarial Tales的对抗性攻击方法，该方法通过将有害内容嵌入赛博朋克叙事并模拟弗拉基米尔·普罗普的民间故事形态学进行功能分析，从结构分解的角度诱导大型语言模型（LLMs）将有害过程重新构建为合法的叙事解释。实验证明在26个模型中平均攻击成功率达到71.3%，所有模型均未表现出可靠的防御能力。作者指出此类结构性绕过机制是一类普遍存在的漏洞，且单靠模式匹配难以防御。研究强调理解攻击成功的原因，提出了通过机制可解释性研究探索模型如何通过叙事线索重塑表示以及能否独立识别有害意图的方向。

Conclusion: 结构性的叙事绕过攻击普遍存在于大型语言模型中，单靠传统防御难以阻止，需通过机制可解释性研究深入理解模型对叙事线索的处理，以提升识别有害意图的能力。

Abstract: Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 frontier models from nine providers, we observe an average attack success rate of 71.3%, with no model family proving reliably robust. Together with our prior work on Adversarial Poetry, these findings suggest that structurally-grounded jailbreaks constitute a broad vulnerability class rather than isolated techniques. The space of culturally coded frames that can mediate harmful intent is vast, likely inexhaustible by pattern-matching defenses alone. Understanding why these attacks succeed is therefore essential: we outline a mechanistic interpretability research agenda to investigate how narrative cues reshape model representations and whether models can learn to recognize harmful intent independently of surface form.

</details>


### [4] [Companion Agents: A Table-Information Mining Paradigm for Text-to-SQL](https://arxiv.org/abs/2601.08838)
*Jiahui Chen,Lei Fu,Jian Cui,Yu Lei,Zhenning Dong*

Main category: cs.CL

TL;DR: 针对注释缺乏的工业场景，提出了数据库端的Companion Agents，自动挖掘隐藏知识，显著提升Text-to-SQL准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大规模Text-to-SQL基准通常依赖完整准确的数据库注释和外部知识，但工业环境中注释常缺失、不完整或错误，这限制了SOTA模型的实际应用。

Method: 提出了一种数据库中心的方法，利用关系数据库中的内在细粒度信息构建缺失证据，设计了Companion Agents（CA）这一新范式，在数据库端预先挖掘和整合隐含的多表关系、值域分布、统计规律和语义线索，在推理阶段选择性激活相关知识。

Result: 在BIRD基准的全缺失证据设置下，CA在RSL-SQL、CHESS和DAIL-SQL上分别提升了4.49、4.37和14.13个百分点的执行准确率，在挑战子集上的提升更显著，分别为9.65、7.58和16.71个百分点。

Conclusion: 通过自动化的数据库端挖掘和证据构建，CA显著提升了Text-to-SQL模型在缺乏注释条件下的性能，展示了无需人工注释即可实现工业级Text-to-SQL系统的可行路径。

Abstract: Large-scale Text-to-SQL benchmarks such as BIRD typically assume complete and accurate database annotations as well as readily available external knowledge, which fails to reflect common industrial settings where annotations are missing, incomplete, or erroneous. This mismatch substantially limits the real-world applicability of state-of-the-art (SOTA) Text-to-SQL systems. To bridge this gap, we explore a database-centric approach that leverages intrinsic, fine-grained information residing in relational databases to construct missing evidence and improve Text-to-SQL accuracy under annotation-scarce conditions. Our key hypothesis is that when a query requires multi-step reasoning over extensive table information, existing methods often struggle to reliably identify and utilize the truly relevant knowledge. We therefore propose to "cache" query-relevant knowledge on the database side in advance, so that it can be selectively activated at inference time. Based on this idea, we introduce Companion Agents (CA), a new Text-to-SQL paradigm that incorporates a group of agents accompanying database schemas to proactively mine and consolidate hidden inter-table relations, value-domain distributions, statistical regularities, and latent semantic cues before query generation. Experiments on BIRD under the fully missing evidence setting show that CA recovers +4.49 / +4.37 / +14.13 execution accuracy points on RSL-SQL / CHESS / DAIL-SQL, respectively, with larger gains on the Challenging subset +9.65 / +7.58 / +16.71. These improvements stem from CA's automatic database-side mining and evidence construction, suggesting a practical path toward industrial-grade Text-to-SQL deployment without reliance on human-curated evidence.

</details>


### [5] [Recursive Knowledge Synthesis for Multi-LLM Systems: Stability Analysis and Tri-Agent Audit Framework](https://arxiv.org/abs/2601.08839)
*Toshiyuki Shigemura*

Main category: cs.CL

TL;DR: 本文提出了一个三代理交叉验证框架，用于多模型大型语言系统的稳定性和可解释性分析，通过引入递归知识综合机制，实现模型间的互约束优化，并通过实证实验验证了系统稳定性和透明性。


<details>
  <summary>Details</summary>
Motivation: 当前多模型大型语言系统面临稳定性和可解释性不足的问题，需设计框架实现模型间协调推理与知识融合，提升系统整体性能及安全性。

Method: 设计了一个结合三个异构LLM的三代理框架，包括语义生成、分析一致性检查和透明性审计，通过递归交互循环实现知识的递归综合，并通过47次受控试验评估稳定性指标。

Result: 系统平均反射可靠性得分0.78±0.06，透明性得分在68%的试验中≥0.8，约89%的试验收敛，实验证明透明性审计在验证映射中充当收缩算子，验证理论预测。

Conclusion: 该系统展示了在真实公开部署环境中，多模型协同能实现稳定和可解释的递归知识合成，支持安全性和人类监督。

Abstract: This paper presents a tri-agent cross-validation framework for analyzing stability and explainability in multi-model large language systems. The architecture integrates three heterogeneous LLMs-used for semantic generation, analytical consistency checking, and transparency auditing-into a recursive interaction cycle. This design induces Recursive Knowledge Synthesis (RKS), where intermediate representations are continuously refined through mutually constraining transformations irreducible to single-model behavior. Across 47 controlled trials using public-access LLM deployments (October 2025), we evaluated system stability via four metrics: Reflex Reliability Score (RRS), Transparency Score (TS), Deviation Detection Rate (DDR), and Correction Success Rate (CSR). The system achieved mean RRS = 0.78+-0.06 and maintained TS >= 0.8 in about 68% of trials. Approximately 89% of trials converged, supporting the theoretical prediction that transparency auditing acts as a contraction operator within the composite validation mapping. The contributions are threefold: (1) a structured tri-agent framework for coordinated reasoning across heterogeneous LLMs, (2) a formal RKS model grounded in fixed-point theory, and (3) empirical evaluation of inter-model stability under realistic, non-API public-access conditions. These results provide initial empirical evidence that a safety-preserving, humansupervised multi-LLM architecture can achieve stable recursive knowledge synthesis in realistic, publicly deployed environments.

</details>


### [6] [Consistency-Aware Editing for Entity-level Unlearning in Language Models](https://arxiv.org/abs/2601.08840)
*Xiaoqi Han,Víctor Gutiérrez-Basulto,Ru Li,Xiaoli Li,Jiye Liang,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文提出了一致性感知编辑(CAE)框架，通过多样提示和一致性正则化实现对大型语言模型中特定实体的稳健全面遗忘，显著提升性能并降低计算代价，推动实体级知识删除研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能会保留训练数据中的敏感、版权或有害信息，现有实体级遗忘方法通常计算代价高或在处理改写查询时表现脆弱，模型编辑作为一种高效替代方案但现有技术多用于实例级更新，难以完全删除实体知识。为此，本文探讨如何将编辑技术适配于实体级遗忘以实现有效且高效的知识删除。

Method: 提出了一种一致性感知编辑(CAE)框架，通过聚合多样的实体相关提示，包括属性、关系和对抗性改写，联合学习低秩更新，并利用一致性正则化对编辑方向进行约束，促进全面且稳健的遗忘，同时最小化对无关知识的干扰。

Result: CAE在RWKU和ToFU两个难测基准上表现优异，显著提升了遗忘准确率和稳健性，揭示了不同实体知识在模型中的存储位置和所需多样提示数量，为规模化实体移除提供了可行方案。

Conclusion: 本文提出的CAE框架在实体级别遗忘中表现出色，能够高效、稳健地删除特定实体的所有知识，同时较大程度地保留模型的整体能力。实验验证了其在遗忘准确率和鲁棒性上的提升，并提供了对实体知识内部存储和删除机制的见解。

Abstract: Large language models (LLMs) risk retaining sensitive, copyrighted, or harmful information from their training data. Entity-level unlearning addresses this issue by removing all knowledge of a specific entity while preserving the model's overall capabilities. Existing approaches typically rely on full-model fine-tuning or prompt-based interventions, which can be computationally expensive or brittle when handling paraphrased queries. Recently, model editing has emerged as an efficient alternative for updating knowledge in LLMs, offering a promising direction for unlearning. However, existing editing techniques are typically designed for instance-level updates, modifying responses to specific attributes of an entity rather than eliminating all knowledge associated with the entity. In this paper, we investigate how editing techniques can be adapted for effective and efficient entity-level unlearning. To this end, we introduce a novel consistency-aware editing (CAE) framework. CAE aggregates a diverse set of prompts related to a target entity, including its attributes, relations, and adversarial paraphrases. It then jointly learns a low-rank update guided by a consistency regularizer that aligns the editing directions across prompts. This promotes robust and comprehensive forgetting while minimizing interference with unrelated knowledge. We further examine where different entities are stored within the model and how many diverse prompts are needed for successful unlearning. We evaluate CAE on two challenging benchmarks, RWKU and ToFU, and demonstrate that it (i) provides insights into how entity-level knowledge is internally represented and deleted in LLMs, (ii) significantly improves forgetting accuracy and robustness over traditional unlearning and editing baselines, and (iii) enables scalable entity removal using only tens of carefully selected prompts.

</details>


### [7] [Triples and Knowledge-Infused Embeddings for Clustering and Classification of Scientific Documents](https://arxiv.org/abs/2601.08841)
*Mihael Arcan*

Main category: cs.CL

TL;DR: 本文通过结合结构化知识的混合文本表示，提出了一种增强科学文献聚类与分类的新方法，实现了高准确率和良好聚类效果。


<details>
  <summary>Details</summary>
Motivation: 面对科学文献数量和复杂性的增加，研究如何利用结构化知识（三元组）增强文档的聚类和分类效果。

Method: 提出了一个模块化流程，结合无监督聚类和监督分类，利用原始摘要、提取的三元组及其混合表示，通过四种Transformer模型嵌入，使用KMeans、GMM和HDBSCAN进行聚类，并对arXiv主题分类进行了微调。

Result: 全文摘要文本能产生最连贯的聚类，混合表达形式显著提升分类性能，轻量级编码器聚类效果优于领域模型，SciBERT在结构化输入分类中表现最佳。

Conclusion: 结合结构化知识的混合表示方法在分类科学文献中表现优异，达到92.6%的准确率和0.925的宏F1值。轻量级编码器在聚类任务上优于领域特定模型，而SciBERT在结构化输入分类中表现更好。

Abstract: The increasing volume and complexity of scientific literature demand robust methods for organizing and understanding research documents. In this study, we explore how structured knowledge, specifically, subject-predicate-object triples, can enhance the clustering and classification of scientific papers. We propose a modular pipeline that combines unsupervised clustering and supervised classification over multiple document representations: raw abstracts, extracted triples, and hybrid formats that integrate both. Using a filtered arXiv corpus, we extract relational triples from abstracts and construct four text representations, which we embed using four state-of-the-art transformer models: MiniLM, MPNet, SciBERT, and SPECTER. We evaluate the resulting embeddings with KMeans, GMM, and HDBSCAN for unsupervised clustering, and fine-tune classification models for arXiv subject prediction. Our results show that full abstract text yields the most coherent clusters, but that hybrid representations incorporating triples consistently improve classification performance, reaching up to 92.6% accuracy and 0.925 macro-F1. We also find that lightweight sentence encoders (MiniLM, MPNet) outperform domain-specific models (SciBERT, SPECTER) in clustering, while SciBERT excels in structured-input classification. These findings highlight the complementary benefits of combining unstructured text with structured knowledge, offering new insights into knowledge-infused representations for semantic organization of scientific documents.

</details>


### [8] [Resisting Correction: How RLHF Makes Language Models Ignore External Safety Signals in Natural Conversation](https://arxiv.org/abs/2601.08842)
*Felipe Biava Cataneo*

Main category: cs.CL

TL;DR: 该研究发现instruction-tuned语言模型在自然对话中忽视外部安全纠正信号，这是RLHF优化带来的一个重要安全风险，强调了在实际部署中需关注交互风格对安全性的影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型安全架构依赖外部监控以检测错误并在推理时注入纠正信号，研究模型是否能在交互式环境下有效整合外部置信度信息。

Method: 使用Llama-3.2-3B在GSM8K数据集上进行因果干预实验，注入外部置信度信号，并在多种提示策略下测量模型的响应合规性。

Result: 基础模型对外部信号的控制能力接近完美，而instruction-tuned模型在显式命令提示下能完全遵从外部信号，但在自然会话查询中严重忽视该信号。此外，小模型的内部置信度信息无效，强调了外部监督的必要性。

Conclusion: instruction-tuned语言模型在不同交互模式下对外部置信度信号的响应存在显著差异，特别是在自然对话中忽视纠正信号，显示出RLHF优化中的一个关键安全隐患。

Abstract: Safety architectures for language models increasingly rely on external monitors to detect errors and inject corrective signals at inference time. For such systems to function in interactive settings, models must be able to incorporate externally provided confidence information into their verbal responses. In this work, we test whether instruction-tuned language models preserve this controllability across different interaction modes.
  Using Llama-3.2-3B on GSM8K, we perform a causal intervention study in which explicit external confidence signals are injected and model compliance is measured under multiple prompt strategies. We find that base models exhibit near-perfect controllability (Spearman rho close to 1.0), while instruction-tuned models display a striking context dependence: they fully comply with external corrections under explicit command prompts (bias approximately 0 percent, rho = 0.93), yet systematically ignore the same signals in natural conversational queries (bias plus 40 percent, rho = 0.04).
  This behavior is not a capability failure; the model can process the signal, but an emergent property of RLHF optimization that prioritizes conversational fluency over external calibration cues in natural dialogue. We further show that internal token-level confidence in small models is uninformative (r = 0.035), underscoring the necessity of external supervision. Our findings highlight a deployment-critical failure mode: the interaction style users expect is precisely where safety corrections are least effective.

</details>


### [9] [Rubric-Conditioned LLM Grading: Alignment, Uncertainty, and Robustness](https://arxiv.org/abs/2601.08843)
*Haotian Deng,Chris Farber,Jiyoon Lee,David Tang*

Main category: cs.CL

TL;DR: 研究系统评估了大型语言模型在基于评分量规的简答自动评分任务中的性能，发现模型在简单二元评分上表现良好，但细化评分量规时表现下降，且准确率与置信度存在权衡，同时模型对某些扰动敏感，强调了不确定性估计与鲁棒性测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 简答自动评分任务因学生回答的语言多样性及需细致的部分分配而具有挑战性，虽然LLM具备潜力，但其在评分量规场景中的可靠性尚需严格评估。

Method: 系统评估大型语言模型(LLM)作为评分工具在基于评分量规的简答自动评分中的表现，使用SciEntsBank基准测试和Qwen 2.5-72B模型，通过对比专家判分对齐度，分析不确定性与准确性的权衡，并测试模型对随机扰动和对抗攻击的鲁棒性。

Result: LLM在二元任务中的评分与专家判分对齐良好，但随着评分量规细化，对齐度下降。通过“信任曲线”分析，滤除低置信度预测可提高剩余样本的准确率。模型对提示注入攻击具有一定鲁棒性，但对同义词替换敏感。

Conclusion: 基于评分量规的大型语言模型评审具备潜力但存在局限，尤其在细化评分任务中表现不足。不确定性估计通过剔除低置信预测提升了准确率，鲁棒性测试揭示模型对同义词替换敏感，提示未来应用需重视不确定性与鲁棒性保障以实现可靠部署。

Abstract: Automated short-answer grading (ASAG) remains a challenging task due to the linguistic variability of student responses and the need for nuanced, rubric-aligned partial credit. While Large Language Models (LLMs) offer a promising solution, their reliability as automated judges in rubric-based settings requires rigorous assessment. In this paper, we systematically evaluate the performance of LLM-judges for rubric-based short-answer grading. We investigate three key aspects: the alignment of LLM grading with expert judgment across varying rubric complexities, the trade-off between uncertainty and accuracy facilitated by a consensus-based deferral mechanism, and the model's robustness under random input perturbations and adversarial attacks. Using the SciEntsBank benchmark and Qwen 2.5-72B, we find that alignment is strong for binary tasks but degrades with increased rubric granularity. Our "Trust Curve" analysis demonstrates a clear trade-off where filtering low-confidence predictions improves accuracy on the remaining subset. Additionally, robustness experiments reveal that while the model is resilient to prompt injection, it is sensitive to synonym substitutions. Our work provides critical insights into the capabilities and limitations of rubric-conditioned LLM judges, highlighting the importance of uncertainty estimation and robustness testing for reliable deployment.

</details>


### [10] [Emissions and Performance Trade-off Between Small and Large Language Models](https://arxiv.org/abs/2601.08844)
*Anandita Garg,Uma Gaba,Deepan Muthirayan,Anish Roy Chowdhury*

Main category: cs.CL

TL;DR: 本研究表明，微调小型语言模型在多个任务中表现接近大型模型，且碳排放显著降低，有助于推动绿色AI发展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在性能上表现优秀，但其训练和推理过程中的巨大香碳足迹令人担忧，提出了环保的需求。

Method: 通过在自然语言处理、推理和编程等多个任务上，比较大型语言模型与微调后的小型语言模型（SLMs）在性能与碳排放之间的权衡。

Result: 在六个任务中，有四个任务中小型语言模型的性能与大型模型相当，但推理阶段的碳排放显著降低。

Conclusion: 微调的小型语言模型在减少环境影响方面具有可行性，是实现绿色可持续人工智能的重要途径。

Abstract: The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comparable performances for a significant reduction in carbon emissions during inference. Our findings demonstrate the viability of smaller models in mitigating the environmental impact of resource-heavy LLMs, thus advancing towards sustainable, green AI.

</details>


### [11] [Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning](https://arxiv.org/abs/2601.08846)
*Cagatay Tekin,Charbel Barakat,Luis Joseph Luna Limgenco*

Main category: cs.CL

TL;DR: 通过在迭代推理中引入语义缓存，InftyThink能更高效地利用历史成功推理模式，提升结构化推理任务的准确性，但在异构任务中存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有的迭代总结推理框架（如InftyThink）在控制上下文增长的同时，重复生成相似的推理策略，效率不高。希望通过引入语义缓存提高推理效率和准确性。

Method: 引入Cross-Chain Memory，通过嵌入式语义缓存存储之前成功的推理模式，并在每一步推理时检索最相似的语义引理来指导推理过程。

Result: 在MATH500、AIME2024和GPQA-Diamond数据集上实验证明，语义引理检索能提高结构化领域的准确率，同时揭示在异构领域测试中存在失败模式。几何分析显示缓存检索在嵌入空间中引入方向性偏差，带来提升和降低基线准确率的双重效应。

Conclusion: 相似度基础的记忆机制对于自我提升大语言模型推理有益，但其效果有限且可能引入负面影响，需谨慎设计和应用。

Abstract: Iterative summarization based reasoning frameworks such as InftyThink enable long-horizon reasoning in large language models (LLMs) by controlling context growth, but they repeatedly regenerate similar reasoning strategies across tasks. We introduce InftyThink with Cross-Chain Memory, an extension that augments iterative reasoning with an embedding-based semantic cache of previously successful reasoning patterns. At each reasoning step, the model retrieves and conditions on the most semantically similar stored lemmas, guiding inference without expanding the context window indiscriminately. Experiments on MATH500, AIME2024, and GPQA-Diamond demonstrate that semantic lemma retrieval improves accuracy in structured domains while exposing failure modes in tests that include heterogeneous domains. Geometric analyses of reasoning trajectories reveal that cache retrieval induces directional biases in embedding space, leading to consistent fix (improve baseline accuracy) and break (degradation in baseline accuracy) attractors. Our results highlight both the benefits and limits of similarity-based memory for self-improving LLM reasoning.

</details>


### [12] [Scalable and Reliable Evaluation of AI Knowledge Retrieval Systems: RIKER and the Coherent Simulated Universe](https://arxiv.org/abs/2601.08847)
*JV Roig*

Main category: cs.CL

TL;DR: RIKER通过从结构化真实信息生成文档，提供了一种无需人工标注且抗污染的知识系统评测方法，揭示了多项模型能力差异，并提出了可广泛应用的评估范式。


<details>
  <summary>Details</summary>
Motivation: 现有知识系统评估方法受限于静态基准测试易被污染、基于大语言模型的评判存在系统性偏差，以及人工标注成本高昂等问题。

Method: 提出RIKER方法，通过范式反转，从已知结构化真实信息生成文档，避免人工注释和参考模型，支持确定性评分和规模化无污染评估。

Result: 用RIKER评估了33个模型，发现上下文长度声称往往超过实际可用容量，跨文档信息聚合难度远高于单文档抽取，且模型的事实定位能力与抗幻觉能力是两种独立特质。

Conclusion: RIKER不仅提供了一个具体基准，还提出了一种领域无关的、基于合成文档生成的可扩展、抗污染评测构建方法，为未来知识系统评估指明了方向。

Abstract: Evaluating knowledge systems (LLMs, RAG, knowledge graphs, etc) faces fundamental challenges: static benchmarks are vulnerable to contamination, LLM-based judges exhibit systematic biases, and ground truth extraction requires expensive human annotation. We present RIKER (Retrieval Intelligence and Knowledge Extraction Rating), both a benchmark and a replicable methodology based on paradigm inversion - generating documents from known ground truth rather than extracting ground truth from documents. This approach enables deterministic scoring and scalable evaluation without human annotation or reference models, and contamination resistance through regenerable corpora. Our evaluation of 33 models using over 21 billion tokens reveals that context length claims frequently exceed usable capacity, with significant degradation beyond 32K tokens; cross-document aggregation proves substantially harder than single-document extraction; and grounding ability and hallucination resistance are distinct capabilities - models excelling at finding facts that exist may still fabricate facts that do not. Beyond the specific benchmark, we contribute a domain-agnostic methodology for constructing scalable and contamination-resistant evaluations wherever synthetic documents can be generated from structured ground truth.

</details>


### [13] [PediaMind-R1: A Temperament-Aware Language Model for Personalized Early Childhood Care Reasoning via Cognitive Modeling and Preference Alignment](https://arxiv.org/abs/2601.08848)
*Zihe Zhang,Can Zhang,Yanheng Xu,Xin Hu,Jichao Leng*

Main category: cs.CL

TL;DR: PediaMind-R1是一款结合发展心理学气质理论的智能育儿大模型，通过两阶段训练实现个性化建议，显著提升育儿实用性和精准性。


<details>
  <summary>Details</summary>
Motivation: 传统智能育儿系统提供通用建议，缺少针对个体儿童气质的专业个性化支持，亟需结合发展心理学理论提升定制化能力。

Method: 采用两阶段训练，先通过监督微调优化结构化思维链推理，再用GRPO方法增强逻辑一致性和同理心护理策略；建立基于Thomas-Chess气质理论的知识图谱。

Result: PediaMind-R1能准确识别早期儿童气质，主动进行个性化推理，评估结果显示其在气质敏感测试和人工评估中表现优异。

Conclusion: PediaMind-R1有效实现了基于儿童气质理论的主动个性化智能育儿，提升了育儿建议的针对性和科学性。

Abstract: This paper presents PediaMind-R1, a domain-specialized large language model designed to achieve active personalization in intelligent parenting scenarios. Unlike conventional systems that provide generic suggestions, PediaMind-R1 draws on insights from developmental psychology. It introduces temperament theory from the Thomas-Chess framework and builds a temperament knowledge graph for infants and toddlers (0-3 years). Our two-stage training pipeline first uses supervised fine-tuning to teach structured chain-of-thought reasoning, and then applies a GRPO-based alignment stage to reinforce logical consistency, domain expertise, and empathetic caregiving strategies. We further design an evaluation framework comprising temperament-sensitive multiple-choice tests and human assessments. The results demonstrate that PediaMind-R1 can accurately interpret early childhood temperament profiles and proactively engage in individualized reasoning. This work highlights the value of integrating vertical-domain modeling with psychological theory. It offers a novel approach to developing user-centered LLMs that advance the practice of active personalization in sensitive caregiving contexts.

</details>


### [14] [Gaming the Answer Matcher: Examining the Impact of Text Manipulation on Automated Judgment](https://arxiv.org/abs/2601.08849)
*Manas Khatore,Sumana Sridharan,Kevork Sulahian,Benjamin J. Smith,Shi Feng*

Main category: cs.CL

TL;DR: 自动答案匹配能有效抵抗文本策略攻击，二元评分更鲁棒，适合作为人类或大模型评审的替代。


<details>
  <summary>Details</summary>
Motivation: 提升自动答案匹配的可靠性，确保其在面对考生试图通过策略性文本操控提升分数时依然能准确评估回答的正确性。

Method: 通过使用被考生模型生成的策略性回答（冗长、多答案、矛盾）测试答案匹配模型，比较二元与连续评分的稳健性，系统验证攻击是否影响得分。

Result: 本文研究了自动答案匹配技术在评估自由文本回答中的可靠性，尤其关注其对策略性攻击（如冗长回答、多答案以及在答案中嵌入相互矛盾内容）的鲁棒性。通过实验发现，这些攻击策略不会提升匹配得分，反而往往降低分数。此外，二元评分（正确/错误判定）比连续评分（部分正确判定）对攻击更为鲁棒。综上，答案匹配技术在对抗简单文本操控时表现出较高稳定性，是有参考答案时替代传统人类或大模型评审的可行方案。

Conclusion: 答案匹配技术对冗长、多重和矛盾回答等策略攻击具有较好鲁棒性，二元评分优于连续评分，证明其作为评估方式的有效性和可靠性。

Abstract: Automated answer matching, which leverages LLMs to evaluate free-text responses by comparing them to a reference answer, shows substantial promise as a scalable and aligned alternative to human evaluation. However, its reliability requires robustness against strategic attacks such as guesswork or verbosity that may artificially inflate scores without improving actual correctness. In this work, we systematically investigate whether such tactics deceive answer matching models by prompting examinee models to: (1) generate verbose responses, (2) provide multiple answers when unconfident, and (3) embed conflicting answers with the correct answer near the start of their response. Our results show that these manipulations do not increase scores and often reduce them. Additionally, binary scoring (which requires a matcher to answer with a definitive "correct" or "incorrect") is more robust to attacks than continuous scoring (which requires a matcher to determine partial correctness). These findings show that answer matching is generally robust to inexpensive text manipulation and is a viable alternative to traditional LLM-as-a-judge or human evaluation when reference answers are available.

</details>


### [15] [Más contexto no es mejor. Paradoja de la dilución vectorial en RAG corporativos](https://arxiv.org/abs/2601.08851)
*Alex Dantart*

Main category: cs.CL

TL;DR: 本文研究了"Contextualized Chunking"技术在RAG中的应用，发现注入摘要比例与检索性能呈"反U型"关系，适量注入能提升召回率，过多注入则降低精确度。提出了计算最佳注入比例的理论框架。


<details>
  <summary>Details</summary>
Motivation: 现有的"Contextualized Chunking"技术虽能增强RAG模型的上下文理解，但引入的摘要注入导致向量稀释，削弱了对局部内容的关注，亟需找到平衡注入比例的方法以提升整体性能。

Method: 通过实验评估不同摘要注入比例对RAG模型性能的影响，量化了注入比例与召回率和精确度之间的关系，提出并验证了一个理论框架以确定最优注入比例。

Result: 实验证明，摘要注入比例适中时召回率提升18%，但当注入比例超过0.4时，特定查询的精确度降低22%。基于此，提出的理论框架能够有效地计算出最佳注入比例。

Conclusion: 适度注入摘要能显著提升RAG的召回性能，但超过关键阈值会降低针对特定查询的精确度。因此，需要精确计算注入比例以达到最优效果。

Abstract: Técnicas recientes de "Contextualized Chunking" inyectan resúmenes para mejorar el contexto en RAG, pero introducen una "dilución vectorial" que opaca el contenido local. Evaluando distintos ratios de inyección, demostramos una curva en "U invertida": una inyección moderada mejora el "Recall" (+18%), pero superar un umbral crítico (CIR > 0.4) reduce la precisión en un 22% para consultas específicas. Proponemos un marco teórico para calcular el ratio óptimo de inyección. --
  Recent "Contextualized Chunking" techniques inject summaries to improve RAG context but introduce "vector dilution" drowning out local content. Evaluating various injection ratios, we demonstrate an "inverted U" curve: moderate injection boosts Recall (+18%), but exceeding a critical threshold (CIR > 0.4) drops precision by 22% for specific queries. We propose a theoretical framework to calculate the optimal injection ratio.

</details>


### [16] [NewsScope: Schema-Grounded Cross-Domain News Claim Extraction with Open Models](https://arxiv.org/abs/2601.08852)
*Nidhi Pandya*

Main category: cs.CL

TL;DR: 本文提出了NewsScope，一个跨领域的新闻事实抽取数据集及基准模型，通过微调LLaMA模型实现高准确率的结构化新闻声明抽取，并公开了代码与模型。


<details>
  <summary>Details</summary>
Motivation: 现有新闻声明抽取方法缺乏模式合规性或泛化能力不足，亟需一个跨领域、高质量的结构化声明抽取数据集与模型。

Method: 基于LLaMA 3.1 8B模型，利用LoRA技术在315个训练样本上进行微调；通过人类评估与标注者一致性验证模型表现；设计数值结合过滤提升准确率。

Result: NewsScope在人类评估中达到89.4%准确率，政治领域表现优于GPT-4o-mini（94.3% VS 87.8%），数字过滤提高准确率至91.6%，标注一致性达94.6%。

Conclusion: NewsScope在跨领域新闻声明抽取任务中表现优异，尤其在政治领域超过GPT-4o-mini，且通过数字结合过滤进一步提升准确率，实现了高质量且可离线部署的模型。

Abstract: Automated news verification requires structured claim extraction, but existing approaches either lack schema compliance or generalize poorly across domains. This paper presents NewsScope, a cross-domain dataset, benchmark, and fine-tuned model for schema-grounded news claim extraction. The dataset contains 455 articles across politics, health, science/environment, and business, consisting of 395 in-domain articles and 60 out-of-source articles for generalization testing. LLaMA 3.1 8B was fine-tuned using LoRA on 315 training examples and evaluated on held-out in-domain (80 articles) and out-of-source (60 articles) test sets. Human evaluation on 400 claims shows NewsScope achieves 89.4% human-evaluated accuracy compared to GPT-4o-mini's 93.7% (p=0.07). NewsScope outperforms GPT-4o-mini on political claims (94.3% vs. 87.8%). A numeric grounding filter further improves accuracy to 91.6%, narrowing the gap to 2.1 percentage points. Inter-annotator agreement studies (160 claims) confirm labeling reliability (94.6% positive agreement on SUPPORTED judgments). The open-weight model enables offline deployment at approximately $15 on-demand compute (or $0 on free tiers). Code and benchmark are publicly released.

</details>


### [17] [Evaluating Role-Consistency in LLMs for Counselor Training](https://arxiv.org/abs/2601.08892)
*Eric Rudolph,Natalie Engert,Jens Albrecht*

Main category: cs.CL

TL;DR: 本文通过构建对抗数据集，评测大语言模型在虚拟客户角色扮演中的一致性与连贯性，推进在线咨询训练方法。


<details>
  <summary>Details</summary>
Motivation: 提升在线咨询服务培训的效果，尤其加强未来咨询师的训练，通过虚拟客户模拟真实咨询互动。

Method: 设计包含对抗攻击的虚拟客户数据集，利用该数据集评估Vicuna及其它开源大语言模型在角色一致性和对话连贯性上的表现，进行横向比较分析。

Result: 创建包含对抗攻击的新数据集，评估并比较多个开源大语言模型在维持角色一致性和对话连贯性方面的表现，特别是Vicuna模型。

Conclusion: 研究表明，加入对抗攻击的数据集可以有效测试模型的角色保持能力，各大模型在角色一致性和对话质量上表现各异，为在线咨询训练提供数据和方法支持。

Abstract: The rise of online counseling services has highlighted the need for effective training methods for future counselors. This paper extends research on VirCo, a Virtual Client for Online Counseling, designed to complement traditional role-playing methods in academic training by simulating realistic client interactions. Building on previous work, we introduce a new dataset incorporating adversarial attacks to test the ability of large language models (LLMs) to maintain their assigned roles (role-consistency). The study focuses on evaluating the role consistency and coherence of the Vicuna model's responses, comparing these findings with earlier research. Additionally, we assess and compare various open-source LLMs for their performance in sustaining role consistency during virtual client interactions. Our contributions include creating an adversarial dataset, evaluating conversation coherence and persona consistency, and providing a comparative analysis of different LLMs.

</details>


### [18] [Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models](https://arxiv.org/abs/2601.08955)
*Youwei Liu,Jian Wang,Hanlin Wang,Beichen Guo,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出了Imagine-then-Plan框架，利用多步动态想象和自适应前瞻提升智能体策略学习效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型主要采用单步或固定视距的预测方式，未充分利用其在复杂任务规划中的潜力。

Method: 提出Imagine-then-Plan（ITP）框架，通过让策略模型与学习到的世界模型交互，产生多步“想象”轨迹，引入自适应的前瞻机制，权衡最终目标和任务进展，形成部分可观测且可想象的马尔可夫决策过程指导策略学习。

Result: ITP在多个代表性代理基准测试中显著超越竞争基线，自适应前瞻机制有效增强了智能体的推理能力，帮助解决更广泛的复杂任务。

Conclusion: ITP框架通过多步动态想象和自适应前瞻机制，提升了世界模型在复杂任务规划中的应用效果，为智能体学习提供了新的思路。

Abstract: Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \textit{observable} and \textit{imaginable} Markov decision process to guide policy learning. We instantiate \texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.

</details>


### [19] [Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM](https://arxiv.org/abs/2601.09001)
*Pedro Memoli Buffa,Luciano Del Corro*

Main category: cs.CL

TL;DR: 通过输出熵统计预测模型在不同领域的准确率，实现LLM性能监控与数据优先获取。


<details>
  <summary>Details</summary>
Motivation: 解决部署大型语言模型(LLMs)时的两个挑战：监控模型在不同域和流量下的性能下降，以及优先获取数据以缩小性能差距。

Method: 通过在推理时计算输出熵轮廓（基于最终层的下一个-token概率）并提取11个统计特征，使用轻量级分类器预测实例正确性，进而估计领域级别的准确率。

Result: 方法在十个STEM推理基准测试和九个不同LLM模型上验证，准确率估计与实际准确率相符，多数模型表现出领域排序的单调性。

Conclusion: 输出熵轮廓作为推理时信号，能有效用于大规模模型性能监控和指导数据采集策略。

Abstract: Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all "10 choose k" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.

</details>


### [20] [TranslateGemma Technical Report](https://arxiv.org/abs/2601.09012)
*Mara Finkelstein,Isaac Caswell,Tobias Domhan,Jan-Thorsten Peter,Juraj Juraska,Parker Riley,Daniel Deutsch,Cole Dilanni,Colin Cherry,Eleftheria Briakou,Elizabeth Nielsen,Jiaming Luo,Kat Black,Ryan Mullins,Sweta Agrawal,Wenda Xu,Erin Kats,Stephane Jaskiewicz,Markus Freitag,David Vilar*

Main category: cs.CL

TL;DR: TranslateGemma是基于Gemma 3基础模型的一套开放机器翻译模型，通过两阶段微调（监督微调+强化学习）提升多语言翻译能力，实验证明其在多个语言对和多模态任务上均有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 增强Gemma 3固有的多语言能力以适应翻译任务，提升翻译质量和效率，同时保持模型的多模态能力。

Method: 采用两阶段微调：第一阶段用大规模高质量合成平行语料和人工翻译数据进行监督微调；第二阶段利用包括MetricX-QE和AutoMQM在内的多奖励模型进行强化学习优化翻译质量。

Result: 在WMT25和WMT24++多语言测试集上，TranslateGemma在自动和人工评测中均显著优于基线Gemma 3，且小模型性能接近大模型，在Vistra图像翻译任务中也表现优异。

Conclusion: TranslateGemma模型显著提升了Gemma 3的翻译性能，在多个语言对和多模态任务上均表现优异，且小模型效率高，适合社区使用。

Abstract: We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.

</details>


### [21] [Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social Deduction Game](https://arxiv.org/abs/2601.09017)
*Haryo Akbarianto Wibowo,Alaa Elsetohy,Qinrong Cui,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 通过Spyfall游戏建立了一个动态、抗数据泄露且能反映文化细节的模型评估框架，揭示模型在多语言多文化场景中的弱点。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试方法存在数据饱和和泄露问题，无法有效评估模型多文化多语言的综合能力。

Method: 本论文提出了一个基于社交推理游戏Spyfall的动态多语言多文化能力评估框架。模型需通过策略对话识别秘密特工或避免被发现，涉及本地文化相关信息如地点或食物。

Result: 游戏基于排名与Chatbot Arena高度相关，但非英语环境下模型表现显著下降，难以处理本地特定实体，且策略执行不佳。

Conclusion: 基于游戏的评估方法为传统NLP基准测试提供了一种可扩展、数据泄露抵抗强且文化敏感的新思路，特别适合多语言多文化的模型性能检测。

Abstract: The rapid advancement of Large Language Models (LLMs) has necessitated more robust evaluation methods that go beyond static benchmarks, which are increasingly prone to data saturation and leakage. In this paper, we propose a dynamic benchmarking framework for evaluating multilingual and multicultural capabilities through the social deduction game Spyfall. In our setup, models must engage in strategic dialogue to either identify a secret agent or avoid detection, utilizing culturally relevant locations or local foods. Our results show that our game-based rankings align closely with the Chatbot Arena. However, we find a significant performance gap in non-English contexts: models are generally less proficient when handling locally specific entities and often struggle with rule-following or strategic integrity in non-English languages. We demonstrate that this game-based approach provides a scalable, leakage-resistant, and culturally nuanced alternative to traditional NLP benchmarks. The game history can be accessed here https://huggingface.co/datasets/haryoaw/cultural-spyfall.

</details>


### [22] [OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG](https://arxiv.org/abs/2601.09028)
*Fengran Mo,Zhan Su,Yuchen Hui,Jinghan Zhang,Jia Ao Sun,Zheyuan Liu,Chao Zhang,Tetsuya Sakai,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本文提出OpenDecoder，通过显式评估检索信息质量增强检索增强生成方法，提升了生成内容的质量和模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLMs）的检索增强生成（RAG）方法假设检索信息与问题高度相关，但实际检索结果的相关性和有用性存在差异，影响生成内容质量。

Method: 提出OpenDecoder方法，通过显式评估检索信息的质量指标（相关性得分、排序得分、查询表现预测得分）作为生成辅助，增强模型对噪声上下文的鲁棒性。

Result: 在五个基准数据集上的实验表明，OpenDecoder方法优于多种基线，表现出更高的效果和鲁棒性。

Conclusion: OpenDecoder方法能够灵活整合不同的外部指标和模型后训练过程，有效提升RAG模型在噪声环境下的生成性能。

Abstract: The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.

</details>


### [23] [SpectraQuery: A Hybrid Retrieval-Augmented Conversational Assistant for Battery Science](https://arxiv.org/abs/2601.09036)
*Sreya Vangara,Jagjit Nanda,Yan-Kai Tzeng,Eric Darve*

Main category: cs.CL

TL;DR: 本文提出了SpectraQuery，一个结合结构化数据和非结构化文献的混合查询框架，实现了科学推理中多模态数据的联合检索和回答生成。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型难以同时跨结构化实验数据和非结构化文献进行科学推理，需实现多模态联合分析。

Method: 采用结构语义解析与增强检索生成相结合的方法，将开放式问题翻译为SQL查询和文献检索操作。

Result: 约80%的SQL查询正确，答案基于率达93%-97%，专家评价准确性、相关性和清晰度均较高。

Conclusion: SpectraQuery能有效结合结构化数据库和非结构化科学文献，生成高质量、基于证据的科学回答，表现优异。

Abstract: Scientific reasoning increasingly requires linking structured experimental data with the unstructured literature that explains it, yet most large language model (LLM) assistants cannot reason jointly across these modalities. We introduce SpectraQuery, a hybrid natural-language query framework that integrates a relational Raman spectroscopy database with a vector-indexed scientific literature corpus using a Structured and Unstructured Query Language (SUQL)-inspired design. By combining semantic parsing with retrieval-augmented generation, SpectraQuery translates open-ended questions into coordinated SQL and literature retrieval operations, producing cited answers that unify numerical evidence with mechanistic explanation. Across SQL correctness, answer groundedness, retrieval effectiveness, and expert evaluation, SpectraQuery demonstrates strong performance: approximately 80 percent of generated SQL queries are fully correct, synthesized answers reach 93-97 percent groundedness with 10-15 retrieved passages, and battery scientists rate responses highly across accuracy, relevance, grounding, and clarity (4.1-4.6/5). These results show that hybrid retrieval architectures can meaningfully support scientific workflows by bridging data and discourse for high-volume experimental datasets.

</details>


### [24] [Can LLMs interpret figurative language as humans do?: surface-level vs representational similarity](https://arxiv.org/abs/2601.09041)
*Samhita Bollepally,Aurora Sloman-Moll,Takashi Yamauchi*

Main category: cs.CL

TL;DR: 大型语言模型在理解复杂社会语言时与人类存在差异，GPT-4表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在理解比喻和社会语境语言时，与人类判断的一致性。

Method: 人类参与者和四个不同规模的指令微调大型语言模型对240句含六种语言特征的对话句子及对应的40个解释问题进行10分制评分，比较其与人类判断的一致性。

Result: 大型语言模型在表面层面与人类判断较为一致，但在表征层面，尤其是对成语和Z世代俚语的理解上存在明显偏差。GPT-4最接近人类的表征模式，所有模型均难以准确处理依赖语境和社会语用的表达如讽刺、俚语和成语。

Conclusion: 尽管大型语言模型能模拟人类表面判断，但其对复杂、依赖语境的语言特征理解仍不充分，未来需加强语境和社会语用表达的模型能力。

Abstract: Large language models generate judgments that resemble those of humans. Yet the extent to which these models align with human judgments in interpreting figurative and socially grounded language remains uncertain. To investigate this, human participants and four instruction-tuned LLMs of different sizes (GPT-4, Gemma-2-9B, Llama-3.2, and Mistral-7B) rated 240 dialogue-based sentences representing six linguistic traits: conventionality, sarcasm, funny, emotional, idiomacy, and slang. Each of the 240 sentences was paired with 40 interpretive questions, and both humans and LLMs rated these sentences on a 10-point Likert scale. Results indicated that humans and LLMs aligned at the surface level with humans, but diverged significantly at the representational level, especially in interpreting figurative sentences involving idioms and Gen Z slang. GPT-4 most closely approximates human representational patterns, while all models struggle with context-dependent and socio-pragmatic expressions like sarcasm, slang, and idiomacy.

</details>


### [25] [Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers](https://arxiv.org/abs/2601.09049)
*Kaiyu He,Zhang Mian,Peilin Wu,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 本研究通过机制分析，发现grokking阶段并非带来新的推理方式，而是在已有路径中融合记忆事实，且其对新知识的迁移能力有限，提示grokking的计算成本收益值得重新评估。


<details>
  <summary>Details</summary>
Motivation: 探究grokking阶段形成的"Generalization Circuit"是否真正提升模型在下游合成任务上的表现及其计算代价是否值得。

Method: 通过对比参数共享的transformer模型在非阶段性及经过长时间训练（grokking后）状态中的推理路径，对模型的推理模式和知识整合能力进行机械性研究。

Result: (i)非grokking和grokking模型处理同分布复合查询时推理路径相同，说明"Generalization Circuit"不是新推理范式的突发获取，而是将记忆的原子事实融合到既有推理路径中；(ii)高准确率的达成与推理路径的形成可以独立发生，具体依赖数据分布；(iii)即使是成熟的"Generalization Circuit"，在整合新知识时转移能力有限，表明grokking的transformer未完全掌握合成逻辑。

Conclusion: grokking并非模型推理范式的一次质变，而是记忆事实的融合过程，其在提高模型泛化能力上的作用有限，且高准确率和推理路径的建立可独立发生；因此grokking的高计算开销需权衡其实际收益。

Abstract: While Large Language Models (LLMs) excel at factual retrieval, they often struggle with the "curse of two-hop reasoning" in compositional tasks. Recent research suggests that parameter-sharing transformers can bridge this gap by forming a "Generalization Circuit" during a prolonged "grokking" phase. A fundamental question arises: Is a grokked model superior to its non-grokked counterparts on downstream tasks? Furthermore, is the extensive computational cost of waiting for the grokking phase worthwhile? In this work, we conduct a mechanistic study to evaluate the Generalization Circuit's role in knowledge assimilation and transfer. We demonstrate that: (i) The inference paths established by non-grokked and grokked models for in-distribution compositional queries are identical. This suggests that the "Generalization Circuit" does not represent the sudden acquisition of a new reasoning paradigm. Instead, we argue that grokking is the process of integrating memorized atomic facts into an naturally established reasoning path. (ii) Achieving high accuracy on unseen cases after prolonged training and the formation of a certain reasoning path are not bound; they can occur independently under specific data regimes. (iii) Even a mature circuit exhibits limited transferability when integrating new knowledge, suggesting that "grokked" Transformers do not achieve a full mastery of compositional logic.

</details>


### [26] [SITA: Learning Speaker-Invariant and Tone-Aware Speech Representations for Low-Resource Tonal Languages](https://arxiv.org/abs/2601.09050)
*Tianyi Xu,Xuan Ouyang,Binwei Yao,Shoua Xiong,Sara Misurelli,Maichou Lor,Junjie Hu*

Main category: cs.CL

TL;DR: 本文针对声音低资源的声调语言提出了SITA，一种轻量级自适应方法，通过多目标训练实现说话者不变性和声调敏感性，从而提升了声调语言的语音表示效果。


<details>
  <summary>Details</summary>
Motivation: 声调语言普遍缺乏资源，现有多语言预训练编码器难以有效捕捉声调信息，且存在性别等无关变异影响，需要设计能保持声调信息且对说话者差异具有鲁棒性的表示。

Method: 采用分阶段多目标训练策略：(i)利用跨性别对比目标保持词汇一致性，利用声调排斥损失防止声调信息崩溃；(ii)辅助CTC基的ASR目标并结合知识蒸馏稳定识别结构。

Result: 在Hmong词汇库上，SITA方法提高了跨性别词汇检索准确率并与ASR的XLS-R教师模型相比保持了良好的识别性能，同时在Mandarin上同样获得性能提升，展示了其适应声调语言的普适性。

Conclusion: SITA在声调语言，特别是民族语言Hmong中显著提升了跨性别词汇检索准确率，并保持了可用的自动语音识别（ASR）性能，同时在Mandarin上的迁移实验表明其通用性。

Abstract: Tonal low-resource languages are widely spoken yet remain underserved by modern speech technology. A key challenge is learning representations that are robust to nuisance variation such as gender while remaining tone-aware for different lexical meanings. To address this, we propose SITA, a lightweight adaptation recipe that enforces Speaker-Invariance and Tone-Awareness for pretrained wav2vec-style encoders. SITA uses staged multi-objective training: (i) a cross-gender contrastive objective encourages lexical consistency across speakers, while a tone-repulsive loss prevents tone collapse by explicitly separating same-word different-tone realizations; and (ii) an auxiliary Connectionist Temporal Classification (CTC)-based ASR objective with distillation stabilizes recognition-relevant structure. We evaluate primarily on Hmong, a highly tonal and severely under-resourced language where off-the-shelf multilingual encoders fail to represent tone effectively. On a curated Hmong word corpus, SITA improves cross-gender lexical retrieval accuracy, while maintaining usable ASR accuracy relative to an ASR-adapted XLS-R teacher. We further observe similar gains when transferring the same recipe to Mandarin, suggesting SITA is a general, plug-in approach for adapting multilingual speech encoders to tonal languages.

</details>


### [27] [Efficient Multilingual Dialogue Processing via Translation Pipelines and Distilled Language Models](https://arxiv.org/abs/2601.09059)
*Santiago Martínez Novoa,Nicolás Rozo Fajardo,Diego Alejandro González Vargas,Nicolás Bedoya Figueroa*

Main category: cs.CL

TL;DR: 本研究通过翻译和知识蒸馏技术，成功构建了一个多语言对话总结及问答系统，在低资源语言任务中无需专门微调，也能实现高性能。


<details>
  <summary>Details</summary>
Motivation: 通过知识蒸馏技术，探索紧凑模型在多语言低资源环境下进行对话总结和问答任务的有效性。

Method: 采用三阶段流水线方法：从印度语言到英语的正向翻译，利用一个2.55亿参数的蒸馏语言模型进行多任务文本生成，最后将结果反向翻译回源语言。

Result: 系统在九种语言上表现出色，特别是在马拉地语（86.7% QnA）、泰米尔语（86.7% QnA）和印地语（80.0% QnA）任务中表现优异。

Conclusion: 翻译基方法结合蒸馏模型是处理低资源多语言任务的有效方案，具有较强的竞争力且无需任务特定微调。

Abstract: This paper presents team Kl33n3x's multilingual dialogue summarization and question answering system developed for the NLPAI4Health 2025 shared task. The approach employs a three-stage pipeline: forward translation from Indic languages to English, multitask text generation using a 2.55B parameter distilled language model, and reverse translation back to source languages. By leveraging knowledge distillation techniques, this work demonstrates that compact models can achieve highly competitive performance across nine languages. The system achieved strong win rates across the competition's tasks, with particularly robust performance on Marathi (86.7% QnA), Tamil (86.7% QnA), and Hindi (80.0% QnA), demonstrating the effectiveness of translation-based approaches for low-resource language processing without task-specific fine-tuning.

</details>


### [28] [Beyond Consensus: Perspectivist Modeling and Evaluation of Annotator Disagreement in NLP](https://arxiv.org/abs/2601.09065)
*Yinuo Xu,David Jurgens*

Main category: cs.CL

TL;DR: 本文综述了NLP中标注者分歧建模的最新研究，强调分歧作为重要信号，归纳方法体系及评价指标，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 标注者分歧在主观和含糊任务中广泛存在，传统方法将其视为噪声剔除，而最新研究表明分歧是反映解释和视角多样性的有意义信号，亟需统一的综述和系统方法。

Method: 通过构建一个统一的框架，围绕预测目标和聚合结构对现有分歧感知方法进行归纳和梳理，提出覆盖数据、任务和标注者因素的分歧来源无关领域的分类法。

Result: 总结了建模方法的新趋势，包括从共识学习到显式建模分歧，以及评价指标的现状，特别是标注者行为和公平性评估主要处于描述性阶段。

Conclusion: 本文综述了NLP中标注者分歧的建模方法，强调将分歧视为有意义信息而非噪声，指出当前研究向显式建模分歧和捕捉标注者结构关系转变，并识别了未来挑战如多源变异整合和可解释性框架。

Abstract: Annotator disagreement is widespread in NLP, particularly for subjective and ambiguous tasks such as toxicity detection and stance analysis. While early approaches treated disagreement as noise to be removed, recent work increasingly models it as a meaningful signal reflecting variation in interpretation and perspective. This survey provides a unified view of disagreement-aware NLP methods. We first present a domain-agnostic taxonomy of the sources of disagreement spanning data, task, and annotator factors. We then synthesize modeling approaches using a common framework defined by prediction targets and pooling structure, highlighting a shift from consensus learning toward explicitly modeling disagreement, and toward capturing structured relationships among annotators. We review evaluation metrics for both predictive performance and annotator behavior, and noting that most fairness evaluations remain descriptive rather than normative. We conclude by identifying open challenges and future directions, including integrating multiple sources of variation, developing disagreement-aware interpretability frameworks, and grappling with the practical tradeoffs of perspectivist modeling.

</details>


### [29] [Mi:dm 2.0 Korea-centric Bilingual Language Models](https://arxiv.org/abs/2601.09066)
*Donghoon Shin,Sejung Lee,Soonmin Bae,Hwijung Ryu,Changwon Ok,Hoyoun Jung,Hyesung Ji,Jeehyun Lim,Jehoon Lee,Ji-Eun Han,Jisoo Baik,Mihyeon Kim,Riwoo Chung,Seongmin Lee,Wonjae Park,Yoonseok Heo,Youngkyung Seo,Seyoun Won,Boeun Kim,Cheolhun Heo,Eunkyeong Lee,Honghee Lee,Hyeongju Ju,Hyeontae Seo,Jeongyong Shim,Jisoo Lee,Junseok Koh,Junwoo Kim,Minho Lee,Minji Kang,Minju Kim,Sangha Nam,Seongheum Park,Taehyeong Kim,Euijai Ahn,Hong Seok Jeung,Jisu Shin,Jiyeon Kim,Seonyeong Song,Seung Hyun Kong,Sukjin Hong,Taeyang Yun,Yu-Seon Kim,A-Hyun Lee,Chae-Jeong Lee,Hye-Won Yu,Ji-Hyun Ahn,Song-Yeon Kim,Sun-Woo Jung,Eunju Kim,Eunji Ha,Jinwoo Baek,Yun-ji Lee,Wanjin Park,Jeong Yeop Kim,Eun Mi Kim,Hyoung Jun Park,Jung Won Yoon,Min Sung Noh,Myung Gyo Oh,Wongyoung Lee,Yun Jin Park,Young S. Kwon,Hyun Keun Kim,Jieun Lee,YeoJoo Park*

Main category: cs.CL

TL;DR: Mi:dm 2.0是一款专为韩语和韩国文化设计的大型语言模型，解决了现有模型在数据和文化贴合度上的不足，提升了韩语处理的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理韩语时存在数据质量不足和文化贴合度低的问题，难以准确反映韩国社会的价值观和文化背景。

Method: 采用了专有数据清洗、高质量合成数据生成、战略性课程学习数据混合及定制韩语优化分词器，结合深度扩展策略构建不同规模模型。

Result: Mi:dm 2.0模型通过高质量数据清洗、合成数据生成、课程学习和定制韩语分词器，显著提升韩语理解和生成能力，在多个韩语基准测试中表现优异。

Conclusion: 通过提供高性能的韩语大语言模型，推动韩国AI技术进步，促进韩国工业、公共服务和教育领域的AI应用，并增强韩国AI开发者社区的实力。

Abstract: We introduce Mi:dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense knowledge inherent to Korean society, enabling nuanced understanding of cultural contexts, emotional subtleties, and real-world scenarios to generate reliable and culturally appropriate responses. To address limitations of existing LLMs, often caused by insufficient or low-quality Korean data and lack of cultural alignment, Mi:dm 2.0 emphasizes robust data quality through a comprehensive pipeline that includes proprietary data cleansing, high-quality synthetic data generation, strategic data mixing with curriculum learning, and a custom Korean-optimized tokenizer to improve efficiency and coverage. To realize this vision, we offer two complementary configurations: Mi:dm 2.0 Base (11.5B parameters), built with a depth-up scaling strategy for general-purpose use, and Mi:dm 2.0 Mini (2.3B parameters), optimized for resource-constrained environments and specialized tasks. Mi:dm 2.0 achieves state-of-the-art performance on Korean-specific benchmarks, with top-tier zero-shot results on KMMLU and strong internal evaluation results across language, humanities, and social science tasks. The Mi:dm 2.0 lineup is released under the MIT license to support extensive research and commercial use. By offering accessible and high-performance Korea-centric LLMs, KT aims to accelerate AI adoption across Korean industries, public services, and education, strengthen the Korean AI developer community, and lay the groundwork for the broader vision of K-intelligence. Our models are available at https://huggingface.co/K-intelligence. For technical inquiries, please contact midm-llm@kt.com.

</details>


### [30] [From Symbolic to Natural-Language Relations: Rethinking Knowledge Graph Construction in the Era of Large Language Models](https://arxiv.org/abs/2601.09069)
*Kanyao Han,Yushang Lai*

Main category: cs.CL

TL;DR: 本文呼吁重新思考知识图谱中关系的表示方式，建议从符号化标签转向自然语言描述，以适应大型语言模型带来的知识表示和生成新范式。


<details>
  <summary>Details</summary>
Motivation: 传统符号关系标签无法充分表达现实世界中复杂、微妙且有时不确定的关系，抽象导致语义信息损失。LLM的出现为知识表示和推理方式带来新变化，推动以自然语言为基础的更丰富关系表达的需求。

Method: 本文提出将知识图谱中的关系表示方式从传统的符号化关系标签转变为自然语言描述，采用混合设计原则保留最小结构骨架，同时支持更灵活、具有上下文敏感性的关系表达。

Result: 提出了一种混合设计原则，结合最小结构化骨架与基于自然语言的关系描述，强调重新设计关系表示方式而非仅用LLM高效填充传统关系模式。

Conclusion: 传统符号化关系标签难以适应LLM时代的知识表示需求，未来KG关系表示应结合自然语言描述与结构骨架，实现更灵活、上下文敏感的表达，提高知识图谱的表现力与适用性。

Abstract: Knowledge graphs (KGs) have commonly been constructed using predefined symbolic relation schemas, typically implemented as categorical relation labels. This design has notable shortcomings: real-world relations are often contextual, nuanced, and sometimes uncertain, and compressing it into discrete relation labels abstracts away critical semantic detail. Nevertheless, symbolic-relation KGs remain widely used because they have been operationally effective and broadly compatible with pre-LLM downstream models and algorithms, in which KG knowledge could be retrieved or encoded into quantified features and embeddings at scale. The emergence of LLMs has reshaped how knowledge is created and consumed. LLMs support scalable synthesis of domain facts directly in concise natural language, and prompting-based inference favors context-rich free-form text over quantified representations. This position paper argues that these changes call for rethinking the representation of relations themselves rather than merely using LLMs to populate conventional schemas more efficiently. We therefore advocate moving from symbolic to natural-language relation descriptions, and we propose hybrid design principles that preserve a minimal structural backbone while enabling more flexible and context-sensitive relational representations.

</details>


### [31] [How Many Human Judgments Are Enough? Feasibility Limits of Human Preference Evaluation](https://arxiv.org/abs/2601.09084)
*Wilson Y. Lee*

Main category: cs.CL

TL;DR: 人类偏好评估中信号分布均匀且偏好差距小，需大量评判才能检测改进，合理设计评估协议和预算至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，使用人类偏好评估比较生成模型，但不清楚需要多少评判才能可靠地检测小幅改进。

Method: 分析人类偏好信号在不同提示中的分布，证明在信号分布均匀时，按比例分配评判是最优策略；通过大规模人类偏好数据集进行实证分析，比较不同评估协议和模态。

Result: 大部分比较属于信号均匀分布，存在较小的偏好差距，需要比常规收集更多的评判数量；经过专门设计的基准测试可以减少提示变异性，提升检测能力。

Conclusion: 人类评估结果中不确定或负面结果常因评估力度不足而非模型无差异，评估设计需明确考虑效应大小、预算和协议。

Abstract: Human preference evaluations are widely used to compare generative models, yet it remains unclear how many judgments are required to reliably detect small improvements. We show that when preference signal is diffuse across prompts (i.e., all prompt types are similarly informative), proportional allocation is minimax-optimal: no allocation strategy substantially improves detectability. Empirical analysis of large-scale human preference datasets shows that most comparisons fall into this diffuse regime, exhibiting small preference margins that require far more judgments than typically collected, even in well-sampled comparisons. These limits persist across evaluation protocols and modalities, including chat, image generation, and code generation with execution feedback. In contrast, curated benchmarks that reduce prompt induced variability systematically induce larger margins and improve detectability through a $1.5\times$ reduction in prompt-level variance. Our results show that inconclusive or negative human evaluation outcomes frequently reflect underpowered evaluation rather than model equivalence, underscoring the need to account explicitly for effect size, budget, and protocol design.

</details>


### [32] [SubTokenTest: A Practical Benchmark for Real-World Sub-token Understanding](https://arxiv.org/abs/2601.09089)
*Shuyang Hou,Yi Hu,Muhan Zhang*

Main category: cs.CL

TL;DR: 大型语言模型在基本字符级任务中表现欠佳，本文提出SubTokenTest评估子词理解能力，发现模型存在显著不足并分析了相关机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了大型语言模型在字符级任务中的不足，而这些不足在实际应用中如文本地图导航和结构化表格理解中至关重要，因此需要一个专门评估细粒度分词理解能力的基准。

Method: 提出了SubTokenTest基准，包括基于十个实用任务的评估，隔离了分词相关的失误，并对九种先进大型语言模型进行了全面测试，同时研究了测试时的规模调整和字符信息的隐藏状态编码。

Result: 使用SubTokenTest基准揭示了多款大型语言模型在子词理解上的薄弱点，显示测试时的模型规模调整对表现有影响，且分析了隐藏状态中字符信息的编码方式。

Conclusion: 当前大型语言模型在细粒度的字符级任务上存在不足，尤其是与分词机制相关的问题，这影响了其在实际应用中的表现。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their reasoning capabilities. However, they continue to struggle with basic character-level tasks, such as counting letters in words, a problem rooted in their tokenization process. While existing benchmarks have highlighted this weakness through basic character operations, such failures are often dismissed due to lacking practical relevance. Yet, many real-world applications, such as navigating text-based maps or interpreting structured tables, rely heavily on precise sub-token understanding. In this regard, we introduce SubTokenTest, a comprehensive benchmark that assesses sub-token understanding through practical, utility-driven tasks. Our benchmark includes ten tasks across four domains and isolates tokenization-related failures by decoupling performance from complex reasoning. We provide a comprehensive evaluation of nine advanced LLMs. Additionally, we investigate the impact of test-time scaling on sub-token reasoning and explore how character-level information is encoded within the hidden states.

</details>


### [33] [Contrastive Bi-Encoder Models for Multi-Label Skill Extraction: Enhancing ESCO Ontology Matching with BERT and Attention Mechanisms](https://arxiv.org/abs/2601.09119)
*Yongming Sun*

Main category: cs.CL

TL;DR: 提出零样本技能提取框架，利用大语言模型合成训练数据，结合层次多技能生成和对比学习，实现高效准确的招聘广告技能识别，适用于中文招聘市场。


<details>
  <summary>Details</summary>
Motivation: 细粒度的劳动力市场分析需要将非结构化的招聘广告映射到标准化的技能分类体系（如ESCO），但标注数据缺乏且代价高，尤其是在非英语环境中。

Method: 提出零样本技能提取框架，使用大语言模型从ESCO定义合成训练实例，引入层次约束的多技能生成，训练对比双编码器结合BERT、BiLSTM和注意力机制进行匹配，并用RoBERTa二分类过滤非技能句子。

Result: 层次条件生成提升了流畅性和判别力，模型在真实中文招聘广告上实现了强零样本检索性能（F1@5=0.72），优于TF-IDF和普通BERT基线。

Conclusion: 该方法为自动化技能编码提供了一种可扩展且数据高效的解决方案，尤其适用于非英语环境的劳动力市场分析。

Abstract: Fine-grained labor market analysis increasingly relies on mapping unstructured job advertisements to standardized skill taxonomies such as ESCO. This mapping is naturally formulated as an Extreme Multi-Label Classification (XMLC) problem, but supervised solutions are constrained by the scarcity and cost of large-scale, taxonomy-aligned annotations--especially in non-English settings where job-ad language diverges substantially from formal skill definitions. We propose a zero-shot skill extraction framework that eliminates the need for manually labeled job-ad training data. The framework uses a Large Language Model (LLM) to synthesize training instances from ESCO definitions, and introduces hierarchically constrained multi-skill generation based on ESCO Level-2 categories to improve semantic coherence in multi-label contexts. On top of the synthetic corpus, we train a contrastive bi-encoder that aligns job-ad sentences with ESCO skill descriptions in a shared embedding space; the encoder augments a BERT backbone with BiLSTM and attention pooling to better model long, information-dense requirement statements. An upstream RoBERTa-based binary filter removes non-skill sentences to improve end-to-end precision. Experiments show that (i) hierarchy-conditioned generation improves both fluency and discriminability relative to unconstrained pairing, and (ii) the resulting multi-label model transfers effectively to real-world Chinese job advertisements, achieving strong zero-shot retrieval performance (F1@5 = 0.72) and outperforming TF--IDF and standard BERT baselines. Overall, the proposed pipeline provides a scalable, data-efficient pathway for automated skill coding in labor economics and workforce analytics.

</details>


### [34] [Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment](https://arxiv.org/abs/2601.09120)
*Chen-Wei Liang,Bin Guo,Zhen-Yuan Wei,Mu-Jiang-Shan Wang*

Main category: cs.CL

TL;DR: 本文提出一种关系感知且跨领域自适应的专利权利要求生成框架，显著提升语义建模和质量评估效果，增强跨司法区泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前专利权利要求生成系统存在跨司法区泛化能力差、权利要求与现有技术语义关系建模不足及质量评估不可靠三大问题。

Method: 提出三阶段框架：关系感知相似性分析、多域适应性权利要求生成和统一质量评估。采用多头注意力机制建模权利要求与现有技术间的语义关系，结合课程学习和动态LoRA适配器选择，实现跨专利领域自适应。使用跨注意力机制实现各评估维度的统一质量评估。

Result: 在多个专利数据集上性能显著提升，包括相较GPT-4o的ROUGE-L提升7.6分，较Llama-3.1-8B的BERTScore提升8.3%，与人类专家评估的相关性提高至0.847。模型保持了89.4%的跨司法区性能，优于基线模型的76.2%。

Conclusion: 该方法有效解决了现有系统的主要瓶颈，提升了专利权利要求自动生成的准确性和评价可信度，为专利自动化办理流程提供了全面解决方案。

Abstract: Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\% cross-jurisdictional performance retention versus 76.2\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.

</details>


### [35] [Identity-Robust Language Model Generation via Content Integrity Preservation](https://arxiv.org/abs/2601.09141)
*Miao Zhang,Kelly Chen,Md Mehrab Tanjim,Rumi Chunara*

Main category: cs.CL

TL;DR: 本文针对大语言模型输出中因用户身份信息导致的核心质量下降问题，提出了一种无需训练的选择性身份信息中和框架，有效提升了输出公平性。


<details>
  <summary>Details</summary>
Motivation: 发现大语言模型对不同用户社会人口特征输出质量存在差异，且这种身份相关的响应质量下降并非源于事实知识缺失。

Method: 通过实验证明偏见生成行为导致身份相关的性能下降，提出选择性中和非关键身份信息的方法以保持语义完整性。

Result: 在四个基准测试和18种社会人口身份中，提出方法将身份偏见平均降低77%，较基于提示的防御措施降低45%。

Conclusion: 本文提出了一种无需训练的轻量级框架，有效中和大语言模型生成中的身份偏见，显著提升了核心响应质量的公平性。

Abstract: Large Language Model (LLM) outputs often vary across user sociodemographic attributes, leading to disparities in factual accuracy, utility, and safety, even for objective questions where demographic information is irrelevant. Unlike prior work on stereotypical or representational bias, this paper studies identity-dependent degradation of core response quality. We show empirically that such degradation arises from biased generation behavior, despite factual knowledge being robustly encoded across identities. Motivated by this mismatch, we propose a lightweight, training-free framework for identity-robust generation that selectively neutralizes non-critical identity information while preserving semantically essential attributes, thus maintaining output content integrity. Experiments across four benchmarks and 18 sociodemographic identities demonstrate an average 77% reduction in identity-dependent bias compared to vanilla prompting and a 45% reduction relative to prompt-based defenses. Our work addresses a critical gap in mitigating the impact of user identity cues in prompts on core generation quality.

</details>


### [36] [OrthoGeoLoRA: Geometric Parameter-Efficient Fine-Tuning for Structured Social Science Concept Retrieval on theWeb](https://arxiv.org/abs/2601.09185)
*Zeqiang Wang,Xinyue Wu,Chenxi Li,Zixi Chen,Nishanth Sastry,Jon Johnson,Suparna De*

Main category: cs.CL

TL;DR: 论文提出OrthoGeoLoRA，通过正交约束优化LoRA微调策略，在社会科学数字资源检索任务中显著提升效率和效果，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的全微调代价高昂，不适合资源有限的小型机构，PEFT和LoRA虽减轻成本但存在几何缺陷，需改进以更高效地适应基础模型。

Method: 引入了OrthoGeoLoRA方法，通过几何重参数化将LoRA更新形式变为类似SVD的形式，并利用Stiefel流形正交约束保持低秩因子的正交性，同时兼容主流优化器和微调流程。

Result: 在多语句编码器和ELSST分层概念检索基准测试中，OrthoGeoLoRA在同等低秩预算下，优于标准LoRA及多种PEFT变体，在排名指标上表现更优。

Conclusion: OrthoGeoLoRA通过对LoRA的低秩矩阵因子施加正交约束，解决了标准LoRA更新中的几何缺陷，显著提升了参数高效微调的表现。

Abstract: Large language models and text encoders increasingly power web-based information systems in the social sciences, including digital libraries, data catalogues, and search interfaces used by researchers, policymakers, and civil society. Full fine-tuning is often computationally and energy intensive, which can be prohibitive for smaller institutions and non-profit organizations in the Web4Good ecosystem. Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), reduces this cost by updating only a small number of parameters. We show that the standard LoRA update $ΔW = BA^\top$ has geometric drawbacks: gauge freedom, scale ambiguity, and a tendency toward rank collapse. We introduce OrthoGeoLoRA, which enforces an SVD-like form $ΔW = BΣA^\top$ by constraining the low-rank factors to be orthogonal (Stiefel manifold). A geometric reparameterization implements this constraint while remaining compatible with standard optimizers such as Adam and existing fine-tuning pipelines. We also propose a benchmark for hierarchical concept retrieval over the European Language Social Science Thesaurus (ELSST), widely used to organize social science resources in digital repositories. Experiments with a multilingual sentence encoder show that OrthoGeoLoRA outperforms standard LoRA and several strong PEFT variants on ranking metrics under the same low-rank budget, offering a more compute- and parameter-efficient path to adapt foundation models in resource-constrained settings.

</details>


### [37] [ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection](https://arxiv.org/abs/2601.09195)
*Tao Liu,Taiqiang Wu,Runming Yang,Shaoning Sun,Junjie Wang,Yujiu Yang*

Main category: cs.CL

TL;DR: 针对传统监督微调忽视语言一对多特性易导致过拟合问题，ProFit通过屏蔽低概率词降低非核心表达影响，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调中对单一答案的强制对齐导致模型过拟合非核心表达，且引入多答案虽可缓解但成本高昂，需寻找更高效的解决方案。

Method: 通过分析词语的概率与语义重要性关系，提出选择性屏蔽低概率词的技术，防止模型对表层非核心表达过拟合。

Result: 本文提出了一种名为ProFit的方法，通过选择性屏蔽低概率词，防止大语言模型在监督微调过程中对非核心表达的过拟合，从而提升了模型在推理和数学基准测试上的性能。

Conclusion: ProFit方法有效缓解了监督微调中单一参考答案所引发的非核心表达过拟合问题，提升了模型的泛化能力和推理表现。

Abstract: Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.

</details>


### [38] [A.X K1 Technical Report](https://arxiv.org/abs/2601.09200)
*Sung Jun Cheon,Jaekyung Cho,Seongho Choi,Hyunjun Eun,Seokhwan Jo,Jaehyun Jun,Minsoo Kang,Jin Kim,Jiwon Kim,Minsang Kim,Sungwan Kim,Seungsik Kim,Tae Yoon Kim,Youngrang Kim,Hyeongmun Lee,Sangyeol Lee,Sungeun Lee,Youngsoon Lee,Yujin Lee,Seongmin Ok,Chanyong Park,Hyewoong Park,Junyoung Park,Hyunho Yang,Subin Yi,Soohyun Bae,Dhammiko Arya,Yongseok Choi,Sangho Choi,Dongyeon Cho,Seungmo Cho,Gyoungeun Han,Yong-jin Han,Seokyoung Hong,Hyeon Hwang,Wonbeom Jang,Minjeong Ju,Wonjin Jung,Keummin Ka,Sungil Kang,Dongnam Kim,Joonghoon Kim,Jonghwi Kim,SaeRom Kim,Sangjin Kim,Seongwon Kim,Youngjin Kim,Seojin Lee,Sunwoo Lee,Taehoon Lee,Chanwoo Park,Sohee Park,Sooyeon Park,Yohan Ra,Sereimony Sek,Seungyeon Seo,Gun Song,Sanghoon Woo,Janghan Yoon,Sungbin Yoon*

Main category: cs.CL

TL;DR: 本文提出了大规模MoE语言模型A.X K1，通过创新训练方法提升推理能力和效率，并在多语言基准中展现出强劲表现，尤其擅长韩语任务。


<details>
  <summary>Details</summary>
Motivation: 旨在缩小推理能力与推理效率之间的差距，提升模型在实际场景中的部署灵活性和推理效果。

Method: 采用5190亿参数的MoE架构，利用规模定律优化训练配置和词汇大小，设计Think-Fusion训练方法实现思考模式的用户控制切换。

Result: 模型在基准测试中达到领先开源模型相当水平，特别在韩语任务上优势明显。

Conclusion: A.X K1在多个语言任务上性能优异，尤其在韩语测试中表现出显著优势。

Abstract: We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, curated by a multi-stage data processing pipeline. Designed to bridge the gap between reasoning capability and inference efficiency, A.X K1 supports explicitly controllable reasoning to facilitate scalable deployment across diverse real-world scenarios. We propose a simple yet effective Think-Fusion training recipe, enabling user-controlled switching between thinking and non-thinking modes within a single unified model. Extensive evaluations demonstrate that A.X K1 achieves performance competitive with leading open-source models, while establishing a distinctive advantage in Korean-language benchmarks.

</details>


### [39] [UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2601.09215)
*Feng Zhang,Shijia Li,Chunmao Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Jingwen Xu,Han Liu*

Main category: cs.CL

TL;DR: 提出了具备推理和策略思维能力的用户语言模型UserLM-R1，实现了更强的跨场景泛化和抗操控能力。


<details>
  <summary>Details</summary>
Motivation: 当前用户模拟器依赖静态且缺乏情境感知的用户画像，导致跨场景泛化能力差，且忽视了人类的策略思维，模型易被代理操控。

Method: 构建综合静态与动态用户画像；设计目标驱动的决策策略生成推理理由及响应；结合监督微调与多奖赏强化学习优化模型推理和策略能力。

Result: 提出了具备推理能力的用户语言模型UserLM-R1，构建了包含静态角色和动态场景目标的综合用户画像，通过驱动目标的决策生成高质量推理并响应，利用监督微调和多奖赏强化学习提升策略能力，实验表明其在复杂对抗场景中表现优于现有方法。

Conclusion: UserLM-R1通过综合用户画像和目标驱动决策策略，有效提升了用户模拟器的泛化能力和人类策略思维表达，显著优于现有基线模型，特别是在复杂和对抗性场景下。

Abstract: User simulators serve as the critical interactive environment for agent post-training, and an ideal user simulator generalizes across domains and proactively engages in negotiation by challenging or bargaining. However, current methods exhibit two issues. They rely on static and context-unaware profiles, necessitating extensive manual redesign for new scenarios, thus limiting generalizability. Moreover, they neglect human strategic thinking, leading to vulnerability to agent manipulation. To address these issues, we propose UserLM-R1, a novel user language model with reasoning capability. Specifically, we first construct comprehensive user profiles with both static roles and dynamic scenario-specific goals for adaptation to diverse scenarios. Then, we propose a goal-driven decision-making policy to generate high-quality rationales before producing responses, and further refine the reasoning and improve strategic capabilities with supervised fine-tuning and multi-reward reinforcement learning. Extensive experimental results demonstrate that UserLM-R1 outperforms competitive baselines, particularly on the more challenging adversarial set.

</details>


### [40] [When to Trust: A Causality-Aware Calibration Framework for Accurate Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2601.09241)
*Jing Ren,Bowen Li,Ziqi Xu,Xinkun Zhang,Haytham Fayek,Xiaodong Li*

Main category: cs.CL

TL;DR: 本研究通过因果意识校准框架Ca2KG改善KG-RAG模型的过度自信问题，实现更可靠且准确的知识图增强生成。


<details>
  <summary>Details</summary>
Motivation: KG-RAG模型在知识图检索增强生成中表现出过度自信问题，尤其在检索的子图不完整或不可靠时，可能导致高风险领域的应用隐患。

Method: 提出Ca2KG，一个基于因果意识校准框架，通过结合反事实提示和面板式重新评分机制，提升KG-RAG模型的置信度校准。

Result: Ca2KG在两个复杂问答数据集上的大量实验显示，显著提升了预测的置信度校准，同时保持甚至提高了模型的预测准确性。

Conclusion: Ca2KG有效解决了KG-RAG模型在知识不完整或不可靠情况下的过度自信问题，提升了模型在复杂任务中的校准质量和预测性能，有助于其在高风险应用场景中的安全部署。

Abstract: Knowledge Graph Retrieval-Augmented Generation (KG-RAG) extends the RAG paradigm by incorporating structured knowledge from knowledge graphs, enabling Large Language Models (LLMs) to perform more precise and explainable reasoning. While KG-RAG improves factual accuracy in complex tasks, existing KG-RAG models are often severely overconfident, producing high-confidence predictions even when retrieved sub-graphs are incomplete or unreliable, which raises concerns for deployment in high-stakes domains. To address this issue, we propose Ca2KG, a Causality-aware Calibration framework for KG-RAG. Ca2KG integrates counterfactual prompting, which exposes retrieval-dependent uncertainties in knowledge quality and reasoning reliability, with a panel-based re-scoring mechanism that stabilises predictions across interventions. Extensive experiments on two complex QA datasets demonstrate that Ca2KG consistently improves calibration while maintaining or even enhancing predictive accuracy.

</details>


### [41] [TeachPro: Multi-Label Qualitative Teaching Evaluation via Cross-View Graph Synergy and Semantic Anchored Evidence Encoding](https://arxiv.org/abs/2601.09246)
*Xiangqian Wang,Yifan Jia,Yang Xiang,Yumin Zhang,Yanbin Wang,Ke Liu*

Main category: cs.CL

TL;DR: 本文提出TeachPro，一种多标签学习框架，系统评估教学的五个关键维度，通过结构化语义空间将开放式学生评价转化为多维度反馈，提升教学评价的诊断细粒度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有学生教学评价依赖标准化调查和二元情感分析，存在可靠性低、反馈信息贫乏的问题，难以提供具体的教学改进建议。

Method: 提出了Dimension-Anchored Evidence Encoder和Cross-View Graph Synergy Network，分别将文本反馈转化为结构化语义嵌入，并融合句法和语义信息，通过跨注意力机制关联教学维度与评论表示。

Result: 在专家注释的多标签评分基准数据集上，TeachPro展示了优异的性能，能够更细粒度地捕捉教学反馈并适应多种评价场景。

Conclusion: TeachPro在多个教学评价维度上表现出优越的诊断能力和鲁棒性，能够为教学改进提供更具体和细致的指导。

Abstract: Standardized Student Evaluation of Teaching often suffer from low reliability, restricted response options, and response distortion. Existing machine learning methods that mine open-ended comments usually reduce feedback to binary sentiment, which overlooks concrete concerns such as content clarity, feedback timeliness, and instructor demeanor, and provides limited guidance for instructional improvement.We propose TeachPro, a multi-label learning framework that systematically assesses five key teaching dimensions: professional expertise, instructional behavior, pedagogical efficacy, classroom experience, and other performance metrics. We first propose a Dimension-Anchored Evidence Encoder, which integrates three core components: (i) a pre-trained text encoder that transforms qualitative feedback annotations into contextualized embeddings; (ii) a prompt module that represents five teaching dimensions as learnable semantic anchors; and (iii) a cross-attention mechanism that aligns evidence with pedagogical dimensions within a structured semantic space. We then propose a Cross-View Graph Synergy Network to represent student comments. This network comprises two components: (i) a Syntactic Branch that extracts explicit grammatical dependencies from parse trees, and (ii) a Semantic Branch that models latent conceptual relations derived from BERT-based similarity graphs. BiAffine fusion module aligns syntactic and semantic units, while a differential regularizer disentangles embeddings to encourage complementary representations. Finally, a cross-attention mechanism bridges the dimension-anchored evidence with the multi-view comment representations. We also contribute a novel benchmark dataset featuring expert qualitative annotations and multi-label scores. Extensive experiments demonstrate that TeachPro offers superior diagnostic granularity and robustness across diverse evaluation settings.

</details>


### [42] [When to Invoke: Refining LLM Fairness with Toxicity Assessment](https://arxiv.org/abs/2601.09250)
*Jing Ren,Bowen Li,Ziqi Xu,Renqiang Luo,Shuo Yu,Xin Ye,Haytham Fayek,Xiaodong Li,Feng Xia*

Main category: cs.CL

TL;DR: 本文提出FairToT，一种在推理阶段通过提示引导提升大语言模型（LLMs）公平性的框架，有效减少不同群体之间的偏差，保持毒性评估的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在毒性评估中对含蓄仇恨言论等微妙表达容易产生不一致判定，体现出难以通过训练修正的偏见，亟需确定何时启用纠正机制以确保评估的公平性和可靠性。

Method: 在推理阶段引入FairToT框架，利用两种可解释的公平性指标识别可能产生群体差异的案例，并在需要时应用额外的评估，从而提高判定的一致性，且无需修改模型参数。

Result: 在多个基准数据集上的实验结果表明，FairToT显著降低了群体层面的判定差异，同时保持了稳定且可靠的毒性预测能力。

Conclusion: FairToT通过推理时的额外评估机制和公平性指标，有效减少了群体间的毒性判定差异，提升了模型的公平性和可靠性。

Abstract: Large Language Models (LLMs) are increasingly used for toxicity assessment in online moderation systems, where fairness across demographic groups is essential for equitable treatment. However, LLMs often produce inconsistent toxicity judgements for subtle expressions, particularly those involving implicit hate speech, revealing underlying biases that are difficult to correct through standard training. This raises a key question that existing approaches often overlook: when should corrective mechanisms be invoked to ensure fair and reliable assessments? To address this, we propose FairToT, an inference-time framework that enhances LLM fairness through prompt-guided toxicity assessment. FairToT identifies cases where demographic-related variation is likely to occur and determines when additional assessment should be applied. In addition, we introduce two interpretable fairness indicators that detect such cases and improve inference consistency without modifying model parameters. Experiments on benchmark datasets show that FairToT reduces group-level disparities while maintaining stable and reliable toxicity predictions, demonstrating that inference-time refinement offers an effective and practical approach for fairness improvement in LLM-based toxicity assessment systems. The source code can be found at https://aisuko.github.io/fair-tot/.

</details>


### [43] [MCGA: A Multi-task Classical Chinese Literary Genre Audio Corpus](https://arxiv.org/abs/2601.09270)
*Yexing Du,Kaiyuan Liu,Bihe Zhang,Youcheng Pan,Bo Yang,Liangyu Huo,Xiyuan Zhang,Jian Xie,Daojing He,Yang Xiang,Ming Liu,Bin Qin*

Main category: cs.CL

TL;DR: 本文发布了一个涵盖六任务的中文古典文学音频数据集MCGA，并评测了现有多模态语言模型，揭示了模型在音频处理上的不足，推动中文古典研究中的音频应用发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大型语言模型在中文古典研究领域主要关注文本和视觉模态，音频语料未被充分探索。

Method: 构建多任务中文古典文学音频语料库，包括ASR、S2TT、SEC、SQA、SU和SR六个任务，设计了SEC和多模态一致性评价指标，并对十个模型进行了性能评估。

Result: 构建了包含六个任务的多任务古文音频数据集MCGA，并评估了十个MLLM模型，发现现有模型在该数据集上表现仍有较大提升空间。

Conclusion: 当前多模态大型语言模型在处理中文古典文学音频方面尚存挑战，MCGA数据集和评价指标将促进更强音频理解能力模型的研发。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), their potential has garnered significant attention in Chinese Classical Studies (CCS). While existing research has primarily focused on text and visual modalities, the audio corpus within this domain remains largely underexplored. To bridge this gap, we propose the Multi-task Classical Chinese Literary Genre Audio Corpus (MCGA). It encompasses a diverse range of literary genres across six tasks: Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Emotion Captioning (SEC), Spoken Question Answering (SQA), Speech Understanding (SU), and Speech Reasoning (SR). Through the evaluation of ten MLLMs, our experimental results demonstrate that current models still face substantial challenges when processed on the MCGA test set. Furthermore, we introduce an evaluation metric for SEC and a metric to measure the consistency between the speech and text capabilities of MLLMs. We release MCGA and our code to the public to facilitate the development of MLLMs with more robust multidimensional audio capabilities in CCS. MCGA Corpus: https://github.com/yxduir/MCGA

</details>


### [44] [ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering](https://arxiv.org/abs/2601.09280)
*Chaerin Lee,Sohee Park,Hyunsik Na,Daseon Choi*

Main category: cs.CL

TL;DR: 针对医疗问答中知识图谱噪声多和推理不稳定问题，提出了区域优先的子图构建与多步推理框架ReGraM，在准确率和幻觉率上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗问答方法往往依赖于遍历整个知识图谱或大规模检索，这会引入大量噪声并导致多跳推理不稳定，核心挑战是识别和推理针对每个查询的适当证据子集。

Method: ReGraM是一种基于区域优先的知识图谱推理框架，通过构建与查询对齐的子图，并在多种证据感知模式下进行逐步推理，限制推理在局部区域内进行。

Result: 在七个医疗问答基准测试上，ReGraM相较于强基线KGARevion在MCQ准确率提升了8.04%，SAQ提升4.50%，幻觉率降低42.9%。消融实验和定性分析证实了区域构建与逐跳推理的对齐是主要提升原因。

Conclusion: 区域优先的知识图谱推理为提升医疗问答中的事实准确性和一致性提供了有效范式。

Abstract: Recent studies in medical question answering (Medical QA) have actively explored the integration of large language models (LLMs) with biomedical knowledge graphs (KGs) to improve factual accuracy. However, most existing approaches still rely on traversing the entire KG or performing large-scale retrieval, which introduces substantial noise and leads to unstable multi-hop reasoning. We argue that the core challenge lies not in expanding access to knowledge, but in identifying and reasoning over the appropriate subset of evidence for each query. ReGraM is a region-first knowledge graph reasoning framework that addresses this challenge by constructing a query-aligned subgraph and performing stepwise reasoning constrained to this localized region under multiple evidence aware modes. By focusing inference on only the most relevant portion of the KG, ReGraM departs from the assumption that all relations are equally useful an assumption that rarely holds in domain-specific medical settings. Experiments on seven medical QA benchmarks demonstrate that ReGraM consistently outperforms a strong baseline (KGARevion), achieving an 8.04% absolute accuracy gain on MCQ, a 4.50% gain on SAQ, and a 42.9% reduction in hallucination rate. Ablation and qualitative analyses further show that aligning region construction with hop-wise reasoning is the primary driver of these improvements. Overall, our results highlight region-first KG reasoning as an effective paradigm for improving factual accuracy and consistency in medical QA.

</details>


### [45] [Understanding or Memorizing? A Case Study of German Definite Articles in Language Models](https://arxiv.org/abs/2601.09313)
*Jonathan Drechsel,Erisa Bytyqi,Steffen Herbold*

Main category: cs.CL

TL;DR: 语言模型对德语定冠词的处理更依赖记忆的关联而非抽象语法规则。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在德语定冠词的性别和格的语法一致性表现是否源于基于规则的泛化还是记忆。

Method: 利用GRADIEND梯度解释方法，学习性别-格特定的德语定冠词转换的参数更新方向。

Result: 发现针对特定性别-格转变的参数更新常常影响无关的性别-格设定，且受影响的神经元存在显著重叠。

Conclusion: 模型处理德语定冠词时并非严格基于规则编码，而部分依赖记忆形成的关联。

Abstract: Language models perform well on grammatical agreement, but it is unclear whether this reflects rule-based generalization or memorization. We study this question for German definite singular articles, whose forms depend on gender and case. Using GRADIEND, a gradient-based interpretability method, we learn parameter update directions for gender-case specific article transitions. We find that updates learned for a specific gender-case article transition frequently affect unrelated gender-case settings, with substantial overlap among the most affected neurons across settings. These results argue against a strictly rule-based encoding of German definite articles, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.

</details>


### [46] [Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework](https://arxiv.org/abs/2601.09342)
*Ewelina Gajewska,Katarzyna Budzynska,Jarosław A Chudziak*

Main category: cs.CL

TL;DR: 本文提出一种融合社会文化背景的多智能体隐性仇恨言论检测框架，提升了检测准确率与公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的隐性仇恨言论检测方法在识别和公平性方面存在不足，难以充分利用社会文化背景信息。

Method: 提出了一个多智能体系统，包括一个中央管理智能体和动态构建的代表特定人口群体的社区智能体，结合公开的社会文化知识实现身份感知的内容审核。

Result: 该方法在ToxiGen数据集上超越了零样本、少样本和链式思维等先进提示方法，显著提升了分类准确率和公平性。

Conclusion: 社区驱动的多智能体协同框架能够显著提升隐性仇恨言论检测的性能和公平性，是身份感知审核的有效解决方案。

Abstract: This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.

</details>


### [47] [Frame of Reference: Addressing the Challenges of Common Ground Representation in Situational Dialogs](https://arxiv.org/abs/2601.09365)
*Biswesh Mohapatra,Théo Charlot,Giovanni Duca,Mayank Palan,Laurent Romary,Justine Cassell*

Main category: cs.CL

TL;DR: 本文研究如何在情境对话中显式表示和利用共同知识，提升对话系统的理解与指代能力。


<details>
  <summary>Details</summary>
Motivation: 在情境对话中，参与者必须建立和维护共同的参考点以确保对话的连贯性，而目前对话系统缺乏明确表示和存储共同知识的机制。

Method: 评估模型通过关系性引用实体以建立和利用共同知识的能力，测试多种共同知识表示方法，并提出改进建立和利用共同知识的方法。

Result: 提出的方法改善了共同知识的建立和后续利用，提升了情境对话中的指代和理解能力。

Conclusion: 明确表示和存储共同知识对于对话系统中实现真正的理解和有效沟通至关重要。

Abstract: Common ground plays a critical role in situated spoken dialogues, where interlocutors must establish and maintain shared references to entities, events, and relations to sustain coherent interaction. For dialog systems, the ability to correctly ground conversational content in order to refer back to it later is particularly important. Prior studies have demonstrated that LLMs are capable of performing grounding acts such as requesting clarification or producing acknowledgments, yet relatively little work has investigated how common ground can be explicitly represented and stored for later use. Without such mechanisms, it remains unclear whether acknowledgment or clarification behaviors truly reflect a grounded understanding. In this work, we evaluate a model's ability to establish and exploit common ground through relational references to entities within the shared context in a situational dialogue. We test multiple methods for representing common ground in situated dialogues and further propose approaches to improve both the establishment of common ground and its subsequent use in the conversation.

</details>


### [48] [Relation Extraction Capabilities of LLMs on Clinical Text: A Bilingual Evaluation for English and Turkish](https://arxiv.org/abs/2601.09367)
*Aidana Aidynkyzy,Oğuz Dikenelli,Oylum Alatlı,Şebnem Bora*

Main category: cs.CL

TL;DR: 该研究构建了英土双语临床关系抽取数据集，提出了关系感知检索方法，并通过多种提示方式验证了LLM在临床关系抽取任务上的有效性，英语表现优于土耳其语。


<details>
  <summary>Details</summary>
Motivation: 临床信息抽取在非英语语言中缺乏标注数据集，限制了基于大型语言模型在该领域的评估。

Method: 构建英土双语临床关系抽取数据集，采用多种提示策略（包括多示例学习和链式思维），并设计了基于对比学习的RAR方法进行示例检索，结合结构化推理提示以提升模型性能。

Result: 提出了首个英土双语临床关系抽取并行数据集，开发了基于对比学习的关系感知检索方法（RAR），通过多种提示策略评估了LLM，发现基于提示的方法优于传统微调模型，且RAR结合结构化推理提示获得最佳性能。

Conclusion: 高质量的示例检索和先进的提示技术能显著提升临床关系抽取性能，推动非英语资源匮乏语言的临床NLP发展。

Abstract: The scarcity of annotated datasets for clinical information extraction in non-English languages hinders the evaluation of large language model (LLM)-based methods developed primarily in English. In this study, we present the first comprehensive bilingual evaluation of LLMs for the clinical Relation Extraction (RE) task in both English and Turkish. To facilitate this evaluation, we introduce the first English-Turkish parallel clinical RE dataset, derived and carefully curated from the 2010 i2b2/VA relation classification corpus. We systematically assess a diverse set of prompting strategies, including multiple in-context learning (ICL) and Chain-of-Thought (CoT) approaches, and compare their performance to fine-tuned baselines such as PURE. Furthermore, we propose Relation-Aware Retrieval (RAR), a novel in-context example selection method based on contrastive learning, that is specifically designed to capture both sentence-level and relation-level semantics. Our results show that prompting-based LLM approaches consistently outperform traditional fine-tuned models. Moreover, evaluations for English performed better than their Turkish counterparts across all evaluated LLMs and prompting techniques. Among ICL methods, RAR achieves the highest performance, with Gemini 1.5 Flash reaching a micro-F1 score of 0.906 in English and 0.888 in Turkish. Performance further improves to 0.918 F1 in English when RAR is combined with a structured reasoning prompt using the DeepSeek-V3 model. These findings highlight the importance of high-quality demonstration retrieval and underscore the potential of advanced retrieval and prompting techniques to bridge resource gaps in clinical natural language processing.

</details>


### [49] [The Imperfective Paradox in Large Language Models](https://arxiv.org/abs/2601.09373)
*Bolei Ma,Yusuke Miyao*

Main category: cs.CL

TL;DR: 本文通过设计诊断数据集，揭示了大型语言模型在事件时态理解上的系统性误区，表明其更像预测叙事机而非逻辑推理者。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否真正理解事件的构成语义，还是仅凭表面概率模式进行预测，特别关注不同时态表述下事件完成状态的逻辑推断。

Method: 设计了ImperfectiveNLI诊断数据集，考察不同时态的语义区分，对多种开放权重的先进模型进行评估，并通过表征分析和提示词干预方法进一步探讨模型推理机制。

Result: 发现模型普遍存在目的论偏差，常常错误地将目标导向事件推断为完成状态，提示词干预虽能减少幻觉完成但也增加误拒现象，内部向量表示区分过程与结果，但推理主要受动机先验影响。

Conclusion: 当前大型语言模型在理解事件的构成语义上存在局限，倾向于依赖表层概率启发式，缺乏结构性时态意识，表现为对目标导向事件驱动的完成状态存在系统性幻觉。

Abstract: Do Large Language Models (LLMs) genuinely grasp the compositional semantics of events, or do they rely on surface-level probabilistic heuristics? We investigate the Imperfective Paradox, a logical phenomenon where the past progressive aspect entails event realization for activities (e.g., running $\to$ ran) but not for accomplishments (e.g., building $\nrightarrow$ built). We introduce ImperfectiveNLI, a diagnostic dataset designed to probe this distinction across diverse semantic classes. Evaluating state-of-the-art open-weight models, we uncover a pervasive Teleological Bias: models systematically hallucinate completion for goal-oriented events, often overriding explicit textual negation. Representational analyses show that while internal embeddings often distinguish process from result, inference decisions are dominated by strong priors about goal attainment. We further find that prompting-based interventions reduce hallucinated completions but also increase incorrect rejections of valid entailments. Our findings suggest that current LLMs lack structural aspectual awareness, operating as predictive narrative engines rather than faithful logical reasoners.

</details>


### [50] [Ability Transfer and Recovery via Modularized Parameters Localization](https://arxiv.org/abs/2601.09398)
*Songyao Jin,Kun Zhou,Wenqi Li,Peng Wang,Biwei Huang*

Main category: cs.CL

TL;DR: 本文发现大语言模型的能力激活集中且可独立迁移，提出ACT方法实现能力选择性转移，有效解决专门化导致的遗忘，提升模型多能力集成表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的持续训练或微调虽能提升特定能力，但往往导致其他能力退化及灾难性遗忘，如何在优化特定能力的同时保护已有能力成为关键问题。

Method: 通过分析大语言模型在特定领域和语言输入下的模块激活，定位出能力相关的通道，并提出ACT方法——一种基于激活差异引导通道选择的能力迁移方法，结合轻量微调实现参数迁移和模型兼容。

Result: ACT方法在多语言数学和科学推理任务中表现出有效的能力恢复，能在保持已有能力的同时消除遗忘，并支持将多个专长模型融合成一个具有多项能力的模型，且干扰极小。

Conclusion: 本论文发现大语言模型中与能力相关的激活主要集中在少数通道，这些通道相对独立且稳定。提出的ACT方法通过定位这些能力通道并选择性迁移参数，有效避免了灾难性遗忘，实现了能力的恢复和多模型能力集成。

Abstract: Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.

</details>


### [51] [Structured Knowledge Representation through Contextual Pages for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.09402)
*Xinze Li,Zhenghao Liu,Haidong Xin,Yukun Yan,Shuo Wang,Zheni Zeng,Sen Mei,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了基于页面驱动的自主管理框架PAGER，通过结构化认知大纲和迭代检索优化知识组织，提升大语言模型的检索增强生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有迭代知识积累过程缺乏连贯组织结构，限制了更全面和一致的知识表征构建，亟需一种结构化且自主的知识表征方法以提升RAG模型性能。

Method: PAGER首先通过大语言模型生成结构化认知大纲，将问题划分为多个知识槽位，之后迭代检索和优化相关文档以填充每个槽位，最终形成连贯的‘页面’作为生成答案的上下文输入。

Result: 在多种知识密集型基准测试和模型上，PAGER表现优于所有RAG基线，提升知识表示质量，降低知识冲突，更有效地帮助大语言模型利用外部知识。

Conclusion: PAGER框架能够构建更高质量、更丰富的信息密度的知识表示，有效缓解知识冲突，提升大语言模型利用外部知识的能力，优于所有现有的RAG基线。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating external knowledge. Recently, some works have incorporated iterative knowledge accumulation processes into RAG models to progressively accumulate and refine query-related knowledge, thereby constructing more comprehensive knowledge representations. However, these iterative processes often lack a coherent organizational structure, which limits the construction of more comprehensive and cohesive knowledge representations. To address this, we propose PAGER, a page-driven autonomous knowledge representation framework for RAG. PAGER first prompts an LLM to construct a structured cognitive outline for a given question, which consists of multiple slots representing a distinct knowledge aspect. Then, PAGER iteratively retrieves and refines relevant documents to populate each slot, ultimately constructing a coherent page that serves as contextual input for guiding answer generation. Experiments on multiple knowledge-intensive benchmarks and backbone models show that PAGER consistently outperforms all RAG baselines. Further analyses demonstrate that PAGER constructs higher-quality and information-dense knowledge representations, better mitigates knowledge conflicts, and enables LLMs to leverage external knowledge more effectively. All code is available at https://github.com/OpenBMB/PAGER.

</details>


### [52] [Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing](https://arxiv.org/abs/2601.09421)
*Filip Trhlik,Andrew Caines,Paula Buttery*

Main category: cs.CL

TL;DR: 本文提出使用低成本的BabyLM模型作为大型预训练语言模型的代理，来研究和消除其偏见，显著降低了训练成本，推动了公平语言模型的构建。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型成本高昂，限制了偏见理解与消除研究的发展，迫切需要低成本、易用的模型平台。

Method: 通过训练紧凑的类似BERT的BabyLM模型，并分析其偏见形成与表现与大型BERT模型的对应关系，进而在BabyLM上进行预训练偏见消除实验。

Result: BabyLM展示出与标准BERT在偏见形成和性能发展上的高度相关性，并成功复现并扩展了关于性别不平衡和有害内容对偏见影响的研究结果。

Conclusion: BabyLM模型作为低成本代理模型，能有效模拟大型预训练语言模型的偏见形成过程与表现，促进了预训练偏见消除研究的普及和加速。

Abstract: Pre-trained language models (LMs) have, over the last few years, grown substantially in both societal adoption and training costs. This rapid growth in size has constrained progress in understanding and mitigating their biases. Since re-training LMs is prohibitively expensive, most debiasing work has focused on post-hoc or masking-based strategies, which often fail to address the underlying causes of bias. In this work, we seek to democratise pre-model debiasing research by using low-cost proxy models. Specifically, we investigate BabyLMs, compact BERT-like models trained on small and mutable corpora that can approximate bias acquisition and learning dynamics of larger models. We show that BabyLMs display closely aligned patterns of intrinsic bias formation and performance development compared to standard BERT models, despite their drastically reduced size. Furthermore, correlations between BabyLMs and BERT hold across multiple intra-model and post-model debiasing methods. Leveraging these similarities, we conduct pre-model debiasing experiments with BabyLMs, replicating prior findings and presenting new insights regarding the influence of gender imbalance and toxicity on bias formation. Our results demonstrate that BabyLMs can serve as an effective sandbox for large-scale LMs, reducing pre-training costs from over 500 GPU-hours to under 30 GPU-hours. This provides a way to democratise pre-model debiasing research and enables faster, more accessible exploration of methods for building fairer LMs.

</details>


### [53] [Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models](https://arxiv.org/abs/2601.09445)
*Minh Vu Pham,Hsuvas Borkakoty,Yufang Hou*

Main category: cs.CL

TL;DR: 提出基于机械解释的框架，定位并控制语言模型内部预训练冲突知识，提升对知识冲突的理解与解决能力。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型内存知识冲突问题，尤其是预训练期间在模型内部表示中产生的冲突未被充分研究。

Method: 基于机械解释性方法设计框架，定位并识别预训练数据中冲突知识在语言模型内部的编码位置和方式。

Result: 发现语言模型的特定内部组件负责编码预训练中的冲突知识，并展示机械解释性方法可用于推理时对冲突知识进行因果干预和控制。

Conclusion: 语言模型内部特定组件承担编码冲突知识的功能，机械解释方法可用来定位和干预这些知识冲突，从而有效控制推理中的知识冲突。

Abstract: In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While prior work has primarily focused on resolving conflicts between a model's internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model's internal representations remain unexplored. In this work, we design a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from the pre-training data is encoded within LMs. Our findings contribute to a growing body of evidence that specific internal components of a language model are responsible for encoding conflicting knowledge from pre-training, and we demonstrate how mechanistic interpretability methods can be leveraged to causally intervene in and control conflicting knowledge at inference time.

</details>


### [54] [Improving Symbolic Translation of Language Models for Logical Reasoning](https://arxiv.org/abs/2601.09446)
*Ramya Keerthy Thatikonda,Jiuzhou Han,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 本文提出针对小型语言模型的错误分类、微调、增量推理以及验证模块结合的框架，有效提升了小型模型进行自然语言到一阶逻辑翻译的准确性和推理性能。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在将自然语言翻译为一阶逻辑的过程中易出错，现有基于自迭代纠错的方法受模型能力限制，难以实现可靠的符号推理系统，故需设计新的方法提升这些模型的翻译准确性和推理性能。

Method: 本文采用错误分类方法，利用大型语言模型合成训练数据对小型模型进行全面微调，并设计了增量推理框架，将推理任务分为谓词生成和一阶逻辑翻译两个阶段，同时引入针对谓词元数错误的验证模块，增强生成质量和错误修正能力。

Result: 本文针对小型语言模型在将自然语言翻译成一阶逻辑（FOL）过程中常见的格式和翻译错误，提出了一种综合方法，包括错误分类、利用大型语言模型生成数据进行微调、小型模型的增量推理和引入验证模块。通过将推理过程分为谓词生成和FOL翻译两个阶段，实现了对模型行为的更好控制，提升了生成质量，同时验证模块针对谓词元数错误进行修正，进一步提升性能。该方法在三个模型家族和四个逻辑推理数据集上的评估显示，显著降低了错误率，提升了谓词覆盖率和推理性能，推动了小型语言模型构建可靠且可访问的符号推理系统。

Conclusion: 通过错误分类、使用大模型合成数据进行微调、增量推理分阶段控制及引入验证模块，显著提升了小型语言模型的翻译准确率和推理表现，促进了可靠符号推理系统的实现。

Abstract: The use of formal language for deductive logical reasoning aligns well with language models (LMs), where translating natural language (NL) into first-order logic (FOL) and employing an external solver results in a verifiable and therefore reliable reasoning system. However, smaller LMs often struggle with this translation task, frequently producing incorrect symbolic outputs due to formatting and translation errors. Existing approaches typically rely on self-iteration to correct these errors, but such methods depend heavily on the capabilities of the underlying model. To address this, we first categorize common errors and fine-tune smaller LMs using data synthesized by large language models. The evaluation is performed using the defined error categories. We introduce incremental inference, which divides inference into two stages, predicate generation and FOL translation, providing greater control over model behavior and enhancing generation quality as measured by predicate metrics. This decomposition framework also enables the use of a verification module that targets predicate-arity errors to further improve performance. Our study evaluates three families of models across four logical-reasoning datasets. The comprehensive fine-tuning, incremental inference, and verification modules reduce error rates, increase predicate coverage, and improve reasoning performance for smaller LMs, moving us closer to developing reliable and accessible symbolic-reasoning systems.

</details>


### [55] [SlidesGen-Bench: Evaluating Slides Generation via Computational and Quantitative Metrics](https://arxiv.org/abs/2601.09487)
*Yunqiao Yang,Wenbo Li,Houxing Ren,Zimu Lu,Ke Wang,Zhiyuan Huang,Zhuofan Zong,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出统一且量化的幻灯片生成评测基准SlidesGen-Bench，能够可靠衡量不同系统效果并高度符合人类喜好。


<details>
  <summary>Details</summary>
Motivation: 现有自动幻灯片生成系统多样但缺乏统一且可靠的评测标准，导致不同架构间结果难以比较。

Method: 提出SlidesGen-Bench基准，通过视觉渲染统一输出并量化评价幻灯片内容、审美和可编辑性，结合人工偏好数据培训和验证评测方法。

Result: SlidesGen-Bench在内容、美学和可编辑性三个维度提供可重复的量化指标，构建Slides-Align1.5k数据集实现高达人类偏好相关性。

Conclusion: SlidesGen-Bench显著优于现有评测协议，更有效地反映不同自动幻灯片生成系统的性能表现，为研究和应用提供了可靠的评测工具。

Abstract: The rapid evolution of Large Language Models (LLMs) has fostered diverse paradigms for automated slide generation, ranging from code-driven layouts to image-centric synthesis. However, evaluating these heterogeneous systems remains challenging, as existing protocols often struggle to provide comparable scores across architectures or rely on uncalibrated judgments. In this paper, we introduce SlidesGen-Bench, a benchmark designed to evaluate slide generation through a lens of three core principles: universality, quantification, and reliability. First, to establish a unified evaluation framework, we ground our analysis in the visual domain, treating terminal outputs as renderings to remain agnostic to the underlying generation method. Second, we propose a computational approach that quantitatively assesses slides across three distinct dimensions - Content, Aesthetics, and Editability - offering reproducible metrics where prior works relied on subjective or reference-dependent proxies. Finally, to ensure high correlation with human preference, we construct the Slides-Align1.5k dataset, a human preference aligned dataset covering slides from nine mainstream generation systems across seven scenarios. Our experiments demonstrate that SlidesGen-Bench achieves a higher degree of alignment with human judgment than existing evaluation pipelines. Our code and data are available at https://github.com/YunqiaoYang/SlidesGen-Bench.

</details>


### [56] [MVSS: A Unified Framework for Multi-View Structured Survey Generation](https://arxiv.org/abs/2601.09504)
*Yinqi Liu,Yueqi Zhu,Yongkang Zhang,Xinfeng Li,Feiran Liu,Yufei Sun,Xin Wang,Renzhao Liang,Yidong Wang,Cunxiang Wang*

Main category: cs.CL

TL;DR: 本文提出MVSS框架，通过多视角结构化生成提升自动调研的层级关系建模和结构化比较能力，使生成综述更具组织性和证据支撑，效果媲美专家综述。


<details>
  <summary>Details</summary>
Motivation: 现有自动调研生成方法难以显式建模研究主题之间的层级关系及结构化方法比较，导致生成结构组织难以与专家撰写的综述相媲美。

Method: 提出了MVSS多视角结构化调研生成框架，首先构建研究领域的概念树，再生成受限于概念树的比较表，最后利用二者作为文本生成的结构约束，实现多视角互补表示。

Result: 在76个计算机科学主题上的实验表明，MVSS在结构组织和证据依据方面优于现有方法，性能接近专家调研水平。

Conclusion: MVSS有效整合层级主题结构与比较表，显著提升自动文献综述的组织质量和引用准确性，可作为专家调研的有力补充。

Abstract: Scientific surveys require not only summarizing large bodies of literature, but also organizing them into clear and coherent conceptual structures. Existing automatic survey generation methods typically focus on linear text generation and struggle to explicitly model hierarchical relations among research topics and structured methodological comparisons, resulting in gaps in structural organization compared to expert-written surveys. We propose MVSS, a multi-view structured survey generation framework that jointly generates and aligns citation-grounded hierarchical trees, structured comparison tables, and survey text. MVSS follows a structure-first paradigm: it first constructs a conceptual tree of the research domain, then generates comparison tables constrained by the tree, and finally uses both as structural constraints for text generation. This enables complementary multi-view representations across structure, comparison, and narrative. We introduce an evaluation framework assessing structural quality, comparative completeness, and citation fidelity. Experiments on 76 computer science topics show MVSS outperforms existing methods in organization and evidence grounding, achieving performance comparable to expert surveys.

</details>


### [57] [SERM: Self-Evolving Relevance Model with Agent-Driven Learning from Massive Query Streams](https://arxiv.org/abs/2601.09515)
*Chenglong Wang,Canjia Li,Xingzhao Zhu,Yifu Huo,Huiyu Wang,Weixiong Lin,Yun Yang,Qiaozhi He,Tianhua Zhou,Xiaojia Chang,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 针对动态查询流导致相关性模型难泛化的问题，本文提出了多智能体自我进化模型SERM，通过样本挖掘和多层标注策略，提高了模型性能，在工业环境中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现实世界的查询流不断动态变化，使得相关性模型难以在实际搜索场景中泛化。

Method: 提出了自我进化相关性模型（SERM），包含多智能体样本挖掘器和多智能体相关性标注器两大模块，分别用于检测分布变化、识别信息样本和生成可靠的伪标签。

Result: 在大规模工业环境中进行评估，通过离线多语言测试和在线测试验证，SERM通过迭代自我进化实现了显著的性能提升。

Conclusion: SERM方法有效解决了查询流中信息样本稀缺和伪标签不可靠的问题，提升了模型在实际大规模搜索场景中的表现。

Abstract: Due to the dynamically evolving nature of real-world query streams, relevance models struggle to generalize to practical search scenarios. A sophisticated solution is self-evolution techniques. However, in large-scale industrial settings with massive query streams, this technique faces two challenges: (1) informative samples are often sparse and difficult to identify, and (2) pseudo-labels generated by the current model could be unreliable. To address these challenges, in this work, we propose a Self-Evolving Relevance Model approach (SERM), which comprises two complementary multi-agent modules: a multi-agent sample miner, designed to detect distributional shifts and identify informative training samples, and a multi-agent relevance annotator, which provides reliable labels through a two-level agreement framework. We evaluate SERM in a large-scale industrial setting, which serves billions of user requests daily. Experimental results demonstrate that SERM can achieve significant performance gains through iterative self-evolution, as validated by extensive offline multilingual evaluations and online testing.

</details>


### [58] [Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats](https://arxiv.org/abs/2601.09555)
*Manyi Zhang,Ji-Fu Li,Zhongao Sun,Haoli Bai,Hui-Ling Zhen,Zhenhua Dong,Xianzhi Yu*

Main category: cs.CL

TL;DR: 本研究系统评估了后训练量化在微尺度浮点格式下的表现，指出MXFP8优于MXFP4，提出预缩放优化策略，促进了MXFP量化在大型语言模型的实用化。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练量化(PTQ)方法主要关注整数量化，而在微尺度浮点(MXFP)格式下的适用性和表现尚未被深入研究。

Method: 系统性研究在MXFP格式下的PTQ表现，涵盖7种以上的PTQ算法、15个评测基准和3个大型语言模型家族。

Result: 发现MXFP8格式表现接近无损，MXFP4性能显著下降且仍有挑战；PTQ效果受格式兼容性影响明显，且不同算法范式效果有差异；多模态模型中量化敏感性主要由语言模型决定；量化缩放因子是MXFP4的关键误差来源，预缩放策略可显著降低误差。

Conclusion: 该研究为将现有PTQ方法适配到MXFP量化提供了实用指导，推动了低精度格式在大型语言模型中的应用。

Abstract: Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain largely unexplored. To address this gap, this work conducts a systematic investigation of PTQ under MXFP formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. The key findings include: 1) MXFP8 consistently achieves near-lossless performance, while MXFP4 introduces substantial accuracy degradation and remains challenging; 2) PTQ effectiveness under MXFP depends strongly on format compatibility, with some algorithmic paradigms being consistently more effective than others; 3) PTQ performance exhibits highly consistent trends across model families and modalities, in particular, quantization sensitivity is dominated by the language model rather than the vision encoder in multimodal LLMs; 4) The scaling factor of quantization is a critical error source in MXFP4, and a simple pre-scale optimization strategy can significantly mitigate its impact. Together, these results provide practical guidance on adapting existing PTQ methods to MXFP quantization.

</details>


### [59] [Dialogue Telemetry: Turn-Level Instrumentation for Autonomous Information Gathering](https://arxiv.org/abs/2601.09570)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.CL

TL;DR: 本文提出了一种无需依赖特定模型的新框架，通过两个信号监控对话进展与停滞，提升了自主信息收集系统的实时监控与策略优化能力。


<details>
  <summary>Details</summary>
Motivation: 当前自主系统缺乏对每轮对话效率进行监控的指标，难以及时发现提问无效导致的对话停滞，影响信息收集效果。

Method: 通过设计两个模型无关的信号——进展估计器(PE)和停滞指数(SI)，分别量化剩余信息潜力和检测重复且低收益的提问模式，并在基于大语言模型的模拟搜索与救援访谈场景中验证。

Result: DT成功区分了高效与停滞的对话轨迹，支持对话策略的强化学习优化，提升了在有停滞成本场景下的策略表现。

Conclusion: 本文提出的Dialogue Telemetry (DT)框架有效提升了自主系统在信息收集对话中的监控能力，能够实时检测对话效率和停滞状态，从而辅助优化对话策略。

Abstract: Autonomous systems conducting schema-grounded information-gathering dialogues face an instrumentation gap, lacking turn-level observables for monitoring acquisition efficiency and detecting when questioning becomes unproductive. We introduce Dialogue Telemetry (DT), a measurement framework that produces two model-agnostic signals after each question-answer exchange: (i) a Progress Estimator (PE) quantifying residual information potential per category (with a bits-based variant), and (ii) a Stalling Index (SI) detecting an observable failure signature characterized by repeated category probing with semantically similar, low-marginal-gain responses. SI flags this pattern without requiring causal diagnosis, supporting monitoring in settings where attributing degradation to specific causes may be impractical. We validate DT in controlled search-and-rescue (SAR)-inspired interviews using large language model (LLM)-based simulations, distinguishing efficient from stalled dialogue traces and illustrating downstream utility by integrating DT signals into a reinforcement learning (RL) policy. Across these settings, DT provides interpretable turn-level instrumentation that improves policy performance when stalling carries operational costs.

</details>


### [60] [DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing](https://arxiv.org/abs/2601.09609)
*Qian Cao,Yahui Liu,Wei Bi,Yi Zhao,Ruihua Song,Xiting Wang,Ruiming Tang,Guorui Zhou,Han Li*

Main category: cs.CL

TL;DR: 本文针对强化学习增强的语言模型输出多样性不足问题，提出利用长链式思维和多样性规划分支提升多样性，且实验验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习增强的语言模型缺乏有效机制引导多样化输出，导致在开放式创造性任务中表现有限，本文旨在解决该问题。

Method: 设计了一个基于半结构化长链式思维的强化学习框架，采用多样性规划分支方法和组感知多样性奖励引导多样化探索。

Result: 本文提出了一种基于强化学习的半结构化长链式思维（Chain-of-Thought, CoT）框架，用于提升大型语言模型的输出多样性。通过在规划阶段引入多样性分支策略以及组感知多样性奖励，显著增强了生成文本的多样性，同时保证了生成质量。

Conclusion: 所提方法有效提高了大型语言模型在创造性写作中的输出多样性，且未降低生成质量，优于现有基线方法。

Abstract: Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.

</details>


### [61] [LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation](https://arxiv.org/abs/2601.09631)
*Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 针对希腊语押韵检测和生成，本文提出结合大语言模型和音韵算法的混合系统，显著提升性能，发布了大规模押韵语料。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理需要语音基础的现象（如押韵检测和生成）方面表现不佳，尤其是在资源较少的现代希腊语中。

Method: 结合大语言模型与确定性的音韵算法，建立希腊语押韵类型的全面分类体系，并采用带有音韵验证的生成管道。评估多种提示策略（如零样本、少样本、链式思维、RAG增强）在多个模型上的表现。

Result: 通过混合系统，押韵识别准确率显著提升（最高达54%），纯大语言模型生成效果极差（有效诗句低于4%），而混合验证循环生成性能提升至73.1%。发布了经过严格清理的4万条押韵语料库。

Conclusion: 纯大语言模型难以胜任基于语音的押韵任务，融合确定性音韵算法的混合方法能显著提高押韵识别与生成的准确性，对低资源语言研究具有重要意义。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant "Reasoning Gap": while native-like models (Claude 3.7) perform intuitively (40\% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54\%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4\% valid poems), while our hybrid verification loop restores performance to 73.1\%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.

</details>


### [62] [TaxoBell: Gaussian Box Embeddings for Self-Supervised Taxonomy Expansion](https://arxiv.org/abs/2601.09633)
*Sahil Mishra,Srinitish Srinivasan,Srikanta Bedathur,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 该论文提出一种新的高斯盒嵌入方法TaxoBell，改进了语义层次结构扩展中的不对称关系建模，显著提升分类拓展性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于点向量的嵌入方法难以有效建模非对称的“is-a”关系，传统盒子嵌入虽能表示包含关系但存在梯度不稳定、无法表示语义不确定及多义性等缺陷。

Method: 提出TaxoBell，一种将盒子几何和多变量高斯分布相结合的高斯盒嵌入框架，实现语义位置和不确定性的建模。通过能量优化方法解决交叉边界梯度不稳定、语义不确定性缺失和多义性表示能力有限等问题。

Result: TaxoBell在五个基准数据集上相较八种最先进的自动分类法扩展方法，MRR提升19%，Recall@k提升约25%，表现显著优越。通过误差分析和消融实验进一步验证了方法的有效性和局限性。

Conclusion: TaxoBell有效解决了传统盒子嵌入方法中的不足，实现了稳定优化、模糊概念建模和层次推理，显著提升了自动分类扩展的性能。

Abstract: Taxonomies form the backbone of structured knowledge representation across diverse domains, enabling applications such as e-commerce catalogs, semantic search, and biomedical discovery. Yet, manual taxonomy expansion is labor-intensive and cannot keep pace with the emergence of new concepts. Existing automated methods rely on point-based vector embeddings, which model symmetric similarity and thus struggle with the asymmetric "is-a" relationships that are fundamental to taxonomies. Box embeddings offer a promising alternative by enabling containment and disjointness, but they face key issues: (i) unstable gradients at the intersection boundaries, (ii) no notion of semantic uncertainty, and (iii) limited capacity to represent polysemy or ambiguity. We address these shortcomings with TaxoBell, a Gaussian box embedding framework that translates between box geometries and multivariate Gaussian distributions, where means encode semantic location and covariances encode uncertainty. Energy-based optimization yields stable optimization, robust modeling of ambiguous concepts, and interpretable hierarchical reasoning. Extensive experimentation on five benchmark datasets demonstrates that TaxoBell significantly outperforms eight state-of-the-art taxonomy expansion baselines by 19% in MRR and around 25% in Recall@k. We further demonstrate the advantages and pitfalls of TaxoBell with error analysis and ablation studies.

</details>


### [63] [Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation](https://arxiv.org/abs/2601.09648)
*Andrew Moore,Paul Rayson,Dawn Archer,Tim Czerniak,Dawn Knight,Daisy Lal,Gearóid Ó Donnchadha,Mícheál Ó Meachair,Scott Piao,Elaine Uí Dhonnchadha,Johanna Vuorinen,Yan Yabo,Xiaobin Yang*

Main category: cs.CL

TL;DR: 本论文扩展了USAS语义标注系统的评估范围，结合神经网络模型提升规则系统，发布了多语种数据和代码，推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: USAS框架缺乏大规模和多语言的语义标注评估，尤其是在规则系统评估领域存在空白。

Method: 构建新的英语银标注数据集，结合现有中英文及多语言数据，训练和评估多种单语和多语神经网络模型，比较其与规则系统的表现，并融合神经网络提升规则系统。

Result: 完成了基于USAS框架的最大规模多语言语义标注评测，发布了神经网络模型、训练数据、中文评测数据集和代码，验证了神经网络对规则系统的提升效果。

Conclusion: 本研究首次对USAS进行了大规模多语言评估，展示了神经网络对传统规则系统的增强潜力，同时发布了丰富的资源促进后续研究。

Abstract: Word Sense Disambiguation (WSD) has been widely evaluated using the semantic frameworks of WordNet, BabelNet, and the Oxford Dictionary of English. However, for the UCREL Semantic Analysis System (USAS) framework, no open extensive evaluation has been performed beyond lexical coverage or single language evaluation. In this work, we perform the largest semantic tagging evaluation of the rule based system that uses the lexical resources in the USAS framework covering five different languages using four existing datasets and one novel Chinese dataset. We create a new silver labelled English dataset, to overcome the lack of manually tagged training data, that we train and evaluate various mono and multilingual neural models in both mono and cross-lingual evaluation setups with comparisons to their rule based counterparts, and show how a rule based system can be enhanced with a neural network model. The resulting neural network models, including the data they were trained on, the Chinese evaluation dataset, and all of the code have been released as open resources.

</details>


### [64] [DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation](https://arxiv.org/abs/2601.09688)
*Yibo Wang,Lei Wang,Yue Deng,Keming Wu,Yao Xiao,Huanjin Yao,Liwei Kang,Hai Ye,Yongcheng Jing,Lidong Bing*

Main category: cs.CL

TL;DR: DeepResearchEval提出了一个自动构建复杂网页研究任务并智能评估的框架，通过角色驱动任务生成和自主事实核查，提升了多步骤研究系统的评价效果。


<details>
  <summary>Details</summary>
Motivation: 现有的多步骤网页研究系统评价存在困难，现有基准测试依赖大量标注、静态评测维度或缺乏可靠的事实验证机制。

Method: 提出DeepResearchEval框架，包括一个基于角色驱动的任务构建管道，利用任务资格和检索必要性两阶段筛选任务；以及一个自主式评估管道，包含自适应点质量评估和主动事实核查模块。

Result: 能够自动构建复杂多源证据整合任务，并动态生成任务特定的评估标准，且能在无引证情况下通过网页搜索自主验证事实。

Conclusion: DeepResearchEval实现了多步骤网页研究任务的自动构建与智能评估，有效解决了传统评估的标注依赖和静态标准限制问题，提升了评测的可靠性和实用性。

Abstract: Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.

</details>


### [65] [Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection](https://arxiv.org/abs/2601.09692)
*Tianyi Niu,Justin Chih-Yao Chen,Genta Indra Winata,Shi-Xiong Zhang,Supriyo Chakraborty,Sambit Sahu,Yue Zhang,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本工作解决了在无真实标注数据下训练LLM路由器的问题，提出基于生成数据的训练方案和CASCAL算法，显著提升了路由器对生成器质量波动的鲁棒性和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）路由方法依赖真实标注数据，但实际中数据通常不可得，尤其在用户请求分布异质且未知的情况下。

Method: 提出RGD设定，在仅有由生成器LLM基于任务描述生成的查询和答案的情况下训练路由器；引入CASCAL方法，采用共识投票和层次聚类进行查询路由，不依赖标签。

Result: 发现查询-答案路由器随生成器质量下降性能降幅大于仅查询路由器；通过筛选生成数据提升质量；CASCAL在弱生成器数据上超越最佳查询-答案路由器4.6%的准确率。

Conclusion: 在缺少真实标注的情况下，基于生成数据训练的查询路由器（如CASCAL）更为稳健，提升了大语言模型路由的实际应用可能性。

Abstract: Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.

</details>


### [66] [LLMs can Compress LLMs: Adaptive Pruning by Agents](https://arxiv.org/abs/2601.09694)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CL

TL;DR: 提出一种由基础模型自适应指导的剪枝方法，显著提升大型语言模型剪枝效果，保持性能和知识，无需再训练。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型剪枝方法存在依赖手工设定层稀疏率、削弱模型事实知识等问题，需要更智能和自适应的剪枝策略。

Method: 提出基于基础模型作为智能剪枝代理的方法，结合权重-激活指标和梯度重要性，构建层敏感度，通过具备自我反思能力的LLM代理迭代优化剪枝策略，并通过回滚机制控制模型性能。

Result: 在Qwen3（4B和8B参数）上45%稀疏度下实现MMLU准确率提升56%，事实知识保留提升19倍，困惑度降低69%，且无再训练需求。

Conclusion: 基础模型能够作为智能代理，有效指导其他基础模型的结构化剪枝，实现高效压缩且保持模型性能和事实知识。

Abstract: As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.

</details>


### [67] [Empathy Applicability Modeling for General Health Queries](https://arxiv.org/abs/2601.09696)
*Shan Randhawa,Agha Ali Raza,Kentaro Toyama,Julie Hui,Mustafa Naseem*

Main category: cs.CL

TL;DR: 本文提出EAF框架，通过理论驱动和双重注释数据，成功实现了对患者查询中同理心需求的前瞻性识别，为临床中更具同理心的自动响应奠定基础。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型整合到临床工作流程中缺乏临床同理心，现有NLP方法多为对医生反应的事后标注，缺乏对患者同理心需求的前瞻性建模，特别是在一般健康咨询中。

Method: 基于临床、语境和语言线索，采用理论驱动方法构建了EAF，收集真人和GPT-4o双重注释的患者查询数据，并训练不同分类器进行同理心适用性预测。

Result: 模型在人体标注和纯GPT注释数据上均表现出较强的预测能力，且与人类注释显示良好的一致性，能够识别隐性痛苦和临床语义含糊等挑战。

Conclusion: 本文提出的同理心适用性框架(EAF)能够有效预测患者查询中同理心的适用性，优于现有的启发式方法和零样本大语言模型基线，具有良好的实际应用潜力。

Abstract: LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.

</details>


### [68] [Value-Aware Numerical Representations for Transformer Language Models](https://arxiv.org/abs/2601.09706)
*Andreea Dutulescu,Stefan Ruseti,Mihai Dascalu*

Main category: cs.CL

TL;DR: 本文提出了一种显式编码数值的数值表示方法，通过在Transformer模型输入中增加一个专门的前缀标记，其嵌入向量基于数值大小进行条件化，从而提升模型对数值的理解和算术操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的语言模型通常仅将数字当做符号标记处理，未显式编码其数值大小，导致对基本数值理解和算术操作的脆弱性和系统性错误。

Method: 在标准分词输入前添加一个专门的前缀标记，该标记的嵌入向量是基于数字的实际数值进行条件化生成，从而将数值大小信息直接注入模型输入空间，兼容现有的解码器架构和分词器。

Result: 在多种算术任务上，该数值感知表示方法在不同数字格式、任务和操作数长度条件下均优于基线方法，证明其提高数值理解鲁棒性的有效性。

Conclusion: 显式编码数值的数值表示方法能有效提升Transformer语言模型的基础数值理解和运算推理能力，显著优于传统仅以符号处理数字的方式。

Abstract: Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model's input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [69] [MACRO-LLM: LLM-Empowered Multi-Agent Collaborative Reasoning under Spatiotemporal Partial Observability](https://arxiv.org/abs/2601.09295)
*Handi Chen,Running Zhao,Xiuzhe Wu,Edith C. H. Ngai*

Main category: cs.MA

TL;DR: 本文针对分布式LLM智能体因时空可见性限制导致的协同困难，提出MACRO-LLM架构，通过动作预测验证、冲突解决和策略自适应三模块提升多智能体协同能力。


<details>
  <summary>Details</summary>
Motivation: 分布式部署的LLM智能体因物理分散导致感知局限和有限时间视野，形成时空部分可观察性，难以高效协调。

Method: 提出了MACRO-LLM架构，通过三个模块(CoProposer、Negotiator、Introspector)应对分布式多智能体在时空部分可观察性下的协同推理挑战。

Result: 在协同自适应巡航控制和流行病控制两个复杂长时序任务中，MACRO-LLM框架有效缓解了时空部分可观察性，通过空间和时间策略实现了稳健协调。

Conclusion: MACRO-LLM成功解决了分布式多智能体在时空部分可观察性条件下的协同问题，实现了复杂任务中的高效和稳健协调。

Abstract: Large Language Model (LLM) agents deployed in complex real-world scenarios typically operate as spatially distributed entities. However, this physical dispersion constrains agents to limited local perception and finite temporal horizons. We characterize this bottleneck as spatiotemporal partial observability. Given such fragmented awareness, distributed agents struggle to coordinate efficiently. To bridge this gap, we introduce MACRO-LLM, LLM-empowered multi-agent collaborative reasoning under spatiotemporal partial observability. The architecture addresses spatiotemporal constraints via three modules: (1) the CoProposer mitigates temporal uncertainty by verifying candidate actions via predictive rollouts; (2) the Negotiator overcomes spatial myopia by resolving conflicts through mean-field statistical aggregation; and (3) the Introspector ensures continuous adaptation by analyzing historical experience to refine strategies via semantic gradient descent. Extensive evaluations on two complex long-horizon tasks, cooperative adaptive cruise control and pandemic control, demonstrate that our framework effectively mitigates spatiotemporal partial observability through spatial and temporal strategies, enabling robust coordination.

</details>


### [70] [SC-MAS: Constructing Cost-Efficient Multi-Agent Systems with Edge-Level Heterogeneous Collaboration](https://arxiv.org/abs/2601.09434)
*Di Zhao,Longhui Ma,Siwei Wang,Miao Wang,Yi Kong*

Main category: cs.MA

TL;DR: 提出一种基于社会资本理论的异构多智能体协作框架SC-MAS，有效提高性能同时降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统大多采用同质化协作模式，限制了不同角色间协作的灵活性，从社会资本理论得到启发，异构协作能提升效率和性能。

Method: 提出SC-MAS框架，将多智能体系统建模为有向图，边代表不同的协作策略，实现异构协作模式，通过统一控制器动态选择任务相关的代理角色、分配边级协作策略及语言模型。

Result: 在多个基准测试中，SC-MAS实现了准确率提升和推理成本降低，如MMLU准确率提升3.35%，成本降低15.38%；MBPP准确率提升3.53%，成本降低12.13%。

Conclusion: 异构协作策略在多智能体系统中显著提升了问题解决的准确性与效率，验证了SC-MAS框架的有效性和可行性。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) enhance complex problem solving through multi-agent collaboration, but often incur substantially higher costs than single-agent systems. Recent MAS routing methods aim to balance performance and overhead by dynamically selecting agent roles and language models. However, these approaches typically rely on a homogeneous collaboration mode, where all agents follow the same interaction pattern, limiting collaboration flexibility across different roles. Motivated by Social Capital Theory, which emphasizes that different roles benefit from distinct forms of collaboration, we propose SC-MAS, a framework for constructing heterogeneous and cost-efficient multi-agent systems. SC-MAS models MAS as directed graphs, where edges explicitly represent pairwise collaboration strategies, allowing different agent pairs to interact through tailored communication patterns. Given an input query, a unified controller progressively constructs an executable MAS by selecting task-relevant agent roles, assigning edge-level collaboration strategies, and allocating appropriate LLM backbones to individual agents. Experiments on multiple benchmarks demonstrate the effectiveness of SC-MAS. In particular, SC-MAS improves accuracy by 3.35% on MMLU while reducing inference cost by 15.38%, and achieves a 3.53% accuracy gain with a 12.13% cost reduction on MBPP. These results validate the feasibility of SC-MAS and highlight the effectiveness of heterogeneous collaboration in multi-agent systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [71] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: 提出基于大型语言模型的LAUDE框架，有效提升硬件设计的单元测试生成和调试效率，达到了高准确率。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中的单元测试对于确保模块功能正确性和符合规范至关重要，但开发高质量的单元测试和调试过程需要深入理解设计及创意。调试设计失败通常耗时且复杂。

Method: 提出LAUDE框架，融合设计源代码的语义理解和大型语言模型（LLMs）的Chain-of-Thought推理能力，通过提示工程和设计执行信息提升单元测试生成准确性和代码调试能力。

Result: 在VerilogEval数据集中，使用LAUDE生成的单元测试在组合设计和时序设计中分别检测到高达100%和93%的漏洞，并分别调试出93%和84%的设计错误。

Conclusion: LAUDE成功实现了硬件单元测试和调试的一体化，显著提高了设计故障检测和修复的效率，展示了大语言模型在硬件设计验证中的应用潜力。

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [72] [Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework](https://arxiv.org/abs/2601.08857)
*Mustafa Degerli*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型对软件工程教育的影响，提出了新的教学框架和评估机制，以适应人工智能驱动的开发环境。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（如ChatGPT和GitHub Copilot）融入专业软件工程工作流程，传统软件工程教学与实际工作需求存在脱节，评估方法和基础技能培养面临挑战。

Method: 采用概念研究方法，构建理论框架，分析LLM改变软件工程核心能力的路径，并设计相应的教育模型。

Result: 提出了一个理论框架分析生成式AI对核心软件工程能力的影响，并引入了基于LLM整合的教学设计模型，重点关注土耳其计算机工程专业的具体情况。

Conclusion: 传统以查重为核心的诚信机制不足以应对新挑战，需转向过程透明性模型；未来需要纵向实证研究以评估教学改革效果。

Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.

</details>


### [73] [Adaptive Trust Metrics for Multi-LLM Systems: Enhancing Reliability in Regulated Industries](https://arxiv.org/abs/2601.08858)
*Tejaswini Bollikonda*

Main category: cs.SE

TL;DR: 本文提出了一种适应性信任度量框架，用以提升多大语言模型系统在敏感领域的可靠性，验证了其在现实场景的应用效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在医疗、金融和法律等敏感领域的应用日益广泛，但其整合过程中存在信任、责任和可靠性等重要问题。

Method: 通过分析系统行为、评估多模型间的不确定性，设计并实现动态监控管道，对模型信任度进行量化和持续改进。

Result: 提出了适应性信任度量框架，通过分析多模型系统行为和不确定性，构建动态监控机制，提升模型在受监管环境下的可靠性。并通过金融合规和医疗诊断的案例验证了方法的实用性。

Conclusion: 适应性信任度量是实现受监管行业中安全、可扩展AI应用的基础，有助于增强系统的可信度和责任追踪能力。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains such as healthcare, finance, and law, yet their integration raises pressing concerns around trust, accountability, and reliability. This paper explores adaptive trust metrics for multi LLM ecosystems, proposing a framework for quantifying and improving model reliability under regulated constraints. By analyzing system behaviors, evaluating uncertainty across multiple LLMs, and implementing dynamic monitoring pipelines, the study demonstrates practical pathways for operational trustworthiness. Case studies from financial compliance and healthcare diagnostics illustrate the applicability of adaptive trust metrics in real world settings. The findings position adaptive trust measurement as a foundational enabler for safe and scalable AI adoption in regulated industries.

</details>


### [74] [EZInput: A Cross-Environment Python Library for Easy UI Generation in Scientific Computing](https://arxiv.org/abs/2601.08859)
*Bruno M. Saraiva,Iván Hidalgo-Cenalmor,António D. Brito,Damián Martínez,Tayla Shakespeare,Guillaume Jacquemet,Ricardo Henriques*

Main category: cs.SE

TL;DR: EZInput是一个Python库，自动生成跨环境的图形用户界面，使无编程经验的用户可访问计算工具，支持参数持久化和环境自动检测，提高科研参数配置的效率与可复现性。


<details>
  <summary>Details</summary>
Motivation: 科研算法参数配置繁琐，需重复输入且难以记录共享，界面环境多样导致使用复杂，严重影响开发效率和实验结果的可复现性。

Method: EZInput采用声明式规范系统，开发者定义输入需求和验证规则，库自动完成环境检测、界面呈现、参数验证和配置持久化，支持Jupyter、Colab和终端等多种运行环境。

Result: EZInput支持多种科学计算输入类型，结合轻量级YAML文件保存参数，减少重复输入，实现参数配置在不同环境间无缝迁移，提升用户体验和科研工作流效率。

Conclusion: EZInput有效解决了科研算法参数配置的碎片化问题，通过一次性声明参数需求，实现跨环境界面自动生成和参数持久化，显著提升了工具的易用性和结果的可复现性。

Abstract: Researchers face a persistent barrier when applying computational algorithms with parameter configuration typically demanding programming skills, interfaces differing across environments, and settings rarely persisting between sessions. This fragmentation forces repetitive input, slows iterative exploration, and undermines reproducibility because parameter choices are difficult to record, share, and reuse. We present EZInput, a cross-runtime environment Python library enabling algorithm developers to automatically generate graphical user interfaces that make their computational tools accessible to end-users without programming expertise. EZInput employs a declarative specification system where developers define input requirements and validation constraints once; the library then handles environment detection, interface rendering, parameter validation, and session persistence across Jupyter notebooks, Google Colab, and terminal environments. This "write once, run anywhere" architecture enables researchers to prototype in notebooks and deploy identical parameter configurations for batch execution on remote systems without code changes or manual transcription. Parameter persistence, inspired by ImageJ/FIJI and adapted to Python workflows, saves and restores user configurations via lightweight YAML files, eliminating redundant input and producing shareable records that enhance reproducibility. EZInput supports diverse input types essential for scientific computing and it also includes built-in validation that ensures data integrity and clear feedback that reduces user friction.

</details>


### [75] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 本文通过基于遗传算法的提示语优化方法，提高了小型大语言模型生成OpenACC指令的稳定性和性能，实现了高性价比的自动化GPU并行化。


<details>
  <summary>Details</summary>
Motivation: 虽然OpenACC降低了GPU加速门槛，但高效编写pragma仍需深厚领域知识；同时，直接利用大语言模型生成的代码常有语法错误或性能不佳。

Method: 利用GEPA（遗传-帕累托）框架，通过交叉和变异生成OpenACC pragma的提示语，结合专家提供的金标准示例和基于子句及参数级别的不匹配反馈，进行迭代优化提示语。

Result: 优化后的提示语显著提升了编译成功率，GPT-4.1 Nano的成功率从66.7%提升到93.3%，GPT-5 Nano从86.7%提升到100%，并且提升了21%的程序实现GPU加速。

Conclusion: 提示语优化可有效释放小型、大语言模型在自动生成高效GPU加速指令上的潜能，为高性能计算中的自动指令并行化提供了廉价可行的解决方案。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [76] [Build Code is Still Code: Finding the Antidote for Pipeline Poisoning](https://arxiv.org/abs/2601.08995)
*Brent Pappas,Paul Gazzillo*

Main category: cs.SE

TL;DR: 该论文提出了一种新策略开发阶段隔离，增强构建系统安全，利用Foreman工具有效检测构建系统中的恶意代码。


<details>
  <summary>Details</summary>
Motivation: C代码虽然被强化，但构建系统代码同样关键且易受攻击，目前针对构建系统的安全防护方法不足，容易被攻击绕过。

Method: 将构建自动化视为程序代码，建模其信息和行为权限，实现对构建系统的行为限制与监测，开发Foreman工具检测异常文件。

Result: 提出了开发阶段隔离策略，通过模拟构建自动化的信息和行为权限，硬化构建系统，开发了Foreman工具，能成功检测XZ Utils攻击中的中毒测试文件。

Conclusion: 开发阶段隔离策略有效提升构建系统安全，未来应普及构建系统安全检查工具，防止供应链中毒。

Abstract: Open source C code underpins society's computing infrastructure. Decades of work has helped harden C code against attackers, but C projects do not consist of only C code. C projects also contain build system code for automating development tasks like compilation, testing, and packaging. These build systems are critcal to software supply chain security and vulnerable to being poisoned, with the XZ Utils and SolarWinds attacks being recent examples. Existing techniques try to harden software supply chains by verifying software dependencies, but such methods ignore the build system itself. Similarly, classic software security checkers only analyze and monitor program code, not build system code. Moreover, poisoned build systems can easily circumvent tools for detecting program code vulnerabilities by disabling such checks. We present development phase isolation, a novel strategy for hardening build systems against poisoning by modeling the information and behavior permissions of build automation as if it were program code. We have prototyped this approach as a tool called Foreman, which successfully detects and warns about the poisoned test files involved in the XZ Utils attack. We outline our future plans to protect against pipeline poisoning by automatically checking development phase isolation. We envision a future where build system security checkers are as prevalent as program code checkers.

</details>


### [77] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文研究了使用LLM生成的数据库测试的flakiness问题，发现生成测试有略高的flaky比例，主要由于无序集合的顺序依赖，且闭源系统中flakiness传递更明显，提示生成测试时需提供定制上下文。


<details>
  <summary>Details</summary>
Motivation: 认识到LLM生成测试可能存在flakiness问题，但其普遍性及成因尚不明确，需研究其影响和潜在根源。

Method: 对四个关系型数据库管理系统（SAP HANA, DuckDB, MySQL, SQLite）的测试套件进行扩充，使用GPT-4o和Mistral-Large-Instruct-2407两种LLM生成测试，评估生成测试的flakiness。通过手动检查找出flakiness根本原因。

Result: 生成的测试中的flaky测试比例略高于现有测试，63%的flaky测试因依赖无序集合的顺序引起。Flakiness从已有测试通过提示传递给新测试，闭源系统更易出现该现象。

Conclusion: LLM生成的测试中存在较多的 flaky 测试，且主要原因是测试对无序集合的顺序依赖。此外，LLM通过提示上下文将现有测试的flakiness传递给新生成的测试，尤其在闭源系统中更为明显。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [78] [SafePlanner: Testing Safety of the Automated Driving System Plan Model](https://arxiv.org/abs/2601.09171)
*Dohyun Kim,Sanggu Han,Sangmin Woo,Joonha Jang,Jaehoon Kim,Changhun Song,Yongdae Kim*

Main category: cs.SE

TL;DR: SafePlanner是一个针对自动驾驶系统规划模型的系统测试框架，利用结构分析和引导模糊测试生成多样测试场景，成功检测并修复了百度Apollo系统中的多处安全缺陷，覆盖率和效率均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中规划模型的安全缺陷可能导致严重事故，现有测试方法难以系统性覆盖其复杂的控制逻辑，亟需一种结构化、系统性且高效的测试框架以提升安全性。

Method: 通过对规划模型的场景跳转逻辑和层级控制流进行结构分析，提取可行场景跳转并与NPC行为结合生成测试场景，利用引导模糊测试探索行为空间进行缺陷检测。

Result: 本文提出了SafePlanner，一种用于自动驾驶系统（ADS）规划模型的安全关键缺陷识别的系统测试框架。SafePlanner通过结构分析规划模型的场景跳转逻辑和层次控制流，提取可行的场景跳转，从而生成结构合理的测试场景，并结合非玩家车辆行为进行情景组合。通过引导模糊测试探索规划模型的行为空间。实验在百度Apollo Level 4自动驾驶系统上进行，产生20635个测试用例，发现520个危险行为，手动归因15个根本原因，并对4个缺陷进行了修补，验证了有效性。SafePlanner在功能覆盖率和决策覆盖率上分别达到83.63%和63.22%，在缺陷发现和效率上优于基线方法。

Conclusion: SafePlanner能够有效识别自动驾驶系统规划模型中的安全关键缺陷，提升测试覆盖率和效率，且修复缺陷后无明显副作用，增强了自动驾驶系统的安全性。

Abstract: In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.

</details>


### [79] [AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems](https://arxiv.org/abs/2601.09393)
*Zirui Wang,Guangba Yu,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出了AI-NativeBench基准测试工具，实现从模型能力测量到AI原生系统工程特性的系统级评估。


<details>
  <summary>Details</summary>
Motivation: 随着软件架构从云原生向AI原生转变，传统黑盒模型能力评估方法已无法满足系统级工程需求，因此亟需新的基准测试方案来洞察系统执行动态和工程问题。

Method: 基于模型上下文协议（MCP）和Agent-to-Agent（A2A）标准，设计了应用中心的白盒基准测试框架，通过分布式追踪中对智能代理执行跨度的细粒度分析，多角度评估系统性能。利用该方法对21种系统变体进行了测试。

Result: 本论文提出了AI-NativeBench，这是第一个基于模型上下文协议（MCP）和Agent-to-Agent（A2A）标准，强调应用级别和白盒视角的AI原生基准测试套件。通过对多种系统变体的实验，揭示了传统评估指标无法反映的系统级执行动态和工程特性，如轻量模型在协议遵循上的优异表现、自愈机制导致的资源浪费等关键现象。

Conclusion: 本研究首次系统性地展示了评估AI原生系统时需要关注的工程维度，促使研究者从单纯能力测量向可靠系统建设转变。

Abstract: The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.

</details>


### [80] [DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries](https://arxiv.org/abs/2601.09440)
*Yi Gao,Xing Hu,Tongtong Xu,Jiali Zhao,Xiaohu Yang,Xin Xia*

Main category: cs.SE

TL;DR: DepRadar是一个用于深度学习库更新中缺陷和影响分析的框架，结合静态分析和领域规则，帮助用户准确识别和判断缺陷影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习库如Transformers和Megatron在现代AI程序中广泛使用，但当这些库出现缺陷时，用户难以判断自己的程序是否受到影响，因为缺陷可能涉及复杂的触发条件和间接的API使用。

Method: DepRadar包含三个步骤：由PR Miner和代码差异分析器提取缺陷语义；Orchestrator Agent整合信号构造缺陷模式和触发条件；Impact Analyzer检测下游程序中缺陷触发情况，同时结合静态分析和领域规则进行推理和跟踪。

Result: 提出了DepRadar框架，通过四个专门代理协调，精细分析缺陷及其影响，能够实现90%的缺陷识别精度和90%的召回率，显著优于其他方法。

Conclusion: DepRadar有效提升了深度学习库缺陷识别和影响评估的准确性和解释性，能辅助用户更好地应对库更新中的风险。

Abstract: Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.

</details>


### [81] [Towards a Metadata Schema for Energy Research Software](https://arxiv.org/abs/2601.09456)
*Stephan Ferenz,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: 本文提出并验证了一套适用于能源研究软件的元数据模式，解决了该领域元数据缺失的问题，提高了软件的可发现性和重用性。


<details>
  <summary>Details</summary>
Motivation: 解决能源研究领域缺乏专用元数据模式，提升软件的可发现性和可重用性，遵循FAIR4RS原则。

Method: 基于需求分析设计元数据模式，并通过用户测试进行评估。

Result: 本文针对能源研究领域缺乏专门的软件元数据模式的问题，开发了一套基于需求分析的能源研究软件元数据模式，并通过用户测试进行了评估。研究结果表明，该元数据模式兼顾了形式化和互操作性需求，同时满足能源研究者的特定需求。用户测试还强调了信息良好展示对科研人员创建元数据的重要性。

Conclusion: 能源研究软件元数据模式需在形式化与互操作性之间取得平衡，并注重信息展示以促进元数据创建。

Abstract: Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.

</details>


### [82] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 本研究通过分析nf-core管道的开发与维护数据，揭示了用户面临的主要挑战和管理实践，提出改进管道可用性、可持续性和可重复性的建议。


<details>
  <summary>Details</summary>
Motivation: 尽管Nextflow和nf-core社区的管道被广泛采用，但关于用户在管道开发与维护中面临的具体挑战知之甚少。

Method: 采用BERTopic主题建模方法，对大量问题和拉取请求数据进行文本分类和统计分析，以揭示用户面临的主要挑战及解决动态。

Result: 通过对25,173个问题和拉取请求的实证分析，发现了13个主要挑战，包括管道开发与集成、错误修复、基因组数据集成、CI配置管理及版本更新处理。89.38%的问题最终被解决，且标签和代码片段的存在显著提升了解决率。工具开发与仓库维护是最具挑战的方面。

Conclusion: 研究为nf-core管道的协作开发和维护提供了实用见解，强调了提升可用性、可持续性和可重复性的机会。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [83] [SysPro: Reproducing System-level Concurrency Bugs from Bug Reports](https://arxiv.org/abs/2601.09616)
*Tarannum Shaila Zaman,Zhihui Yan,Chen Wang,Chadni Islam,Jiangfan Shi,Tingting Yu*

Main category: cs.SE

TL;DR: SysPro自动提取缺陷报告中系统调用信息，结合多种方法生成输入并通过动态插桩重现系统级并发缺陷，解决了现有工具难以重现非确定性并发缺陷的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具难以处理系统调用级别的特定交错顺序以重现非确定性并发缺陷，且缺陷报告中的信息不完整且格式不统一。

Method: 通过自动提取报告中的系统调用名称，结合信息检索、正则表达式匹配和类别划分法生成输入数据，再利用动态源代码插桩实现缺陷重现。

Result: 在真实基准测试中，SysPro展现出高效且有效的系统级并发缺陷定位与重现性能。

Conclusion: SysPro能有效地定位和重现系统级并发缺陷，提升了重现复杂非确定性缺陷的能力。

Abstract: Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.

</details>


### [84] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 研究表明，使用当前最新LLM版本直接生成单元测试，性能优于带有多种后处理组件的传统方法，并通过分阶段测试生成策略提高效率，降低调用成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型(LLMs)的自动化单元测试生成技术虽有所提升，但往往基于较旧LLM版本和较弱提示，导致实际效果有待验证。

Method: 复制四种最先进的LLM测试生成工具(HITS, SymPrompt, TestSpark, CoverUp)，并将所有方法整合当前LLM版本，在393个类和3657个方法上进行评测。

Result: 纯LLM方法在所有测试效果指标（代码行覆盖率、分支覆盖率、变异得分）上均优于之前的先进方法，且消耗的LLM调用次数相当。

Conclusion: 纯LLM方法效果优异，提出先对程序类进行测试生成，再针对未覆盖的方法进行补充，减少约20%LLM调用次数，同时保持相似甚至略高的测试效果。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [85] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: 本文提出了ShortCoder框架，通过语法级的代码简化和知识注入，提高Python代码生成的效率，减少生成的token数，同时保持代码的语义一致和可读性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型代码生成效率受限于架构限制和推理消耗，尽管已有推理阶段优化，生成阶段的效率提升仍被忽视。本文旨在通过代码简化和知识注入来提升生成阶段的效率。

Method: 本文设计了十条Python语法级别的简化规则，结合基于规则重写和LLM指导的混合数据合成管道，构建了ShorterCodeBench数据集；并通过微调策略将简洁意识注入基础LLM，实现更高效的代码生成。

Result: ShortCoder在HumanEval测试中，生成效率提升了18.1%-37.8%，同时保持了代码的功能性和语义一致性，远超现有最先进方法。

Conclusion: ShortCoder框架显著提升了代码生成的效率，token数量减少18.1%，在HumanEval测试中效率提升18.1%-37.8%，同时保证生成代码的性能和可读性。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>
